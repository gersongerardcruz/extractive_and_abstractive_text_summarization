{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-gc-data-cleaning-and-preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Preprocessing\n",
        "\n",
        "In order to clean and preprocess the data, I created a data cleaning and preprocessing function with the following capabilities: \n",
        "\n",
        "- Lemmatization\n",
        "- Stopword removal\n",
        "- Lowercase\n",
        "- Punctuation cleaning\n",
        "- Emoji cleaning\n",
        "- Number cleaning\n",
        "- Weblinks cleaning\n",
        "- Unnecessary spaces removal\n",
        "\n",
        "I gave the user the freedom to choose which cleaning to apply by creating a unified function where every cleaning step is a boolean. For the purpose of this project, I do not lemmatize, remove stopwords, lowercase, and remove punctuations so that the summarization will still have its semantic context in place."
      ],
      "metadata": {
        "id": "Jh-ZJyAwJaoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Individual cleaning functions\n",
        "def remove_web_links(text):\n",
        "  text = re.sub(r'http://www.\\w+.org/','', text)\n",
        "  text = re.sub(r'http://www.\\w+.org/','', text)\n",
        "  text = re.sub(r'http://www.([\\w\\S]+).org/\\w+\\W\\w+','',text)\n",
        "  text = re.sub(r'https://www.\\w+.org/','', text)\n",
        "  text = re.sub(r'https://www.([\\w\\S]+).org/\\w+\\W\\w+','',text)\n",
        "  text = re.sub(r'https://\\w+.\\w+/\\d+.\\d+/\\w\\d+\\W\\w+','',text)\n",
        "  text = re.sub(r'https://\\w+.\\w+/\\d+.\\d+/\\w\\d+\\W\\w+','',text)\n",
        "  text = re.sub(r'Figure\\s\\d:','', text)\n",
        "  text = re.sub(r'\\Wwww.\\w+\\W\\w+\\W','',text)\n",
        "  text = re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n",
        "  text = re.sub(r'www.\\w+','',text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def remove_emojis(text):\n",
        "  regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  text = regrex_pattern.sub('', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def remove_spaces(text):\n",
        "  text = re.sub(r'\\n',\"\",text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  words=word_tokenize(text)\n",
        "  sentence=[w for w in words if w not in stop_words]\n",
        "  return \" \".join(sentence)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  wordlist=[]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sentences=sent_tokenize(text)\n",
        "  for sentence in sentences:\n",
        "      words=word_tokenize(sentence)\n",
        "      for word in words:\n",
        "          wordlist.append(lemmatizer.lemmatize(word))\n",
        "  return ' '.join(wordlist)\n",
        "\n",
        "def lowercase_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  additional_punctuations = ['’', '…'] # punctuations not in string.punctuation  \n",
        "  for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation, '')\n",
        "  \n",
        "  for punctuation in additional_punctuations:\n",
        "    text = text.replace(punctuation, '')\n",
        "    \n",
        "  return text\n",
        "\n",
        "def remove_numbers(text):\n",
        "  if text is not None:\n",
        "    text = text.replace(r'^\\d+\\.\\s+','')\n",
        "  \n",
        "  text = re.sub(\"[0-9]\", '', text)\n",
        "  return text\n",
        "\n",
        "# Unified boolean controlled cleaning function \n",
        "def clean_and_preprocess_data(text, lowercase=True, clean_stopwords=True, clean_punctuations=True, clean_links=True, \n",
        "                              clean_emojis=True, clean_spaces=True, clean_numbers=True,  lemmatize=True):\n",
        "  \n",
        "  if clean_stopwords == True:\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "  if clean_punctuations == True:\n",
        "    text = remove_punctuations(text)\n",
        "  \n",
        "  if clean_links == True:\n",
        "    text = remove_web_links(text)\n",
        "  \n",
        "  if clean_emojis == True:\n",
        "    text = remove_emojis(text)\n",
        "  \n",
        "  if clean_spaces == True:\n",
        "    text = remove_spaces(text)\n",
        "  \n",
        "  if clean_numbers == True:\n",
        "    text = remove_numbers(text)\n",
        "  \n",
        "  if lemmatize == True:\n",
        "    text = lemmatize_text(text)\n",
        "  \n",
        "  if lowercase == True:\n",
        "    return text.lower()\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "6fcQ4N41J3KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Note: The final preprocessing function will be found on the `src/data` directory.\n",
        "\n",
        "## Preprocessing and cleaning the raw data"
      ],
      "metadata": {
        "id": "utW4rbDzJ-Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_df = pd.read_csv(\"top100.csv\")\n",
        "\n",
        "text_df['abstract'] = text_df['abstract'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
        "text_df['full_text'] = text_df['full_text'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
        "text_df['conclusion'] = text_df['conclusion'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
        "\n",
        "print(text_df.head())\n",
        "\n",
        "# Saving the preprocessed data into a .csv file\n",
        "text_df.to_csv(\"top100_cleaned.csv\")"
      ],
      "metadata": {
        "id": "zja_qIJoKJRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}