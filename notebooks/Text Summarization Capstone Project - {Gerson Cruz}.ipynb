{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Summarization Capstone Project - {Gerson Cruz}.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyvCVD/zr3ZIM4XBvOBBSF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b05403da39bd41b9b5d6d99048f91afe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4bf10acadfc43fa935b9be0a7cd304c","IPY_MODEL_98b35fecb8ba40a687471630acdb1631","IPY_MODEL_8d1373f224f14c3a940050cd3e328a71"],"layout":"IPY_MODEL_1678b07b8c4e48849ca8c4e89c83bebe"}},"c4bf10acadfc43fa935b9be0a7cd304c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c6d6f2c74bc4f61a74ba0053647dbe2","placeholder":"​","style":"IPY_MODEL_1379ce5bbfcb4fc99398a8b34758ee59","value":"Downloading: 100%"}},"98b35fecb8ba40a687471630acdb1631":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc42fb955e1748c082fbc8888946d1ec","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff1f3e0ecc304de68c506cccc0fe206b","value":385}},"8d1373f224f14c3a940050cd3e328a71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc18d0a2c6874c2ca0aa7becf036c3e5","placeholder":"​","style":"IPY_MODEL_0503599684d9420aa464ceb1c008bb07","value":" 385/385 [00:00&lt;00:00, 9.15kB/s]"}},"1678b07b8c4e48849ca8c4e89c83bebe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c6d6f2c74bc4f61a74ba0053647dbe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1379ce5bbfcb4fc99398a8b34758ee59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc42fb955e1748c082fbc8888946d1ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff1f3e0ecc304de68c506cccc0fe206b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc18d0a2c6874c2ca0aa7becf036c3e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0503599684d9420aa464ceb1c008bb07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"070f8a90f8104097adb08c548bf457ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e20656d893984a58b822d3900d8cbdca","IPY_MODEL_5b06e63a678b44ab8a45ec3499a373a9","IPY_MODEL_6266f16d32484bdd8d0835f4b96a0f61"],"layout":"IPY_MODEL_888d57ce90ae4cc4b280dafdc86639e7"}},"e20656d893984a58b822d3900d8cbdca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58e4adba5591473f96f483f4a3a1d9a0","placeholder":"​","style":"IPY_MODEL_0b71c019f9964e5994fae71a1141f56a","value":"Downloading: 100%"}},"5b06e63a678b44ab8a45ec3499a373a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b360d1232c24fd0a9e9e781c149d432","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93a38ea035bb45c49165e99bf6c6ffa6","value":227845}},"6266f16d32484bdd8d0835f4b96a0f61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c39df8a975034402b58b3d48c2e4e7b0","placeholder":"​","style":"IPY_MODEL_09504bc3537b4a8f9faab674d0e4d7fa","value":" 223k/223k [00:00&lt;00:00, 910kB/s]"}},"888d57ce90ae4cc4b280dafdc86639e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58e4adba5591473f96f483f4a3a1d9a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b71c019f9964e5994fae71a1141f56a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b360d1232c24fd0a9e9e781c149d432":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a38ea035bb45c49165e99bf6c6ffa6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c39df8a975034402b58b3d48c2e4e7b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09504bc3537b4a8f9faab674d0e4d7fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb338536c1e6456b8d75c5e6c19f2e10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc6f86c13715402d941cf465b4057293","IPY_MODEL_c5d10622f98b4510a26a9dec85c91647","IPY_MODEL_102b257c3d37420f9a9c73fee1bf7dd8"],"layout":"IPY_MODEL_afc3621bb0fe4a7bb55c107aba9dd5a4"}},"cc6f86c13715402d941cf465b4057293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9e95a6c03b74215ba414c15cb0109ad","placeholder":"​","style":"IPY_MODEL_ea053977f4e44ab2a0b0bb52fba1cdf2","value":"Downloading: 100%"}},"c5d10622f98b4510a26a9dec85c91647":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0c426dca2df421780027840a51fc364","max":442221694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b329b83fd1b44d69af7b50de8a6936e","value":442221694}},"102b257c3d37420f9a9c73fee1bf7dd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f925fccaa254171b23e77481fcd4d9e","placeholder":"​","style":"IPY_MODEL_0fa2d5968c3b47a2a4299ea9bbe45c5f","value":" 422M/422M [00:22&lt;00:00, 28.4MB/s]"}},"afc3621bb0fe4a7bb55c107aba9dd5a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e95a6c03b74215ba414c15cb0109ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea053977f4e44ab2a0b0bb52fba1cdf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0c426dca2df421780027840a51fc364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b329b83fd1b44d69af7b50de8a6936e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f925fccaa254171b23e77481fcd4d9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fa2d5968c3b47a2a4299ea9bbe45c5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Extractive and Abstractive Text Summarization for Long Documents\n","##### ***by Gerson Gerard Cruz***\n","\n","In an increasingly information-dependent world, the ability to provide the most important and accurate information in the least amount of time is exceedingly valuable. Text summarization can provide this value. It is the process of summarizing a certain document in order to get the most important information from the original one. Essentially, text summarization produces a concise summary which preserves the valuable information and meaning of a document. \n","\n","\n","<img src='https://drive.google.com/uc?id=1caGEZmT4ODf0R7zNZLqYh-8plpv55sIh'>\n","\n","\n","There are two general types of text summarization: Extractive and Abstractive summarization. \n","\n","#### Extractive Summarization \n","\n","Extractive summarization, from the word itself, is a method of extracting a subset of words that contain the most important information in a text. This approach takes into consideration the most important parts of document sentences and uses them to form the summarization. Then, algorithms to give weights to these parts and rank them based on similarity and importance are used. \n","\n","The general workflow for extractive summarization goes like: \n","\n","**Text input --> Get similar sentences --> Assign weights to sentences --> Rank sentences --> Choose sentences with highest ranks to form the summary**\n","\n","#### Abstractive Summarization\n","\n","In contrast, abstractive summarization aims to **abstract** and use words that did not appear in the input document based on the semantic information of the text. This means abstractive summarization produces a new summary. Abstractive summarization interprets and examines the document using advanced NLP techniques and generates a new concise summary based the most important information in the text. \n","\n","The general workflow for abstractive summarization goes like:\n","\n","**Text input --> Understand the context of the document --> Use semantic understanding --> Abstract and create a new summary**\n","\n","In general, abstractive summarization is desired more than extractive summarization because it is akin to how a human would summarize a text by first understanding its meaning and putting it into his/her own words. However, given the challenges in semantic representation, extractive summarization often gives better results. \n","\n","## The Project\n","\n","In addition to the advantages and disadvantages of these two summarization techniques, there is also difficulty in summarizing long text documents. For example, in this Github issue [Bart now enforces maximum sequence length in Summarization Pipeline](https://github.com/huggingface/transformers/issues/4224), there are limits to the maximum length of a text document for abstractive summarization of some transformer models like BART. Given this, I researched on how to solve this problem and came across this paper: [Combination of abstractive and extractive approaches for summarization of long scientific texts](https://arxiv.org/abs/2006.05354) which applied extractive summarization to get a summary with the important extracted information from the text and then performed abstractive summarization on the extracted summary along with the scientific paper's abstract and conclusion. \n","\n","While I won't be going as detailed as the paper, in this project, I still aim to apply extractive and abstractive summarization in order to summarize long scientific documents. \n","\n","## The Dataset\n","\n","The dataset I will use for this project consists of 100 scientific papers from the WING NUS group's Scisumm corpus found at this [github link](https://github.com/WING-NUS/scisumm-corpus). According to the authors, [Scisumm](https://cs.stanford.edu/~myasu/projects/scisumm_net/) is a summary of scientific papers should ideally incorporate the impact of the papers on the research community reflected by citations. To facilitate research in citation-aware scientific paper summarization (Scisumm), the CL-Scisumm shared task has been organized since 2014 for papers in the computational linguistics and NLP domain. \n","\n","## The Methodology\n","\n","The project workflow consists of three main steps: data collection and preprocessing, modelling, and model deployment. \n","\n","### Data Collection and Preprocessing\n","\n","In this step, I choose 100 scientific papers from the Scisumm corpus. I selectively decide which papers to include because the project requires papers which explicitly have an `abstract` and a `conclusion` in the .xml file. Some papers, after investigation, did not have an `abstract` section and instead was found directly in the text section of the document. This will lead to extraction errors as the .xml extraction pipeline was explicitly designed for xml documents which explicity have an `abstract` and `conclusion` section. \n","\n","For the data preprocessing step, I create a data cleaning and preprocessing functions with the following capabilities:\n","* Lemmatization\n","* Stopword removal\n","* Lowercase\n","* Punctuation cleaning\n","* Emoji cleaning\n","* Number cleaning\n","* Weblinks cleaning\n","* Unnecessary spaces removal\n","\n","I gave the user the freedom to choose which cleaning to apply by creating a unified function where every cleaning step is a boolean. For the purpose of this project, I do not lemmatize, remove stopwords, lowercase, and remove punctuations so that the summarization will still have its semantic context in place. \n","\n","### Model Training\n","\n","For modelling, I perform both extractive and abstractive summarization. For extractive summarization, I use the BERT transformer model and customize it to use the pre-trained weights of the **sciBERT** model which specializes in scientific texts, which fit our purpose. For every text, I determine the optimal number of sentences for the extracted summary.\n","\n","For abstractive summarization, I first concatenate the abstract, extractive summary, and conclusion together since much of the important information can be found in them. Then, I use the **facebook-BART-large-cnn** transformer model to perform the abstraction. \n","\n","### Model Deployment\n","\n","For deployment, I use Streamlit to create a simple user interface which requires a long text input to summarize. Then, I deploy this model using the package `localtunnel` so that I can serve this project to the web. \n","\n","# Table of Contents\n","I. [Importing Libraries and Installing External Dependencies](#s1) <br>\n","II. [Data Collection and Preprocessing](#s2) <br>\n","III. [Modelling](#s3) <br>\n","IV. [Model Deployment](#s4) <br>\n","V. [Recommendations](#s5) <br>\n"],"metadata":{"id":"yWi9w1ZkIrFS"}},{"cell_type":"markdown","source":["### Importing Libraries and Installing External Dependencies <a name=\"s1\"></a>"],"metadata":{"id":"K_WXEk4hOxyD"}},{"cell_type":"code","source":["!pip install lxml\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install tensorflow-gpu # For CPMTokenizer\n","!pip install neuralcoref\n","!pip install bert-extractive-summarizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KhQ-5D_W1v-","executionInfo":{"status":"ok","timestamp":1648303275836,"user_tz":-480,"elapsed":88974,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"2e6fbb7e-6085-44e9-d78e-11ba64a5b204"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.9 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.4 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 41.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 30.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 37.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n","Collecting tensorflow-gpu\n","  Downloading tensorflow_gpu-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n","\u001b[K     |████████████████████████████████| 497.5 MB 25 kB/s \n","\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.5.3)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 46.6 MB/s \n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.8.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.24.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (13.0.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.5)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.0.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.8.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.44.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.10.0.2)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (3.3.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.2.0)\n","Installing collected packages: tf-estimator-nightly, tensorflow-gpu\n","Successfully installed tensorflow-gpu-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting neuralcoref\n","  Downloading neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286 kB)\n","\u001b[K     |████████████████████████████████| 286 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.21.5)\n","Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.2.4)\n","Collecting boto3\n","  Downloading boto3-1.21.27-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 45.1 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.10.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (3.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (4.63.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref) (3.10.0.2)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.7 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n","Collecting botocore<1.25.0,>=1.24.27\n","  Downloading botocore-1.24.27-py3-none-any.whl (8.6 MB)\n","\u001b[K     |████████████████████████████████| 8.6 MB 42.5 MB/s \n","\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 45.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.27->boto3->neuralcoref) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.27->boto3->neuralcoref) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.21.27 botocore-1.24.27 jmespath-1.0.0 neuralcoref-4.0 s3transfer-0.5.2 urllib3-1.25.11\n","Collecting bert-extractive-summarizer\n","  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.17.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.21.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.63.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.9.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2021.10.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.4.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.49)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.11.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->bert-extractive-summarizer) (3.0.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n","Installing collected packages: bert-extractive-summarizer\n","Successfully installed bert-extractive-summarizer-0.10.1\n"]}]},{"cell_type":"code","source":["# Data collection\n","from lxml import objectify\n","import pandas as pd\n","import numpy as np\n","import os\n","import glob\n","from glob import iglob\n","\n","# Data preprocessing\n","import string\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Text Summarization\n","from transformers import *\n","from summarizer import Summarizer\n","from summarizer.text_processors.coreference_handler import CoreferenceHandler"],"metadata":{"id":"wfJeqU63QxFz","executionInfo":{"status":"ok","timestamp":1648303311180,"user_tz":-480,"elapsed":35357,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"811665ab-d78c-4073-acd1-80129532819d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n","  return f(*args, **kwds)\n","100%|██████████| 40155833/40155833 [00:01<00:00, 34448343.47B/s]\n"]}]},{"cell_type":"markdown","source":["## Data Collection and Preprocessing <a name=\"s2\"></a>\n","\n","The data from Scisumm is in the .xml format. The [XML](https://www.indeed.com/career-advice/career-development/xml-file), also known as the extensible markup language file, is used to structure data for storage and transport. It contains tags to provide structure to the data and also contains the text. Put simply, XML is a standard text file that utilizes customized tags, to describe the structure of the document and how it should be stored and transported.\n","\n","The structure of a sample paper is shown below: \n","\n","\n","<img src='https://drive.google.com/uc?id=1wheFaobd2Bw6QSmMIn0azJIhC584lNzV'>\n","\n","Each part of the paper is contained in a `SECTION` tag and the succeeding paragraphs of the section are found below it. Each sentence is given a unique `security identifier` or `sid`. \n","\n","In order to properly extract this data, I follow this process:\n","1. Get the list of all `.xml` files names\n","2. Use the library `objectify` in order to extract all text contents of the data.\n","3. Extract the `abstract` and `conclusion` columns into separate lists for abstractive summarization. \n","4. Collate the text from every section into one whole text.\n","5. Append the `abstract`, `entire_text`, and `conclusion` into a pandas dataframe"],"metadata":{"id":"IG1cVo3VQ6s2"}},{"cell_type":"markdown","source":["#### Getting the list of all .xml file names. "],"metadata":{"id":"Sxjc7r-si3kj"}},{"cell_type":"code","source":["%cd ..\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"],"metadata":{"id":"njjDOJq-i8lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648303333705,"user_tz":-480,"elapsed":22531,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"a73e81bc-ea35-455b-b628-ba5081a28a86"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/\n","Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["%cd /mydrive/Omdena\\ School/Solving\\ Business\\ Problems\\ with\\ NLP/Capstone\\ Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gyB3PnOjcNk","executionInfo":{"status":"ok","timestamp":1648303334163,"user_tz":-480,"elapsed":463,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"8b9205b7-7586-4c12-eb3a-8fb4c14c011f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Omdena School/Solving Business Problems with NLP/Capstone Project\n"]}]},{"cell_type":"code","source":["# Run this to unzip the scisumm zip file into google drive\n","# !unzip scisummnet_release1.1__20190413.zip"],"metadata":{"id":"iawoWLxIjjHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, I filtered the unzipped folder to only include **100 selected documents** that follow the format in the screenshot above for the purpose of this project. The 100 documents are contained in a folder named `top100`."],"metadata":{"id":"97laKcsCm0VS"}},{"cell_type":"code","source":["%cd scisummnet_release1.1__20190413"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TZYCFEIvlRG","executionInfo":{"status":"ok","timestamp":1647326858774,"user_tz":-480,"elapsed":723,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"d2746407-16d3-43c9-e65f-5e70fd00c481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Omdena School/Solving Business Problems with NLP/Capstone Project/scisummnet_release1.1__20190413\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZevEc44hvokd","executionInfo":{"status":"ok","timestamp":1647326861177,"user_tz":-480,"elapsed":558,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"051aeae6-08db-4962-9afe-e112ade91522"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset_Documentation.txt  log.txt  top100  top1000_complete  top100.zip\n"]}]},{"cell_type":"code","source":["# # Run this to unzip the top100 zip file into the top100 folder\n","# !unzip top100.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6LpCuKbvpr4","executionInfo":{"status":"ok","timestamp":1647317010695,"user_tz":-480,"elapsed":5852,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"810bb045-52e5-41d2-8803-8432fd5baadf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  top100.zip\n","   creating: top100/\n","   creating: top100/A00-1031/\n","  inflating: top100/A00-1031/citing_sentences_annotated.json  \n","   creating: top100/A00-1031/Documents_xml/\n","  inflating: top100/A00-1031/Documents_xml/A00-1031.xml  \n","   creating: top100/A00-1031/summary/\n","  inflating: top100/A00-1031/summary/A00-1031.gold.txt  \n","   creating: top100/A00-1043/\n","  inflating: top100/A00-1043/citing_sentences_annotated.json  \n","   creating: top100/A00-1043/Documents_xml/\n","  inflating: top100/A00-1043/Documents_xml/A00-1043.xml  \n","   creating: top100/A00-1043/summary/\n","  inflating: top100/A00-1043/summary/A00-1043.gold.txt  \n","   creating: top100/A00-2004/\n","  inflating: top100/A00-2004/citing_sentences_annotated.json  \n","   creating: top100/A00-2004/Documents_xml/\n","  inflating: top100/A00-2004/Documents_xml/A00-2004.xml  \n","   creating: top100/A00-2004/summary/\n","  inflating: top100/A00-2004/summary/A00-2004.gold.txt  \n","   creating: top100/A00-2009/\n","  inflating: top100/A00-2009/citing_sentences_annotated.json  \n","   creating: top100/A00-2009/Documents_xml/\n","  inflating: top100/A00-2009/Documents_xml/A00-2009.xml  \n","   creating: top100/A00-2009/summary/\n","  inflating: top100/A00-2009/summary/A00-2009.gold.txt  \n","   creating: top100/A00-2018/\n","  inflating: top100/A00-2018/citing_sentences_annotated.json  \n","   creating: top100/A00-2018/Documents_xml/\n","  inflating: top100/A00-2018/Documents_xml/A00-2018.xml  \n","   creating: top100/A00-2018/summary/\n","  inflating: top100/A00-2018/summary/A00-2018.gold.txt  \n","   creating: top100/A00-2019/\n","  inflating: top100/A00-2019/citing_sentences_annotated.json  \n","   creating: top100/A00-2019/Documents_xml/\n","  inflating: top100/A00-2019/Documents_xml/A00-2019.xml  \n","   creating: top100/A00-2019/summary/\n","  inflating: top100/A00-2019/summary/A00-2019.gold.txt  \n","   creating: top100/A00-2024/\n","  inflating: top100/A00-2024/citing_sentences_annotated.json  \n","   creating: top100/A00-2024/Documents_xml/\n","  inflating: top100/A00-2024/Documents_xml/A00-2024.xml  \n","   creating: top100/A00-2024/summary/\n","  inflating: top100/A00-2024/summary/A00-2024.gold.txt  \n","   creating: top100/A00-2026/\n","  inflating: top100/A00-2026/citing_sentences_annotated.json  \n","   creating: top100/A00-2026/Documents_xml/\n","  inflating: top100/A00-2026/Documents_xml/A00-2026.xml  \n","   creating: top100/A00-2026/summary/\n","  inflating: top100/A00-2026/summary/A00-2026.gold.txt  \n","   creating: top100/A00-2030/\n","  inflating: top100/A00-2030/citing_sentences_annotated.json  \n","   creating: top100/A00-2030/Documents_xml/\n","  inflating: top100/A00-2030/Documents_xml/A00-2030.xml  \n","   creating: top100/A00-2030/summary/\n","  inflating: top100/A00-2030/summary/A00-2030.gold.txt  \n","   creating: top100/A00-2031/\n","  inflating: top100/A00-2031/citing_sentences_annotated.json  \n","   creating: top100/A00-2031/Documents_xml/\n","  inflating: top100/A00-2031/Documents_xml/A00-2031.xml  \n","   creating: top100/A00-2031/summary/\n","  inflating: top100/A00-2031/summary/A00-2031.gold.txt  \n","   creating: top100/A00-2034/\n","  inflating: top100/A00-2034/citing_sentences_annotated.json  \n","   creating: top100/A00-2034/Documents_xml/\n","  inflating: top100/A00-2034/Documents_xml/A00-2034.xml  \n","   creating: top100/A00-2034/summary/\n","  inflating: top100/A00-2034/summary/A00-2034.gold.txt  \n","   creating: top100/A88-1019/\n","  inflating: top100/A88-1019/citing_sentences_annotated.json  \n","   creating: top100/A88-1019/Documents_xml/\n","  inflating: top100/A88-1019/Documents_xml/A88-1019.xml  \n","   creating: top100/A88-1019/summary/\n","  inflating: top100/A88-1019/summary/A88-1019.gold.txt  \n","   creating: top100/A92-1018/\n","  inflating: top100/A92-1018/citing_sentences_annotated.json  \n","   creating: top100/A92-1018/Documents_xml/\n","  inflating: top100/A92-1018/Documents_xml/A92-1018.xml  \n","   creating: top100/A92-1018/summary/\n","  inflating: top100/A92-1018/summary/A92-1018.gold.txt  \n","   creating: top100/A92-1021/\n","  inflating: top100/A92-1021/citing_sentences_annotated.json  \n","   creating: top100/A92-1021/Documents_xml/\n","  inflating: top100/A92-1021/Documents_xml/A92-1021.xml  \n","   creating: top100/A92-1021/summary/\n","  inflating: top100/A92-1021/summary/A92-1021.gold.txt  \n","   creating: top100/A94-1006/\n","  inflating: top100/A94-1006/citing_sentences_annotated.json  \n","   creating: top100/A94-1006/Documents_xml/\n","  inflating: top100/A94-1006/Documents_xml/A94-1006.xml  \n","   creating: top100/A94-1006/summary/\n","  inflating: top100/A94-1006/summary/A94-1006.gold.txt  \n","   creating: top100/A94-1009/\n","  inflating: top100/A94-1009/citing_sentences_annotated.json  \n","   creating: top100/A94-1009/Documents_xml/\n","  inflating: top100/A94-1009/Documents_xml/A94-1009.xml  \n","   creating: top100/A94-1009/summary/\n","  inflating: top100/A94-1009/summary/A94-1009.gold.txt  \n","   creating: top100/A97-1004/\n","  inflating: top100/A97-1004/citing_sentences_annotated.json  \n","   creating: top100/A97-1004/Documents_xml/\n","  inflating: top100/A97-1004/Documents_xml/A97-1004.xml  \n","   creating: top100/A97-1004/summary/\n","  inflating: top100/A97-1004/summary/A97-1004.gold.txt  \n","   creating: top100/A97-1011/\n","  inflating: top100/A97-1011/citing_sentences_annotated.json  \n","   creating: top100/A97-1011/Documents_xml/\n","  inflating: top100/A97-1011/Documents_xml/A97-1011.xml  \n","   creating: top100/A97-1011/summary/\n","  inflating: top100/A97-1011/summary/A97-1011.gold.txt  \n","   creating: top100/A97-1014/\n","  inflating: top100/A97-1014/citing_sentences_annotated.json  \n","   creating: top100/A97-1014/Documents_xml/\n","  inflating: top100/A97-1014/Documents_xml/A97-1014.xml  \n","   creating: top100/A97-1014/summary/\n","  inflating: top100/A97-1014/summary/A97-1014.gold.txt  \n","   creating: top100/A97-1029/\n","  inflating: top100/A97-1029/citing_sentences_annotated.json  \n","   creating: top100/A97-1029/Documents_xml/\n","  inflating: top100/A97-1029/Documents_xml/A97-1029.xml  \n","   creating: top100/A97-1029/summary/\n","  inflating: top100/A97-1029/summary/A97-1029.gold.txt  \n","   creating: top100/A97-1030/\n","  inflating: top100/A97-1030/citing_sentences_annotated.json  \n","   creating: top100/A97-1030/Documents_xml/\n","  inflating: top100/A97-1030/Documents_xml/A97-1030.xml  \n","   creating: top100/A97-1030/summary/\n","  inflating: top100/A97-1030/summary/A97-1030.gold.txt  \n","   creating: top100/A97-1052/\n","  inflating: top100/A97-1052/citing_sentences_annotated.json  \n","   creating: top100/A97-1052/Documents_xml/\n","  inflating: top100/A97-1052/Documents_xml/A97-1052.xml  \n","   creating: top100/A97-1052/summary/\n","  inflating: top100/A97-1052/summary/A97-1052.gold.txt  \n","   creating: top100/C02-1011/\n","  inflating: top100/C02-1011/citing_sentences_annotated.json  \n","   creating: top100/C02-1011/Documents_xml/\n","  inflating: top100/C02-1011/Documents_xml/C02-1011.xml  \n","   creating: top100/C02-1011/summary/\n","  inflating: top100/C02-1011/summary/C02-1011.gold.txt  \n","   creating: top100/C02-1054/\n","  inflating: top100/C02-1054/citing_sentences_annotated.json  \n","   creating: top100/C02-1054/Documents_xml/\n","  inflating: top100/C02-1054/Documents_xml/C02-1054.xml  \n","   creating: top100/C02-1054/summary/\n","  inflating: top100/C02-1054/summary/C02-1054.gold.txt  \n","   creating: top100/C02-1114/\n","  inflating: top100/C02-1114/citing_sentences_annotated.json  \n","   creating: top100/C02-1114/Documents_xml/\n","  inflating: top100/C02-1114/Documents_xml/C02-1114.xml  \n","   creating: top100/C02-1114/summary/\n","  inflating: top100/C02-1114/summary/C02-1114.gold.txt  \n","   creating: top100/C02-1144/\n","  inflating: top100/C02-1144/citing_sentences_annotated.json  \n","   creating: top100/C02-1144/Documents_xml/\n","  inflating: top100/C02-1144/Documents_xml/C02-1144.xml  \n","   creating: top100/C02-1144/summary/\n","  inflating: top100/C02-1144/summary/C02-1144.gold.txt  \n","   creating: top100/C02-1145/\n","  inflating: top100/C02-1145/citing_sentences_annotated.json  \n","   creating: top100/C02-1145/Documents_xml/\n","  inflating: top100/C02-1145/Documents_xml/C02-1145.xml  \n","   creating: top100/C02-1145/summary/\n","  inflating: top100/C02-1145/summary/C02-1145.gold.txt  \n","   creating: top100/C02-1150/\n","  inflating: top100/C02-1150/citing_sentences_annotated.json  \n","   creating: top100/C02-1150/Documents_xml/\n","  inflating: top100/C02-1150/Documents_xml/C02-1150.xml  \n","   creating: top100/C02-1150/summary/\n","  inflating: top100/C02-1150/summary/C02-1150.gold.txt  \n","   creating: top100/C04-1010/\n","  inflating: top100/C04-1010/citing_sentences_annotated.json  \n","   creating: top100/C04-1010/Documents_xml/\n","  inflating: top100/C04-1010/Documents_xml/C04-1010.xml  \n","   creating: top100/C04-1010/summary/\n","  inflating: top100/C04-1010/summary/C04-1010.gold.txt  \n","   creating: top100/C04-1024/\n","  inflating: top100/C04-1024/citing_sentences_annotated.json  \n","   creating: top100/C04-1024/Documents_xml/\n","  inflating: top100/C04-1024/Documents_xml/C04-1024.xml  \n","   creating: top100/C04-1024/summary/\n","  inflating: top100/C04-1024/summary/C04-1024.gold.txt  \n","   creating: top100/C04-1041/\n","  inflating: top100/C04-1041/citing_sentences_annotated.json  \n","   creating: top100/C04-1041/Documents_xml/\n","  inflating: top100/C04-1041/Documents_xml/C04-1041.xml  \n","   creating: top100/C04-1041/summary/\n","  inflating: top100/C04-1041/summary/C04-1041.gold.txt  \n","   creating: top100/C04-1051/\n","  inflating: top100/C04-1051/citing_sentences_annotated.json  \n","   creating: top100/C04-1051/Documents_xml/\n","  inflating: top100/C04-1051/Documents_xml/C04-1051.xml  \n","   creating: top100/C04-1051/summary/\n","  inflating: top100/C04-1051/summary/C04-1051.gold.txt  \n","   creating: top100/C04-1059/\n","  inflating: top100/C04-1059/citing_sentences_annotated.json  \n","   creating: top100/C04-1059/Documents_xml/\n","  inflating: top100/C04-1059/Documents_xml/C04-1059.xml  \n","   creating: top100/C04-1059/summary/\n","  inflating: top100/C04-1059/summary/C04-1059.gold.txt  \n","   creating: top100/C04-1072/\n","  inflating: top100/C04-1072/citing_sentences_annotated.json  \n","   creating: top100/C04-1072/Documents_xml/\n","  inflating: top100/C04-1072/Documents_xml/C04-1072.xml  \n","   creating: top100/C04-1072/summary/\n","  inflating: top100/C04-1072/summary/C04-1072.gold.txt  \n","   creating: top100/C04-1080/\n","  inflating: top100/C04-1080/citing_sentences_annotated.json  \n","   creating: top100/C04-1080/Documents_xml/\n","  inflating: top100/C04-1080/Documents_xml/C04-1080.xml  \n","   creating: top100/C04-1080/summary/\n","  inflating: top100/C04-1080/summary/C04-1080.gold.txt  \n","   creating: top100/C04-1081/\n","  inflating: top100/C04-1081/citing_sentences_annotated.json  \n","   creating: top100/C04-1081/Documents_xml/\n","  inflating: top100/C04-1081/Documents_xml/C04-1081.xml  \n","   creating: top100/C04-1081/summary/\n","  inflating: top100/C04-1081/summary/C04-1081.gold.txt  \n","   creating: top100/C04-1111/\n","  inflating: top100/C04-1111/citing_sentences_annotated.json  \n","   creating: top100/C04-1111/Documents_xml/\n","  inflating: top100/C04-1111/Documents_xml/C04-1111.xml  \n","   creating: top100/C04-1111/summary/\n","  inflating: top100/C04-1111/summary/C04-1111.gold.txt  \n","   creating: top100/C04-1146/\n","  inflating: top100/C04-1146/citing_sentences_annotated.json  \n","   creating: top100/C04-1146/Documents_xml/\n","  inflating: top100/C04-1146/Documents_xml/C04-1146.xml  \n","   creating: top100/C04-1146/summary/\n","  inflating: top100/C04-1146/summary/C04-1146.gold.txt  \n","   creating: top100/C04-1180/\n","  inflating: top100/C04-1180/citing_sentences_annotated.json  \n","   creating: top100/C04-1180/Documents_xml/\n","  inflating: top100/C04-1180/Documents_xml/C04-1180.xml  \n","   creating: top100/C04-1180/summary/\n","  inflating: top100/C04-1180/summary/C04-1180.gold.txt  \n","   creating: top100/C04-1197/\n","  inflating: top100/C04-1197/citing_sentences_annotated.json  \n","   creating: top100/C04-1197/Documents_xml/\n","  inflating: top100/C04-1197/Documents_xml/C04-1197.xml  \n","   creating: top100/C04-1197/summary/\n","  inflating: top100/C04-1197/summary/C04-1197.gold.txt  \n","   creating: top100/C04-1200/\n","  inflating: top100/C04-1200/citing_sentences_annotated.json  \n","   creating: top100/C04-1200/Documents_xml/\n","  inflating: top100/C04-1200/Documents_xml/C04-1200.xml  \n","   creating: top100/C04-1200/summary/\n","  inflating: top100/C04-1200/summary/C04-1200.gold.txt  \n","   creating: top100/C08-1018/\n","  inflating: top100/C08-1018/citing_sentences_annotated.json  \n","   creating: top100/C08-1018/Documents_xml/\n","  inflating: top100/C08-1018/Documents_xml/C08-1018.xml  \n","   creating: top100/C08-1018/summary/\n","  inflating: top100/C08-1018/summary/C08-1018.gold.txt  \n","   creating: top100/C08-1022/\n","  inflating: top100/C08-1022/citing_sentences_annotated.json  \n","   creating: top100/C08-1022/Documents_xml/\n","  inflating: top100/C08-1022/Documents_xml/C08-1022.xml  \n","   creating: top100/C08-1022/summary/\n","  inflating: top100/C08-1022/summary/C08-1022.gold.txt  \n","   creating: top100/C08-1098/\n","  inflating: top100/C08-1098/citing_sentences_annotated.json  \n","   creating: top100/C08-1098/Documents_xml/\n","  inflating: top100/C08-1098/Documents_xml/C08-1098.xml  \n","   creating: top100/C08-1098/summary/\n","  inflating: top100/C08-1098/summary/C08-1098.gold.txt  \n","   creating: top100/C08-1107/\n","  inflating: top100/C08-1107/citing_sentences_annotated.json  \n","   creating: top100/C08-1107/Documents_xml/\n","  inflating: top100/C08-1107/Documents_xml/C08-1107.xml  \n","   creating: top100/C08-1107/summary/\n","  inflating: top100/C08-1107/summary/C08-1107.gold.txt  \n","   creating: top100/C08-1109/\n","  inflating: top100/C08-1109/citing_sentences_annotated.json  \n","   creating: top100/C08-1109/Documents_xml/\n","  inflating: top100/C08-1109/Documents_xml/C08-1109.xml  \n","   creating: top100/C08-1109/summary/\n","  inflating: top100/C08-1109/summary/C08-1109.gold.txt  \n","   creating: top100/C08-1114/\n","  inflating: top100/C08-1114/citing_sentences_annotated.json  \n","   creating: top100/C08-1114/Documents_xml/\n","  inflating: top100/C08-1114/Documents_xml/C08-1114.xml  \n","   creating: top100/C08-1114/summary/\n","  inflating: top100/C08-1114/summary/C08-1114.gold.txt  \n","   creating: top100/C10-1011/\n","  inflating: top100/C10-1011/citing_sentences_annotated.json  \n","   creating: top100/C10-1011/Documents_xml/\n","  inflating: top100/C10-1011/Documents_xml/C10-1011.xml  \n","   creating: top100/C10-1011/summary/\n","  inflating: top100/C10-1011/summary/C10-1011.gold.txt  \n","   creating: top100/C10-1152/\n","  inflating: top100/C10-1152/citing_sentences_annotated.json  \n","   creating: top100/C10-1152/Documents_xml/\n","  inflating: top100/C10-1152/Documents_xml/C10-1152.xml  \n","   creating: top100/C10-1152/summary/\n","  inflating: top100/C10-1152/summary/C10-1152.gold.txt  \n","   creating: top100/C10-2005/\n","  inflating: top100/C10-2005/citing_sentences_annotated.json  \n","   creating: top100/C10-2005/Documents_xml/\n","  inflating: top100/C10-2005/Documents_xml/C10-2005.xml  \n","   creating: top100/C10-2005/summary/\n","  inflating: top100/C10-2005/summary/C10-2005.gold.txt  \n","   creating: top100/C10-2028/\n","  inflating: top100/C10-2028/citing_sentences_annotated.json  \n","   creating: top100/C10-2028/Documents_xml/\n","  inflating: top100/C10-2028/Documents_xml/C10-2028.xml  \n","   creating: top100/C10-2028/summary/\n","  inflating: top100/C10-2028/summary/C10-2028.gold.txt  \n","   creating: top100/C86-1045/\n","  inflating: top100/C86-1045/citing_sentences_annotated.json  \n","   creating: top100/C86-1045/Documents_xml/\n","  inflating: top100/C86-1045/Documents_xml/C86-1045.xml  \n","   creating: top100/C86-1045/summary/\n","  inflating: top100/C86-1045/summary/C86-1045.gold.txt  \n","   creating: top100/C90-2067/\n","  inflating: top100/C90-2067/citing_sentences_annotated.json  \n","   creating: top100/C90-2067/Documents_xml/\n","  inflating: top100/C90-2067/Documents_xml/C90-2067.xml  \n","   creating: top100/C90-2067/summary/\n","  inflating: top100/C90-2067/summary/C90-2067.gold.txt  \n","   creating: top100/C90-3045/\n","  inflating: top100/C90-3045/citing_sentences_annotated.json  \n","   creating: top100/C90-3045/Documents_xml/\n","  inflating: top100/C90-3045/Documents_xml/C90-3045.xml  \n","   creating: top100/C90-3045/summary/\n","  inflating: top100/C90-3045/summary/C90-3045.gold.txt  \n","   creating: top100/C92-1019/\n","  inflating: top100/C92-1019/citing_sentences_annotated.json  \n","   creating: top100/C92-1019/Documents_xml/\n","  inflating: top100/C92-1019/Documents_xml/C92-1019.xml  \n","   creating: top100/C92-1019/summary/\n","  inflating: top100/C92-1019/summary/C92-1019.gold.txt  \n","   creating: top100/C92-2066/\n","  inflating: top100/C92-2066/citing_sentences_annotated.json  \n","   creating: top100/C92-2066/Documents_xml/\n","  inflating: top100/C92-2066/Documents_xml/C92-2066.xml  \n","   creating: top100/C92-2066/summary/\n","  inflating: top100/C92-2066/summary/C92-2066.gold.txt  \n","   creating: top100/C94-1079/\n","  inflating: top100/C94-1079/citing_sentences_annotated.json  \n","   creating: top100/C94-1079/Documents_xml/\n","  inflating: top100/C94-1079/Documents_xml/C94-1079.xml  \n","   creating: top100/C94-1079/summary/\n","  inflating: top100/C94-1079/summary/C94-1079.gold.txt  \n","   creating: top100/C94-2178/\n","  inflating: top100/C94-2178/citing_sentences_annotated.json  \n","   creating: top100/C94-2178/Documents_xml/\n","  inflating: top100/C94-2178/Documents_xml/C94-2178.xml  \n","   creating: top100/C94-2178/summary/\n","  inflating: top100/C94-2178/summary/C94-2178.gold.txt  \n","   creating: top100/C94-2195/\n","  inflating: top100/C94-2195/citing_sentences_annotated.json  \n","   creating: top100/C94-2195/Documents_xml/\n","  inflating: top100/C94-2195/Documents_xml/C94-2195.xml  \n","   creating: top100/C94-2195/summary/\n","  inflating: top100/C94-2195/summary/C94-2195.gold.txt  \n","   creating: top100/C96-1021/\n","  inflating: top100/C96-1021/citing_sentences_annotated.json  \n","   creating: top100/C96-1021/Documents_xml/\n","  inflating: top100/C96-1021/Documents_xml/C96-1021.xml  \n","   creating: top100/C96-1021/summary/\n","  inflating: top100/C96-1021/summary/C96-1021.gold.txt  \n","   creating: top100/C96-2141/\n","  inflating: top100/C96-2141/citing_sentences_annotated.json  \n","   creating: top100/C96-2141/Documents_xml/\n","  inflating: top100/C96-2141/Documents_xml/C96-2141.xml  \n","   creating: top100/C96-2141/summary/\n","  inflating: top100/C96-2141/summary/C96-2141.gold.txt  \n","   creating: top100/D07-1002/\n","  inflating: top100/D07-1002/citing_sentences_annotated.json  \n","   creating: top100/D07-1002/Documents_xml/\n","  inflating: top100/D07-1002/Documents_xml/D07-1002.xml  \n","   creating: top100/D07-1002/summary/\n","  inflating: top100/D07-1002/summary/D07-1002.gold.txt  \n","   creating: top100/P08-1043/\n","  inflating: top100/P08-1043/citing_sentences_annotated.json  \n","   creating: top100/P08-1043/Documents_xml/\n","  inflating: top100/P08-1043/Documents_xml/P08-1043.xml  \n","   creating: top100/P08-1043/summary/\n","  inflating: top100/P08-1043/summary/P08-1043.gold.txt  \n","   creating: top100/P08-1064/\n","  inflating: top100/P08-1064/citing_sentences_annotated.json  \n","   creating: top100/P08-1064/Documents_xml/\n","  inflating: top100/P08-1064/Documents_xml/P08-1064.xml  \n","   creating: top100/P08-1064/summary/\n","  inflating: top100/P08-1064/summary/P08-1064.gold.txt  \n","   creating: top100/P08-1066/\n","  inflating: top100/P08-1066/citing_sentences_annotated.json  \n","   creating: top100/P08-1066/Documents_xml/\n","  inflating: top100/P08-1066/Documents_xml/P08-1066.xml  \n","   creating: top100/P08-1066/summary/\n","  inflating: top100/P08-1066/summary/P08-1066.gold.txt  \n","   creating: top100/P08-1067/\n","  inflating: top100/P08-1067/citing_sentences_annotated.json  \n","   creating: top100/P08-1067/Documents_xml/\n","  inflating: top100/P08-1067/Documents_xml/P08-1067.xml  \n","   creating: top100/P08-1067/summary/\n","  inflating: top100/P08-1067/summary/P08-1067.gold.txt  \n","   creating: top100/P08-1068/\n","  inflating: top100/P08-1068/citing_sentences_annotated.json  \n","   creating: top100/P08-1068/Documents_xml/\n","  inflating: top100/P08-1068/Documents_xml/P08-1068.xml  \n","   creating: top100/P08-1068/summary/\n","  inflating: top100/P08-1068/summary/P08-1068.gold.txt  \n","   creating: top100/P08-1076/\n","  inflating: top100/P08-1076/citing_sentences_annotated.json  \n","   creating: top100/P08-1076/Documents_xml/\n","  inflating: top100/P08-1076/Documents_xml/P08-1076.xml  \n","   creating: top100/P08-1076/summary/\n","  inflating: top100/P08-1076/summary/P08-1076.gold.txt  \n","   creating: top100/P08-1084/\n","  inflating: top100/P08-1084/citing_sentences_annotated.json  \n","   creating: top100/P08-1084/Documents_xml/\n","  inflating: top100/P08-1084/Documents_xml/P08-1084.xml  \n","   creating: top100/P08-1084/summary/\n","  inflating: top100/P08-1084/summary/P08-1084.gold.txt  \n","   creating: top100/P08-1085/\n","  inflating: top100/P08-1085/citing_sentences_annotated.json  \n","   creating: top100/P08-1085/Documents_xml/\n","  inflating: top100/P08-1085/Documents_xml/P08-1085.xml  \n","   creating: top100/P08-1085/summary/\n","  inflating: top100/P08-1085/summary/P08-1085.gold.txt  \n","   creating: top100/P08-1086/\n","  inflating: top100/P08-1086/citing_sentences_annotated.json  \n","   creating: top100/P08-1086/Documents_xml/\n","  inflating: top100/P08-1086/Documents_xml/P08-1086.xml  \n","   creating: top100/P08-1086/summary/\n","  inflating: top100/P08-1086/summary/P08-1086.gold.txt  \n","   creating: top100/P08-1088/\n","  inflating: top100/P08-1088/citing_sentences_annotated.json  \n","   creating: top100/P08-1088/Documents_xml/\n","  inflating: top100/P08-1088/Documents_xml/P08-1088.xml  \n","   creating: top100/P08-1088/summary/\n","  inflating: top100/P08-1088/summary/P08-1088.gold.txt  \n","   creating: top100/P08-1090/\n","  inflating: top100/P08-1090/citing_sentences_annotated.json  \n","   creating: top100/P08-1090/Documents_xml/\n","  inflating: top100/P08-1090/Documents_xml/P08-1090.xml  \n","   creating: top100/P08-1090/summary/\n","  inflating: top100/P08-1090/summary/P08-1090.gold.txt  \n","   creating: top100/P08-1101/\n","  inflating: top100/P08-1101/citing_sentences_annotated.json  \n","   creating: top100/P08-1101/Documents_xml/\n","  inflating: top100/P08-1101/Documents_xml/P08-1101.xml  \n","   creating: top100/P08-1101/summary/\n","  inflating: top100/P08-1101/summary/P08-1101.gold.txt  \n","   creating: top100/P08-1102/\n","  inflating: top100/P08-1102/citing_sentences_annotated.json  \n","   creating: top100/P08-1102/Documents_xml/\n","  inflating: top100/P08-1102/Documents_xml/P08-1102.xml  \n","   creating: top100/P08-1102/summary/\n","  inflating: top100/P08-1102/summary/P08-1102.gold.txt  \n","   creating: top100/P08-1108/\n","  inflating: top100/P08-1108/citing_sentences_annotated.json  \n","   creating: top100/P08-1108/Documents_xml/\n","  inflating: top100/P08-1108/Documents_xml/P08-1108.xml  \n","   creating: top100/P08-1108/summary/\n","  inflating: top100/P08-1108/summary/P08-1108.gold.txt  \n","   creating: top100/P08-1109/\n","  inflating: top100/P08-1109/citing_sentences_annotated.json  \n","   creating: top100/P08-1109/Documents_xml/\n","  inflating: top100/P08-1109/Documents_xml/P08-1109.xml  \n","   creating: top100/P08-1109/summary/\n","  inflating: top100/P08-1109/summary/P08-1109.gold.txt  \n","   creating: top100/P08-1114/\n","  inflating: top100/P08-1114/citing_sentences_annotated.json  \n","   creating: top100/P08-1114/Documents_xml/\n","  inflating: top100/P08-1114/Documents_xml/P08-1114.xml  \n","   creating: top100/P08-1114/summary/\n","  inflating: top100/P08-1114/summary/P08-1114.gold.txt  \n","   creating: top100/P08-1115/\n","  inflating: top100/P08-1115/citing_sentences_annotated.json  \n","   creating: top100/P08-1115/Documents_xml/\n","  inflating: top100/P08-1115/Documents_xml/P08-1115.xml  \n","   creating: top100/P08-1115/summary/\n","  inflating: top100/P08-1115/summary/P08-1115.gold.txt  \n","   creating: top100/P08-1119/\n","  inflating: top100/P08-1119/citing_sentences_annotated.json  \n","   creating: top100/P08-1119/Documents_xml/\n","  inflating: top100/P08-1119/Documents_xml/P08-1119.xml  \n","   creating: top100/P08-1119/summary/\n","  inflating: top100/P08-1119/summary/P08-1119.gold.txt  \n","   creating: top100/P08-2007/\n","  inflating: top100/P08-2007/citing_sentences_annotated.json  \n","   creating: top100/P08-2007/Documents_xml/\n","  inflating: top100/P08-2007/Documents_xml/P08-2007.xml  \n","   creating: top100/P08-2007/summary/\n","  inflating: top100/P08-2007/summary/P08-2007.gold.txt  \n","   creating: top100/P08-2012/\n","  inflating: top100/P08-2012/citing_sentences_annotated.json  \n","   creating: top100/P08-2012/Documents_xml/\n","  inflating: top100/P08-2012/Documents_xml/P08-2012.xml  \n","   creating: top100/P08-2012/summary/\n","  inflating: top100/P08-2012/summary/P08-2012.gold.txt  \n","   creating: top100/P08-2026/\n","  inflating: top100/P08-2026/citing_sentences_annotated.json  \n","   creating: top100/P08-2026/Documents_xml/\n","  inflating: top100/P08-2026/Documents_xml/P08-2026.xml  \n","   creating: top100/P08-2026/summary/\n","  inflating: top100/P08-2026/summary/P08-2026.gold.txt  \n","   creating: top100/P09-1010/\n","  inflating: top100/P09-1010/citing_sentences_annotated.json  \n","   creating: top100/P09-1010/Documents_xml/\n","  inflating: top100/P09-1010/Documents_xml/P09-1010.xml  \n","   creating: top100/P09-1010/summary/\n","  inflating: top100/P09-1010/summary/P09-1010.gold.txt  \n","   creating: top100/P09-1011/\n","  inflating: top100/P09-1011/citing_sentences_annotated.json  \n","   creating: top100/P09-1011/Documents_xml/\n","  inflating: top100/P09-1011/Documents_xml/P09-1011.xml  \n","   creating: top100/P09-1011/summary/\n","  inflating: top100/P09-1011/summary/P09-1011.gold.txt  \n","   creating: top100/P09-1019/\n","  inflating: top100/P09-1019/citing_sentences_annotated.json  \n","   creating: top100/P09-1019/Documents_xml/\n","  inflating: top100/P09-1019/Documents_xml/P09-1019.xml  \n","   creating: top100/P09-1019/summary/\n","  inflating: top100/P09-1019/summary/P09-1019.gold.txt  \n","   creating: top100/P09-1026/\n","  inflating: top100/P09-1026/citing_sentences_annotated.json  \n","   creating: top100/P09-1026/Documents_xml/\n","  inflating: top100/P09-1026/Documents_xml/P09-1026.xml  \n","   creating: top100/P09-1026/summary/\n","  inflating: top100/P09-1026/summary/P09-1026.gold.txt  \n","   creating: top100/P09-1027/\n","  inflating: top100/P09-1027/citing_sentences_annotated.json  \n","   creating: top100/P09-1027/Documents_xml/\n","  inflating: top100/P09-1027/Documents_xml/P09-1027.xml  \n","   creating: top100/P09-1027/summary/\n","  inflating: top100/P09-1027/summary/P09-1027.gold.txt  \n","   creating: top100/P09-1039/\n","  inflating: top100/P09-1039/citing_sentences_annotated.json  \n","   creating: top100/P09-1039/Documents_xml/\n","  inflating: top100/P09-1039/Documents_xml/P09-1039.xml  \n","   creating: top100/P09-1039/summary/\n","  inflating: top100/P09-1039/summary/P09-1039.gold.txt  \n","   creating: top100/P09-1040/\n","  inflating: top100/P09-1040/citing_sentences_annotated.json  \n","   creating: top100/P09-1040/Documents_xml/\n","  inflating: top100/P09-1040/Documents_xml/P09-1040.xml  \n","   creating: top100/P09-1040/summary/\n","  inflating: top100/P09-1040/summary/P09-1040.gold.txt  \n","   creating: top100/P09-1042/\n","  inflating: top100/P09-1042/citing_sentences_annotated.json  \n","   creating: top100/P09-1042/Documents_xml/\n","  inflating: top100/P09-1042/Documents_xml/P09-1042.xml  \n","   creating: top100/P09-1042/summary/\n","  inflating: top100/P09-1042/summary/P09-1042.gold.txt  \n","   creating: top100/P09-1057/\n","  inflating: top100/P09-1057/citing_sentences_annotated.json  \n","   creating: top100/P09-1057/Documents_xml/\n","  inflating: top100/P09-1057/Documents_xml/P09-1057.xml  \n","   creating: top100/P09-1057/summary/\n","  inflating: top100/P09-1057/summary/P09-1057.gold.txt  \n","   creating: top100/P09-1058/\n","  inflating: top100/P09-1058/citing_sentences_annotated.json  \n","   creating: top100/P09-1058/Documents_xml/\n","  inflating: top100/P09-1058/Documents_xml/P09-1058.xml  \n","   creating: top100/P09-1058/summary/\n","  inflating: top100/P09-1058/summary/P09-1058.gold.txt  \n","   creating: top100/P09-1068/\n","  inflating: top100/P09-1068/citing_sentences_annotated.json  \n","   creating: top100/P09-1068/Documents_xml/\n","  inflating: top100/P09-1068/Documents_xml/P09-1068.xml  \n","   creating: top100/P09-1068/summary/\n","  inflating: top100/P09-1068/summary/P09-1068.gold.txt  \n","   creating: top100/P09-1074/\n","  inflating: top100/P09-1074/citing_sentences_annotated.json  \n","   creating: top100/P09-1074/Documents_xml/\n","  inflating: top100/P09-1074/Documents_xml/P09-1074.xml  \n","   creating: top100/P09-1074/summary/\n","  inflating: top100/P09-1074/summary/P09-1074.gold.txt  \n","   creating: top100/P09-1077/\n","  inflating: top100/P09-1077/citing_sentences_annotated.json  \n","   creating: top100/P09-1077/Documents_xml/\n","  inflating: top100/P09-1077/Documents_xml/P09-1077.xml  \n","   creating: top100/P09-1077/summary/\n","  inflating: top100/P09-1077/summary/P09-1077.gold.txt  \n","   creating: top100/P09-1088/\n","  inflating: top100/P09-1088/citing_sentences_annotated.json  \n","   creating: top100/P09-1088/Documents_xml/\n","  inflating: top100/P09-1088/Documents_xml/P09-1088.xml  \n","   creating: top100/P09-1088/summary/\n","  inflating: top100/P09-1088/summary/P09-1088.gold.txt  \n","   creating: top100/P10-1001/\n","  inflating: top100/P10-1001/citing_sentences_annotated.json  \n","   creating: top100/P10-1001/Documents_xml/\n","  inflating: top100/P10-1001/Documents_xml/P10-1001.xml  \n","   creating: top100/P10-1001/summary/\n","  inflating: top100/P10-1001/summary/P10-1001.gold.txt  \n","   creating: top100/P10-1040/\n","  inflating: top100/P10-1040/citing_sentences_annotated.json  \n","   creating: top100/P10-1040/Documents_xml/\n","  inflating: top100/P10-1040/Documents_xml/P10-1040.xml  \n","   creating: top100/P10-1040/summary/\n","  inflating: top100/P10-1040/summary/P10-1040.gold.txt  \n","   creating: top100/P10-1044/\n","  inflating: top100/P10-1044/citing_sentences_annotated.json  \n","   creating: top100/P10-1044/Documents_xml/\n","  inflating: top100/P10-1044/Documents_xml/P10-1044.xml  \n","   creating: top100/P10-1044/summary/\n","  inflating: top100/P10-1044/summary/P10-1044.gold.txt  \n"]}]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhOxPIBPvuW3","executionInfo":{"status":"ok","timestamp":1647326865535,"user_tz":-480,"elapsed":484,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"f7324533-ebb2-4bcf-d137-9ddd616cce58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Omdena School/Solving Business Problems with NLP/Capstone Project\n"]}]},{"cell_type":"code","source":["# Get the list of directories for all .xml files\n","file_directory = glob.glob(\"scisummnet_release1.1__20190413/top100/*/*/*.xml\", recursive=True)\n","\n","# Check if the paths directory is correct\n","file_directory[0:5]"],"metadata":{"id":"d0P4WK1mkcDi","executionInfo":{"status":"ok","timestamp":1647329487415,"user_tz":-480,"elapsed":23183,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"986eba75-5360-4ac7-b660-a01f045048ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['scisummnet_release1.1__20190413/top100/P08-1115/Documents_xml/P08-1115.xml',\n"," 'scisummnet_release1.1__20190413/top100/P09-1039/Documents_xml/P09-1039.xml',\n"," 'scisummnet_release1.1__20190413/top100/P08-1102/Documents_xml/P08-1102.xml',\n"," 'scisummnet_release1.1__20190413/top100/P08-1066/Documents_xml/P08-1066.xml',\n"," 'scisummnet_release1.1__20190413/top100/P09-1040/Documents_xml/P09-1040.xml']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Create xml file extraction function\n","def extract_xml(directory):\n","  xml_data = objectify.parse(directory)  # Parse XML data\n","  root = xml_data.getroot()  # Root element\n","\n","  data = []\n","  cols = []\n","  for i in range(len(root.getchildren())):\n","      child = root.getchildren()[i]\n","      data.append([subchild.text for subchild in child.getchildren()])\n","\n","      # If the tag is not 'SECTION', it is a section header, append that header\n","      # If it is, it means it is a subsection, and append the title of that \n","      # subsection\n","      if child.tag != \"SECTION\":\n","        cols.append(child.tag)\n","      else:\n","        cols.append(child.attrib.get('title'))\n","\n","  df = pd.DataFrame(data).T  # Create DataFrame and transpose it\n","  df.columns = cols  # Update column names\n","\n","  # Get the abstract column (second column)\n","  abstract_list = df.iloc[:, 1].dropna()\n","  abstract = \" \".join(abstract_list)\n","\n","  # Get the conclusion column (penultimate column)\n","  conclusion_text = df.iloc[:, -2].dropna()\n","  conclusion = \" \".join(conclusion_text)\n","\n","  # Drop last column of a dataframe\n","  df = df.iloc[: , :-1]\n","\n","  # Drop first column: S \n","  df = df.iloc[:, 1:]\n","\n","  # Iterate over all sections and join them together to get the text document\n","  text_list = []\n","  for column in df.columns:\n","    text_filtered = df[column].dropna()\n","    text = \" \".join(text_filtered)\n","    text_list.append(text)\n","\n","  text_list\n","  final_text = \" \".join(text_list)\n","\n","  return abstract, final_text, conclusion"],"metadata":{"id":"Op4-AWExT0vv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","abstract_list = []\n","full_text_list = []\n","conclusion_list = []\n","\n","counter = 0\n","for directory in file_directory:\n","  abstract, full_text, conclusion = extract_xml(directory)\n","  abstract_list.append(abstract)\n","  full_text_list.append(full_text)\n","  conclusion_list.append(conclusion)\n","\n","  print(f\"XML extraction for document {counter} done! \\n\")\n","  counter += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgd82V_-o3y_","executionInfo":{"status":"ok","timestamp":1647329653167,"user_tz":-480,"elapsed":1228,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"e8437d66-5f41-4222-bcbc-b5d8a73f352f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["XML extraction for document 0 done! \n","\n","XML extraction for document 1 done! \n","\n","XML extraction for document 2 done! \n","\n","XML extraction for document 3 done! \n","\n","XML extraction for document 4 done! \n","\n","XML extraction for document 5 done! \n","\n","XML extraction for document 6 done! \n","\n","XML extraction for document 7 done! \n","\n","XML extraction for document 8 done! \n","\n","XML extraction for document 9 done! \n","\n","XML extraction for document 10 done! \n","\n","XML extraction for document 11 done! \n","\n","XML extraction for document 12 done! \n","\n","XML extraction for document 13 done! \n","\n","XML extraction for document 14 done! \n","\n","XML extraction for document 15 done! \n","\n","XML extraction for document 16 done! \n","\n","XML extraction for document 17 done! \n","\n","XML extraction for document 18 done! \n","\n","XML extraction for document 19 done! \n","\n","XML extraction for document 20 done! \n","\n","XML extraction for document 21 done! \n","\n","XML extraction for document 22 done! \n","\n","XML extraction for document 23 done! \n","\n","XML extraction for document 24 done! \n","\n","XML extraction for document 25 done! \n","\n","XML extraction for document 26 done! \n","\n","XML extraction for document 27 done! \n","\n","XML extraction for document 28 done! \n","\n","XML extraction for document 29 done! \n","\n","XML extraction for document 30 done! \n","\n","XML extraction for document 31 done! \n","\n","XML extraction for document 32 done! \n","\n","XML extraction for document 33 done! \n","\n","XML extraction for document 34 done! \n","\n","XML extraction for document 35 done! \n","\n","XML extraction for document 36 done! \n","\n","XML extraction for document 37 done! \n","\n","XML extraction for document 38 done! \n","\n","XML extraction for document 39 done! \n","\n","XML extraction for document 40 done! \n","\n","XML extraction for document 41 done! \n","\n","XML extraction for document 42 done! \n","\n","XML extraction for document 43 done! \n","\n","XML extraction for document 44 done! \n","\n","XML extraction for document 45 done! \n","\n","XML extraction for document 46 done! \n","\n","XML extraction for document 47 done! \n","\n","XML extraction for document 48 done! \n","\n","XML extraction for document 49 done! \n","\n","XML extraction for document 50 done! \n","\n","XML extraction for document 51 done! \n","\n","XML extraction for document 52 done! \n","\n","XML extraction for document 53 done! \n","\n","XML extraction for document 54 done! \n","\n","XML extraction for document 55 done! \n","\n","XML extraction for document 56 done! \n","\n","XML extraction for document 57 done! \n","\n","XML extraction for document 58 done! \n","\n","XML extraction for document 59 done! \n","\n","XML extraction for document 60 done! \n","\n","XML extraction for document 61 done! \n","\n","XML extraction for document 62 done! \n","\n","XML extraction for document 63 done! \n","\n","XML extraction for document 64 done! \n","\n","XML extraction for document 65 done! \n","\n","XML extraction for document 66 done! \n","\n","XML extraction for document 67 done! \n","\n","XML extraction for document 68 done! \n","\n","XML extraction for document 69 done! \n","\n","XML extraction for document 70 done! \n","\n","XML extraction for document 71 done! \n","\n","XML extraction for document 72 done! \n","\n","XML extraction for document 73 done! \n","\n","XML extraction for document 74 done! \n","\n","XML extraction for document 75 done! \n","\n","XML extraction for document 76 done! \n","\n","XML extraction for document 77 done! \n","\n","XML extraction for document 78 done! \n","\n","XML extraction for document 79 done! \n","\n","XML extraction for document 80 done! \n","\n","XML extraction for document 81 done! \n","\n","XML extraction for document 82 done! \n","\n","XML extraction for document 83 done! \n","\n","XML extraction for document 84 done! \n","\n","XML extraction for document 85 done! \n","\n","XML extraction for document 86 done! \n","\n","XML extraction for document 87 done! \n","\n","XML extraction for document 88 done! \n","\n","XML extraction for document 89 done! \n","\n","XML extraction for document 90 done! \n","\n","XML extraction for document 91 done! \n","\n","XML extraction for document 92 done! \n","\n","XML extraction for document 93 done! \n","\n","XML extraction for document 94 done! \n","\n","XML extraction for document 95 done! \n","\n","XML extraction for document 96 done! \n","\n","XML extraction for document 97 done! \n","\n","XML extraction for document 98 done! \n","\n","XML extraction for document 99 done! \n","\n","CPU times: user 844 ms, sys: 48.7 ms, total: 892 ms\n","Wall time: 1.08 s\n"]}]},{"cell_type":"code","source":["print(abstract_list[0:2])\n","print(full_text_list[0:2])\n","print(conclusion_list[0:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPIi5gJqXffq","executionInfo":{"status":"ok","timestamp":1647329655527,"user_tz":-480,"elapsed":5,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"d48c91f5-0dfb-4c61-8a77-ac9625764f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation.', 'We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.']\n","[\"Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation. When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight – harkening back to Weaver in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f' E F(o) and we seek An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max max Pr(e)Pr(f'|e)Pr(o|f')�(4) e f�EF(o) Following Och and Ney (2002), we use the maximum entropy framework (Berger et al., 1996) to directly model the posterior Pr(e, f'|o) with parameters tuned to minimize a loss function representing the quality only of the resulting translations. Thus, we make use of the following general decision rule: In principle, one could decode according to (2) simply by enumerating and decoding each f� ∈ F(o); however, for any interestingly large F(o) this will be impractical. We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate. In Section 2, we discuss decoding with this model in general, and then show how two classes of translation models can easily be adapted for lattice translation; we achieve a unified treatment of finite-state and hierarchical phrase-based models by treating lattices as a subcase of weighted finite state automata (FSAs). In Section 3, we identify and solve issues that arise with reordering in non-linear FSAs, i.e. FSAs where every path does not pass through every node. Section 4 presents two applications of the noisier channel paradigm, demonstrating substantial performance gains in Arabic-English and Chinese-English translation. In Section 5 we discuss relevant prior work, and we conclude in Section 6. Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008). In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices. The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007). The second is a phrase-based decoder implementing the model of Koehn et al. (2003). A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA). We further stipulate that exactly one node has no outgoing edges and is designated the ‘end node’. Figure 1 illustrates three classes of word lattices. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multiple members of the set to be represented with a single piece of structure. Additionally, all paths from one node to another form an equivalence class representing, in our model, alternative expressions of the same underlying communicative intent. For translation, we will find it useful to encode G in a chart based on a topological ordering of the nodes, as described by Cheppalier et al. (1999). The nodes in the lattices shown in Figure 1 are labeled according to an appropriate numbering. The chart-representation of the graph is a triple of 2-dimensional matrices hF, p, Ri, which can be constructed from the numbered graph. Fi,j is the word label of the jth transition leaving node i. The corresponding transition cost is pi,j. Ri,j is the node number of the node on the right side of the jth transition leaving node i. Note that Ri,j > i for all i, j. Table 1 shows the word lattice from Figure 1 represented in matrix form as hF, p, Ri. Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. The parser consists of two kinds of items, the first with the form [X —* α • Q, i, j] representing rules that have yet to be completed and span node i to node j. The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j]. As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [5, 0, |V  |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents. A target language model is necessary to generate fluent output. To do so, the grammar is intersected with an n-gram LM. To mitigate the effects of the combinatorial explosion of non-terminals the LM intersection entails, we use cube-pruning to only consider the most promising expansions (Chiang, 2007). A second important class of translation models includes those based formally on FSTs. We present a description of the decoding process for a word lattice using a representative FST model, the phrase-based translation model described in Koehn et al. (2003). Phrase-based models translate a foreign sentence f into the target language e by breaking up f into a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words. Each word in f must be translated exactly once. To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1. Although the number of source phrases in a word lattice can be exponential in the number of nodes, enumerating the possible translations of every span in a lattice is in practice tractable, as described by Bertoldi et al. (2007). We adapted the Moses phrase-based decoder to translate word lattices (Koehn et al., 2007). The unmodified decoder builds a translation hypothesis from left to right by selecting a range of untranslated words and adding translations of this phrase to the end of the hypothesis being extended. When no untranslated words remain, the translation process is complete. The word lattice decoder works similarly, only now the decoder keeps track not of the words that have been covered, but of the nodes, given a topological ordering of the nodes. For example, assuming the third lattice in Figure 1 is our input, if the edge with word a is translated, this will cover two untranslated nodes [0,1] in the coverage vector, even though it is only a single word. As with sentencebased decoding, a translation hypothesis is complete when all nodes in the input lattice are covered. The changes described thus far are straightforward adaptations of the underlying phrase-based sentence decoder; however, dealing properly with non-monotonic decoding of word lattices introduces some minor complexity that is worth mentioning. In the sentence decoder, any translation of any span of untranslated words is an allowable extension of a partial translation hypothesis, provided that the coverage vectors of the extension and the partial hypothesis do not intersect. In a non-linear word lattice, a further constraint must be enforced ensuring that there is always a path from the starting node of the translation extension’s source to the node representing the nearest right edge of the already-translated material, as well as a path from the ending node of the translation extension’s source to future translated spans. Figure 3 illustrates the problem. If [0,1] is translated, the decoder must not consider translating [2,3] as a possible extension of this hypothesis since there is no path from node 1 to node 2 and therefore the span [1,2] would never be covered. In the parser that forms the basis of the hierarchical decoder described in Section 2.3, no such restriction is necessary since grammar rules are processed in a strictly left-to-right fashion without any skips. In both hierarchical and phrase-based models, the distance between words in the source sentence is used to limit where in the target sequence their translations will be generated. In phrase based translation, distortion is modeled explicitly. Models that support non-monotonic decoding generally include a distortion cost, such as |ai − bi−1 − 1 |where ai is the starting position of the foreign phrase fi and bi−1 is the ending position of phrase fi−1 (Koehn et al., 2003). The intuition behind this model is that since most translation is monotonic, the cost of skipping ahead or back in the source should be proportional to the number of words that are skipped. Additionally, a maximum distortion limit is used to restrict the size of the search space. In linear word lattices, such as confusion networks, the distance metric used for the distortion penalty and for distortion limits is well defined; however, in a non-linear word lattice, it poses the problem illustrated in Figure 4. Assuming the leftto-right decoding strategy described in the previous section, if c is generated by the first target word, the distortion penalty associated with “skipping ahead” should be either 3 or 2, depending on what path is chosen to translate the span [0,3]. In large lattices, where a single arc may span many nodes, the possible distances may vary quite substantially depending on what path is ultimately taken, and handling this properly therefore crucial. Although hierarchical phrase-based models do not model distortion explicitly, Chiang (2007) suggests using a span length limit to restrict the window in which reordering can take place.1 The decoder enforces the constraint that a synchronous rule learned from the training data (the only mechanism by which reordering can be introduced) can span maximally A words in f. Like the distortion cost used in phrase-based systems, A is also poorly defined for non-linear lattices. Since we want a distance metric that will restrict as few local reorderings as possible on any path, we use a function �(a, b) returning the length of the shortest path between nodes a and b. Since this function is not dependent on the exact path chosen, it can be computed in advance of decoding using an allpairs shortest path algorithm (Cormen et al., 1989). We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices. We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance. For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005). Table 2 summarizes the results of the phrasebased systems. On both test sets, the shortest path metric improved the BLEU scores. As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline). Table 3 summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder’s span limit. The pattern is the same, showing a clear increase in BLEU for the shortest path metric over the baseline. Chinese word segmentation. A necessary first step in translating Chinese using standard models is segmenting the character stream into a sequence of words. Word-lattice translation offers two possible improvements over the conventional approach. First, a lattice may represent multiple alternative segmentations of a sentence; input represented in this way will be more robust to errors made by the segmenter.2 Second, different segmentation granularities may be more or less optimal for translating different spans. By encoding alternatives in the input in a word lattice, the decision as to which granularity to use for a given span can be resolved during decoding rather than when constructing the system. Figure 5 illustrates a lattice based on three different segmentations. Arabic morphological variation. Arabic orthography is problematic for lexical and phrase-based MT approaches since a large class of functional elements (prepositions, pronouns, tense markers, conjunctions, definiteness markers) are attached to their host stems. Thus, while the training data may provide good evidence for the translation of a particular stem by itself, the same stem may not be attested when attached to a particular conjunction. The general solution taken is to take the best possible morphological analysis of the text (it is often ambiguous whether a piece of a word is part of the stem or merely a neighboring functional element), and then make a subset of the bound functional elements in the language into freestanding tokens. Figure 6 illustrates the unsegmented Arabic surface form as well as the morphological segmentation variant we made use of. The limitation of this approach is that as the amount and variety of training data increases, the optimal segmentation strategy changes: more aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbin and Stanford segmenters (hs+ss), and one which uses all three segmentations (hs+ss+cs). Data and Settings. The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences). The corpus was analyzed with the three segmentation schemes. For the systems using word lattices, the training data contained the versions of the corpus appropriate for the segmentation schemes used in the input. That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.3 A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments. The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate training (Och, 2003). The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach. All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05. The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant. Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices. For example, for MT06 the number of OOVs in the hs translation is 484. The number of OOVs decreased by 19% for hs+ss and by 75% for hs+ss+cs. As mentioned in Section 3, using lexical reordering for word-lattices further improves the translation quality. We created lattices from an unsegmented version of the Arabic test data and generated alternative arcs where clitics as well as the definiteness marker and the future tense marker were segmented into tokens. We used the Buckwalter morphological analyzer and disambiguated the analysis using a simple unigram model trained on the Penn Arabic Treebank. Data and Settings. For these experiments we made use of the entire NIST MT08 training data, although for training of the system, we used a subsampling method proposed by Kishore Papineni that aims to include training sentences containing ngrams in the test data (personal communication). For all systems, we used a 5-gram English LM trained on 250M words of English training data. The NIST MT03 test set was used as development set for optimizing the interpolation weights using MER training (Och, 2003). Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al. (2004) only report results of monotonic decoding for the systems they describe. Bertoldi et al. (2007) solve the problem by requiring that their input be in the format of a confusion network, which enables the standard distortion penalty to be used. Finally, the system described by Zhang et al. (2005) uses IBM Model 4 features to translate lattices. For the distortion model, they use the maximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simplifications of a surface token when the surface token can not be translated. Parsing and formal language theory. There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998). Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG. Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices. We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model. Additionally, we have shown that although word lattices complicate modeling of word reordering, a simple heuristic offers good performance and enables many standard distortion models to be used directly with lattice input.\", \"We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective dependency parsing. We believe that our formulations can pave the way for efficient exploitation of global features and constraints in parsing applications, leading to more powerful models. Riedel and Clarke (2006) cast dependency parsing as an ILP, but efficient formulations remain an open problem. Our formulations offer the following comparative advantages: from data. In particular, our formulations handle higher-order arc interactions (like siblings and grandparents), model word valency, and can learn to favor nearly-projective parses. We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1) directed graph D = hV, Ai, with vertices in V = {0, ... , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2. Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed graph D if hV, Bi is a (directed) tree rooted at r. We define the set of legal dependency parse trees of x (denoted Y(x)) as the set of 0-arborescences of D, i.e., we admit each arborescence as a potential dependency tree. Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i. We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) < k < max(i, j), there is a directed path in y from i to k). A dependency tree is called projective if it only contains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z °_ hzaia∈A with each component defined as za = ff(a ∈ y) (here, ff(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal dependency trees and taking the convex hull, we obtain a polyhedron that we call the arborescence polytope, denoted by Z(x). Each vertex of Z(x) can be identified with a dependency tree in Y(x). The Minkowski-Weyl theorem (Rockafellar, 1970) ensures that Z(x) has a representation of the form Z(x) = {z ∈ R|A  ||Az ≤ b}, for some p-by-|A| matrix A and some vector b in Rp. However, it is not easy to obtain a compact representation (where p grows polynomially with the number of words n). In §3, we will provide a compact representation of an outer polytope ¯Z(x) ⊇ Z(x) whose integer vertices correspond to dependency trees. Hence, the problem of finding the dependency tree that maximizes some linear function of the inci1The general case where A C_ V 2 is also of interest; it arises whenever a constraint or a lexicon forbids some arcs from appearing in dependency tree. It may also arise as a consequence of a first-stage pruning step where some candidate arcs are eliminated; this will be further discussed in §4. where only the backbone structure (i.e., the arcs without the labels depicted in Fig. 1) is to be predicted. tences;examples from McDonald and Satta (2007). those that assume each dependency decision denceevectorsmcan befcastdas ansILP. A similar idea was aplied to word alignment by Lacoste-Julien that dependency graphs must be trees. Such mod their parameters facor relative to individual edges et al. (2006), where permutations (rather than arof the graph (Paskin, 2001; McDonald et a., l are comny d o as gefacd 2005a). Edge-factored models have many computah pm cto ativ ndidl dge borescences) were the combinatorial structure bef th gah (Pki 2001 MDld t l ing requiring representation. ai to earn a parse, i.e., a functo h : X → Y mary problem in treating each dependency s in Nonlocal information such as arity (o valy that given x ∈ X ouputs a legal dependency parse depedent is that it is not a realistic assumption. and neighbouring dependencies can be crucial to y ∈ Y(x). Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview. Typcal graph-bsed methods invetigating and xtendng he edge-factored model rojtie prsin lgoiths for bth leaig nd consider liear classifiers of the fom inference of McDonald et al. (2005b) In partic ithin the datadri en ettin where f(x, y) is a vector of features and w s the tion over all possble depndency graphs for a givn correspondingyweight vector. One wants hw. to g bh pttion io a dge pect haveasmallcexpected loss; the typictlnloss functionnis thereHamming loss,cle(y'; y)n°_  |{hi, jid∈ we sho y0: hi, ji ∈/ y}|. Tractability s usually ensured ing raiing gloally normalized log-linear modht they can be sed in many important earning bystrong factorization assumptions, like the one els, syntactic language modeling, and nsupervied nd inference problem including minrisk decod underlying the arc-factored mode (Esne, 1996; ing training globally normalized log-linear modMcDonald et a., 2005), which forbids any feature els syntactic language modeling and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f(x, y) as: Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. Our approach will build a graph-based parser without the drawback of a restriction to local features. By formulating inference as an ILP, nonlocal features can be easily accommodated in our model; furthermore, by using a relaxation technique we can still make learning tractable. The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009). A linear program (LP) is an optimization problem of the form If the problem is feasible, the optimum is attained at a vertex of the polyhedron that defines the constraint space. If we add the constraint x E Zd, then the above is called an integer linear program (ILP). For some special parameter settings—e.g., when b is an integer vector and A is totally unimodular5—all vertices of the constraining polyhedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003). Of course, this need not happen: solving a general ILP is an NP-complete problem. Despite this fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Since it is intractable to throw in all constraints at once, they propose a cuttingplane algorithm, where the cycle constraints are only invoked when violated by the current solution. The resulting algorithm is still slow, and an arc-factored model is used as a surrogate during training (i.e., the hard constraints are only used at test time), which implies a discrepancy between the model that is optimized and the one that is actually going to be used. Here, we propose ILP formulations that eliminate the need for cycle constraints; in fact, they require only a polynomial number of constraints. Not only does our model allow expert knowledge to be injected in the form of constraints, it is also capable of learning soft versions of those constraints from data; indeed, it can handle features that are not arc-factored (correlating, for example, siblings and grandparents, modeling valency, or preferring nearly projective parses). While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NPhard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. In this paper, we focus on unlabeled dependency parsing, for clarity of exposition. If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size. We start by describing our constraint space. Our formulations rely on a concise polyhedral representation of the set of candidate dependency parse trees, as sketched in §2.1. This will be accomplished by drawing an analogy with a network flow problem. Let D = (V, A) be the complete directed graph S+(v) , {hi, ji ∈ A  |i = v} denote its set of outgoing arcs. The two first conditions can be easily expressed by linear constraints on the incidence vector z: Condition 3 is somewhat harder to express. Rather than adding exponentially many constraints, one for each potential cycle (like Riedel and Clarke, 2006), we equivalently replace condition 3 by 30. B is connected. Note that conditions 1-2-3 are equivalent to 1-230, in the sense that both define the same set Y(x). However, as we will see, the latter set of conditions is more convenient. Connectedness of graphs can be imposed via flow constraints (by requiring that, for any v ∈ V \\\\ {0}, there is a directed path in B connecting 0 to v). We adapt the single commodity flow formulation for the (undirected) minimum spanning tree problem, due to Magnanti and Wolsey (1994), that requires O(n2) variables and constraints. Under this model, the root node must send one unit of flow to every other node. By making use of extra variables, 0i , h0aiaEA, to denote the flow of commodities through each arc, we are led to the following constraints in addition to Eqs. 4–5 (we denote U , [0, 1], and B , {0, 1} = U ∩ Z): These constraints project an outer bound of the arborescence polytope, i.e., Furthermore, the integer points of �Z(x) are precisely the incidence vectors of dependency trees in Y(x); these are obtained by replacing Eq. 9 by za ∈ B, a ∈ A. (11) Given our polyhedral representation of (an outer bound of) the arborescence polytope, we can now formulate dependency parsing with an arcfactored model as an ILP. By storing the arclocal feature vectors into the columns of a matrix F(x) , [fa(x)]aEA, and defining the score vector s , F(x)Tw (each entry is an arc score) the inference problem can be written as where A is a sparse constraint matrix (with O(|A|) non-zero elements), and b is the constraint vector; A and b encode the constraints (4–9). This is an ILP with O(|A|) variables and constraints (hence, quadratic in n); if we drop the integer constraint the problem becomes the LP relaxation. As is, this formulation is no more attractive than solving the problem with the existing combinatorial algorithms discussed in §2.2; however, we can now start adding non-local features to build a more powerful model. To cope with higher-order features of the form fa1,...,aK(x) (i.e., features whose values depend on the simultaneous inclusion of arcs a1, ... , aK on a candidate dependency tree), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables zal...aK , zal ∧...∧zaK. This logical relation can be expressed by the following O(K) agreement constraints:6 As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki  |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki  |hi, ji such features in our formulation, we need to add extra variables zsibl , hzrir∈Rsibl and zgrand , hzrir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator variables, i.e., zsibl Hence, these features can be handled in our formulation by adding the following O(|A |· |V |) variables and constraints: for all triples hi, j, ki ∈ Rgrand. Let R , A ∪ Rsibl ∪ Rgrand; by redefining z , hzrir∈R and F(x) , [fr(x)]r∈R, we may express our inference problem as in Eq. 12, with O(|A |· |V |) variables and constraints. Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features 6Actually, any logical condition can be encoded with linear constraints involving binary variables; see e.g. Clarke and Lapata (2008) for an overview. 7By sibling features we mean features that depend on pairs of sibling arcs (i.e., of the form (i, j) and (i, k)); by grandparent features we mean features that depend on pairs of grandparent arcs (of the form (i, j) and (j, k)). can be incorporated in our model by adding “dynamic” constraints to our ILP. Define: zfirst child , ij 0 otherwise. but this would yield a constraint matrix with O(n4) non-zero elements. Instead, we define auxiliary variables βjk and γij: sibl ijk z γi(j+1)≤ γij +zij analogously for the case n Then, we have that The following constraints encode the logical relations for the auxiliary vari Auxiliary variables and constraints are defined A crucial fact about dependency grammars is that words have preferences about the number and arrangement of arguments an d modifiers they accept. Therefore, it is desirable to include features that indicate, for a candidate arborescence, how many outgoing arcs depart from each vertex; denote these quantities by vi , Pa∈δ+(i) za, for each i ∈ V . We call vi the valency of the ith vertex. We add valency indicators zval ik , ff(vi = k) for i ∈ V and k = 0,... , n − 1. This way, we are able to penalize candidate dependency trees that assign unusual valencies to some of their vertices, by specifying a individual cost for each possible value of valency. The following O(|V |2) constraints encode the agreement between valency indicators and the other variables: For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3) variables and constraints. In this model, every node k =6 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22): where δk j, ff(j = k) is the Kronecker delta. We next define auxiliary variables ψjk that indicate if there is a path from j to k. Since each vertex except the root has only one incoming arc, the following linear equalities are enough to describe these new variables: a , ff(a ∈ y and a is nonprojective). From the definition of projective arcs in §2.1, we have that znp There are other ways to introduce nonprojectivity indicators and alternative definitions of “nonprojective arc.” For example, by using dynamic constraints of the same kind as those in §3.3, we can indicate arcs that “cross” other arcs with O(n3) variables and constraints, and a cubic number of non-zero elements in the constraint matrix (omitted for space). It would be straightforward to adapt the constraints in §3.5 to allow only projective parse trees: simply force znp a = 0 for any a ∈ A. But there are more efficient ways of accomplish this. While it is difficult to impose projectivity constraints or cycle constraints individually, there is a simpler way of imposing both. Consider 3 (or 30) from §3.1. 300. If hi, ji ∈ B, then, for any k = 1, ... , n such that k =6 j, the parent of k must satisfy (defining i0 , min(i, j) and j0 , max(i, j)): Then, Y(x) will be redefined as the set ofprojective dependency parse trees. We omit the proof for space. Conditions 1, 2, and 3&quot; can be encoded with O(n2) constraints. We report experiments on seven languages, six (Danish, Dutch, Portuguese, Slovene, Swedish and Turkish) from the CoNLL-X shared task (Buchholz and Marsi, 2006), and one (English) from the CoNLL-2008 shared task (Surdeanu et al., 2008).8 All experiments are evaluated using the unlabeled attachment score (UAS), using the default settings.9 We used the same arc-factored features as McDonald et al. (2005) (included in the MSTParser toolkit10); for the higher-order models described in §3.3–3.5, we employed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V  |� |A|) constraints and variables, which, when A = V 2, grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8We used the provided train/test splits except for English, for which we tested on the development partition. For training, sentences longer than 80 words were discarded. For testing, all sentences were kept (the longest one has length 118). 11Note that, unlike reranking approaches, there are still exponentially many candidate parse trees after pruning. The oracle constrained to pick parents from these lists achieves > 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observe that, for some languages, the valency features do not seem to help. Merely modeling the number of dependents of a word may not be as valuable as knowing what kinds of dependents they are (for example, distinguishing among arguments and adjuncts). Comparing with the baselines, we observe that our full model outperforms that of McDonald and Pereira (2006), and is in line with the most accurate dependency parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers). Unlike the ensembles, it directly handles non-local output features by optimizing a single global objective. Perhaps more importantly, it makes it possible to exploit expert knowledge through the form of hard global constraints. Although not pursued here, the same kind of constraints employed by Riedel and Clarke (2006) can straightforwardly fit into our model, after extending it to perform labeled dependency parsing. We believe that a careful design of fea13Unlike our model, the hybrid models used here as baselines make use of the dependency labels at training time; indeed, the transition-based parser is trained to predict a labeled dependency parse tree, and the graph-based parser use these predicted labels as input features. Our model ignores this information at training time; therefore, this comparison is slightly unfair to us. model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008). The four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features (see §3.2–§3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according to Dan Bikel’s randomized method (http://www.cis.upenn.edu/-dbikel/software.html). tures and constraints can lead to further improvements on accuracy. We now turn to a different issue: scalability. In previous work (Martins et al., 2009), we showed that training the model via LP-relaxed inference (as we do here) makes it learn to avoid fractional solutions; as a consequence, ILP solvers will converge faster to the optimum (on average). Yet, it is known from worst case complexity theory that solving a general ILP is NP-hard; hence, these solvers may not scale well with the sentence length. Merely considering the LP-relaxed version of the problem at test time is unsatisfactory, as it may lead to a fractional solution (i.e., a solution whose components indexed by arcs, z = (z-)-EA, are not all integer), which does not correspond to a valid dependency tree. We propose the following approximate algorithm to obtain an actual parse: first, solve the LP relaxation (which can be done in polynomial time with interior-point methods); then, if the solution is fractional, project it onto the feasible set Y(x). Fortunately, the Euclidean projection can be computed in a straightforward way by finding a maximal arborescence in the directed graph whose weights are defined by z (we omit the proof for space); as we saw in §2.2, the ChuLiu-Edmonds algorithm can do this in polynomial time. The overall parsing runtime becomes polynomial with respect to the length of the sentence. The last column of Table 1 compares the accuracy of this approximate method with the exact one. We observe that there is not a substantial drop in accuracy; on the other hand, we observed a considerable speed-up with respect to exact inference, particularly for long sentences. The average runtime (across all languages) is 0.632 seconds per sentence, which is in line with existing higher-order parsers and is much faster than the runtimes reported by Riedel and Clarke (2006). We presented new dependency parsers based on concise ILP formulations. We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints. These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints. Learning through a max-margin framework is made effective by the means of a LPrelaxation. Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.\"]\n","['We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model. Additionally, we have shown that although word lattices complicate modeling of word reordering, a simple heuristic offers good performance and enables many standard distortion models to be used directly with lattice input.', 'We presented new dependency parsers based on concise ILP formulations. We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints. These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints. Learning through a max-margin framework is made effective by the means of a LPrelaxation. Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.']\n"]}]},{"cell_type":"code","source":["pd.set_option('max_colwidth', 100)"],"metadata":{"id":"tSmCZ3tKpAuE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_df = pd.DataFrame(list(zip(abstract_list, full_text_list, conclusion_list)), columns=[\"abstract\", \"full_text\", \"conclusion\"])\n","text_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"L4FAb4UEpAwf","executionInfo":{"status":"ok","timestamp":1647329658112,"user_tz":-480,"elapsed":366,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"ed0554c9-173c-4d77-86cc-63d00436723d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                               abstract  \\\n","0   Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1   We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2   We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3   In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4   We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","5   Morphological processes in Semitic languages deliver space-delimited words which introduce multi...   \n","6   Previous studies of data-driven dependency parsing have shown that the distribution of parsing e...   \n","7   This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a...   \n","8   We present a phrasal synchronous grammar model of translational equivalence. Unlike previous app...   \n","9   Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poo...   \n","10  Many phrase alignment models operate over combinatorial space of phrase We prove that finding an...   \n","11  mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg ...   \n","12  Discriminative feature-based methods are widely used in natural language processing, but sentenc...   \n","13  In this paper, we present a discriminative word-character hybrid model for joint Chinese word se...   \n","14  In this paper, we present a reinforcement learning approach for mapping natural language instruc...   \n","15  We describe an unsupervised system for learncoherent sequences or sets events whose arguments ar...   \n","16  A desirable quality of a coreference resolution system is the ability to handle transitivity con...   \n","17  We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that ...   \n","18  In statistical language modeling, one technique to reduce the problematic effects of data sparsi...   \n","19  We present a simple and effective semisupervised method for training dependency parsers. We focu...   \n","20  We address the task of unsupervised POS tagging. We demonstrate that good results can be obtaine...   \n","21  In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic a...   \n","22  This paper provides evidence that the use of more unlabeled data in semi-supervised learning can...   \n","23  The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classifi...   \n","24  Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most curren...   \n","25  used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring...   \n","26  A central problem in grounded language acquisition is learning the correspondences between a ric...   \n","27  We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the d...   \n","28  For centuries, the deep connection between languages has brought about major discoveries about h...   \n","29  We present a method for learning bilingual translation lexicons from monolingual corpora. Word t...   \n","30  Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmenta...   \n","31  We present a series of experiments on auidentifying the sense of imrelations, i.e. relations tha...   \n","32  Parser self-training is the technique of taking an existing parser, parsing extra data and then ...   \n","33  We present a novel approach to weakly supervised semantic class learning from the web, using a s...   \n","34  reranking techniques ofsuffer from the limited scope of the best list, which rules out many pote...   \n","35  In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammati...   \n","36  Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims foun...   \n","37  Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing acc...   \n","38  There are five missing brackets which are indicated as &quot;*[&quot; or &quot;1&quot;. Words wi...   \n","39  Automatic part of speech tagging is an area of natural language processing where statistical tec...   \n","40  We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1%...   \n","41  This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble...   \n","42  We present an unsupervised method for detecting grammatical errors by inferring negative evidenc...   \n","43  We propose a method for identifying diathesis alternations where a particular argument type is s...   \n","44  We present three systems for surface natural language generation that are trainable from annotat...   \n","45  Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown ...   \n","46  This paper describes a method for linear text segmentation which is twice as accurate and over s...   \n","47  and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguist...   \n","48  1993). We that part of speech tagging and word alignment could have an important role in glossar...   \n","49  We present an implementation of a part-of-speech tagger based on a hidden Markov model. The meth...   \n","50  We present a cut and paste based text summarizer, which uses operations derived from an analysis...   \n","51  We present a trainable model for identifying sentence boundaries in raw text. Given a corpus ann...   \n","52  This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-lin...   \n","53  This paper presents a statistical, learned approach to finding names and other nonrecursive enti...   \n","54  trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, ...   \n","55  In this paper we address issues related to building a large-scale Chinese corpus. We try to answ...   \n","56  In order to respond correctly to a free form factual question given a large collection of texts,...   \n","57  This paper presents a deterministic dependency parser based on memory-based learning, which pars...   \n","58  We describe an annotation scheme and a tool developed for creating linguistically annotated corp...   \n","59  An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser...   \n","60  Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extra...   \n","61  of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455...   \n","62  This paper presents an unsupervised method forassembling semantic knowledge from a part-of speec...   \n","63  We consider here the problem of Base Noun Phrase translation. We propose a new method to perform...   \n","64  Broad-coverage lexical resources such as WordNet are extremely useful. However, they often inclu...   \n","65  We describe a practical parser for unrestricted dependencies. The parser creates links between w...   \n","66  In this paper, we consider sentence sim plification as a special form of translation with the co...   \n","67  Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a ...   \n","68  Comparisons of automatic evaluation metrics for machine translation are usually conducted on cor...   \n","69  In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentenc...   \n","70  In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-n...   \n","71  Most work on unsupervised entailment rule acquisition focused on rules between templates with tw...   \n","72  Although vast amounts of textual data are freely available, many NLP algorithms exploit only a m...   \n","73  We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets wit...   \n","74  This work investigates the variation in a word?s dis tributionally nearest neighbours with respe...   \n","75  Automated identification of diverse sen timent types can be beneficial for manyNLP systems such ...   \n","76  We explore unsupervised language model adaptation techniques for Statistical Machine Translation...   \n","77  We present a system for the semantic role la beling task. The system combines a machine learning...   \n","78  This paper shows how to construct semantic representations from the derivations producedby a wid...   \n","79  We present a new HMM tagger that exploits context on both sides of a word to be tagged, and eval...   \n","80  Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, re...   \n","81  In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tw...   \n","82  Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem...   \n","83  In addition to a high accuracy, short parsing and training times are the most important properti...   \n","84  In this paper, we present an approach to the automatic identification and correction ofprepositi...   \n","85  We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from...   \n","86  The unique properties of lree-adjoining grammars (TAG) present a challenge for the application o...   \n","87  Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Ba...   \n","88  Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese se...   \n","89  Categorial unification grammars (CUGs) embody the essential properties of both unification and c...   \n","90  In this paper, we describe a means for automatically building very large neural networks (VLNNs)...   \n","91  Various methods have been proposed for aligning texts in two or more languages such as the Canad...   \n","92  We present an efI\\]cient, broad-coverage, principle-based parser for English. The parser has bee...   \n","93  Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, h...   \n","94  In this paper, we describe a new model for word alignment in statistical trans- lation and prese...   \n","95  We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...   \n","96  I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...   \n","97  computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...   \n","98  If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...   \n","99  We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...   \n","\n","                                                                                              full_text  \\\n","0   Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1   We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2   We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3   In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4   We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","5   Morphological processes in Semitic languages deliver space-delimited words which introduce multi...   \n","6   Previous studies of data-driven dependency parsing have shown that the distribution of parsing e...   \n","7   This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a...   \n","8   We present a phrasal synchronous grammar model of translational equivalence. Unlike previous app...   \n","9   Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poo...   \n","10  Many phrase alignment models operate over combinatorial space of phrase We prove that finding an...   \n","11  mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg ...   \n","12  Discriminative feature-based methods are widely used in natural language processing, but sentenc...   \n","13  In this paper, we present a discriminative word-character hybrid model for joint Chinese word se...   \n","14  In this paper, we present a reinforcement learning approach for mapping natural language instruc...   \n","15  We describe an unsupervised system for learncoherent sequences or sets events whose arguments ar...   \n","16  A desirable quality of a coreference resolution system is the ability to handle transitivity con...   \n","17  We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that ...   \n","18  In statistical language modeling, one technique to reduce the problematic effects of data sparsi...   \n","19  We present a simple and effective semisupervised method for training dependency parsers. We focu...   \n","20  We address the task of unsupervised POS tagging. We demonstrate that good results can be obtaine...   \n","21  In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic a...   \n","22  This paper provides evidence that the use of more unlabeled data in semi-supervised learning can...   \n","23  The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classifi...   \n","24  Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most curren...   \n","25  used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring...   \n","26  A central problem in grounded language acquisition is learning the correspondences between a ric...   \n","27  We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the d...   \n","28  For centuries, the deep connection between languages has brought about major discoveries about h...   \n","29  We present a method for learning bilingual translation lexicons from monolingual corpora. Word t...   \n","30  Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmenta...   \n","31  We present a series of experiments on auidentifying the sense of imrelations, i.e. relations tha...   \n","32  Parser self-training is the technique of taking an existing parser, parsing extra data and then ...   \n","33  We present a novel approach to weakly supervised semantic class learning from the web, using a s...   \n","34  reranking techniques ofsuffer from the limited scope of the best list, which rules out many pote...   \n","35  In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammati...   \n","36  Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims foun...   \n","37  Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing acc...   \n","38  There are five missing brackets which are indicated as &quot;*[&quot; or &quot;1&quot;. Words wi...   \n","39  Automatic part of speech tagging is an area of natural language processing where statistical tec...   \n","40  We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1%...   \n","41  This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble...   \n","42  We present an unsupervised method for detecting grammatical errors by inferring negative evidenc...   \n","43  We propose a method for identifying diathesis alternations where a particular argument type is s...   \n","44  We present three systems for surface natural language generation that are trainable from annotat...   \n","45  Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown ...   \n","46  This paper describes a method for linear text segmentation which is twice as accurate and over s...   \n","47  and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguist...   \n","48  1993). We that part of speech tagging and word alignment could have an important role in glossar...   \n","49  We present an implementation of a part-of-speech tagger based on a hidden Markov model. The meth...   \n","50  We present a cut and paste based text summarizer, which uses operations derived from an analysis...   \n","51  We present a trainable model for identifying sentence boundaries in raw text. Given a corpus ann...   \n","52  This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-lin...   \n","53  This paper presents a statistical, learned approach to finding names and other nonrecursive enti...   \n","54  trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, ...   \n","55  In this paper we address issues related to building a large-scale Chinese corpus. We try to answ...   \n","56  In order to respond correctly to a free form factual question given a large collection of texts,...   \n","57  This paper presents a deterministic dependency parser based on memory-based learning, which pars...   \n","58  We describe an annotation scheme and a tool developed for creating linguistically annotated corp...   \n","59  An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser...   \n","60  Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extra...   \n","61  of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455...   \n","62  This paper presents an unsupervised method forassembling semantic knowledge from a part-of speec...   \n","63  We consider here the problem of Base Noun Phrase translation. We propose a new method to perform...   \n","64  Broad-coverage lexical resources such as WordNet are extremely useful. However, they often inclu...   \n","65  We describe a practical parser for unrestricted dependencies. The parser creates links between w...   \n","66  In this paper, we consider sentence sim plification as a special form of translation with the co...   \n","67  Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a ...   \n","68  Comparisons of automatic evaluation metrics for machine translation are usually conducted on cor...   \n","69  In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentenc...   \n","70  In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-n...   \n","71  Most work on unsupervised entailment rule acquisition focused on rules between templates with tw...   \n","72  Although vast amounts of textual data are freely available, many NLP algorithms exploit only a m...   \n","73  We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets wit...   \n","74  This work investigates the variation in a word?s dis tributionally nearest neighbours with respe...   \n","75  Automated identification of diverse sen timent types can be beneficial for manyNLP systems such ...   \n","76  We explore unsupervised language model adaptation techniques for Statistical Machine Translation...   \n","77  We present a system for the semantic role la beling task. The system combines a machine learning...   \n","78  This paper shows how to construct semantic representations from the derivations producedby a wid...   \n","79  We present a new HMM tagger that exploits context on both sides of a word to be tagged, and eval...   \n","80  Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, re...   \n","81  In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tw...   \n","82  Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem...   \n","83  In addition to a high accuracy, short parsing and training times are the most important properti...   \n","84  In this paper, we present an approach to the automatic identification and correction ofprepositi...   \n","85  We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from...   \n","86  The unique properties of lree-adjoining grammars (TAG) present a challenge for the application o...   \n","87  Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Ba...   \n","88  Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese se...   \n","89  Categorial unification grammars (CUGs) embody the essential properties of both unification and c...   \n","90  In this paper, we describe a means for automatically building very large neural networks (VLNNs)...   \n","91  Various methods have been proposed for aligning texts in two or more languages such as the Canad...   \n","92  We present an efI\\]cient, broad-coverage, principle-based parser for English. The parser has bee...   \n","93  Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, h...   \n","94  In this paper, we describe a new model for word alignment in statistical trans- lation and prese...   \n","95  We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...   \n","96  I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...   \n","97  computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...   \n","98  If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...   \n","99  We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...   \n","\n","                                                                                             conclusion  \n","0   We have achieved substantial gains in translation performance by decoding compact representation...  \n","1   We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...  \n","2   We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sour...  \n","3   In this paper, we propose a novel string-todependency algorithm for statistical machine translat...  \n","4   We have presented a novel transition system for dependency parsing that can handle unrestricted ...  \n","5   The accuracy results for segmentation, tagging and parsing using our different models and our st...  \n","6   Combinations of graph-based and transition-based models for data-driven dependency parsing have ...  \n","7   This paper addresses challenges faced by opinion analysis in the debate genre. In our method, fa...  \n","8   We have presented a Bayesian model of SCFG induction capable of capturing phrasal units of trans...  \n","9   In this paper, we proposed a novel and effective learning scheme for transferring dependency par...  \n","10  Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipar...  \n","11  We conducted Chinese-to-English translation experiments. We trained the translation model on the...  \n","12  We have presented a new, feature-rich, dynamic programming based discriminative parser which is ...  \n","13  In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word s...  \n","14  In this paper, we presented a reinforcement learning approach for inducing a mapping between ins...  \n","15  Our significant improvement in the cloze evaluation shows that even though narrative cloze does ...  \n","16  We showed how to use integer linear programming to encode transitivity constraints in a corefere...  \n","17  The method proposed in this paper is simple— once an integer program is produced, there are solv...  \n","18  We trained a number of predictive class-based language models on different Arabic and English co...  \n","19  In this paper, we have presented a simple but effective semi-supervised learning approach and de...  \n","20  We now apply the same technique to English semisupervised POS tagging. Recent investigations of ...  \n","21  When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new...  \n","22  In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized ...  \n","23  In this paper, we propose to use the co-training approach to address the problem of cross-lingua...  \n","24  We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show h...  \n","25  Up till this point, we have learned narrative relations across all possible events, including th...  \n","26  Two important aspects of our model are the segmentation of the text and the modeling of the coIn...  \n","27  The bulk of the relevant related work is described in earlier sections, as appropriate. This pap...  \n","28  Table 1 shows the performance of the various automatic segmentation methods. The first three row...  \n","29  We have presented a novel generative model for bilingual lexicon induction and presented results...  \n","30  We proposed a joint Chinese word segmentation and POS tagging model, which achieved a considerab...  \n","31  We have presented the first study that predicts implicit discourse relations in a realistic sett...  \n","32  We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts. By doing so w...  \n","33  Combining hyponym patterns with pattern linkage graphs is an effective way to produce a highly a...  \n","34  We compare the performance of our forest reranker against n-best reranking on the Penn English T...  \n","35  From the observations in the previous section, we propose the following guidelines for how to tr...  \n","36  We have shown that a tagger based on Markov models yields state-of-the-art results, despite cont...  \n","37  We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free p...  \n","38  Consider once again the sentence, &quot;I see a bird.&quot; The problem is to find an assignment...  \n","39  The tagger was tested on 5% of the Brown Corpus including sections from every genre. First, the ...  \n","40  In the previous sections we have concentrated on the relation of the parser to a maximumentropy ...  \n","41  This paper shows that word sense disambiguation accuracy can be improved by combining a number o...  \n","42  The unsupervised techniques that we have presented for inferring negative evidence are effective...  \n","43  We have discovered a significant relationship between the similarity of selectional preferences ...  \n","44  This paper presents the first systems (known to the author) that use a statistical learning appr...  \n","45  We present a novel sentence reduction system which removes extraneous phrases from sentences tha...  \n","46  A segmentation algorithm has two key elements, a, clustering strategy and a similarity measure. ...  \n","47  There are, it seems, two reasonable baselines for this and future work. First of all, most const...  \n","48  We have shown that terminology research provides a good application for robust natural language ...  \n","49  We have used the tagger in a number of applications. We describe three applications here: phrase...  \n","50  This paper presents a novel architecture for text summarization using cut and paste techniques o...  \n","51  We have described an approach to identifying sentence boundaries which performs comparably to ot...  \n","52  The previous section showed how to combine the supertagger and parser for the purpose of creatin...  \n","53  While our initial results have been quite favorable, there is still much that can be done potent...  \n","54  An evaluation of an earlier version of Nominator, was performed on 88 Wall Street Journal docume...  \n","55  In this paper we address issues related to building a large-scale Chinese corpus. We try to answ...  \n","56  We designed two experiments to test the accuracy ofour classifier on TREC questions. The first e...  \n","57  n onto the stack, giving the configuration ?n|S,I,A?. 2We use nil to denote the empty list and a...  \n","58  As the annotation scheme described in this paper focusses on annotating argument structure rathe...  \n","59  Large context-free grammars extracted from tree banks achieve high coverage and accuracy, but th...  \n","60  The above methods can also be applied to othertasks in natural language processing such as chunk...  \n","61  Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiti...  \n","62  In this section we give examples of lexical cat egories extracted by our method and evaluatethem...  \n","63  We conducted experiments on translation of the Base NPs from English to Chinese. We extracted Ba...  \n","64  We generated clusters from a news corpus using CBC and compared them with classes extracted from...  \n","65  In this paper, we have presented some main features of our new framework for dependency syntax. ...  \n","66  Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from ...  \n","67  The first experiment examines the two word sentiment classifier models and the second the three ...  \n","68  Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward. To...  \n","69  In this section we present our experimental set up for assessing the performance of our model. W...  \n","70  One aspect of automatic error detection that usu ally is under-reported is an analysis of the er...  \n","71  We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT al...  \n","72  In this section, we empirically compare the pattern-based and co-occurrence-based models pre sen...  \n","73  Our tagger combines two ideas, the decompositionof the probability of complex POS tags into a pr...  \n","74  In its most general sense, a collocation is a habitual or lexicalised word combination. How ever...  \n","75  The purpose of our evaluation was to learn how well our framework can identify and distinguishbe...  \n","76  Experiments are carried out on a standard statistical machine translation task defined in the NI...  \n","77  We show that linguistic information is useful for se mantic role labeling, both in extracting fe...  \n","78  This paper shows how to construct semantic representations from the derivations producedby a wid...  \n","79  We have presented a comprehensive evaluation of several methods for unsupervised part-of-speech ...  \n","80  The main limitation of PairClass is the need for a large corpus. Phrases that contain a pair of ...  \n","81  There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang...  \n","82  To make a comprehensive evaluation, we use allfour of the datasets from a recent Chinese word se...  \n","83  ThresholdFor non-projective parsing, we use the NonProjective Approximation Algorithm of McDon a...  \n","84  In developing this model, our first aim was not to create something which learns like a human, b...  \n","85  To explore some of the differences between the training sets, we hand-examined a random sample o...  \n","86  The synchronous TAG formalism is inherently nondirec- tional. Derivation is not defined in terms...  \n","87  Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Ba...  \n","88  Chinese sentences arc cx)mposed with string of characters without blanks to mark words. However ...  \n","89  Both terms, unification grammar and categorial grammar are used for classes of grammar formalism...  \n","90  Our approach to WSD takes advantage of both strategies outlined above, but enables us to address...  \n","91  There have been quite a number of recent papers on parallel text: Brown et al(1990, 1991, 1993),...  \n","92  Principle-based grammars, such as Govern- ment-Binding (GB) theory (Chomsky, 1981; Haegeman, 199...  \n","93  Our evaluation was motivated by the following ques tions: (1) How does the incompleteness of Fra...  \n","94  The goal is the translation of a text given in some language F into a target language E. For con...  \n","95  Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate ...  \n","96  Prel)ositioual phrase attachment disambiguation is a difficult problem. Take, for example, the s...  \n","97  We have presented an application of topic modeling to the problem of automatically computing sel...  \n","98  Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic ...  \n","99  We have presented new parsing algorithms that are capable of efficiently parsing third-order fac...  "],"text/html":["\n","  <div id=\"df-da112c94-78d1-4b7a-8449-373f37c2be1b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>full_text</th>\n","      <th>conclusion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>We have achieved substantial gains in translation performance by decoding compact representation...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We proposed a cascaded linear model for Chinese Joint S&amp;T. Under this model, many knowledge sour...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We have presented a novel transition system for dependency parsing that can handle unrestricted ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Morphological processes in Semitic languages deliver space-delimited words which introduce multi...</td>\n","      <td>Morphological processes in Semitic languages deliver space-delimited words which introduce multi...</td>\n","      <td>The accuracy results for segmentation, tagging and parsing using our different models and our st...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Previous studies of data-driven dependency parsing have shown that the distribution of parsing e...</td>\n","      <td>Previous studies of data-driven dependency parsing have shown that the distribution of parsing e...</td>\n","      <td>Combinations of graph-based and transition-based models for data-driven dependency parsing have ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a...</td>\n","      <td>This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a...</td>\n","      <td>This paper addresses challenges faced by opinion analysis in the debate genre. In our method, fa...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>We present a phrasal synchronous grammar model of translational equivalence. Unlike previous app...</td>\n","      <td>We present a phrasal synchronous grammar model of translational equivalence. Unlike previous app...</td>\n","      <td>We have presented a Bayesian model of SCFG induction capable of capturing phrasal units of trans...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poo...</td>\n","      <td>Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poo...</td>\n","      <td>In this paper, we proposed a novel and effective learning scheme for transferring dependency par...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an...</td>\n","      <td>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an...</td>\n","      <td>Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipar...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg ...</td>\n","      <td>mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg ...</td>\n","      <td>We conducted Chinese-to-English translation experiments. We trained the translation model on the...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Discriminative feature-based methods are widely used in natural language processing, but sentenc...</td>\n","      <td>Discriminative feature-based methods are widely used in natural language processing, but sentenc...</td>\n","      <td>We have presented a new, feature-rich, dynamic programming based discriminative parser which is ...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>In this paper, we present a discriminative word-character hybrid model for joint Chinese word se...</td>\n","      <td>In this paper, we present a discriminative word-character hybrid model for joint Chinese word se...</td>\n","      <td>In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word s...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>In this paper, we present a reinforcement learning approach for mapping natural language instruc...</td>\n","      <td>In this paper, we present a reinforcement learning approach for mapping natural language instruc...</td>\n","      <td>In this paper, we presented a reinforcement learning approach for inducing a mapping between ins...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>We describe an unsupervised system for learncoherent sequences or sets events whose arguments ar...</td>\n","      <td>We describe an unsupervised system for learncoherent sequences or sets events whose arguments ar...</td>\n","      <td>Our significant improvement in the cloze evaluation shows that even though narrative cloze does ...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>A desirable quality of a coreference resolution system is the ability to handle transitivity con...</td>\n","      <td>A desirable quality of a coreference resolution system is the ability to handle transitivity con...</td>\n","      <td>We showed how to use integer linear programming to encode transitivity constraints in a corefere...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that ...</td>\n","      <td>We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that ...</td>\n","      <td>The method proposed in this paper is simple— once an integer program is produced, there are solv...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>In statistical language modeling, one technique to reduce the problematic effects of data sparsi...</td>\n","      <td>In statistical language modeling, one technique to reduce the problematic effects of data sparsi...</td>\n","      <td>We trained a number of predictive class-based language models on different Arabic and English co...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>We present a simple and effective semisupervised method for training dependency parsers. We focu...</td>\n","      <td>We present a simple and effective semisupervised method for training dependency parsers. We focu...</td>\n","      <td>In this paper, we have presented a simple but effective semi-supervised learning approach and de...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>We address the task of unsupervised POS tagging. We demonstrate that good results can be obtaine...</td>\n","      <td>We address the task of unsupervised POS tagging. We demonstrate that good results can be obtaine...</td>\n","      <td>We now apply the same technique to English semisupervised POS tagging. Recent investigations of ...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic a...</td>\n","      <td>In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic a...</td>\n","      <td>When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>This paper provides evidence that the use of more unlabeled data in semi-supervised learning can...</td>\n","      <td>This paper provides evidence that the use of more unlabeled data in semi-supervised learning can...</td>\n","      <td>In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized ...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classifi...</td>\n","      <td>The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classifi...</td>\n","      <td>In this paper, we propose to use the co-training approach to address the problem of cross-lingua...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most curren...</td>\n","      <td>Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most curren...</td>\n","      <td>We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show h...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring...</td>\n","      <td>used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring...</td>\n","      <td>Up till this point, we have learned narrative relations across all possible events, including th...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>A central problem in grounded language acquisition is learning the correspondences between a ric...</td>\n","      <td>A central problem in grounded language acquisition is learning the correspondences between a ric...</td>\n","      <td>Two important aspects of our model are the segmentation of the text and the modeling of the coIn...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the d...</td>\n","      <td>We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the d...</td>\n","      <td>The bulk of the relevant related work is described in earlier sections, as appropriate. This pap...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>For centuries, the deep connection between languages has brought about major discoveries about h...</td>\n","      <td>For centuries, the deep connection between languages has brought about major discoveries about h...</td>\n","      <td>Table 1 shows the performance of the various automatic segmentation methods. The first three row...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>We present a method for learning bilingual translation lexicons from monolingual corpora. Word t...</td>\n","      <td>We present a method for learning bilingual translation lexicons from monolingual corpora. Word t...</td>\n","      <td>We have presented a novel generative model for bilingual lexicon induction and presented results...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmenta...</td>\n","      <td>Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmenta...</td>\n","      <td>We proposed a joint Chinese word segmentation and POS tagging model, which achieved a considerab...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>We present a series of experiments on auidentifying the sense of imrelations, i.e. relations tha...</td>\n","      <td>We present a series of experiments on auidentifying the sense of imrelations, i.e. relations tha...</td>\n","      <td>We have presented the first study that predicts implicit discourse relations in a realistic sett...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>Parser self-training is the technique of taking an existing parser, parsing extra data and then ...</td>\n","      <td>Parser self-training is the technique of taking an existing parser, parsing extra data and then ...</td>\n","      <td>We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts. By doing so w...</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>We present a novel approach to weakly supervised semantic class learning from the web, using a s...</td>\n","      <td>We present a novel approach to weakly supervised semantic class learning from the web, using a s...</td>\n","      <td>Combining hyponym patterns with pattern linkage graphs is an effective way to produce a highly a...</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>reranking techniques ofsuffer from the limited scope of the best list, which rules out many pote...</td>\n","      <td>reranking techniques ofsuffer from the limited scope of the best list, which rules out many pote...</td>\n","      <td>We compare the performance of our forest reranker against n-best reranking on the Penn English T...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammati...</td>\n","      <td>In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammati...</td>\n","      <td>From the observations in the previous section, we propose the following guidelines for how to tr...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims foun...</td>\n","      <td>Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims foun...</td>\n","      <td>We have shown that a tagger based on Markov models yields state-of-the-art results, despite cont...</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing acc...</td>\n","      <td>Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing acc...</td>\n","      <td>We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free p...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>There are five missing brackets which are indicated as &amp;quot;*[&amp;quot; or &amp;quot;1&amp;quot;. Words wi...</td>\n","      <td>There are five missing brackets which are indicated as &amp;quot;*[&amp;quot; or &amp;quot;1&amp;quot;. Words wi...</td>\n","      <td>Consider once again the sentence, &amp;quot;I see a bird.&amp;quot; The problem is to find an assignment...</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>Automatic part of speech tagging is an area of natural language processing where statistical tec...</td>\n","      <td>Automatic part of speech tagging is an area of natural language processing where statistical tec...</td>\n","      <td>The tagger was tested on 5% of the Brown Corpus including sections from every genre. First, the ...</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1%...</td>\n","      <td>We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1%...</td>\n","      <td>In the previous sections we have concentrated on the relation of the parser to a maximumentropy ...</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble...</td>\n","      <td>This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble...</td>\n","      <td>This paper shows that word sense disambiguation accuracy can be improved by combining a number o...</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>We present an unsupervised method for detecting grammatical errors by inferring negative evidenc...</td>\n","      <td>We present an unsupervised method for detecting grammatical errors by inferring negative evidenc...</td>\n","      <td>The unsupervised techniques that we have presented for inferring negative evidence are effective...</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>We propose a method for identifying diathesis alternations where a particular argument type is s...</td>\n","      <td>We propose a method for identifying diathesis alternations where a particular argument type is s...</td>\n","      <td>We have discovered a significant relationship between the similarity of selectional preferences ...</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>We present three systems for surface natural language generation that are trainable from annotat...</td>\n","      <td>We present three systems for surface natural language generation that are trainable from annotat...</td>\n","      <td>This paper presents the first systems (known to the author) that use a statistical learning appr...</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown ...</td>\n","      <td>Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown ...</td>\n","      <td>We present a novel sentence reduction system which removes extraneous phrases from sentences tha...</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>This paper describes a method for linear text segmentation which is twice as accurate and over s...</td>\n","      <td>This paper describes a method for linear text segmentation which is twice as accurate and over s...</td>\n","      <td>A segmentation algorithm has two key elements, a, clustering strategy and a similarity measure. ...</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguist...</td>\n","      <td>and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguist...</td>\n","      <td>There are, it seems, two reasonable baselines for this and future work. First of all, most const...</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>1993). We that part of speech tagging and word alignment could have an important role in glossar...</td>\n","      <td>1993). We that part of speech tagging and word alignment could have an important role in glossar...</td>\n","      <td>We have shown that terminology research provides a good application for robust natural language ...</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>We present an implementation of a part-of-speech tagger based on a hidden Markov model. The meth...</td>\n","      <td>We present an implementation of a part-of-speech tagger based on a hidden Markov model. The meth...</td>\n","      <td>We have used the tagger in a number of applications. We describe three applications here: phrase...</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>We present a cut and paste based text summarizer, which uses operations derived from an analysis...</td>\n","      <td>We present a cut and paste based text summarizer, which uses operations derived from an analysis...</td>\n","      <td>This paper presents a novel architecture for text summarization using cut and paste techniques o...</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>We present a trainable model for identifying sentence boundaries in raw text. Given a corpus ann...</td>\n","      <td>We present a trainable model for identifying sentence boundaries in raw text. Given a corpus ann...</td>\n","      <td>We have described an approach to identifying sentence boundaries which performs comparably to ot...</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-lin...</td>\n","      <td>This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-lin...</td>\n","      <td>The previous section showed how to combine the supertagger and parser for the purpose of creatin...</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>This paper presents a statistical, learned approach to finding names and other nonrecursive enti...</td>\n","      <td>This paper presents a statistical, learned approach to finding names and other nonrecursive enti...</td>\n","      <td>While our initial results have been quite favorable, there is still much that can be done potent...</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, ...</td>\n","      <td>trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, ...</td>\n","      <td>An evaluation of an earlier version of Nominator, was performed on 88 Wall Street Journal docume...</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>In this paper we address issues related to building a large-scale Chinese corpus. We try to answ...</td>\n","      <td>In this paper we address issues related to building a large-scale Chinese corpus. We try to answ...</td>\n","      <td>In this paper we address issues related to building a large-scale Chinese corpus. We try to answ...</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>In order to respond correctly to a free form factual question given a large collection of texts,...</td>\n","      <td>In order to respond correctly to a free form factual question given a large collection of texts,...</td>\n","      <td>We designed two experiments to test the accuracy ofour classifier on TREC questions. The first e...</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>This paper presents a deterministic dependency parser based on memory-based learning, which pars...</td>\n","      <td>This paper presents a deterministic dependency parser based on memory-based learning, which pars...</td>\n","      <td>n onto the stack, giving the configuration ?n|S,I,A?. 2We use nil to denote the empty list and a...</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>We describe an annotation scheme and a tool developed for creating linguistically annotated corp...</td>\n","      <td>We describe an annotation scheme and a tool developed for creating linguistically annotated corp...</td>\n","      <td>As the annotation scheme described in this paper focusses on annotating argument structure rathe...</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser...</td>\n","      <td>An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser...</td>\n","      <td>Large context-free grammars extracted from tree banks achieve high coverage and accuracy, but th...</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extra...</td>\n","      <td>Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extra...</td>\n","      <td>The above methods can also be applied to othertasks in natural language processing such as chunk...</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455...</td>\n","      <td>of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455...</td>\n","      <td>Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiti...</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>This paper presents an unsupervised method forassembling semantic knowledge from a part-of speec...</td>\n","      <td>This paper presents an unsupervised method forassembling semantic knowledge from a part-of speec...</td>\n","      <td>In this section we give examples of lexical cat egories extracted by our method and evaluatethem...</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>We consider here the problem of Base Noun Phrase translation. We propose a new method to perform...</td>\n","      <td>We consider here the problem of Base Noun Phrase translation. We propose a new method to perform...</td>\n","      <td>We conducted experiments on translation of the Base NPs from English to Chinese. We extracted Ba...</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>Broad-coverage lexical resources such as WordNet are extremely useful. However, they often inclu...</td>\n","      <td>Broad-coverage lexical resources such as WordNet are extremely useful. However, they often inclu...</td>\n","      <td>We generated clusters from a news corpus using CBC and compared them with classes extracted from...</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>We describe a practical parser for unrestricted dependencies. The parser creates links between w...</td>\n","      <td>We describe a practical parser for unrestricted dependencies. The parser creates links between w...</td>\n","      <td>In this paper, we have presented some main features of our new framework for dependency syntax. ...</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>In this paper, we consider sentence sim plification as a special form of translation with the co...</td>\n","      <td>In this paper, we consider sentence sim plification as a special form of translation with the co...</td>\n","      <td>Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from ...</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a ...</td>\n","      <td>Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a ...</td>\n","      <td>The first experiment examines the two word sentiment classifier models and the second the three ...</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>Comparisons of automatic evaluation metrics for machine translation are usually conducted on cor...</td>\n","      <td>Comparisons of automatic evaluation metrics for machine translation are usually conducted on cor...</td>\n","      <td>Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward. To...</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentenc...</td>\n","      <td>In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentenc...</td>\n","      <td>In this section we present our experimental set up for assessing the performance of our model. W...</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-n...</td>\n","      <td>In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-n...</td>\n","      <td>One aspect of automatic error detection that usu ally is under-reported is an analysis of the er...</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>Most work on unsupervised entailment rule acquisition focused on rules between templates with tw...</td>\n","      <td>Most work on unsupervised entailment rule acquisition focused on rules between templates with tw...</td>\n","      <td>We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT al...</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>Although vast amounts of textual data are freely available, many NLP algorithms exploit only a m...</td>\n","      <td>Although vast amounts of textual data are freely available, many NLP algorithms exploit only a m...</td>\n","      <td>In this section, we empirically compare the pattern-based and co-occurrence-based models pre sen...</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets wit...</td>\n","      <td>We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets wit...</td>\n","      <td>Our tagger combines two ideas, the decompositionof the probability of complex POS tags into a pr...</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>This work investigates the variation in a word?s dis tributionally nearest neighbours with respe...</td>\n","      <td>This work investigates the variation in a word?s dis tributionally nearest neighbours with respe...</td>\n","      <td>In its most general sense, a collocation is a habitual or lexicalised word combination. How ever...</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>Automated identification of diverse sen timent types can be beneficial for manyNLP systems such ...</td>\n","      <td>Automated identification of diverse sen timent types can be beneficial for manyNLP systems such ...</td>\n","      <td>The purpose of our evaluation was to learn how well our framework can identify and distinguishbe...</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>We explore unsupervised language model adaptation techniques for Statistical Machine Translation...</td>\n","      <td>We explore unsupervised language model adaptation techniques for Statistical Machine Translation...</td>\n","      <td>Experiments are carried out on a standard statistical machine translation task defined in the NI...</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>We present a system for the semantic role la beling task. The system combines a machine learning...</td>\n","      <td>We present a system for the semantic role la beling task. The system combines a machine learning...</td>\n","      <td>We show that linguistic information is useful for se mantic role labeling, both in extracting fe...</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>This paper shows how to construct semantic representations from the derivations producedby a wid...</td>\n","      <td>This paper shows how to construct semantic representations from the derivations producedby a wid...</td>\n","      <td>This paper shows how to construct semantic representations from the derivations producedby a wid...</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>We present a new HMM tagger that exploits context on both sides of a word to be tagged, and eval...</td>\n","      <td>We present a new HMM tagger that exploits context on both sides of a word to be tagged, and eval...</td>\n","      <td>We have presented a comprehensive evaluation of several methods for unsupervised part-of-speech ...</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, re...</td>\n","      <td>Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, re...</td>\n","      <td>The main limitation of PairClass is the need for a large corpus. Phrases that contain a pair of ...</td>\n","    </tr>\n","    <tr>\n","      <th>81</th>\n","      <td>In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tw...</td>\n","      <td>In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tw...</td>\n","      <td>There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang...</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem...</td>\n","      <td>Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem...</td>\n","      <td>To make a comprehensive evaluation, we use allfour of the datasets from a recent Chinese word se...</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>In addition to a high accuracy, short parsing and training times are the most important properti...</td>\n","      <td>In addition to a high accuracy, short parsing and training times are the most important properti...</td>\n","      <td>ThresholdFor non-projective parsing, we use the NonProjective Approximation Algorithm of McDon a...</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>In this paper, we present an approach to the automatic identification and correction ofprepositi...</td>\n","      <td>In this paper, we present an approach to the automatic identification and correction ofprepositi...</td>\n","      <td>In developing this model, our first aim was not to create something which learns like a human, b...</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from...</td>\n","      <td>We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from...</td>\n","      <td>To explore some of the differences between the training sets, we hand-examined a random sample o...</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>The unique properties of lree-adjoining grammars (TAG) present a challenge for the application o...</td>\n","      <td>The unique properties of lree-adjoining grammars (TAG) present a challenge for the application o...</td>\n","      <td>The synchronous TAG formalism is inherently nondirec- tional. Derivation is not defined in terms...</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Ba...</td>\n","      <td>Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Ba...</td>\n","      <td>Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Ba...</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese se...</td>\n","      <td>Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese se...</td>\n","      <td>Chinese sentences arc cx)mposed with string of characters without blanks to mark words. However ...</td>\n","    </tr>\n","    <tr>\n","      <th>89</th>\n","      <td>Categorial unification grammars (CUGs) embody the essential properties of both unification and c...</td>\n","      <td>Categorial unification grammars (CUGs) embody the essential properties of both unification and c...</td>\n","      <td>Both terms, unification grammar and categorial grammar are used for classes of grammar formalism...</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>In this paper, we describe a means for automatically building very large neural networks (VLNNs)...</td>\n","      <td>In this paper, we describe a means for automatically building very large neural networks (VLNNs)...</td>\n","      <td>Our approach to WSD takes advantage of both strategies outlined above, but enables us to address...</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>Various methods have been proposed for aligning texts in two or more languages such as the Canad...</td>\n","      <td>Various methods have been proposed for aligning texts in two or more languages such as the Canad...</td>\n","      <td>There have been quite a number of recent papers on parallel text: Brown et al(1990, 1991, 1993),...</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>We present an efI\\]cient, broad-coverage, principle-based parser for English. The parser has bee...</td>\n","      <td>We present an efI\\]cient, broad-coverage, principle-based parser for English. The parser has bee...</td>\n","      <td>Principle-based grammars, such as Govern- ment-Binding (GB) theory (Chomsky, 1981; Haegeman, 199...</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, h...</td>\n","      <td>Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, h...</td>\n","      <td>Our evaluation was motivated by the following ques tions: (1) How does the incompleteness of Fra...</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>In this paper, we describe a new model for word alignment in statistical trans- lation and prese...</td>\n","      <td>In this paper, we describe a new model for word alignment in statistical trans- lation and prese...</td>\n","      <td>The goal is the translation of a text given in some language F into a target language E. For con...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...</td>\n","      <td>We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...</td>\n","      <td>Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate ...</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...</td>\n","      <td>I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...</td>\n","      <td>Prel)ositioual phrase attachment disambiguation is a difficult problem. Take, for example, the s...</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...</td>\n","      <td>computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...</td>\n","      <td>We have presented an application of topic modeling to the problem of automatically computing sel...</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...</td>\n","      <td>If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...</td>\n","      <td>Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic ...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...</td>\n","      <td>We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...</td>\n","      <td>We have presented new parsing algorithms that are capable of efficiently parsing third-order fac...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da112c94-78d1-4b7a-8449-373f37c2be1b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-da112c94-78d1-4b7a-8449-373f37c2be1b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-da112c94-78d1-4b7a-8449-373f37c2be1b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjUebwUhV_j2","executionInfo":{"status":"ok","timestamp":1647327502941,"user_tz":-480,"elapsed":383,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"ca15fdc0-56f2-44be-bf39-29d3be95c470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Capstone Project - {Gerson Cruz}.ipynb'  'Text Summarization Image.png'\n"," __MACOSX\t\t\t\t   top100.csv\n"," scisummnet_release1.1__20190413\t  'XML Structure.png'\n"," scisummnet_release1.1__20190413.zip\n"]}]},{"cell_type":"code","source":["text_df.to_csv(\"top100.csv\")"],"metadata":{"id":"8kh1XCr4WBkE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Cleaning and Preprocessing"],"metadata":{"id":"ndBDVkvRv6-s"}},{"cell_type":"code","source":["# Individual cleaning functions\n","def remove_web_links(text):\n","  text = re.sub(r'http://www.\\w+.org/','', text)\n","  text = re.sub(r'http://www.\\w+.org/','', text)\n","  text = re.sub(r'http://www.([\\w\\S]+).org/\\w+\\W\\w+','',text)\n","  text = re.sub(r'https://www.\\w+.org/','', text)\n","  text = re.sub(r'https://www.([\\w\\S]+).org/\\w+\\W\\w+','',text)\n","  text = re.sub(r'https://\\w+.\\w+/\\d+.\\d+/\\w\\d+\\W\\w+','',text)\n","  text = re.sub(r'https://\\w+.\\w+/\\d+.\\d+/\\w\\d+\\W\\w+','',text)\n","  text = re.sub(r'Figure\\s\\d:','', text)\n","  text = re.sub(r'\\Wwww.\\w+\\W\\w+\\W','',text)\n","  text = re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n","  text = re.sub(r'www.\\w+','',text)\n","\n","  return text\n","\n","def remove_emojis(text):\n","  regrex_pattern = re.compile(pattern = \"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002500-\\U00002BEF\"  # chinese char\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\" \n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u200d\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\ufe0f\"  # dingbats\n","        u\"\\u3030\"  # flags (iOS)\n","                           \"]+\", flags = re.UNICODE)\n","  text = regrex_pattern.sub('', text)\n","\n","  return text\n","\n","def remove_spaces(text):\n","  text = re.sub(r'\\n',\"\",text)\n","\n","  return text\n","\n","def remove_stopwords(text):\n","  stop_words=set(stopwords.words('english'))\n","  words=word_tokenize(text)\n","  sentence=[w for w in words if w not in stop_words]\n","  return \" \".join(sentence)\n","\n","def lemmatize_text(text):\n","  wordlist=[]\n","  lemmatizer = WordNetLemmatizer()\n","  sentences=sent_tokenize(text)\n","  for sentence in sentences:\n","      words=word_tokenize(sentence)\n","      for word in words:\n","          wordlist.append(lemmatizer.lemmatize(word))\n","  return ' '.join(wordlist)\n","\n","def lowercase_text(text):\n","  return text.lower()\n","\n","def remove_punctuations(text):\n","  additional_punctuations = ['’', '…'] # punctuations not in string.punctuation  \n","  for punctuation in string.punctuation:\n","    text = text.replace(punctuation, '')\n","  \n","  for punctuation in additional_punctuations:\n","    text = text.replace(punctuation, '')\n","    \n","  return text\n","\n","def remove_numbers(text):\n","  if text is not None:\n","    text = text.replace(r'^\\d+\\.\\s+','')\n","  \n","  text = re.sub(\"[0-9]\", '', text)\n","  return text\n","\n","# Unified boolean controlled cleaning function \n","def clean_and_preprocess_data(text, lowercase=True, clean_stopwords=True, clean_punctuations=True, clean_links=True, \n","                              clean_emojis=True, clean_spaces=True, clean_numbers=True,  lemmatize=True):\n","  \n","  if clean_stopwords == True:\n","    text = remove_stopwords(text)\n","\n","  if clean_punctuations == True:\n","    text = remove_punctuations(text)\n","  \n","  if clean_links == True:\n","    text = remove_web_links(text)\n","  \n","  if clean_emojis == True:\n","    text = remove_emojis(text)\n","  \n","  if clean_spaces == True:\n","    text = remove_spaces(text)\n","  \n","  if clean_numbers == True:\n","    text = remove_numbers(text)\n","  \n","  if lemmatize == True:\n","    text = lemmatize_text(text)\n","  \n","  if lowercase == True:\n","    return text.lower()\n","\n","  return text"],"metadata":{"id":"5BnHm7KbVu2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","text_df['abstract'] = text_df['abstract'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n","text_df['full_text'] = text_df['full_text'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n","text_df['conclusion'] = text_df['conclusion'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PAsND-fhWICx","executionInfo":{"status":"ok","timestamp":1647568493388,"user_tz":-480,"elapsed":359,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"ceebc362-380f-4240-dc40-1c8e2b65acb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 185 ms, sys: 3.89 ms, total: 189 ms\n","Wall time: 189 ms\n"]}]},{"cell_type":"code","source":["text_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"2aNMHxxPWbw6","executionInfo":{"status":"ok","timestamp":1647568496367,"user_tz":-480,"elapsed":319,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"87be9646-9e04-43be-cc1c-f2ce9fcad5e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                              abstract  \\\n","0  Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1  We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2  We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3  In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4  We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","\n","                                                                                             full_text  \\\n","0  Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1  We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2  We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3  In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4  We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","\n","                                                                                            conclusion  \n","0  We have achieved substantial gains in translation performance by decoding compact representation...  \n","1  We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...  \n","2  We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sour...  \n","3  In this paper, we propose a novel string-todependency algorithm for statistical machine translat...  \n","4  We have presented a novel transition system for dependency parsing that can handle unrestricted ...  "],"text/html":["\n","  <div id=\"df-926a0508-0591-4cb5-a2fb-87eada0561db\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>full_text</th>\n","      <th>conclusion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>We have achieved substantial gains in translation performance by decoding compact representation...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We proposed a cascaded linear model for Chinese Joint S&amp;T. Under this model, many knowledge sour...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We have presented a novel transition system for dependency parsing that can handle unrestricted ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-926a0508-0591-4cb5-a2fb-87eada0561db')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-926a0508-0591-4cb5-a2fb-87eada0561db button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-926a0508-0591-4cb5-a2fb-87eada0561db');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["text_df.to_csv(\"top100_cleaned.csv\")"],"metadata":{"id":"k_BCOPdpX3EA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modelling: Data Explorations, Feature Extraction, Extractive Summarization, and Abstractive Summarization <a name=\"s3\"></a>\n","\n","Before moving on to our summarization, I'll first explore some important data characteristics like the average word length and token length per text.\n","\n","See this reference [article](https://towardsdatascience.com/beginners-guide-for-data-cleaning-and-feature-extraction-in-nlp-756f311d8083) for a basic introduction to NLP feature extraction.\n","\n","In order to perform the feature extraction, I created a `class TextSummarizer` which serves both as the feature extractor and the text summarizer. The class has the following functions and their corresponding capabilities: \n","* `avg_word`: Function for getting the average length of a word in a text\n","* `count_punctuation`: Function for getting the number of punctuations in a text\n","* `get_optimal_number_sentences`: Function for getting the optimal number of sentences for extractive summarization\n","* `extract_text_features`: Function which returns the following features number of stopwords, punctuations, numerical characters, words, average word length and stopwords to word ratio. \n","* `extractive_summarizer`: Function for performing extractive text summarization\n","* `join_extracted_summary`: Function for concatenating the abstract, extractive text summary, and conclusion\n","* `abstractive_summarizer`: Function for performing abstractive summarization with the text. \n","\n","Using this class, I perform all the necessary explorations, and then directly proceed to performing extractive text summarization and abstractive text summarization with BERT and BART respectively. \n","\n","For extractive text summarization, I used the following references:\n","* [bert-extractive-summarizer](https://github.com/dmmiller612/bert-extractive-summarizer)\n","* [Handling coreference resolution with Python](https://kaveeshabaddage.medium.com/how-to-resolve-coreference-resolution-using-python-97fcd6b2cedb) \n","* [sciBERT](https://github.com/allenai/scibert)\n","\n","*Note: Due to the lack of computational resources, coreference handling could not be applied as the memory provided by Google Collab is not enough and leads to a session crash.*\n","\n","For abstractive text summarization, I used the following reference:\n","* [facebook-bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)"],"metadata":{"id":"zJq4yPUAYqgm"}},{"cell_type":"code","source":["pd.set_option('max_colwidth', 100)\n","text_df = pd.read_csv(\"top100_cleaned.csv\")\n","text_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n","text_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"eEpjPd0qZNtM","executionInfo":{"status":"ok","timestamp":1647780149465,"user_tz":-480,"elapsed":635,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"9f826855-8bd0-4de7-d4ff-8a86777fffcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                              abstract  \\\n","0  Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1  We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2  We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3  In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4  We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","\n","                                                                                             full_text  \\\n","0  Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1  We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2  We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3  In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4  We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","\n","                                                                                            conclusion  \n","0  We have achieved substantial gains in translation performance by decoding compact representation...  \n","1  We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...  \n","2  We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sour...  \n","3  In this paper, we propose a novel string-todependency algorithm for statistical machine translat...  \n","4  We have presented a novel transition system for dependency parsing that can handle unrestricted ...  "],"text/html":["\n","  <div id=\"df-3450ab87-5b23-4a70-9d1a-464297ca51f5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>full_text</th>\n","      <th>conclusion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>We have achieved substantial gains in translation performance by decoding compact representation...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We proposed a cascaded linear model for Chinese Joint S&amp;T. Under this model, many knowledge sour...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We have presented a novel transition system for dependency parsing that can handle unrestricted ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3450ab87-5b23-4a70-9d1a-464297ca51f5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3450ab87-5b23-4a70-9d1a-464297ca51f5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3450ab87-5b23-4a70-9d1a-464297ca51f5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["class TextSummarizer:\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Helper functions\n","\n","  # Get average word length in a document\n","  def avg_word(self, data):\n","    words = data.split()\n","    length = (sum(len(word) for word in words)/(len(words)+0.000001))\n","\n","    return length\n","  \n","  # Get number of punctuations in a document\n","  def count_punctuation(self, data):\n","    punctuation_count = sum([1 for char in data if char in string.punctuation])\n","\n","    return punctuation_count\n","  \n","  # Get optimal number of sentences for extractive summarization\n","  def get_optimal_number_sentences(self, data, model):\n","\n","    optimal_num_sentences = model.calculate_optimal_k(data, k_max=10)\n","\n","    return optimal_num_sentences\n","  \n","  # Extract numerical text features\n","  def extract_text_features(self, text_column):\n","    \n","    \"\"\"\n","    Extracts text features such as number of stopwords, punctuations,\n","    numerical characters, average word length, average document length\n","    :param text_column: dataframe column to perform feature extraction on\n","    :return: dataframe with new feature columns\n","    \"\"\"\n","    \n","    # Get number of stop words\n","    stop_words = stopwords.words('english')\n","    self.data[\"num_stopwords\"] = self.data[text_column].apply(lambda x: \n","    len([x for x in x.split() if x in stop_words]))\n","\n","    # Get number of punctuations\n","    self.data[\"num_punctuations\"] = self.data[text_column].apply(lambda x: \n","    self.count_punctuation(x))\n","\n","    # Get number of numerical characters\n","    self.data[\"num_numerics\"] = self.data[text_column].apply(lambda x:\n","    len([x for x in x.split() if x.isdigit()]))\n","\n","    # Get number of words in the document\n","    self.data[\"num_words\"] = self.data[text_column].apply(lambda x: \n","    len(str(x).split(\" \")))\n","\n","    # Get average word length in document\n","\n","    self.data[\"avg_word_length\"] = self.data[text_column].apply(lambda x: \n","    round(self.avg_word(x),1))\n","\n","    # Get the stopwords to word ratio\n","    self.data[\"stopwords_to_words_ratio\"] = round(self.data[\"num_stopwords\"] / self.data[\"num_words\"], 3)\n","\n","    return self.data\n","  \n","  def extractive_summarizer(self, model, text_column):\n","    \n","    \"\"\"\n","    Performs extractive text summarization with BERT and allows for different \n","    pretrained model loading and configurations.\n","    :param model: initialized pretrained model\n","    :param text_column: dataframe column to perform text_summarization on\n","    :return: dataframe with summarized text columns\n","    \"\"\"\n","\n","    self.data[\"extractive_summarized_text\"] = self.data[text_column].apply(lambda x:\n","    \"\".join(model(x, num_sentences=self.get_optimal_number_sentences(x, model))))\n","\n","    return self.data   \n","\n","\n","  def join_extracted_summary(self, abstract, extracted_summary, conclusion):\n","\n","    \"\"\"\n","    Concatenates the abstract, extractive_summarized_text, and conclusion columns\n","    into one column for abstractive summarization\n","    :param abstract: abstract column\n","    :param extracted_summary: extractive_summarized_text column\n","    :param conclusion: conclusion column\n","    :return: dataframe with concatenated abstract, extracted summary and conclusion \n","    columns\n","    \"\"\"\n","\n","    self.data[\"combined_text\"] = self.data[[abstract, extracted_summary, conclusion]].agg(\n","        \" \".join, axis=1\n","    )\n","\n","    return self.data\n","\n","  def abstractive_summarizer(self, model, text_column, max_length=750, min_length=250):\n","    \n","    \"\"\"\n","    Performs abstract text summarization with BART using the extracted summary combined\n","    with the abstract and conclusion of the text.\n","    :param model: pipeline of the abstractive summarizer model\n","    :param text_column: dataframe column to perform text_summarization on\n","    :return: dataframe with summarized text columns\n","    \"\"\"\n","\n","    summaries_list = []\n","    for i in range(len(self.data[text_column])):\n","      text = self.data[text_column][i]\n","      try:\n","        summary = model(text, max_length = max_length, \n","        min_length = min_length, do_sample=False)[-1][\"summary_text\"]\n","      except:\n","        # Decrease the length of the token to 1024 if it exceeds\n","        text = text[:1024]\n","        summary = model(text, max_length = max_length, \n","        min_length = min_length, do_sample=False)[-1][\"summary_text\"]\n","      \n","      summaries_list.append(summary)\n","    \n","    self.data[\"abstractive_summaries\"] = summaries_list\n","      \n","    return self.data"],"metadata":{"id":"gzn0Y84sZvcU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extractive Summarization"],"metadata":{"id":"Ucd0SptMPB7p"}},{"cell_type":"code","source":["text_class = TextSummarizer(text_df)"],"metadata":{"id":"bmr8i9ntPmlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_class.extract_text_features(\"full_text\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PFhGYsEUPprf","executionInfo":{"status":"ok","timestamp":1647780150625,"user_tz":-480,"elapsed":715,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"27a6e721-5144-46f4-f15b-bfbb924324b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                               abstract  \\\n","0   Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1   We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2   We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3   In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4   We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","..                                                                                                  ...   \n","95  We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...   \n","96  I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...   \n","97  computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...   \n","98  If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...   \n","99  We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...   \n","\n","                                                                                              full_text  \\\n","0   Word lattice decoding has proven useful in spoken language translation; we argue that it provide...   \n","1   We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...   \n","2   We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...   \n","3   In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4   We present a novel transition system for dependency parsing, which constructs arcs only between ...   \n","..                                                                                                  ...   \n","95  We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...   \n","96  I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...   \n","97  computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...   \n","98  If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...   \n","99  We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...   \n","\n","                                                                                             conclusion  \\\n","0   We have achieved substantial gains in translation performance by decoding compact representation...   \n","1   We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...   \n","2   We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sour...   \n","3   In this paper, we propose a novel string-todependency algorithm for statistical machine translat...   \n","4   We have presented a novel transition system for dependency parsing that can handle unrestricted ...   \n","..                                                                                                  ...   \n","95  Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate ...   \n","96  Prel)ositioual phrase attachment disambiguation is a difficult problem. Take, for example, the s...   \n","97  We have presented an application of topic modeling to the problem of automatically computing sel...   \n","98  Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic ...   \n","99  We have presented new parsing algorithms that are capable of efficiently parsing third-order fac...   \n","\n","    num_stopwords  num_punctuations  num_numerics  num_words  avg_word_length  \\\n","0            1330               723            22       3826              5.4   \n","1            1615              1209            13       4683              5.1   \n","2            1315               707            27       3573              5.2   \n","3            1306               880            22       3774              5.2   \n","4            1728               882            12       4416              4.9   \n","..            ...               ...           ...        ...              ...   \n","95           1794              1286            44       5024              5.2   \n","96            742              1185            31       2528              4.9   \n","97           1731              1049            38       4870              5.5   \n","98           1568              1140            39       4740              5.4   \n","99           1739              1258            56       4888              5.4   \n","\n","    stopwords_to_words_ratio  \n","0                      0.348  \n","1                      0.345  \n","2                      0.368  \n","3                      0.346  \n","4                      0.391  \n","..                       ...  \n","95                     0.357  \n","96                     0.294  \n","97                     0.355  \n","98                     0.331  \n","99                     0.356  \n","\n","[100 rows x 9 columns]"],"text/html":["\n","  <div id=\"df-cd35ce78-fb0b-477c-a89f-3677a1c94216\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>full_text</th>\n","      <th>conclusion</th>\n","      <th>num_stopwords</th>\n","      <th>num_punctuations</th>\n","      <th>num_numerics</th>\n","      <th>num_words</th>\n","      <th>avg_word_length</th>\n","      <th>stopwords_to_words_ratio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>Word lattice decoding has proven useful in spoken language translation; we argue that it provide...</td>\n","      <td>We have achieved substantial gains in translation performance by decoding compact representation...</td>\n","      <td>1330</td>\n","      <td>723</td>\n","      <td>22</td>\n","      <td>3826</td>\n","      <td>5.4</td>\n","      <td>0.348</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...</td>\n","      <td>We presented new dependency parsers based on concise ILP formulations. We have shown how non-loc...</td>\n","      <td>1615</td>\n","      <td>1209</td>\n","      <td>13</td>\n","      <td>4683</td>\n","      <td>5.1</td>\n","      <td>0.345</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...</td>\n","      <td>We proposed a cascaded linear model for Chinese Joint S&amp;T. Under this model, many knowledge sour...</td>\n","      <td>1315</td>\n","      <td>707</td>\n","      <td>27</td>\n","      <td>3573</td>\n","      <td>5.2</td>\n","      <td>0.368</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>In this paper, we propose a novel string-todependency algorithm for statistical machine translat...</td>\n","      <td>1306</td>\n","      <td>880</td>\n","      <td>22</td>\n","      <td>3774</td>\n","      <td>5.2</td>\n","      <td>0.346</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We present a novel transition system for dependency parsing, which constructs arcs only between ...</td>\n","      <td>We have presented a novel transition system for dependency parsing that can handle unrestricted ...</td>\n","      <td>1728</td>\n","      <td>882</td>\n","      <td>12</td>\n","      <td>4416</td>\n","      <td>4.9</td>\n","      <td>0.391</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...</td>\n","      <td>We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...</td>\n","      <td>Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate ...</td>\n","      <td>1794</td>\n","      <td>1286</td>\n","      <td>44</td>\n","      <td>5024</td>\n","      <td>5.2</td>\n","      <td>0.357</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...</td>\n","      <td>I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...</td>\n","      <td>Prel)ositioual phrase attachment disambiguation is a difficult problem. Take, for example, the s...</td>\n","      <td>742</td>\n","      <td>1185</td>\n","      <td>31</td>\n","      <td>2528</td>\n","      <td>4.9</td>\n","      <td>0.294</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...</td>\n","      <td>computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...</td>\n","      <td>We have presented an application of topic modeling to the problem of automatically computing sel...</td>\n","      <td>1731</td>\n","      <td>1049</td>\n","      <td>38</td>\n","      <td>4870</td>\n","      <td>5.5</td>\n","      <td>0.355</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...</td>\n","      <td>If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...</td>\n","      <td>Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic ...</td>\n","      <td>1568</td>\n","      <td>1140</td>\n","      <td>39</td>\n","      <td>4740</td>\n","      <td>5.4</td>\n","      <td>0.331</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...</td>\n","      <td>We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...</td>\n","      <td>We have presented new parsing algorithms that are capable of efficiently parsing third-order fac...</td>\n","      <td>1739</td>\n","      <td>1258</td>\n","      <td>56</td>\n","      <td>4888</td>\n","      <td>5.4</td>\n","      <td>0.356</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 9 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd35ce78-fb0b-477c-a89f-3677a1c94216')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cd35ce78-fb0b-477c-a89f-3677a1c94216 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cd35ce78-fb0b-477c-a89f-3677a1c94216');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["%%time\n","\n","pretrained_model = 'allenai/scibert_scivocab_uncased'\n","# Load model, model config and tokenizer via Transformers\n","custom_config = AutoConfig.from_pretrained(pretrained_model)\n","custom_config.output_hidden_states=True\n","custom_tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n","custom_model = AutoModel.from_pretrained(pretrained_model, config=custom_config)\n","\n","# Create pretrained-model object\n","model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b05403da39bd41b9b5d6d99048f91afe","c4bf10acadfc43fa935b9be0a7cd304c","98b35fecb8ba40a687471630acdb1631","8d1373f224f14c3a940050cd3e328a71","1678b07b8c4e48849ca8c4e89c83bebe","4c6d6f2c74bc4f61a74ba0053647dbe2","1379ce5bbfcb4fc99398a8b34758ee59","fc42fb955e1748c082fbc8888946d1ec","ff1f3e0ecc304de68c506cccc0fe206b","bc18d0a2c6874c2ca0aa7becf036c3e5","0503599684d9420aa464ceb1c008bb07","070f8a90f8104097adb08c548bf457ec","e20656d893984a58b822d3900d8cbdca","5b06e63a678b44ab8a45ec3499a373a9","6266f16d32484bdd8d0835f4b96a0f61","888d57ce90ae4cc4b280dafdc86639e7","58e4adba5591473f96f483f4a3a1d9a0","0b71c019f9964e5994fae71a1141f56a","2b360d1232c24fd0a9e9e781c149d432","93a38ea035bb45c49165e99bf6c6ffa6","c39df8a975034402b58b3d48c2e4e7b0","09504bc3537b4a8f9faab674d0e4d7fa","fb338536c1e6456b8d75c5e6c19f2e10","cc6f86c13715402d941cf465b4057293","c5d10622f98b4510a26a9dec85c91647","102b257c3d37420f9a9c73fee1bf7dd8","afc3621bb0fe4a7bb55c107aba9dd5a4","a9e95a6c03b74215ba414c15cb0109ad","ea053977f4e44ab2a0b0bb52fba1cdf2","e0c426dca2df421780027840a51fc364","3b329b83fd1b44d69af7b50de8a6936e","6f925fccaa254171b23e77481fcd4d9e","0fa2d5968c3b47a2a4299ea9bbe45c5f"]},"id":"lOekw5sG2LP9","executionInfo":{"status":"ok","timestamp":1647780238312,"user_tz":-480,"elapsed":87711,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"96ac1851-35ae-44d3-b634-966381a373b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprlux9unu\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05403da39bd41b9b5d6d99048f91afe"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","creating metadata file for /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq_w5vwax\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/223k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070f8a90f8104097adb08c548bf457ec"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","creating metadata file for /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxiu7cmsd\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb338536c1e6456b8d75c5e6c19f2e10"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","creating metadata file for /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 12s, sys: 3.85 s, total: 1min 16s\n","Wall time: 1min 27s\n"]}]},{"cell_type":"code","source":["%%time \n","extractive_summarized_text = text_class.extractive_summarizer(model, \"full_text\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AaSC4hzrOn1z","executionInfo":{"status":"ok","timestamp":1647781019097,"user_tz":-480,"elapsed":744408,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"2db9fccb-9896-4747-acf9-972f91d16645"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 12min 30s, sys: 2min 26s, total: 14min 57s\n","Wall time: 12min 24s\n"]}]},{"cell_type":"code","source":["# Save entire dataframe \n","extractive_summarized_text.to_csv(\"extractive_summarized_dataframe_final.csv\")\n","extractive_summaries = extractive_summarized_text[\"extractive_summarized_text\"]"],"metadata":{"id":"AL0NID0NQ-PF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extractive_summaries"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYmIDAOBRHyK","executionInfo":{"status":"ok","timestamp":1647781054853,"user_tz":-480,"elapsed":314,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"bbf96252-a343-46db-8287-34dd1e78cdc7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     Word lattice decoding has proven useful in spoken language translation; we argue that it provide...\n","1     We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linea...\n","2     We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging...\n","3     In this paper, we propose a novel string-todependency algorithm for statistical machine translat...\n","4     We present a novel transition system for dependency parsing, which constructs arcs only between ...\n","                                                     ...                                                 \n","95    We present an algorithm for anaphora res- olutkm which is a modified and extended version of tha...\n","96    I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment dis...\n","97    computation of preferthe admissible argument values for a relation, is a well-known NLP task wit...\n","98    If we take an existing supervised NLP system, a simple and general way to improve accuracy is to...\n","99    We present algorithms for higher-order dependency parsing that are “third-order” in the sense th...\n","Name: extractive_summarized_text, Length: 100, dtype: object"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#### Abstractive Summarization with BART"],"metadata":{"id":"S9uyM7jxRJMv"}},{"cell_type":"code","source":["abstractive_text = pd.read_csv(\"extractive_summarized_dataframe_final.csv\")\n","abstractive_text.drop(\"Unnamed: 0\", axis=1, inplace=True)\n","abstractive_text.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"3oGjW80yBvmi","executionInfo":{"status":"ok","timestamp":1647784331709,"user_tz":-480,"elapsed":353,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"f007fa41-51f5-4537-fc1c-83e1a343ca7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            abstract  \\\n","0  Word lattice decoding has proven useful in spo...   \n","1  We formulate the problem of nonprojective depe...   \n","2  We propose a cascaded linear model for joint C...   \n","3  In this paper, we propose a novel string-todep...   \n","4  We present a novel transition system for depen...   \n","\n","                                           full_text  \\\n","0  Word lattice decoding has proven useful in spo...   \n","1  We formulate the problem of nonprojective depe...   \n","2  We propose a cascaded linear model for joint C...   \n","3  In this paper, we propose a novel string-todep...   \n","4  We present a novel transition system for depen...   \n","\n","                                          conclusion  num_stopwords  \\\n","0  We have achieved substantial gains in translat...           1330   \n","1  We presented new dependency parsers based on c...           1615   \n","2  We proposed a cascaded linear model for Chines...           1315   \n","3  In this paper, we propose a novel string-todep...           1306   \n","4  We have presented a novel transition system fo...           1728   \n","\n","   num_punctuations  num_numerics  num_words  avg_word_length  \\\n","0               723            22       3826              5.4   \n","1              1209            13       4683              5.1   \n","2               707            27       3573              5.2   \n","3               880            22       3774              5.2   \n","4               882            12       4416              4.9   \n","\n","   stopwords_to_words_ratio                         extractive_summarized_text  \n","0                     0.348  Word lattice decoding has proven useful in spo...  \n","1                     0.345  We formulate the problem of nonprojective depe...  \n","2                     0.368  We propose a cascaded linear model for joint C...  \n","3                     0.346  In this paper, we propose a novel string-todep...  \n","4                     0.391  We present a novel transition system for depen...  "],"text/html":["\n","  <div id=\"df-c6201e07-7075-450a-b42d-6bfb2498a47c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>full_text</th>\n","      <th>conclusion</th>\n","      <th>num_stopwords</th>\n","      <th>num_punctuations</th>\n","      <th>num_numerics</th>\n","      <th>num_words</th>\n","      <th>avg_word_length</th>\n","      <th>stopwords_to_words_ratio</th>\n","      <th>extractive_summarized_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","      <td>We have achieved substantial gains in translat...</td>\n","      <td>1330</td>\n","      <td>723</td>\n","      <td>22</td>\n","      <td>3826</td>\n","      <td>5.4</td>\n","      <td>0.348</td>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","      <td>We presented new dependency parsers based on c...</td>\n","      <td>1615</td>\n","      <td>1209</td>\n","      <td>13</td>\n","      <td>4683</td>\n","      <td>5.1</td>\n","      <td>0.345</td>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","      <td>We proposed a cascaded linear model for Chines...</td>\n","      <td>1315</td>\n","      <td>707</td>\n","      <td>27</td>\n","      <td>3573</td>\n","      <td>5.2</td>\n","      <td>0.368</td>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>1306</td>\n","      <td>880</td>\n","      <td>22</td>\n","      <td>3774</td>\n","      <td>5.2</td>\n","      <td>0.346</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>We present a novel transition system for depen...</td>\n","      <td>We present a novel transition system for depen...</td>\n","      <td>We have presented a novel transition system fo...</td>\n","      <td>1728</td>\n","      <td>882</td>\n","      <td>12</td>\n","      <td>4416</td>\n","      <td>4.9</td>\n","      <td>0.391</td>\n","      <td>We present a novel transition system for depen...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6201e07-7075-450a-b42d-6bfb2498a47c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6201e07-7075-450a-b42d-6bfb2498a47c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6201e07-7075-450a-b42d-6bfb2498a47c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["text_class = TextSummarizer(abstractive_text)"],"metadata":{"id":"H5rc5mvjbOu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_class.join_extracted_summary(\"abstract\", \"extractive_summarized_text\", \"conclusion\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DRC8PQtXCD7_","executionInfo":{"status":"ok","timestamp":1647784341988,"user_tz":-480,"elapsed":8,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"f86e057c-6a27-4aec-af6a-694281f741e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             abstract  \\\n","0   Word lattice decoding has proven useful in spo...   \n","1   We formulate the problem of nonprojective depe...   \n","2   We propose a cascaded linear model for joint C...   \n","3   In this paper, we propose a novel string-todep...   \n","4   We present a novel transition system for depen...   \n","..                                                ...   \n","95  We present an algorithm for anaphora res- olut...   \n","96  I:n this paper, we describe a new corpus-based...   \n","97  computation of preferthe admissible argument v...   \n","98  If we take an existing supervised NLP system, ...   \n","99  We present algorithms for higher-order depende...   \n","\n","                                            full_text  \\\n","0   Word lattice decoding has proven useful in spo...   \n","1   We formulate the problem of nonprojective depe...   \n","2   We propose a cascaded linear model for joint C...   \n","3   In this paper, we propose a novel string-todep...   \n","4   We present a novel transition system for depen...   \n","..                                                ...   \n","95  We present an algorithm for anaphora res- olut...   \n","96  I:n this paper, we describe a new corpus-based...   \n","97  computation of preferthe admissible argument v...   \n","98  If we take an existing supervised NLP system, ...   \n","99  We present algorithms for higher-order depende...   \n","\n","                                           conclusion  num_stopwords  \\\n","0   We have achieved substantial gains in translat...           1330   \n","1   We presented new dependency parsers based on c...           1615   \n","2   We proposed a cascaded linear model for Chines...           1315   \n","3   In this paper, we propose a novel string-todep...           1306   \n","4   We have presented a novel transition system fo...           1728   \n","..                                                ...            ...   \n","95  Quantitative evaluation shows the anaphora res...           1794   \n","96  Prel)ositioual phrase attachment disambiguatio...            742   \n","97  We have presented an application of topic mode...           1731   \n","98  Word features can be learned in advance in an ...           1568   \n","99  We have presented new parsing algorithms that ...           1739   \n","\n","    num_punctuations  num_numerics  num_words  avg_word_length  \\\n","0                723            22       3826              5.4   \n","1               1209            13       4683              5.1   \n","2                707            27       3573              5.2   \n","3                880            22       3774              5.2   \n","4                882            12       4416              4.9   \n","..               ...           ...        ...              ...   \n","95              1286            44       5024              5.2   \n","96              1185            31       2528              4.9   \n","97              1049            38       4870              5.5   \n","98              1140            39       4740              5.4   \n","99              1258            56       4888              5.4   \n","\n","    stopwords_to_words_ratio  \\\n","0                      0.348   \n","1                      0.345   \n","2                      0.368   \n","3                      0.346   \n","4                      0.391   \n","..                       ...   \n","95                     0.357   \n","96                     0.294   \n","97                     0.355   \n","98                     0.331   \n","99                     0.356   \n","\n","                           extractive_summarized_text  \\\n","0   Word lattice decoding has proven useful in spo...   \n","1   We formulate the problem of nonprojective depe...   \n","2   We propose a cascaded linear model for joint C...   \n","3   In this paper, we propose a novel string-todep...   \n","4   We present a novel transition system for depen...   \n","..                                                ...   \n","95  We present an algorithm for anaphora res- olut...   \n","96  I:n this paper, we describe a new corpus-based...   \n","97  computation of preferthe admissible argument v...   \n","98  If we take an existing supervised NLP system, ...   \n","99  We present algorithms for higher-order depende...   \n","\n","                                        combined_text  \n","0   Word lattice decoding has proven useful in spo...  \n","1   We formulate the problem of nonprojective depe...  \n","2   We propose a cascaded linear model for joint C...  \n","3   In this paper, we propose a novel string-todep...  \n","4   We present a novel transition system for depen...  \n","..                                                ...  \n","95  We present an algorithm for anaphora res- olut...  \n","96  I:n this paper, we describe a new corpus-based...  \n","97  computation of preferthe admissible argument v...  \n","98  If we take an existing supervised NLP system, ...  \n","99  We present algorithms for higher-order depende...  \n","\n","[100 rows x 11 columns]"],"text/html":["\n","  <div id=\"df-301deabd-9e8c-4f78-b2a6-b6454ed31633\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>full_text</th>\n","      <th>conclusion</th>\n","      <th>num_stopwords</th>\n","      <th>num_punctuations</th>\n","      <th>num_numerics</th>\n","      <th>num_words</th>\n","      <th>avg_word_length</th>\n","      <th>stopwords_to_words_ratio</th>\n","      <th>extractive_summarized_text</th>\n","      <th>combined_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","      <td>We have achieved substantial gains in translat...</td>\n","      <td>1330</td>\n","      <td>723</td>\n","      <td>22</td>\n","      <td>3826</td>\n","      <td>5.4</td>\n","      <td>0.348</td>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","      <td>Word lattice decoding has proven useful in spo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","      <td>We presented new dependency parsers based on c...</td>\n","      <td>1615</td>\n","      <td>1209</td>\n","      <td>13</td>\n","      <td>4683</td>\n","      <td>5.1</td>\n","      <td>0.345</td>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","      <td>We formulate the problem of nonprojective depe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","      <td>We proposed a cascaded linear model for Chines...</td>\n","      <td>1315</td>\n","      <td>707</td>\n","      <td>27</td>\n","      <td>3573</td>\n","      <td>5.2</td>\n","      <td>0.368</td>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","      <td>We propose a cascaded linear model for joint C...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>1306</td>\n","      <td>880</td>\n","      <td>22</td>\n","      <td>3774</td>\n","      <td>5.2</td>\n","      <td>0.346</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","      <td>In this paper, we propose a novel string-todep...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>We present a novel transition system for depen...</td>\n","      <td>We present a novel transition system for depen...</td>\n","      <td>We have presented a novel transition system fo...</td>\n","      <td>1728</td>\n","      <td>882</td>\n","      <td>12</td>\n","      <td>4416</td>\n","      <td>4.9</td>\n","      <td>0.391</td>\n","      <td>We present a novel transition system for depen...</td>\n","      <td>We present a novel transition system for depen...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>We present an algorithm for anaphora res- olut...</td>\n","      <td>We present an algorithm for anaphora res- olut...</td>\n","      <td>Quantitative evaluation shows the anaphora res...</td>\n","      <td>1794</td>\n","      <td>1286</td>\n","      <td>44</td>\n","      <td>5024</td>\n","      <td>5.2</td>\n","      <td>0.357</td>\n","      <td>We present an algorithm for anaphora res- olut...</td>\n","      <td>We present an algorithm for anaphora res- olut...</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>I:n this paper, we describe a new corpus-based...</td>\n","      <td>I:n this paper, we describe a new corpus-based...</td>\n","      <td>Prel)ositioual phrase attachment disambiguatio...</td>\n","      <td>742</td>\n","      <td>1185</td>\n","      <td>31</td>\n","      <td>2528</td>\n","      <td>4.9</td>\n","      <td>0.294</td>\n","      <td>I:n this paper, we describe a new corpus-based...</td>\n","      <td>I:n this paper, we describe a new corpus-based...</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>computation of preferthe admissible argument v...</td>\n","      <td>computation of preferthe admissible argument v...</td>\n","      <td>We have presented an application of topic mode...</td>\n","      <td>1731</td>\n","      <td>1049</td>\n","      <td>38</td>\n","      <td>4870</td>\n","      <td>5.5</td>\n","      <td>0.355</td>\n","      <td>computation of preferthe admissible argument v...</td>\n","      <td>computation of preferthe admissible argument v...</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>If we take an existing supervised NLP system, ...</td>\n","      <td>If we take an existing supervised NLP system, ...</td>\n","      <td>Word features can be learned in advance in an ...</td>\n","      <td>1568</td>\n","      <td>1140</td>\n","      <td>39</td>\n","      <td>4740</td>\n","      <td>5.4</td>\n","      <td>0.331</td>\n","      <td>If we take an existing supervised NLP system, ...</td>\n","      <td>If we take an existing supervised NLP system, ...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>We present algorithms for higher-order depende...</td>\n","      <td>We present algorithms for higher-order depende...</td>\n","      <td>We have presented new parsing algorithms that ...</td>\n","      <td>1739</td>\n","      <td>1258</td>\n","      <td>56</td>\n","      <td>4888</td>\n","      <td>5.4</td>\n","      <td>0.356</td>\n","      <td>We present algorithms for higher-order depende...</td>\n","      <td>We present algorithms for higher-order depende...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 11 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-301deabd-9e8c-4f78-b2a6-b6454ed31633')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-301deabd-9e8c-4f78-b2a6-b6454ed31633 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-301deabd-9e8c-4f78-b2a6-b6454ed31633');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["%%time\n","summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n","abstractive_summarized_text = text_class.abstractive_summarizer(summarizer, \"combined_text\")\n","\n","# Save to csv\n","abstractive_summarized_text.to_csv(\"abstractive_summarized_dataframe_final.csv\")\n","abstractive_summaries = abstractive_summarized_text[\"abstractive_summaries\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InLVbLsmCNsW","executionInfo":{"status":"ok","timestamp":1647790347587,"user_tz":-480,"elapsed":5999481,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"de8ddb88-34a7-4984-bf73-0414c661ca16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","Your max_length is set to 750, but you input_length is only 419. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=209)\n","Your max_length is set to 750, but you input_length is only 468. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=234)\n","Your max_length is set to 750, but you input_length is only 531. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=265)\n","Your max_length is set to 750, but you input_length is only 361. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=180)\n","Your max_length is set to 750, but you input_length is only 600. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=300)\n","Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 1024). Running this sequence through the model will result in indexing errors\n","Your max_length is set to 750, but you input_length is only 197. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=98)\n","Your max_length is set to 750, but you input_length is only 309. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=154)\n","Your max_length is set to 750, but you input_length is only 428. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=214)\n","Your max_length is set to 750, but you input_length is only 452. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=226)\n","Your max_length is set to 750, but you input_length is only 368. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=184)\n","Your max_length is set to 750, but you input_length is only 242. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=121)\n","Your max_length is set to 750, but you input_length is only 455. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=227)\n","Your max_length is set to 750, but you input_length is only 446. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=223)\n","Your max_length is set to 750, but you input_length is only 357. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=178)\n","Your max_length is set to 750, but you input_length is only 481. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=240)\n","Your max_length is set to 750, but you input_length is only 387. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=193)\n","Your max_length is set to 750, but you input_length is only 725. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=362)\n","Your max_length is set to 750, but you input_length is only 183. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=91)\n","Your max_length is set to 750, but you input_length is only 599. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=299)\n","Your max_length is set to 750, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n","Your max_length is set to 750, but you input_length is only 601. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=300)\n","Your max_length is set to 750, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n","Your max_length is set to 750, but you input_length is only 431. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=215)\n","Your max_length is set to 750, but you input_length is only 229. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n","Your max_length is set to 750, but you input_length is only 730. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=365)\n","Your max_length is set to 750, but you input_length is only 211. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=105)\n","Your max_length is set to 750, but you input_length is only 674. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=337)\n","Your max_length is set to 750, but you input_length is only 525. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=262)\n","Your max_length is set to 750, but you input_length is only 717. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=358)\n","Your max_length is set to 750, but you input_length is only 567. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=283)\n","Your max_length is set to 750, but you input_length is only 542. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=271)\n","Your max_length is set to 750, but you input_length is only 389. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=194)\n","Your max_length is set to 750, but you input_length is only 681. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=340)\n","Your max_length is set to 750, but you input_length is only 216. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=108)\n","Your max_length is set to 750, but you input_length is only 204. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=102)\n","Your max_length is set to 750, but you input_length is only 544. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=272)\n","Your max_length is set to 750, but you input_length is only 337. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=168)\n","Your max_length is set to 750, but you input_length is only 415. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=207)\n","Your max_length is set to 750, but you input_length is only 225. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n","Your max_length is set to 750, but you input_length is only 342. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=171)\n","Your max_length is set to 750, but you input_length is only 604. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=302)\n","Your max_length is set to 750, but you input_length is only 398. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=199)\n","Your max_length is set to 750, but you input_length is only 432. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=216)\n","Your max_length is set to 750, but you input_length is only 264. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=132)\n","Your max_length is set to 750, but you input_length is only 449. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=224)\n","Your max_length is set to 750, but you input_length is only 216. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=108)\n","Your max_length is set to 750, but you input_length is only 595. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=297)\n","Your max_length is set to 750, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n","Your max_length is set to 750, but you input_length is only 362. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=181)\n","Your max_length is set to 750, but you input_length is only 467. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=233)\n","Your max_length is set to 750, but you input_length is only 209. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=104)\n","Your max_length is set to 750, but you input_length is only 295. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=147)\n","Your max_length is set to 750, but you input_length is only 605. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=302)\n","Your max_length is set to 750, but you input_length is only 146. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n","Your max_length is set to 750, but you input_length is only 195. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n","Your max_length is set to 750, but you input_length is only 206. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=103)\n","Your max_length is set to 750, but you input_length is only 634. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=317)\n","Your max_length is set to 750, but you input_length is only 619. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=309)\n","Your max_length is set to 750, but you input_length is only 246. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n","Your max_length is set to 750, but you input_length is only 195. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n","Your max_length is set to 750, but you input_length is only 216. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=108)\n","Your max_length is set to 750, but you input_length is only 206. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=103)\n","Your max_length is set to 750, but you input_length is only 452. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=226)\n","Your max_length is set to 750, but you input_length is only 210. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=105)\n","Your max_length is set to 750, but you input_length is only 199. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=99)\n","Your max_length is set to 750, but you input_length is only 590. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=295)\n","Your max_length is set to 750, but you input_length is only 207. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=103)\n","Your max_length is set to 750, but you input_length is only 527. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=263)\n","Your max_length is set to 750, but you input_length is only 211. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=105)\n","Your max_length is set to 750, but you input_length is only 229. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n","Your max_length is set to 750, but you input_length is only 277. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=138)\n","Your max_length is set to 750, but you input_length is only 307. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=153)\n","Your max_length is set to 750, but you input_length is only 189. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=94)\n","Your max_length is set to 750, but you input_length is only 200. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=100)\n","Your max_length is set to 750, but you input_length is only 653. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=326)\n","Your max_length is set to 750, but you input_length is only 213. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n","Your max_length is set to 750, but you input_length is only 675. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=337)\n","Your max_length is set to 750, but you input_length is only 596. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=298)\n","Your max_length is set to 750, but you input_length is only 676. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=338)\n","Your max_length is set to 750, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n","Your max_length is set to 750, but you input_length is only 210. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=105)\n","Your max_length is set to 750, but you input_length is only 229. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n","Your max_length is set to 750, but you input_length is only 291. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=145)\n","Your max_length is set to 750, but you input_length is only 619. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=309)\n","Your max_length is set to 750, but you input_length is only 210. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=105)\n","Your max_length is set to 750, but you input_length is only 609. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=304)\n","Your max_length is set to 750, but you input_length is only 626. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=313)\n","Your max_length is set to 750, but you input_length is only 198. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=99)\n","Your max_length is set to 750, but you input_length is only 439. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=219)\n","Your max_length is set to 750, but you input_length is only 257. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=128)\n","Your max_length is set to 750, but you input_length is only 285. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=142)\n","Your max_length is set to 750, but you input_length is only 491. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=245)\n","Your max_length is set to 750, but you input_length is only 616. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=308)\n","Your max_length is set to 750, but you input_length is only 664. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=332)\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 39min 20s, sys: 1min 22s, total: 1h 40min 42s\n","Wall time: 1h 39min 59s\n"]}]},{"cell_type":"code","source":["abstractive_summaries"],"metadata":{"id":"YqTZ4AbO6RCZ","executionInfo":{"status":"ok","timestamp":1647790991334,"user_tz":-480,"elapsed":298,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"6f77c591-b035-4e38-86aa-dbebc4fbb9f4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     We show that prior work in translating lattice...\n","1     We formulate the problem of nonprojective depe...\n","2     We propose a cascaded linear model for joint C...\n","3     In this paper, we propose a novel string-todep...\n","4     System constructs arcs only between adjacent w...\n","                            ...                        \n","95    We present an algorithm for anaphora res- olut...\n","96    We describe a new corpus-based ap- proach to p...\n","97     computation of preferthe admissible argument ...\n","98    If we take an existing supervised NLP system, ...\n","99    In order to parse a sentence x, it suffices to...\n","Name: abstractive_summaries, Length: 100, dtype: object"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Model Deployment with Streamlit and Localtunnel <a name=\"s4\"></a>\n","\n","[Streamlit](https://streamlit.io/) is an open-source app framework for Machine Learning and Data Science teams. It turns data scripts into shareable web apps in minutes. \n","\n","To deploy my model with streamlit, I create a python file `app.py` which contains the following code:\n","\n","```\n","from transformers import *\n","from summarizer import Summarizer\n","from summarizer.text_processors.coreference_handler import CoreferenceHandler\n","import streamlit as st\n","\n","st.title('Extractive and Abstractive Text Summarization')\n","st.markdown('Using BERT and BART Transformer Models')\n","\n","text = st.text_area('Please Input a Long Scientific Text')\n","abstract = st.text_area(\"Please Input Scientific Text Abstract\")\n","conclusion = st.text_area(\"Please Input Scientific Text Conclusion\")\n","\n","pretrained_model = 'allenai/scibert_scivocab_uncased'\n","\n","max_length = 750\n","min_length = 250\n","\n","@st.cache(suppress_st_warning=True)\n","def get_summary(text, abstract, conclusion, pretrained_model):\n","    # Extractive Summarizer\n","    # Load model, model config and tokenizer via Transformers\n","    custom_config = AutoConfig.from_pretrained(pretrained_model)\n","    custom_config.output_hidden_states=True\n","    custom_tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n","    custom_model = AutoModel.from_pretrained(pretrained_model, config=custom_config)\n","\n","    # Create pretrained-model object\n","    extractive_model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n","\n","    # Abstractive Summarizer\n","    abstractive_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n","\n","    optimal_num_sentences = extractive_model.calculate_optimal_k(text, k_max=10)\n","    extractive_summarized_text = \"\".join(extractive_model(text, num_sentences=optimal_num_sentences))\n","    \n","    text_list = [abstract, extractive_summarized_text, conclusion]\n","    joined_text = \" \".join(text_list)\n","\n","    abstractive_summary = abstractive_summarizer(joined_text, max_length=max_length, min_length=min_length, \n","                                                do_sample=False)[-1][\"summary_text\"]\n","    st.write(\"Summary\")\n","    st.success(abstractive_summary)\n","\n","if st.button(\"Summarize\"):\n","    get_summary(text, abstract, conclusion, pretrained_model)\n","```\n","\n","The code above in `app.py` performs the following:\n","1. Creates a streamlit text area for the user to place a long scientific text document.\n","2. Create a `Summarize` button for the user to click.\n","3. Once `Summarize` is click, the model performs extractive summarization and then abstractive summarization. The result is presented to the user. \n","\n","In order for this to be shared through the web, I make use of [localtunnel](https://github.com/localtunnel/localtunnel). Localtunnel exposes your localhost to the world for easy testing and sharing! No need to mess with DNS or deploy just to have others test out your changes. It is great for working with browser testing tools like browserling or external api callback services like twilio which require a public url for callbacks.\n","\n","With localtunnel, I am able to deploy the streamlit interface to the web. \n","\n","To achieve all this, first ensure `app.py` is in the same directory as this notebook. Then, run the codeblocks below. "],"metadata":{"id":"es2bkAYOWBkn"}},{"cell_type":"code","source":["!pip install streamlit"],"metadata":{"id":"LKciRIyTYTCy","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1648303361229,"user_tz":-480,"elapsed":15992,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"58002440-7c39-4438-b0b5-4ddfa8546c4a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.8.0-py2.py3-none-any.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n","Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 34.6 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.10.0.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Collecting validators\n","  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n","Collecting pympler>=0.9\n","  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n","\u001b[K     |████████████████████████████████| 164 kB 48.9 MB/s \n","\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n","Collecting blinker\n","  Downloading blinker-1.4.tar.gz (111 kB)\n","\u001b[K     |████████████████████████████████| 111 kB 45.8 MB/s \n","\u001b[?25hCollecting gitpython!=3.1.19\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 46.2 MB/s \n","\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n","Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.11.3)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n","Collecting toml\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n","Collecting watchdog\n","  Downloading watchdog-2.1.7-py3-none-manylinux2014_x86_64.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.7.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.4.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n","Collecting ipykernel>=5.1.2\n","  Downloading ipykernel-6.9.2-py3-none-any.whl (130 kB)\n","\u001b[K     |████████████████████████████████| 130 kB 47.6 MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.7.0)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n","Collecting ipython>=7.23.1\n","  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 40.8 MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.4.8)\n","Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.4)\n","Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n","Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n","\u001b[K     |████████████████████████████████| 380 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.1.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.6.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.9.2)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.3.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n","Building wheels for collected packages: blinker\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=5da4a23ca2a6bf867307c5f667618075f750848be23fc9b1fec10a0616deaac1\n","  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n","Successfully built blinker\n","Installing collected packages: prompt-toolkit, ipython, ipykernel, smmap, gitdb, watchdog, validators, toml, pympler, pydeck, gitpython, blinker, streamlit\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.2 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\n","Successfully installed blinker-1.4 gitdb-4.0.9 gitpython-3.1.27 ipykernel-6.9.2 ipython-7.32.0 prompt-toolkit-3.0.28 pydeck-0.7.1 pympler-1.0.1 smmap-5.0.0 streamlit-1.8.0 toml-0.10.2 validators-0.18.2 watchdog-2.1.7\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","ipykernel","prompt_toolkit"]}}},"metadata":{}}]},{"cell_type":"code","source":["!streamlit run app.py & npx localtunnel --port 8501"],"metadata":{"id":"JClXLqUPYc4_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648309839059,"user_tz":-480,"elapsed":345307,"user":{"displayName":"Gerson Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07825640613715802477"}},"outputId":"337601af-d1f5-467d-fbc6-c87a5cdace88"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-03-26 15:44:56.167 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n","\u001b[K\u001b[?25hnpx: installed 22 in 4.559s\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.68.157.228:8501\u001b[0m\n","\u001b[0m\n","your url is: https://calm-hound-63.loca.lt\n","2022-03-26 15:45:30.922080: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n","  return f(*args, **kwds)\n","2022-03-26 15:45:32.684 Loading model from /root/.neuralcoref_cache/neuralcoref\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n","Model config BartConfig {\n","  \"_name_or_path\": \"facebook/bart-large-cnn\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Your max_length is set to 750, but you input_length is only 378. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=189)\n","\u001b[34m  Stopping...\u001b[0m\n","^C\n"]}]},{"cell_type":"markdown","source":["These codeblocks will expose your work to the web. It will return a URL for you to share in order to access the streamlit model. \n","\n","After that, enjoy testing out the streamlit model :) "],"metadata":{"id":"AURqFCIsYd_R"}},{"cell_type":"markdown","source":["## Recommendations <a name=\"s5\"></a>\n","\n","For improvements to this project, the following recommendations are provided:\n","1. Given more computational resources, use coreference handlers to improve upon the semantic capabilities of the summarizer.\n","2. Finetune the abstractive summarizer with BART by training it on a dataset consisting of the other papers in the Sciscumm corpus each with its respective summary made by a human. This will improve the performance of the summarizer with the Scisumm dataset, and in turn, with scientific documents. \n","3. Check out other transformer models and compare the resulting summaries in order to find a best model for this dataset.\n","4. Research about methods to decrease inference time on deployed Streamlit model.  \n","\n","**Thank you very much for checking this project out! Hope you learned and have a nice day!**"],"metadata":{"id":"bVO458NGDRhV"}},{"cell_type":"code","source":[""],"metadata":{"id":"B807m9hvggpt"},"execution_count":null,"outputs":[]}]}