,abstract,full_text,conclusion
0,"Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation.","Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation. When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight – harkening back to Weaver in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f' E F(o) and we seek An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max max Pr(e)Pr(f'|e)Pr(o|f')�(4) e f�EF(o) Following Och and Ney (2002), we use the maximum entropy framework (Berger et al., 1996) to directly model the posterior Pr(e, f'|o) with parameters tuned to minimize a loss function representing the quality only of the resulting translations. Thus, we make use of the following general decision rule: In principle, one could decode according to (2) simply by enumerating and decoding each f� ∈ F(o); however, for any interestingly large F(o) this will be impractical. We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate. In Section 2, we discuss decoding with this model in general, and then show how two classes of translation models can easily be adapted for lattice translation; we achieve a unified treatment of finite-state and hierarchical phrase-based models by treating lattices as a subcase of weighted finite state automata (FSAs). In Section 3, we identify and solve issues that arise with reordering in non-linear FSAs, i.e. FSAs where every path does not pass through every node. Section 4 presents two applications of the noisier channel paradigm, demonstrating substantial performance gains in Arabic-English and Chinese-English translation. In Section 5 we discuss relevant prior work, and we conclude in Section 6. Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008). In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices. The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007). The second is a phrase-based decoder implementing the model of Koehn et al. (2003). A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA). We further stipulate that exactly one node has no outgoing edges and is designated the ‘end node’. Figure 1 illustrates three classes of word lattices. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multiple members of the set to be represented with a single piece of structure. Additionally, all paths from one node to another form an equivalence class representing, in our model, alternative expressions of the same underlying communicative intent. For translation, we will find it useful to encode G in a chart based on a topological ordering of the nodes, as described by Cheppalier et al. (1999). The nodes in the lattices shown in Figure 1 are labeled according to an appropriate numbering. The chart-representation of the graph is a triple of 2-dimensional matrices hF, p, Ri, which can be constructed from the numbered graph. Fi,j is the word label of the jth transition leaving node i. The corresponding transition cost is pi,j. Ri,j is the node number of the node on the right side of the jth transition leaving node i. Note that Ri,j > i for all i, j. Table 1 shows the word lattice from Figure 1 represented in matrix form as hF, p, Ri. Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. The parser consists of two kinds of items, the first with the form [X —* α • Q, i, j] representing rules that have yet to be completed and span node i to node j. The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j]. As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [5, 0, |V  |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents. A target language model is necessary to generate fluent output. To do so, the grammar is intersected with an n-gram LM. To mitigate the effects of the combinatorial explosion of non-terminals the LM intersection entails, we use cube-pruning to only consider the most promising expansions (Chiang, 2007). A second important class of translation models includes those based formally on FSTs. We present a description of the decoding process for a word lattice using a representative FST model, the phrase-based translation model described in Koehn et al. (2003). Phrase-based models translate a foreign sentence f into the target language e by breaking up f into a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words. Each word in f must be translated exactly once. To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1. Although the number of source phrases in a word lattice can be exponential in the number of nodes, enumerating the possible translations of every span in a lattice is in practice tractable, as described by Bertoldi et al. (2007). We adapted the Moses phrase-based decoder to translate word lattices (Koehn et al., 2007). The unmodified decoder builds a translation hypothesis from left to right by selecting a range of untranslated words and adding translations of this phrase to the end of the hypothesis being extended. When no untranslated words remain, the translation process is complete. The word lattice decoder works similarly, only now the decoder keeps track not of the words that have been covered, but of the nodes, given a topological ordering of the nodes. For example, assuming the third lattice in Figure 1 is our input, if the edge with word a is translated, this will cover two untranslated nodes [0,1] in the coverage vector, even though it is only a single word. As with sentencebased decoding, a translation hypothesis is complete when all nodes in the input lattice are covered. The changes described thus far are straightforward adaptations of the underlying phrase-based sentence decoder; however, dealing properly with non-monotonic decoding of word lattices introduces some minor complexity that is worth mentioning. In the sentence decoder, any translation of any span of untranslated words is an allowable extension of a partial translation hypothesis, provided that the coverage vectors of the extension and the partial hypothesis do not intersect. In a non-linear word lattice, a further constraint must be enforced ensuring that there is always a path from the starting node of the translation extension’s source to the node representing the nearest right edge of the already-translated material, as well as a path from the ending node of the translation extension’s source to future translated spans. Figure 3 illustrates the problem. If [0,1] is translated, the decoder must not consider translating [2,3] as a possible extension of this hypothesis since there is no path from node 1 to node 2 and therefore the span [1,2] would never be covered. In the parser that forms the basis of the hierarchical decoder described in Section 2.3, no such restriction is necessary since grammar rules are processed in a strictly left-to-right fashion without any skips. In both hierarchical and phrase-based models, the distance between words in the source sentence is used to limit where in the target sequence their translations will be generated. In phrase based translation, distortion is modeled explicitly. Models that support non-monotonic decoding generally include a distortion cost, such as |ai − bi−1 − 1 |where ai is the starting position of the foreign phrase fi and bi−1 is the ending position of phrase fi−1 (Koehn et al., 2003). The intuition behind this model is that since most translation is monotonic, the cost of skipping ahead or back in the source should be proportional to the number of words that are skipped. Additionally, a maximum distortion limit is used to restrict the size of the search space. In linear word lattices, such as confusion networks, the distance metric used for the distortion penalty and for distortion limits is well defined; however, in a non-linear word lattice, it poses the problem illustrated in Figure 4. Assuming the leftto-right decoding strategy described in the previous section, if c is generated by the first target word, the distortion penalty associated with “skipping ahead” should be either 3 or 2, depending on what path is chosen to translate the span [0,3]. In large lattices, where a single arc may span many nodes, the possible distances may vary quite substantially depending on what path is ultimately taken, and handling this properly therefore crucial. Although hierarchical phrase-based models do not model distortion explicitly, Chiang (2007) suggests using a span length limit to restrict the window in which reordering can take place.1 The decoder enforces the constraint that a synchronous rule learned from the training data (the only mechanism by which reordering can be introduced) can span maximally A words in f. Like the distortion cost used in phrase-based systems, A is also poorly defined for non-linear lattices. Since we want a distance metric that will restrict as few local reorderings as possible on any path, we use a function �(a, b) returning the length of the shortest path between nodes a and b. Since this function is not dependent on the exact path chosen, it can be computed in advance of decoding using an allpairs shortest path algorithm (Cormen et al., 1989). We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices. We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance. For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005). Table 2 summarizes the results of the phrasebased systems. On both test sets, the shortest path metric improved the BLEU scores. As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline). Table 3 summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder’s span limit. The pattern is the same, showing a clear increase in BLEU for the shortest path metric over the baseline. Chinese word segmentation. A necessary first step in translating Chinese using standard models is segmenting the character stream into a sequence of words. Word-lattice translation offers two possible improvements over the conventional approach. First, a lattice may represent multiple alternative segmentations of a sentence; input represented in this way will be more robust to errors made by the segmenter.2 Second, different segmentation granularities may be more or less optimal for translating different spans. By encoding alternatives in the input in a word lattice, the decision as to which granularity to use for a given span can be resolved during decoding rather than when constructing the system. Figure 5 illustrates a lattice based on three different segmentations. Arabic morphological variation. Arabic orthography is problematic for lexical and phrase-based MT approaches since a large class of functional elements (prepositions, pronouns, tense markers, conjunctions, definiteness markers) are attached to their host stems. Thus, while the training data may provide good evidence for the translation of a particular stem by itself, the same stem may not be attested when attached to a particular conjunction. The general solution taken is to take the best possible morphological analysis of the text (it is often ambiguous whether a piece of a word is part of the stem or merely a neighboring functional element), and then make a subset of the bound functional elements in the language into freestanding tokens. Figure 6 illustrates the unsegmented Arabic surface form as well as the morphological segmentation variant we made use of. The limitation of this approach is that as the amount and variety of training data increases, the optimal segmentation strategy changes: more aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbin and Stanford segmenters (hs+ss), and one which uses all three segmentations (hs+ss+cs). Data and Settings. The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences). The corpus was analyzed with the three segmentation schemes. For the systems using word lattices, the training data contained the versions of the corpus appropriate for the segmentation schemes used in the input. That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.3 A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments. The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate training (Och, 2003). The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach. All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05. The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant. Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices. For example, for MT06 the number of OOVs in the hs translation is 484. The number of OOVs decreased by 19% for hs+ss and by 75% for hs+ss+cs. As mentioned in Section 3, using lexical reordering for word-lattices further improves the translation quality. We created lattices from an unsegmented version of the Arabic test data and generated alternative arcs where clitics as well as the definiteness marker and the future tense marker were segmented into tokens. We used the Buckwalter morphological analyzer and disambiguated the analysis using a simple unigram model trained on the Penn Arabic Treebank. Data and Settings. For these experiments we made use of the entire NIST MT08 training data, although for training of the system, we used a subsampling method proposed by Kishore Papineni that aims to include training sentences containing ngrams in the test data (personal communication). For all systems, we used a 5-gram English LM trained on 250M words of English training data. The NIST MT03 test set was used as development set for optimizing the interpolation weights using MER training (Och, 2003). Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al. (2004) only report results of monotonic decoding for the systems they describe. Bertoldi et al. (2007) solve the problem by requiring that their input be in the format of a confusion network, which enables the standard distortion penalty to be used. Finally, the system described by Zhang et al. (2005) uses IBM Model 4 features to translate lattices. For the distortion model, they use the maximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simplifications of a surface token when the surface token can not be translated. Parsing and formal language theory. There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998). Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG. Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices. We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model. Additionally, we have shown that although word lattices complicate modeling of word reordering, a simple heuristic offers good performance and enables many standard distortion models to be used directly with lattice input.","We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model. Additionally, we have shown that although word lattices complicate modeling of word reordering, a simple heuristic offers good performance and enables many standard distortion models to be used directly with lattice input."
1,"We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.","We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective dependency parsing. We believe that our formulations can pave the way for efficient exploitation of global features and constraints in parsing applications, leading to more powerful models. Riedel and Clarke (2006) cast dependency parsing as an ILP, but efficient formulations remain an open problem. Our formulations offer the following comparative advantages: from data. In particular, our formulations handle higher-order arc interactions (like siblings and grandparents), model word valency, and can learn to favor nearly-projective parses. We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1) directed graph D = hV, Ai, with vertices in V = {0, ... , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2. Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed graph D if hV, Bi is a (directed) tree rooted at r. We define the set of legal dependency parse trees of x (denoted Y(x)) as the set of 0-arborescences of D, i.e., we admit each arborescence as a potential dependency tree. Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i. We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) < k < max(i, j), there is a directed path in y from i to k). A dependency tree is called projective if it only contains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z °_ hzaia∈A with each component defined as za = ff(a ∈ y) (here, ff(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal dependency trees and taking the convex hull, we obtain a polyhedron that we call the arborescence polytope, denoted by Z(x). Each vertex of Z(x) can be identified with a dependency tree in Y(x). The Minkowski-Weyl theorem (Rockafellar, 1970) ensures that Z(x) has a representation of the form Z(x) = {z ∈ R|A  ||Az ≤ b}, for some p-by-|A| matrix A and some vector b in Rp. However, it is not easy to obtain a compact representation (where p grows polynomially with the number of words n). In §3, we will provide a compact representation of an outer polytope ¯Z(x) ⊇ Z(x) whose integer vertices correspond to dependency trees. Hence, the problem of finding the dependency tree that maximizes some linear function of the inci1The general case where A C_ V 2 is also of interest; it arises whenever a constraint or a lexicon forbids some arcs from appearing in dependency tree. It may also arise as a consequence of a first-stage pruning step where some candidate arcs are eliminated; this will be further discussed in §4. where only the backbone structure (i.e., the arcs without the labels depicted in Fig. 1) is to be predicted. tences;examples from McDonald and Satta (2007). those that assume each dependency decision denceevectorsmcan befcastdas ansILP. A similar idea was aplied to word alignment by Lacoste-Julien that dependency graphs must be trees. Such mod their parameters facor relative to individual edges et al. (2006), where permutations (rather than arof the graph (Paskin, 2001; McDonald et a., l are comny d o as gefacd 2005a). Edge-factored models have many computah pm cto ativ ndidl dge borescences) were the combinatorial structure bef th gah (Pki 2001 MDld t l ing requiring representation. ai to earn a parse, i.e., a functo h : X → Y mary problem in treating each dependency s in Nonlocal information such as arity (o valy that given x ∈ X ouputs a legal dependency parse depedent is that it is not a realistic assumption. and neighbouring dependencies can be crucial to y ∈ Y(x). Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview. Typcal graph-bsed methods invetigating and xtendng he edge-factored model rojtie prsin lgoiths for bth leaig nd consider liear classifiers of the fom inference of McDonald et al. (2005b) In partic ithin the datadri en ettin where f(x, y) is a vector of features and w s the tion over all possble depndency graphs for a givn correspondingyweight vector. One wants hw. to g bh pttion io a dge pect haveasmallcexpected loss; the typictlnloss functionnis thereHamming loss,cle(y'; y)n°_  |{hi, jid∈ we sho y0: hi, ji ∈/ y}|. Tractability s usually ensured ing raiing gloally normalized log-linear modht they can be sed in many important earning bystrong factorization assumptions, like the one els, syntactic language modeling, and nsupervied nd inference problem including minrisk decod underlying the arc-factored mode (Esne, 1996; ing training globally normalized log-linear modMcDonald et a., 2005), which forbids any feature els syntactic language modeling and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f(x, y) as: Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. Our approach will build a graph-based parser without the drawback of a restriction to local features. By formulating inference as an ILP, nonlocal features can be easily accommodated in our model; furthermore, by using a relaxation technique we can still make learning tractable. The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009). A linear program (LP) is an optimization problem of the form If the problem is feasible, the optimum is attained at a vertex of the polyhedron that defines the constraint space. If we add the constraint x E Zd, then the above is called an integer linear program (ILP). For some special parameter settings—e.g., when b is an integer vector and A is totally unimodular5—all vertices of the constraining polyhedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003). Of course, this need not happen: solving a general ILP is an NP-complete problem. Despite this fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Since it is intractable to throw in all constraints at once, they propose a cuttingplane algorithm, where the cycle constraints are only invoked when violated by the current solution. The resulting algorithm is still slow, and an arc-factored model is used as a surrogate during training (i.e., the hard constraints are only used at test time), which implies a discrepancy between the model that is optimized and the one that is actually going to be used. Here, we propose ILP formulations that eliminate the need for cycle constraints; in fact, they require only a polynomial number of constraints. Not only does our model allow expert knowledge to be injected in the form of constraints, it is also capable of learning soft versions of those constraints from data; indeed, it can handle features that are not arc-factored (correlating, for example, siblings and grandparents, modeling valency, or preferring nearly projective parses). While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NPhard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. In this paper, we focus on unlabeled dependency parsing, for clarity of exposition. If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size. We start by describing our constraint space. Our formulations rely on a concise polyhedral representation of the set of candidate dependency parse trees, as sketched in §2.1. This will be accomplished by drawing an analogy with a network flow problem. Let D = (V, A) be the complete directed graph S+(v) , {hi, ji ∈ A  |i = v} denote its set of outgoing arcs. The two first conditions can be easily expressed by linear constraints on the incidence vector z: Condition 3 is somewhat harder to express. Rather than adding exponentially many constraints, one for each potential cycle (like Riedel and Clarke, 2006), we equivalently replace condition 3 by 30. B is connected. Note that conditions 1-2-3 are equivalent to 1-230, in the sense that both define the same set Y(x). However, as we will see, the latter set of conditions is more convenient. Connectedness of graphs can be imposed via flow constraints (by requiring that, for any v ∈ V \ {0}, there is a directed path in B connecting 0 to v). We adapt the single commodity flow formulation for the (undirected) minimum spanning tree problem, due to Magnanti and Wolsey (1994), that requires O(n2) variables and constraints. Under this model, the root node must send one unit of flow to every other node. By making use of extra variables, 0i , h0aiaEA, to denote the flow of commodities through each arc, we are led to the following constraints in addition to Eqs. 4–5 (we denote U , [0, 1], and B , {0, 1} = U ∩ Z): These constraints project an outer bound of the arborescence polytope, i.e., Furthermore, the integer points of �Z(x) are precisely the incidence vectors of dependency trees in Y(x); these are obtained by replacing Eq. 9 by za ∈ B, a ∈ A. (11) Given our polyhedral representation of (an outer bound of) the arborescence polytope, we can now formulate dependency parsing with an arcfactored model as an ILP. By storing the arclocal feature vectors into the columns of a matrix F(x) , [fa(x)]aEA, and defining the score vector s , F(x)Tw (each entry is an arc score) the inference problem can be written as where A is a sparse constraint matrix (with O(|A|) non-zero elements), and b is the constraint vector; A and b encode the constraints (4–9). This is an ILP with O(|A|) variables and constraints (hence, quadratic in n); if we drop the integer constraint the problem becomes the LP relaxation. As is, this formulation is no more attractive than solving the problem with the existing combinatorial algorithms discussed in §2.2; however, we can now start adding non-local features to build a more powerful model. To cope with higher-order features of the form fa1,...,aK(x) (i.e., features whose values depend on the simultaneous inclusion of arcs a1, ... , aK on a candidate dependency tree), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables zal...aK , zal ∧...∧zaK. This logical relation can be expressed by the following O(K) agreement constraints:6 As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki  |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki  |hi, ji such features in our formulation, we need to add extra variables zsibl , hzrir∈Rsibl and zgrand , hzrir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator variables, i.e., zsibl Hence, these features can be handled in our formulation by adding the following O(|A |· |V |) variables and constraints: for all triples hi, j, ki ∈ Rgrand. Let R , A ∪ Rsibl ∪ Rgrand; by redefining z , hzrir∈R and F(x) , [fr(x)]r∈R, we may express our inference problem as in Eq. 12, with O(|A |· |V |) variables and constraints. Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features 6Actually, any logical condition can be encoded with linear constraints involving binary variables; see e.g. Clarke and Lapata (2008) for an overview. 7By sibling features we mean features that depend on pairs of sibling arcs (i.e., of the form (i, j) and (i, k)); by grandparent features we mean features that depend on pairs of grandparent arcs (of the form (i, j) and (j, k)). can be incorporated in our model by adding “dynamic” constraints to our ILP. Define: zfirst child , ij 0 otherwise. but this would yield a constraint matrix with O(n4) non-zero elements. Instead, we define auxiliary variables βjk and γij: sibl ijk z γi(j+1)≤ γij +zij analogously for the case n Then, we have that The following constraints encode the logical relations for the auxiliary vari Auxiliary variables and constraints are defined A crucial fact about dependency grammars is that words have preferences about the number and arrangement of arguments an d modifiers they accept. Therefore, it is desirable to include features that indicate, for a candidate arborescence, how many outgoing arcs depart from each vertex; denote these quantities by vi , Pa∈δ+(i) za, for each i ∈ V . We call vi the valency of the ith vertex. We add valency indicators zval ik , ff(vi = k) for i ∈ V and k = 0,... , n − 1. This way, we are able to penalize candidate dependency trees that assign unusual valencies to some of their vertices, by specifying a individual cost for each possible value of valency. The following O(|V |2) constraints encode the agreement between valency indicators and the other variables: For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3) variables and constraints. In this model, every node k =6 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22): where δk j, ff(j = k) is the Kronecker delta. We next define auxiliary variables ψjk that indicate if there is a path from j to k. Since each vertex except the root has only one incoming arc, the following linear equalities are enough to describe these new variables: a , ff(a ∈ y and a is nonprojective). From the definition of projective arcs in §2.1, we have that znp There are other ways to introduce nonprojectivity indicators and alternative definitions of “nonprojective arc.” For example, by using dynamic constraints of the same kind as those in §3.3, we can indicate arcs that “cross” other arcs with O(n3) variables and constraints, and a cubic number of non-zero elements in the constraint matrix (omitted for space). It would be straightforward to adapt the constraints in §3.5 to allow only projective parse trees: simply force znp a = 0 for any a ∈ A. But there are more efficient ways of accomplish this. While it is difficult to impose projectivity constraints or cycle constraints individually, there is a simpler way of imposing both. Consider 3 (or 30) from §3.1. 300. If hi, ji ∈ B, then, for any k = 1, ... , n such that k =6 j, the parent of k must satisfy (defining i0 , min(i, j) and j0 , max(i, j)): Then, Y(x) will be redefined as the set ofprojective dependency parse trees. We omit the proof for space. Conditions 1, 2, and 3&quot; can be encoded with O(n2) constraints. We report experiments on seven languages, six (Danish, Dutch, Portuguese, Slovene, Swedish and Turkish) from the CoNLL-X shared task (Buchholz and Marsi, 2006), and one (English) from the CoNLL-2008 shared task (Surdeanu et al., 2008).8 All experiments are evaluated using the unlabeled attachment score (UAS), using the default settings.9 We used the same arc-factored features as McDonald et al. (2005) (included in the MSTParser toolkit10); for the higher-order models described in §3.3–3.5, we employed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V  |� |A|) constraints and variables, which, when A = V 2, grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8We used the provided train/test splits except for English, for which we tested on the development partition. For training, sentences longer than 80 words were discarded. For testing, all sentences were kept (the longest one has length 118). 11Note that, unlike reranking approaches, there are still exponentially many candidate parse trees after pruning. The oracle constrained to pick parents from these lists achieves > 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observe that, for some languages, the valency features do not seem to help. Merely modeling the number of dependents of a word may not be as valuable as knowing what kinds of dependents they are (for example, distinguishing among arguments and adjuncts). Comparing with the baselines, we observe that our full model outperforms that of McDonald and Pereira (2006), and is in line with the most accurate dependency parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers). Unlike the ensembles, it directly handles non-local output features by optimizing a single global objective. Perhaps more importantly, it makes it possible to exploit expert knowledge through the form of hard global constraints. Although not pursued here, the same kind of constraints employed by Riedel and Clarke (2006) can straightforwardly fit into our model, after extending it to perform labeled dependency parsing. We believe that a careful design of fea13Unlike our model, the hybrid models used here as baselines make use of the dependency labels at training time; indeed, the transition-based parser is trained to predict a labeled dependency parse tree, and the graph-based parser use these predicted labels as input features. Our model ignores this information at training time; therefore, this comparison is slightly unfair to us. model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008). The four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features (see §3.2–§3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according to Dan Bikel’s randomized method (http://www.cis.upenn.edu/-dbikel/software.html). tures and constraints can lead to further improvements on accuracy. We now turn to a different issue: scalability. In previous work (Martins et al., 2009), we showed that training the model via LP-relaxed inference (as we do here) makes it learn to avoid fractional solutions; as a consequence, ILP solvers will converge faster to the optimum (on average). Yet, it is known from worst case complexity theory that solving a general ILP is NP-hard; hence, these solvers may not scale well with the sentence length. Merely considering the LP-relaxed version of the problem at test time is unsatisfactory, as it may lead to a fractional solution (i.e., a solution whose components indexed by arcs, z = (z-)-EA, are not all integer), which does not correspond to a valid dependency tree. We propose the following approximate algorithm to obtain an actual parse: first, solve the LP relaxation (which can be done in polynomial time with interior-point methods); then, if the solution is fractional, project it onto the feasible set Y(x). Fortunately, the Euclidean projection can be computed in a straightforward way by finding a maximal arborescence in the directed graph whose weights are defined by z (we omit the proof for space); as we saw in §2.2, the ChuLiu-Edmonds algorithm can do this in polynomial time. The overall parsing runtime becomes polynomial with respect to the length of the sentence. The last column of Table 1 compares the accuracy of this approximate method with the exact one. We observe that there is not a substantial drop in accuracy; on the other hand, we observed a considerable speed-up with respect to exact inference, particularly for long sentences. The average runtime (across all languages) is 0.632 seconds per sentence, which is in line with existing higher-order parsers and is much faster than the runtimes reported by Riedel and Clarke (2006). We presented new dependency parsers based on concise ILP formulations. We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints. These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints. Learning through a max-margin framework is made effective by the means of a LPrelaxation. Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.","We presented new dependency parsers based on concise ILP formulations. We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints. These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints. Learning through a max-margin framework is made effective by the means of a LPrelaxation. Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration."
2,"We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.","We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources. Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models. We will describe it in detail in Section 4. In this architecture, knowledge sources that are intractable to incorporate into the perceptron, can be easily incorporated into the outside linear model. In addition, as these knowledge sources are regarded as separate features, we can train their corresponding models independently with each other. This is an interesting approach when the training corpus is large as it reduces the time and space consumption. Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T. 2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l < r) denotes character sequence ranges from Cl to Cr. We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag. It is a better idea to perform segmentation and POS tagging jointly in a uniform framework. According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively. In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). As each tag is now composed of a boundary part and a POS part, the joint S&T problem is transformed to a uniform boundary-POS labelling problem. A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN. The perceptron algorithm introduced into NLP by Collins (2002), is a simple but effective discriminative training method. It has comparable performance to CRFs, while with much faster training. The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on. We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T. In following subsections, we describe the feature templates and the perceptron training algorithm. The feature templates we adopted are selected from those of Ng and Low (2004). To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation. All feature templates and their instances are shown in Table 1. C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0). Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target. We called them non-lexical-target because predications derived from them can predicate without considering the current character C0. Templates in the column below are expanded from the upper ones. We add a field C0 to each template in the upper column, so that it can carry out predication according to not only the context but also the current character itself. As predications generated from such templates depend on the current character, we name these templates lexical-target. Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails. We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model. For an input character sequence x, we aim to find an output F(x) satisfying: vector 4)(x, y) and the parameter vector a. We used the algorithm depicted in Algorithm 1 to tune the parameter vector a. To alleviate overfitting on the training examples, we use the refinement strategy called “averaged parameters” (Collins, 2002) to the algorithm in Algorithm 1. In theory, any useful knowledge can be incorporated into the perceptron directly, besides the characterbased features already adopted. Additional features most widely used are related to word or POS ngrams. However, such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly. Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones. We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones, not to mention the higher-order grams such as trigrams or 4-grams. In addition, even though these higher grams were managed to be used, there still remains another problem: as the current predication relies on the results of prior ones, the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position, which evokes a potential risk to depress the training. To alleviate the drawbacks, we propose a cascaded linear model. It has a two-layer architecture, with a perceptron as the core and another linear model as the outside-layer. Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron. Since the perceptron is fixed during the second training step, the whole training procedure need relative small time and memory cost. The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates. In this layer, each knowledge source is treated as a feature with a corresponding weight denoting its relative importance. Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability. As all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms. In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence. As shown in Figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer. Besides the output of the perceptron, the outside-layer also receive the outputs of the word LM, the POS LM, the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT. Language model (LM) provides linguistic probabilities of a word sequence. It is an important measure of fluency of the translation in SMT. Formally, an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM. Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence. Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: λwt and λtw denote the corresponding weights of the two components. Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus. For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach. To facilitate tuning the weights, we use two components of the co-occurrence model Co(W,T) to represent the co-occurrence probability of W and T, rather than use Co(W, T) itself. In the rest of the paper, we will call them labelling model and generating model respectively. Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure. In Chinese Joint S&T, the mission of the decoder is to find the boundary-POS labelled sequence with the highest score. Given a Chinese character sequence C1:n, the decoding procedure can proceed in a left-right fashion with a dynamic programming approach. By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding. At each position i, we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i, K) (K is assigned 20 in all our experiments) and ending at position i, then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l), and select for position i a N-best list of candidate results from all these candidates. When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model. In addition, we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates. By equation 2, we can synthetically evaluate all these scores to perform more accurately comparing between candidates. Algorithm 2 shows the decoding algorithm. Lines 3 — 11 generate a N-best list for each character position i. Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character). Line 6 enumerates all POS’s for the word w spanning length l and ending at position i. Line 8 considers each candidate result in N-best list at prior position of the current word. Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p. We reported results from two set of experiments. The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR). The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T. In all experiments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure. With precision P and recall R, the balance F-measure is defined as: F = 2PR/(P + R). For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus. In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features. Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations. We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7. Then we trained LEX on each of the four corpora for 7 iterations. Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR). On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). However, the accuracy on PKU corpus is obvious lower than the best score SIGHAN reported, we need to conduct further research on this problem. We turned to experiments on CTB 5.0 to test the performance of the cascaded model. According to the usual practice in syntactic analysis, we choose chapters 1 − 260 (18074 sentences) as training set, chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set. At the first step, we conducted a group of contrasting experiments on the core perceptron, the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only, while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T. Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled. The evaluation results are shown in Table 3. We find that Joint S&T can also improve the segmentation accuracy. However, the F-measure on Joint S&T is obvious lower, about a rate of 95% to the F-measure on segmentation. Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation. As the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs. Here the core perceptron was just the POS+ model in experiments above. Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model. We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing. To obtain their corresponding weights, we adapted the minimum-error-rate training algorithm (Och, 2003) to train the outside-layer model. In order to inspect how much improvement each feature brings into the cascaded model, every time we removed a feature while retaining others, then retrained the model and tested its performance on the test set. Table 4 shows experiments results. We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T, over the perceptron-only model POS+. We also find that the perceptron model functions as the kernel of the outside-layer linear model. Without the perceptron, the cascaded model (if we can still call it “cascaded”) performs poorly on both segmentation and Joint S&T. Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T. Another important feature is the labelling model. Without it, the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points. The generating model, which functions as that in HMM, brings an improvement of about 0.1 points to each test item. However unlike the three features, the word LM brings very tiny improvement. We suppose that the character-based features used in the perceptron play a similar role as the lowerorder word LM, and it would be helpful if we train a higher-order word LM on a larger scale corpus. Finally, the word count penalty gives improvement to the cascaded model, 0.13 points on segmentation and 0.16 points on Joint S&T. In summary, the cascaded model can utilize these knowledge sources effectively, without causing the feature space of the percptron becoming even larger. Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&T, with error reductions of 18.5% and 12% respectively. We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model. This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. However, can the perceptron incorporate all the knowledge used in the outside-layer linear model? If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)? In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). How can we utilize these knowledge sources effectively? We will investigate these problems in the following work.","We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model. This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. However, can the perceptron incorporate all the knowledge used in the outside-layer linear model? If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)? In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). How can we utilize these knowledge sources effectively? We will investigate these problems in the following work."
3,"In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.","In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restrict the target side to the so called wellformed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. We incorporate a dependency language model during decoding, in order to exploit long-distance word relations which are unavailable with a traditional n-gram language model on target strings. For comparison purposes, we replicated the Hiero decoder (Chiang, 2005) as our baseline. Our stringto-dependency decoder shows 1.48 point improvement in BLEU and 2.53 point improvement in TER on the NIST 04 Chinese-English MT evaluation set. In the rest of this section, we will briefly discuss previous work on hierarchical MT and dependency representations, which motivated our research. In section 2, we introduce the model of string-to-dependency decoding. Section 3 illustrates of the use of dependency language models. In section 4, we describe the implementation details of our MT system. We discuss experimental results in section 5, compare to related work in section 6, and draw conclusions in section 7. Graehl and Knight (2004) proposed the use of targettree-to-source-string transducers (xRS) to model translation. In xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) followed the tree-to-tree approach (Shieber and Schabes, 1990) for translation. In their models, dependency treelets are used to represent both the source and the target sides. Decoding is implemented as tree transduction preceded by source side dependency parsing. While tree-to-tree models can represent richer structural information, existing tree-totree models did not show advantage over string-totree models on translation accuracy due to a much larger search space. One of the motivations of our work is to achieve desirable trade-off between model capability and search space through the use of the so called wellformed dependency structures in rule representation. Dependency trees reveal long-distance relations between words. For a given sentence, each word has a parent word which it depends on, except for the root word. Figure 1 shows an example of a dependency tree. Arrows point from the child to the parent. In this example, the word find is the root. Dependency trees are simpler in form than CFG trees since there are no constituent labels. However, dependency relations directly model semantic structure of a sentence. As such, dependency trees are a desirable prior model of the target sentence. We restrict ourselves to the so-called well-formed target dependency structures based on the following considerations. In (Ding and Palmer, 2005; Quirk et al., 2005), there is no restriction on dependency treelets used in transfer rules except for the size limit. This may result in a high dimensionality in hypothesis representation and make it hard to employ shared structures for efficient dynamic programming. In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots. Therefore, the search space becomes much smaller. Furthermore, shared structures can be easily defined based on the labels of the slots. In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be attached by specifying NTs in dependency trees. Marcu et al. (2006) showed that many useful phrasal rules cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al., 2006). For example, the following rule A number of techniques have been proposed to improve rule coverage. (Marcu et al., 2006) and (Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. A string-to-dependency grammar G is a 4-tuple G =< R, X, Tf, Te >, where R is a set of transfer rules. X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007). Tf is a set of terminals in the source language, and Te is a set of terminals in the target language1. A string-to-dependency transfer rule R E R is a 4-tuple R =< 5f, 5e, D, A >, where 5f E (Tf U {X})+ is a source string, 5e E (Te U {X})+ is a target string, D represents the dependency structure for 5e, and A is the alignment between 5f and 5e. Non-terminal alignments in A must be one-to-one. In order to exclude undesirable structures, we only allow 5e whose dependency structure D is well-formed, which we will define below. In addition, the same well-formedness requirement will be applied to partial decoding results. Thus, we will be able to employ shared structures to merge multiple partial results. Based on the results in previous work (DeNeefe et al., 2007), we want to keep two kinds of dependency structures. In one kind, we keep dependency trees with a sub-root, where all the children of the sub-root are complete. We call them fixed dependency structures because the head is known or fixed. In the other, we keep dependency structures of sibling nodes of a common head, but the head itself is unspecified or floating. Each of the siblings must be a complete constituent. We call them floating dependency structures. Floating structures can represent many linguistically meaningful non-constituent structures: for example, like the red, a modifier of a noun. Only those two kinds of dependency structures are well-formed structures in our system. Furthermore, we operate over well-formed structures in a bottom-up style in decoding. However, the description given above does not provide a clear definition on how to combine those two types of structures. In the rest of this section, we will provide formal definitions of well-formed structures and combinatory operations over them, so that we can easily manipulate well-formed structures in decoding. Formal definitions also allow us to easily extend the framework to incorporate a dependency language model in decoding. Examples will be provided along with the formal definitions. Consider a sentence 5 = w1w2...wn. Let d1d2...dn represent the parent word IDs for each word. For example, d4 = 2 means that w4 depends 'We ignore the left hand side here because there is only one non-terminal X. Of course, this formalism can be extended to have multiple NTs. Definition 1 A dependency structure di..j is fixed on head h, where h E [i, j], or fixed for short, if and only if it meets the following conditions In addition, we say the category of di..j is (−, h, −), where − means this field is undefined. We say the category of di..j is (C, −, −) if j < h, or (−, −, C) otherwise. A category is composed of the three fields (A, h, B), where h is used to represent the head, and A and B are designed to model left and right dependents of the head respectively. A dependency structure is well-formed if and only if it is either fixed or floating. We can represent dependency structures with graphs. Figure 2 shows examples of fixed structures, Figure 3 shows examples of floating structures, and Figure 4 shows ill-formed dependency structures. It is easy to verify that the structures in Figures 2 and 3 are well-formed. 4(a) is ill-formed because boy does not have its child word the in the tree. 4(b) is ill-formed because it is not a continuous segment. As for the example the red mentioned above, it is a well-formed floating dependency structure. One of the purposes of introducing floating dependency structures is that siblings having a common parent will become a well-defined entity, although they are not considered a constituent. We always build well-formed partial structures on the target side in decoding. Furthermore, we combine partial dependency structures in a way such that we can obtain all possible well-formed but no ill-formed dependency structures during bottom-up decoding. The solution is to employ categories introduced above. Each well-formed dependency structure has a category. We can apply four combinatory operations over the categories. If we can combine two categories with a certain category operation, we can use a corresponding tree operation to combine two dependency structures. The category of the combined dependency structure is the result of the combinatory category operations. We first introduce three meta category operations. Two of them are unary operations, left raising (LR) and right raising (RR), and one is the binary operation unification (UF). First, the raising operations are used to turn a completed fixed structure into a floating structure. It is easy to verify the following theorem according to the definitions. Therefore we can always raise a fixed structure if we assume it is complete, i.e. (1) holds. Unification is well-defined if and only if we can unify all three elements and the result is a valid fixed or floating category. For example, we can unify a fixed structure with a floating structure or two floating structures in the same direction, but we cannot unify two fixed structures. Next we introduce the four tree operations on dependency structures. Instead of providing the formal definition, we use figures to illustrate these operations to make it easy to understand. Figure 1 shows a traditional dependency tree. Figure 5 shows the four operations to combine partial dependency structures, which are left adjoining (LA), right adjoining (RA), left concatenation (LC) and right concatenation (RC). Child and parent subtrees can be combined with adjoining which is similar to the traditional dependency formalism. We can either adjoin a fixed structure or a floating structure to the head of a fixed structure. Complete siblings can be combined via concatenation. We can concatenate two fixed structures, one fixed structure with one floating structure, or two floating structures in the same direction. The flexibility of the order of operation allows us to take adWe use the same names for the operations on categories for the sake of convenience. We can easily use the meta category operations to define the four combinatory operations. The definition of the operations in the left direction is as follows. Those in the right direction are similar. It is easy to verify the soundness and completeness of category operations based on one-to-one mapping of the conditions in the definitions of corresponding operations on dependency structures and on categories. Suppose we have a dependency tree for a red apple, where both a and red depend on apple. There are two ways to compute the category of this string from the bottom up. cat(Da red apple) = LA(cat(Da),LA(cat(Dred),cat(Dapple))) = LA(LC(cat(Da), cat(Dred)), cat(Dapple)) Based on Theorem 2, it follows that combinatory operation of categories has the confluence property, since the result dependency structure is determined. Corollary 1 (confluence) The category of a wellformed dependency tree does not depend on the order of category calculation. With categories, we can easily track the types of dependency structures and constrain operations in decoding. For example, we have a rule with dependency structure find +— X, where X right adjoins to find. Suppose we have two floating structures2, We can replace X by X2, but not by X1 based on the definition of category operations. Now we explain how we get the string-todependency rules from training data. The procedure is similar to (Chiang, 2007) except that we maintain tree structures on the target side, instead of strings. Given sentence-aligned bi-lingual training data, we first use GIZA++ (Och and Ney, 2003) to generate word level alignment. We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magerman’s rules (1995). Then we use heuristic rules to extract transferrules recursively based on the GIZA alignment and the target dependency trees. The rule extraction procedure is as follows. All the 4-tuples (P? ',� phrase alignments, where source phrase P ? ',� �is e under alignment3 A, and D, the dependency structure for P m,n e , is well-formed. All valid phrase templates are valid rules templates. Let (Pi,j f , Pm,n e , D1, A) be a valid rule template, and (Pp,q f , Ps,t e , D2, A) a valid phrase alignment, where [p, q] C [i, j], [s, t] C [m, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned. We create a new valid rule template (P0 f, P0e, D0, A), where we obtain Pf0 by replacing Pp,q f with label X in Pi,j f , and obtain Among all valid rule templates, we collect those that contain at most two NTs and at most seven elements in the source as transfer rules in our system. Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing. We view target dependency as the hidden structure of source fragments. The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side. Once there is a completed rule, we build a larger dependency structure by substituting component dependency structures for corresponding NTs in the target dependency structure of rules. Hypothesis dependency structures are organized in a shared forest, or AND-OR structures. An ANDf aligned to Pm,n e , we mean all words in Pi,j f are either aligned to words in Pm,n e or unaligned, and vice versa. Furthermore, at least one word in Pi,j structure represents an application of a rule over component OR-structures, and an OR-structure represents a set of alternative AND-structures with the same state. A state means a n-tuple that characterizes the information that will be inquired by up-level AND-structures. Supposing we use a traditional tri-gram language model in decoding, we need to specify the leftmost two words and the rightmost two words in a state. Since we only have a single NT X in the formalism described above, we do not need to add the NT label in states. However, we need to specify one of the three types of the dependency structure: fixed, floating on the left side, or floating on the right side. This information is encoded in the category of the dependency structure. In the next section, we will explain how to extend categories and states to exploit a dependency language model during decoding. For the dependency tree in Figure 1, we calculate the probability of the tree as follows ×PL(will|find-as-head) ×PL(boy|will, find-as-head) ×PL(the|boy-as-head) ×PR(it|find-as-head) ×PR(interesting|it, find-as-head) Here PT(x) is the probability that word x is the root of a dependency tree. PL and PR are left and right side generative probabilities respectively. Let wh be the head, and wL1wL2...wLn be the children on the left side from the nearest to the farthest. Suppose we use a tri-gram dependency LM, wh-as-head represents wh used as the head, and it is different from wh in the dependency language model. The right side probability is similar. In order to calculate the dependency language model score, or depLM score for short, on the fly for partial hypotheses in a bottom-up decoding, we need to save more information in categories and states. We use a 5-tuple (LF, LN, h, RN, RF) to represent the category of a dependency structure. h represents the head. LF and RF represent the farthest two children on the left and right sides respectively. Similarly, LN and RN represent the nearest two children on the left and right sides respectively. The three types of categories are as follows. Similar operations as described in Section 2.2 are used to keep track of the head and boundary child nodes which are then used to compute depLM scores in decoding. Due to the limit of space, we skip the details here. 8. Discount on ill-formed dependency structures We have eight features in our system. The values of the first four features are accumulated on the rules used in a translation. Following (Chiang, 2005), we also use concatenation rules like X —* XX for backup. The 5th feature counts the number of concatenation rules used in a translation. In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions. We tune the weights with several rounds of decoding-optimization. Following (Och, 2003), the k-best results are accumulated as the input of the optimizer. Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration. Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline. We carried out experiments on three models. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems use only one non-terminal label in rules. The major difference is in the representation of target structures. We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding. All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset was also used to train a 3-gram dependency LM. Traditional 3-gram and 5-gram LMs were trained on a corpus of 6G words composed of the LDC Gigaword corpus and text downloaded from Web (Bulyko et al., 2007). We tuned the weights on NIST MT05 and tested on MT04. Table 1 shows the number of transfer rules extracted from the training data for the tuning and test sets. The constraint of well-formed dependency structures greatly reduced the size of the rule set. Although the rule size increased a little bit after incorporating dependency structures in rules, the size of string-to-dependency rule set is less than 20% of the baseline rule set size. Table 2 shows the BLEU and TER scores on MT04. On decoding output, the string-todependency system achieved 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to the baseline hierarchical stringto-string system. After 5-gram rescoring, it achieved 1.21 point improvement in BLEU and 1.19 improvement in TER. The filtered model does not show improvement on BLEU. The filtered string-to-string rules can be viewed the string projection of stringto-dependency rules. It means that just using dependency structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P(FIE) was employed in the construction of the target forest due to the complexity of the syntax-based LM. Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system. This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees. The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations. The depLM is used in a bottom-up style, while SLM is employed in a left-to-right style. In this paper, we propose a novel string-todependency algorithm for statistical machine translation. For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005). Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set. Dependency structures provide a desirable platform to employ linguistic knowledge in MT. In the future, we will continue our research in this direction to carry out translation with deeper features, for example, propositional structures (Palmer et al., 2005). We believe that the fixed and floating structures proposed in this paper can be extended to model predicates and arguments.","In this paper, we propose a novel string-todependency algorithm for statistical machine translation. For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005). Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set. Dependency structures provide a desirable platform to employ linguistic knowledge in MT. In the future, we will continue our research in this direction to carry out translation with deeper features, for example, propositional structures (Palmer et al., 2005). We believe that the fixed and floating structures proposed in this paper can be extended to model predicates and arguments."
4,"We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score.","We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one problem that still has not found a satisfactory solution in data-driven dependency parsing is the treatment of discontinuous syntactic constructions, usually modeled by non-projective dependency trees, as illustrated in Figure 1. In a projective dependency tree, the yield of every subtree is a contiguous substring of the sentence. This is not the case for the tree in Figure 1, where the subtrees rooted at node 2 (hearing) and node 4 (scheduled) both have discontinuous yields. Allowing non-projective trees generally makes parsing computationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency parsing typically use one of two strategies to deal with non-projective trees (unless they ignore them completely). Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover nonprojective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency parsing in linear time; we then extend the system with a single transition for swapping the order of words in the input and demonstrate that the extended system can be used to parse unrestricted dependency trees with a time complexity that is quadratic in the worst case but still linear in the best case. In Section 4, we present experiments indicating that the expected running time of the new system on naturally occurring data is in fact linear and that the system achieves state-ofthe-art parsing accuracy. We discuss related work in Section 5 and conclude in Section 6. Given a set L of dependency labels, a dependency graph for a sentence x = w1, ... , wn is a directed graph G = (Vx, A), where The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra artificial root node 0. The set A of arcs is a set of triples (i, l, j), where i and j are nodes and l is a label. For a dependency graph G = (Vx, A) to be well-formed, we in addition require that it is a tree rooted at the node 0, as illustrated in Figure 1. Following Nivre (2008a), we define a transition system for dependency parsing as a quadruple 5 = (C, T, cs, Ct), where In this paper, we take the set C of configurations to be the set of all triples c = (E, B, A) such that E and B are disjoint sublists of the nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and some label set L); we take the initial configuration for a sentence x = w1, ... , wn to be cs(x) = ([0], [1, ... , n], 11); and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], A) (for any arc set A). The set T of transitions will be discussed in detail in Sections 3.1–3.2. We will refer to the list E as the stack and the list B as the buffer, and we will use the variables Q and 0 for arbitrary sublists of E and B, respectively. For reasons of perspicuity, we will write E with its head (top) to the right and B with its head to the left. Thus, c = ([QIi], [j10], A) is a configuration with the node i on top of the stack E and the node j as the first node in the buffer B. Given a transition system 5 = (C, T, cs, Ct), a transition sequence for a sentence x is a sequence C0,m = (c0, c1, ... , cm) of configurations, such that The parse assigned to S by C0,m is the dependency graph Gcm = (Vx, Acm), where Acm is the set of arcs in cm. A transition system S is sound for a class G of dependency graphs iff, for every sentence x and transition sequence C0,m for x in S, Gcm E G. S is complete for G iff, for every sentence x and dependency graph G for x in G, there is a transition sequence C0,m for x in S such that Gcm = G. An oracle for a transition system S is a function o : C —* T. Ideally, o should always return the optimal transition t for a given configuration c, but all we require formally is that it respects the preconditions of transitions in T. That is, if o(c) = t then t is permissible in c. Given an oracle o, deterministic transition-based parsing can be achieved by the following simple algorithm: Starting in the initial configuration cs(x), the parser repeatedly calls the oracle function o for the current configuration c and updates c according to the oracle transition t. The iteration stops when a terminal configuration is reached. It is easy to see that, provided that there is at least one transition sequence in S for every sentence, the parser constructs exactly one transition sequence C0,m for a sentence x and returns the parse defined by the terminal configuration cm, i.e., Gcm = (Vx, Acm). Assuming that the calls o(c) and t(c) can both be performed in constant time, the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S. When building practical parsing systems, the oracle can be approximated by a classifier trained on treebank data, a technique that has been used successfully in a number of systems (Yamada and Matsumoto, 2003; Nivre et al., 2004; Attardi, 2006). This is also the approach we will take in the experimental evaluation in Section 4. Having defined the set of configurations, including initial and terminal configurations, we will now focus on the transition set T required for dependency parsing. The total set of transitions that will be considered is given in Figure 2, but we will start in Section 3.1 with the subset Tp (p for projective) consisting of the first three. In Section 3.2, we will add the fourth transition (SWAP) to get the full transition set Tu (u for unrestricted). The minimal transition set Tp for projective dependency parsing contains three transitions: The system Sp = (C, Tp, cs, Ct) is sound and complete for the set of projective dependency trees (over some label set L) and has been used, in slightly different variants, by a number of transition-based dependency parsers (Yamada and Matsumoto, 2003; Nivre, 2004; Attardi, 2006; Nivre, 2008a). For proofs of soundness and completeness, see Nivre (2008a). As noted in section 2, the worst-case time complexity of a deterministic transition-based parser is given by an upper bound on the length of transition sequences. In 5p, the number of transitions for a sentence x = w1, ... , wn is always exactly 2n, since a terminal configuration can only be reached after n SHIFT transitions (moving nodes 1, ... , n from B to E) and n applications of LEFT-ARCl or RIGHT-ARCl (removing the same nodes from E). Hence, the complexity of deterministic parsing is O(n) in the worst case (as well as in the best case). We now consider what happens when we add the fourth transition from Figure 2 to get the extended transition set T,. The SWAP transition updates a configuration with stack [aJi, j] by moving the node i back to the buffer. This has the effect that the order of the nodes i and j in the appended list E+B is reversed compared to the original word order in the sentence. It is important to note that SWAP is only permissible when the two nodes on top of the stack are in the original word order, which prevents the same two nodes from being swapped more than once, and when the leftmost node i is distinct from the root node 0. Note also that SWAP moves the node i back to the buffer, so that LEFT-ARCl, RIGHT-ARCl or SWAP can subsequently apply with the node j on top of the stack. The fact that we can swap the order of nodes, implicitly representing subtrees, means that we can construct non-projective trees by applying to denote the subset of A that only contains the outgoing arcs of the node i. LEFT-ARCl or RIGHT-ARCl to subtrees whose yields are not adjacent according to the original word order. This is illustrated in Figure 3, which shows the transition sequence needed to parse the example in Figure 1. For readability, we represent both the stack E and the buffer B as lists of tokens, indexed by position, rather than abstract nodes. The last column records the arc that is added to the arc set A in a given transition (if any). Given the simplicity of the extension, it is rather remarkable that the system Su = (C, Tu, cs, Ct) is sound and complete for the set of all dependency trees (over some label set L), including all non-projective trees. The soundness part is trivial, since any terminating transition sequence will have to move all the nodes 1, ... , n from B to E (using SHIFT) and then remove them from E (using LEFT-ARCl or RIGHT-ARCl), which will produce a tree with root 0. For completeness, we note first that projectivity is not a property of a dependency tree in itself, but of the tree in combination with a word order, and that a tree can always be made projective by reordering the nodes. For instance, let x be a sentence with dependency tree G = (Vx, A), and let <G be the total order on Vx defined by an inorder traversal of G that respects the local ordering of a node and its children given by the original word order. Regardless of whether G is projective with respect to x, it must by necessity be projective with respect to <G. We call <G the projective order corresponding to x and G and use it as our canonical way of finding a node order that makes the tree projective. By way of illustration, the projective order for the sentence and tree in Figure 1 is: A1 <G hearing2 <G on5 <G the6 <G issue7 <G is3 <G scheduled4 <G today8 <G .9. If the words of a sentence x with dependency tree G are already in projective order, this means that G is projective with respect to x and that we can parse the sentence using only transitions in Tp„ because nodes can be pushed onto the stack in projective order using only the SHIFT transition. If the words are not in projective order, we can use a combination of SHIFT and SWAP transitions to ensure that nodes are still pushed onto the stack in projective order. More precisely, if the next node in the projective order is the kth node in the buffer, we perform k SHIFT transitions, to get this node onto the stack, followed by k−1 SWAP transitions, to move the preceding k − 1 nodes back to the buffer.1 In this way, the parser can effectively sort the input nodes into projective order on the stack, repeatedly extracting the minimal element of <G from the buffer, and build a tree that is projective with respect to the sorted order. Since any input can be sorted using SHIFT and SWAP, and any projective tree can be built using SHIFT, LEFT-ARCl and RIGHT-ARCl, the system Su is complete for the set of all dependency trees. In Figure 4, we define an oracle function o for the system Su, which implements this “sort and parse” strategy and predicts the optimal transition t out of the current configuration c, given the target dependency tree G = (Vx, A) and the projective order <G. The oracle predicts LEFT-ARCl or RIGHT-ARCl if the two top nodes on the stack should be connected by an arc and if the dependent node of this arc is already connected to all its dependents; it predicts SWAP if the two top nodes are not in projective order; and it predicts SHIFT otherwise. This is the oracle that has been used to generate training data for classifiers in the experimental evaluation in Section 4. Let us now consider the time complexity of the extended system Su = (C, Tu, cs, Ct) and let us begin by observing that 2n is still a lower bound on the number of transitions required to reach a terminal configuration. A sequence of 2n transitions occurs when no SWAP transitions are performed, in which case the behavior of the system is identical to the simpler system 5p. This is important, because it means that the best-case complexity of the deterministic parser is still O(n) and that the we can expect to observe the best case for all sentences with projective dependency trees. The exact number of additional transitions needed to reach a terminal configuration is determined by the number of SWAP transitions. Since SWAP moves one node from E to B, there will be one additional SHIFT for every SWAP, which means that the total number of transitions is 2n + 2k, where k is the number of SWAP transitions. Given the condition that SWAP can only apply in a configuration c = ([a|i, j], B, A) if 0 < i < j, the number of SWAP transitions is bounded by n(n�1) 2 , which means that 2n + n(n − 1) = n + n2 is an upper bound on the number of transitions in a terminating sequence. Hence, the worst-case complexity of the deterministic parser is O(n2). The running time of a deterministic transitionbased parser using the system 5,, is O(n) in the best case and O(n2) in the worst case. But what about the average case? Empirical studies, based on data from a wide range of languages, have shown that dependency trees tend to be projective and that most non-projective trees only contain a small number of discontinuities (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). This should mean that the expected number of swaps per sentence is small, and that the running time is linear on average for the range of inputs that occur in natural languages. This is a hypothesis that will be tested experimentally in the next section. Our experiments are based on five data sets from the CoNLL-X shared task: Arabic, Czech, Danish, Slovene, and Turkish (Buchholz and Marsi, 2006). These languages have been selected because the data come from genuine dependency treebanks, whereas all the other data sets are based on some kind of conversion from another type of representation, which could potentially distort the distribution of different types of structures in the data. In section 3.2, we hypothesized that the expected running time of a deterministic parser using the transition system 5,, would be linear, rather than quadratic. To test this hypothesis, we examine how the number of transitions varies as a function of sentence length. We call this the abstract running time, since it abstracts over the actual time needed to compute each oracle prediction and transition, which is normally constant but dependent on the type of classifier used. We first measured the abstract running time on the training sets, using the oracle to derive the transition sequence for every sentence, to see how many transitions are required in the ideal case. We then performed the same measurement on the test sets, using classifiers trained on the oracle transition sequences from the training sets (as described below in Section 4.2), to see whether the trained parsers deviate from the ideal case. The result for Arabic and Danish can be seen in Figure 5, where black dots represent training sentences (parsed with the oracle) and white dots represent test sentences (parsed with a classifier). For Arabic there is a very clear linear relationship in both cases with very few outliers. Fitting the data with a linear function using the least squares method gives us m = 2.06n (R2 = 0.97) for the training data and m = 2.02n (R2 = 0.98) for the test data, where m is the number of transitions in parsing a sentence of length n. For Danish, there is clearly more variation, especially for the training data, but the least-squares approximation still explains most of the variance, with m = 2.22n (R2 = 0.85) for the training data and m = 2.07n (R2 = 0.96) for the test data. For both languages, we thus see that the classifier-based parsers have a lower mean number of transitions and less variance than the oracle parsers. And in both cases, the expected number of transitions is only marginally greater than the 2n of the strictly projective transition system Sp. We have chosen to display results for Arabic and Danish because they are the two extremes in our sample. Arabic has the smallest variance and the smallest linear coefficients, and Danish has the largest variance and the largest coefficients. The remaining three languages all lie somewhere in the middle, with Czech being closer to Arabic and Slovene closer to Danish. Together, the evidence from all five languages strongly corroborates the hypothesis that the expected running time for the system Su is linear in sentence length for naturally occurring data. In order to assess the parsing accuracy that can be achieved with the new transition system, we trained a deterministic parser using the new transition system Su for each of the five languages. For comparison, we also trained two parsers using Sp, one that is strictly projective and one that uses the pseudo-projective parsing technique to recover non-projective dependencies in a post-processing step (Nivre and Nilsson, 2005). We will refer to the latter system as Spp. All systems use SVM classifiers with a polynomial kernel to approximate the oracle function, with features and parameters taken from Nivre et al. (2006), which was the best performing transition-based system in the CoNLL-X shared task.2 Table 1 shows the labeled parsing accuracy of the parsers measured in two ways: attachment score (AS) is the percentage of tokens with the correct head and dependency label; exact match (EM) is the percentage of sentences with a completely correct labeled dependency tree. The score in brackets is the attachment score for the (small) subset of tokens that are connected to their head by a non-projective arc in the gold standard parse. For comparison, the table also includes results for the two best performing systems in the original CoNLL-X shared task, Malt-06 (Nivre et al., 2006) and MST-06 (McDonald et al., 2006), as well as the integrated system MSTMalt, which is a graph-based parser guided by the predictions of a transition-based parser and currently has the best reported results on the CoNLL-X data sets (Nivre and McDonald, 2008). Looking first at the overall attachment score, we see that Su gives a substantial improvement over Sp (and outperforms Spp) for Czech and Slovene, where the scores achieved are rivaled only by the combo system MSTMalt. For these languages, there is no statistical difference between Su and MSTMalt, which are both significantly better than all the other parsers, except Spp for Czech (McNemar’s test, α = .05). This is accompanied by an improvement on non-projective arcs, where Su outperforms all other systems for Czech and is second only to the two MST parsers (MST-06 and MSTMalt) for Slovene. It is worth noting that the percentage of non-projective arcs is higher for Czech (1.9%) and Slovene (1.9%) than for any of the other languages. For the other three languages, Su has a drop in overall attachment score compared to Sp, but none of these differences is statistically significant. In fact, the only significant differences in attachment score here are the positive differences between MSTMalt and all other systems for Arabic and Danish, and the negative difference between MST-06 and all other systems for Turkish. The attachment scores for non-projective arcs are generally very low for these languages, except for the two MST parsers on Danish, but Su performs at least as well as Spp on Danish and Turkish. (The results for Arabic are not very meaningful, given that there are only eleven non-projective arcs in the entire test set, of which the (pseudo-)projective parsers found two and Su one, while MSTMalt and MST-06 found none at all.) Considering the exact match scores, finally, it is very interesting to see that Su almost consistently outperforms all other parsers, including the combo system MSTMalt, and sometimes by a fairly wide margin (Czech, Slovene). The difference is statistically significant with respect to all other systems except MSTMalt for Slovene, all except MSTMalt and Spp for Czech, and with respect to MSTMalt for Turkish. For Arabic and Danish, there are no significant differences in the exact match scores. We conclude that Su may increase the probability of finding a completely correct analysis, which is sometimes reflected also in the overall attachment score, and we conjecture that the strength of the positive effect is dependent on the frequency of non-projective arcs in the language. Processing non-projective trees by swapping the order of words has recently been proposed by both Nivre (2008b) and Titov et al. (2009), but these systems cannot handle unrestricted non-projective trees. It is worth pointing out that, although the system described in Nivre (2008b) uses four transitions bearing the same names as the transitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two consecutive SWAP transitions are permitted. This restriction preserves linear worstcase complexity at the expense of completeness. Finally, the algorithm first described by Covington (2001) and used for data-driven parsing by Nivre (2007), is complete but has quadratic complexity even in the best case. We have presented a novel transition system for dependency parsing that can handle unrestricted non-projective trees. The system reuses standard techniques for building projective trees by combining adjacent nodes (representing subtrees with adjacent yields), but adds a simple mechanism for swapping the order of nodes on the stack, which gives a system that is sound and complete for the set of all dependency trees over a given label set but behaves exactly like the standard system for the subset of projective trees. As a result, the time complexity of deterministic parsing is O(n2) in the worst case, which is rare, but O(n) in the best case, which is common, and experimental results on data from five languages support the conclusion that expected running time is linear in the length of the sentence. Experimental results also show that parsing accuracy is competitive, especially for languages like Czech and Slovene where nonprojective dependency structures are common, and especially with respect to the exact match score, where it has the best reported results for four out of five languages. Finally, the simplicity of the system makes it very easy to implement. Future research will include an in-depth error analysis to find out why the system works better for some languages than others and why the exact match score improves even when the attachment score goes down. In addition, we want to explore alternative oracle functions, which try to minimize the number of swaps by allowing the stack to be temporarily “unsorted”.","We have presented a novel transition system for dependency parsing that can handle unrestricted non-projective trees. The system reuses standard techniques for building projective trees by combining adjacent nodes (representing subtrees with adjacent yields), but adds a simple mechanism for swapping the order of nodes on the stack, which gives a system that is sound and complete for the set of all dependency trees over a given label set but behaves exactly like the standard system for the subset of projective trees. As a result, the time complexity of deterministic parsing is O(n2) in the worst case, which is rare, but O(n) in the best case, which is common, and experimental results on data from five languages support the conclusion that expected running time is linear in the length of the sentence. Experimental results also show that parsing accuracy is competitive, especially for languages like Czech and Slovene where nonprojective dependency structures are common, and especially with respect to the exact match score, where it has the best reported results for four out of five languages. Finally, the simplicity of the system makes it very easy to implement. Future research will include an in-depth error analysis to find out why the system works better for some languages than others and why the exact match score improves even when the attachment score goes down. In addition, we want to explore alternative oracle functions, which try to minimize the number of swaps by allowing the stack to be temporarily “unsorted”."
5,"Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.","Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, ‘tokens’) that constitute the unanalyzed surface forms (utterances). In Semitic languages the situation is very different. In Modern Hebrew (Hebrew), a Semitic language with very rich morphology, particles marking conjunctions, prepositions, complementizers and relativizers are bound elements prefixed to the word (Glinert, 1989). The Hebrew token ‘bcl’1, for example, stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al., 2001). “in the shadow”. This token may further embed into a larger utterance, e.g., ‘bcl hneim’ (literally “in-the-shadow the-pleasant”, meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective. It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”). This leads to word- and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies. Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2 Segmental morphology Hebrew consists of seven particles m(“from”) f(“when”/“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem. Several such particles may be prefixed onto a single stem, in which case the affixation is subject to strict linear precedence constraints. Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem. While the linear precedence of segmental morphemes within a token is subject to constraints, the dominance relations among their mother and sister constituents is rather free. The relativizer f(“that”) for example, may attach to an arbitrarily long relative clause that goes beyond token boundaries. The attachment in such cases encompasses a long distance dependency that cannot be captured by Markovian processes that are typically used for morphological disambiguation. The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind. A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology. An additional case of super-segmental morphology is the case of Pronominal Clitics. Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements. The additional morphological material in such cases appears after the stem and realizes the extended meaning. The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008). Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens. The form fmnh, for example, can be understood as the verb “lubricated”, the possessed noun “her oil”, the adjective “fat” or the verb “got fat”. Furthermore, the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities. The same form fmnh can be segmented as f-mnh, f (“that”) functioning as a reletivizer with the form mnh. The form mnh itself can be read as at least three different verbs (“counted”, “appointed”, “was appointed”), a noun (“a portion”), and a possessed noun (“her kind”). Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form. Such discrepancies can be aligned via an intermediate level of PoS tags. PoS tags impose a unique morphological segmentation on surface tokens and present a unique valid yield for syntactic trees. The correct ambiguity resolution of the syntactic level therefore helps to resolve the morphological one, and vice versa. Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006). Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). The development of the very first Hebrew Treebank (Sima’an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited. Sima’an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation. Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank. Tsarfaty and Sima’an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation. The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty, 2006; Tsarfaty and Sima’an, 2004) and empirically explored in (Tsarfaty, 2006). Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task. Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task. Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks. A Hebrew surface token may have several readings, each of which corresponding to a sequence of segments and their corresponding PoS tags. We refer to different readings as different analyses whereby the segments are deterministic given the sequence of PoS tags. We refer to a segment and its assigned PoS tag as a lexeme, and so analyses are in fact sequences of lexemes. For brevity we omit the segments from the analysis, and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB. Such tag sequences are often treated as “complex tags” (e.g. REL+VB) (cf. (Bar-Haim et al., 2007; Habash and Rambow, 2005)) and probabilities are assigned to different analyses in accordance with the likelihood of their tags (e.g., “fmnh is 30% likely to be tagged NN and 70% likely to be tagged REL+VB”). Here we do not submit to this view. When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB, the analysis introduces two distinct entities, the relativizer f (“that”) and the verb mnh (“counted”), and not as the complex entity “that counted”. When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”. There is no relation between these two interpretations other then the fact that their surface forms coincide, and we argue that the only reason to prefer one analysis over the other is compositional. A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al., 2005) present similar models. We suggest that in unlexicalized PCFGs the syntactic context may be explicitly modeled in the derivation probabilities. Hence, we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags, and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context. In our model, however, all lattice paths are taken to be a-priori equally likely. We represent all morphological analyses of a given utterance using a lattice structure. Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance. This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler, 2001; Bar-Haim et al., 2005; Smith et al., 2005; Cohen and Smith, 2007; Adler, 2007). Figure 1 depicts the lattice for a 2-words sentence bclm hneim. We use double-circles to indicate the space-delimited token boundaries. Note that in our construction arcs can never cross token boundaries. Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token. Furthermore, some of the arcs represent lexemes not present in the input tokens (e.g. h/DT, fl/POS), however these are parts of valid analyses of the token (cf. super-segmental morphology section 2). Segments with the same surface form but different PoS tags are treated as different lexemes, and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7). A similar structure is used in speech recognition. There, a lattice is used to represent the possible sentences resulting from an interpretation of an acoustic model. In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions. Given that weights on all outgoing arcs sum up to one, weights induce a probability distribution on the lattice paths. In sequential tagging models such as (Adler and Elhadad, 2006; Bar-Haim et al., 2007; Smith et al., 2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1, ... , wn of space-delimited tokens. Each token may admit multiple analyses, each of which a sequence of one or more lexemes (we use li to denote a lexeme) belonging a presupposed Hebrew lexicon LEX. The entries in such a lexicon may be thought of as meaningful surface segments paired up with their PoS tags li = (si, pi), but note that a surface segment s need not be a space-delimited token. The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1. A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L). We define the lattice L to be the concatenation of the lattices Li corresponding to the input words wi (s.t. M(wi) = Li). Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer, we look for the most probable parse tree π s.t. Since the lattice L for a given sentence W is determined by the morphological analyzer M we have which is precisely the formula corresponding to the so-called lattice parsing familiar from speech recognition. Every parse π selects a specific morphological segmentation (l1...lk) (a path through the lattice). This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g. (Charniak et al., 1996). Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations. A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger, and concludes that these should be P(w|t) “and nothing fancier”.3 In our setting, therefore, the Lattice is not used to induce a probability distribution on a linear context, but rather, it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form. This is a unique object for which we are able to define a proper probability model. Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to. The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes. This is done using a simple PCFG which is lexemebased. This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes). The possible analyses of a surface token pose constraints on the analyses of specific segments. In order to pass these constraints onto the parser, the lexical rules in the grammar are of the form pi —* (si, pi) Parameter Estimation The grammar probabilities are estimated from the corpus using simple relative frequency estimates. Lexical rules are estimated in a similar manner. We smooth Prf(p —* (s, p)) for rare and OOV segments (s E l, l E L, s unseen) using a “per-tag” probability distribution over rare segments which we estimate using relative frequency estimates for once-occurring segments. 3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc. Handling Unknown tokens When handling unknown tokens in a language such as Hebrew various important aspects have to be borne in mind. Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags. Secondly, some segments in a proposed segment sequence may in fact be seen lexical events, i.e., for some p tag Prf(p —* (s, p)) > 0, while other segments have never been observed as a lexical event before. The latter arcs correspond to OOV words in English. Finally, the assignments of PoS tags to OOV segments is subject to language specific constraints relative to the token it was originated from. Our smoothing procedure takes into account all the aforementioned aspects and works as follows. We first make use of our morphological analyzer to find all segmentation possibilities by chopping off all prefix sequence possibilities (including the empty prefix) and construct a lattice off of them. The remaining arcs are marked OOV. At this stage the lattice path corresponds to segments only, with no PoS assigned to them. In turn we use two sorts of heuristics, orthogonal to one another, to prune segmentation possibilities based on lexical and grammatical constraints. We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes. This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments. For the remaining arcs, if the segment is in fact a known lexeme it is tagged as usual, but for the OOV arcs which are valid Hebrew entries lacking tags assignment, we assign all possible tags and then simulate a grammatical constraint. Here, all tokeninternal collocations of tags unseen in our training data are pruned away. From now on all lattice arcs are tagged segments and the assignment of probability P(p —* (s, p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice. In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation, and let the statistics (including OOV) decide. We empirically control for the effect of our heuristics to make sure our pruning does not undermine the objectives of our joint task. Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data, different splits, differing annotation schemes, and different evaluation measures. Our experimental setup therefore is designed to serve two goals. Our primary goal is to exploit the resources that are most appropriate for the task at hand, and our secondary goal is to allow for comparison of our models’ performance against previously reported results. When a comparison against previous results requires additional pre-processing, we state it explicitly to allow for the reader to replicate the reported results. Data We use the Hebrew Treebank, (Sima’an et al., 2001), provided by the knowledge center for processing Hebrew, in which sentences from the daily newspaper “Ha’aretz” are morphologically segmented and syntactically annotated. The treebank has two versions, v1.0 and v2.0, containing 5001 and 6501 sentences respectively. We use v1.0 mainly because previous studies on joint inference reported results w.r.t. v1.0 only.5 We expect that using the same setup on v2.0 will allow a crosstreebank comparison.6 We used the first 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and “malformed”7 were removed. The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. (we ignored the 419 trees in their development set.) Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007). We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses. Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5. We use the HSPELL9 (Har’el and Kenigsberg, 2004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments. Models that employ this strategy are denoted hsp. To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning. For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the treebank. Our first model is GTplain, a PCFG learned from the treebank after removing all functional features from the syntactic categories. In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as 10Lattice parsing can be performed by special initialization of the chart in a CKY parser (Chappelier et al., 1999). We currently simulate this by crafting a WCFG and feeding it to BitPar. Given a PCFG grammar G and a lattice L with nodes n1 ... nk, we construct the weighted grammar GL as follows: for every arc (lexeme) l E L from node ni to node nj, we add to GL the rule [l --+ tni, tni+1, ... , tnj_1] with a probability of 1 (this indicates the lexeme l spans from node ni to node nj). GL is then used to parse the string tn1 ... tnk_1, where tni is a terminal corresponding to the lattice span between node ni and ni+1. Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GTnph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task. To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)). SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure. Evaluating parsing results in our joint framework, as argued by Tsarfaty (2006), is not trivial under the joint disambiguation task, as the hypothesized yield need not coincide with the correct one. Our parsing performance measures (SY N) thus report the PARSEVAL extension proposed in Tsarfaty (2006). We further report SYNCS, the parsing metric of Cohen and Smith (2007), to facilitate the comparison. We report the F1 value of both measures. Finally, our U (unparsed) measure is used to report the number of sentences to which our system could not propose a joint analysis. The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1. In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task. The table makes clear that enriching our grammar improves the syntactic performance as well as morphological disambiguation (segmentation and POS tagging) accuracy. This supports our main thesis that decisions taken by single, improved, grammar are beneficial for both tasks. When using the segmentation pruning (using HSPELL) for unseen tokens, performance improves for all tasks as well. Yet we note that the better grammars without pruning outperform the poorer grammars using this technique, indicating that the syntactic context aids, to some extent, the disambiguation of unknown tokens. Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks. We first note that the accuracy results of our system are overall higher on their setup, on all measures, indicating that theirs may be an easier dataset. Secondly, for all our models we provide better fine- and coarse-grained POS-tagging accuracy, and all pruned models outperform the Oracle results reported by them.12 In terms of syntactic disambiguation, even the simplest grammar pruned with HSPELL outperforms their non-Oracle results. Without HSPELL-pruning, our simpler grammars are somewhat lagging behind, but as the grammars improve the gap is bridged. The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks. This essentially means that their model does not result in a true joint inference, as executions for different tasks involve tuning a parameter separately. In our model there are no such hyper-parameters, and the performance is the result of truly joint disambiguation. sults. Furthermore, the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith. This essentially means that a better grammar tunes the joint model for optimized syntactic disambiguation at least in as much as their hyper parameters do. An interesting observation is that while vertical markovization benefits all our models, its effect is less evident in Cohen and Smith. On the surface, our model may seem as a special case of Cohen and Smith in which α = 0. However, there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context. Many morphological decisions are based on long distance dependencies, and when the global syntactic evidence disagrees with evidence based on local linear context, the two models compete with one another, despite the fact that the PCFG takes also local context into account. In addition, as the CRF and PCFG look at similar sorts of information from within two inherently different models, they are far from independent and optimizing their product is meaningless. Cohen and Smith approach this by introducing the α hyperparameter, which performs best when optimized independently for each sentence (cf. Oracle results). In contrast, our morphological probabilities are based on a unigram, lexeme-based model, and all other (local and non-local) contextual considerations are delegated to the PCFG. This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.","The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1. In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task. The table makes clear that enriching our grammar improves the syntactic performance as well as morphological disambiguation (segmentation and POS tagging) accuracy. This supports our main thesis that decisions taken by single, improved, grammar are beneficial for both tasks. When using the segmentation pruning (using HSPELL) for unseen tokens, performance improves for all tasks as well. Yet we note that the better grammars without pruning outperform the poorer grammars using this technique, indicating that the syntactic context aids, to some extent, the disambiguation of unknown tokens. Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks. We first note that the accuracy results of our system are overall higher on their setup, on all measures, indicating that theirs may be an easier dataset. Secondly, for all our models we provide better fine- and coarse-grained POS-tagging accuracy, and all pruned models outperform the Oracle results reported by them.12 In terms of syntactic disambiguation, even the simplest grammar pruned with HSPELL outperforms their non-Oracle results. Without HSPELL-pruning, our simpler grammars are somewhat lagging behind, but as the grammars improve the gap is bridged. The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks. This essentially means that their model does not result in a true joint inference, as executions for different tasks involve tuning a parameter separately. In our model there are no such hyper-parameters, and the performance is the result of truly joint disambiguation. sults. Furthermore, the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith. This essentially means that a better grammar tunes the joint model for optimized syntactic disambiguation at least in as much as their hyper parameters do. An interesting observation is that while vertical markovization benefits all our models, its effect is less evident in Cohen and Smith. On the surface, our model may seem as a special case of Cohen and Smith in which α = 0. However, there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context. Many morphological decisions are based on long distance dependencies, and when the global syntactic evidence disagrees with evidence based on local linear context, the two models compete with one another, despite the fact that the PCFG takes also local context into account. In addition, as the CRF and PCFG look at similar sorts of information from within two inherently different models, they are far from independent and optimizing their product is meaningless. Cohen and Smith approach this by introducing the α hyperparameter, which performs best when optimized independently for each sentence (cf. Oracle results). In contrast, our morphological probabilities are based on a unigram, lexeme-based model, and all other (local and non-local) contextual considerations are delegated to the PCFG. This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process."
6,"Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.","Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task. Syntactic dependency graphs have recently gained a wide interest in the natural language processing community and have been used for many problems ranging from machine translation (Ding and Palmer, 2004) to ontology construction (Snow et al., 2005). A dependency graph for a sentence represents each word and its syntactic dependents through labeled directed arcs, as shown in figure 1. One advantage of this representation is that it extends naturally to discontinuous constructions, which arise due to long distance dependencies or in languages where syntactic structure is encoded in morphology rather than in word order. This is undoubtedly one of the reasons for the emergence of dependency parsers for a wide range of languages. Many of these parsers are based on data-driven parsing models, which learn to produce dependency graphs for sentences solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist. Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007). In graph-based parsing, we learn a model for scoring possible dependency graphs for a given sentence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorithms, but define features over a limited history of parsing decisions. The transitionbased models are essentially the opposite. They use local training and greedy inference algorithms, but define features over a rich history of parsing decisions. This is a fundamental trade-off that is hard to overcome by tractable means. Both models have been used to achieve state-of-the-art accuracy for a wide range of languages, as shown in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), but McDonald and Nivre (2007) showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models. In this paper, we consider a simple way of integrating graph-based and transition-based models in order to exploit their complementary strengths and thereby improve parsing accuracy beyond what is possible by either model in isolation. The method integrates the two models by allowing the output of one model to define features for the other. This method is simple – requiring only the definition of new features – and robust by allowing a model to learn relative to the predictions of the other. Given a set L = 1l1, ... ,l|L|} of arc labels (dependency relations), a dependency graph for an input sentence x = w0, w1, ... , w, (where w0 = ROOT) is a labeled directed graph G = (V, A) consisting of a set of nodes V = 10, 1, ... , n}1 and a set of labeled directed arcs A C_ V xV xL, i.e., if (i, j, l) E A for i, j E V and l E L, then there is an arc from node i to node j with label l in the graph. A dependency graph G for a sentence x must be a directed tree originating out of the root node 0 and spanning all nodes in V , as exemplified by the graph in figure 1. This is a common constraint in many dependency parsing theories and their implementations. Graph-based dependency parsers parameterize a model over smaller substructures in order to search the space of valid dependency graphs and produce the most likely one. The simplest parameterization is the arc-factored model that defines a real-valued score function for arcs s(i, j, l) and further defines the score of a dependency graph as the sum of the score of all the arcs it contains. As a result, the dependency parsing problem is written: This problem is equivalent to finding the highest scoring directed spanning tree in the complete graph over the input sentence, which can be solved in O(n2) time (McDonald et al., 2005b). Additional parameterizations are possible that take more than one arc into account, but have varying effects on complexity (McDonald and Satta, 2007). An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a). The primary disadvantage of these models is that scores – and as a result any feature representations – are restricted to a single arc or a small number of arcs in the graph. The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 Transition-based dependency parsing systems use a model parameterized over transitions of an abstract machine for deriving dependency graphs, such that every transition sequence from the designated initial configuration to some terminal configuration derives a valid dependency graph. Given a real-valued score function s(c, t) (for transition t out of configuration c), parsing can be performed by starting from the initial configuration and taking the optimal transition t* = arg maxtET s(c, t) out of every configuration c until a terminal configuration is reached. This can be seen as a greedy search for the optimal dependency graph, based on a sequence of locally optimal decisions in terms of the transition system. Many transition systems for data-driven dependency parsing are inspired by shift-reduce parsing, where each configuration c contains a stack Q, for storing partially processed nodes and a buffer Q, containing the remaining input. Transitions in such a system add arcs to the dependency graph and manipulate the stack and buffer. One example is the transition system defined by Nivre (2003), which parses a sentence x = wo, wi, ... , w,,, in O(n) time. To learn a scoring function on transitions, these systems rely on discriminative learning methods, such as memory-based learning or support vector machines, using a strictly local learning procedure where only single transitions are scored (not complete transition sequences). The main advantage of these models is that features are not restricted to a limited number of graph arcs but can take into account the entire dependency graph built so far. The major disadvantage is that the greedy parsing strategy may lead to error propagation. The specific transition-based model studied in this work is that presented by Nivre et al. (2006), which uses support vector machines to learn transition scores. We call this system MaltParser, or Malt for short, which is also the name of the freely available implementation.3 These models differ primarily with respect to three properties: inference, learning, and feature representation. MaltParser uses an inference algorithm that greedily chooses the best parsing decision based on the current parser history whereas MSTParser uses exhaustive search algorithms over the space of all valid dependency graphs to find the graph that maximizes the score. MaltParser trains a model to make a single classification decision (choose the next transition) whereas MSTParser trains a model to maximize the global score of correct graphs. MaltParser can introduce a rich feature history based on previous parser decisions, whereas MSTParser is forced to restrict features to a single decision or a pair of nearby decisions in order to retain efficiency. These differences highlight an inherent trade-off between global inference/learning and expressiveness of feature representations. MSTParser favors the former at the expense of the latter and MaltParser the opposite. This difference was highlighted in the study of McDonald and Nivre (2007), which showed that the difference is reflected directly in the error distributions of the parsers. Thus, MaltParser is less accurate than MSTParser for long dependencies and those closer to the root of the graph, but more accurate for short dependencies and those farthest away from the root. Furthermore, MaltParser is more accurate for dependents that are nouns and pronouns, whereas MSTParser is more accurate for verbs, adjectives, adverbs, adpositions, and conjunctions. Given that there is a strong negative correlation between dependency length and tree depth, and given that nouns and pronouns tend to be more deeply embedded than (at least) verbs and conjunctions, these patterns can all be explained by the same underlying factors. Simply put, MaltParser has an advantage in its richer feature representations, but this advantage is gradually diminished by the negative effect of error propagation due to the greedy inference strategy as sentences and dependencies get longer. MSTParser has a more even distribution of errors, which is expected given that the inference algorithm and feature representation should not prefer one type of arc over another. This naturally leads one to ask: Is it possible to integrate the two models in order to exploit their complementary strengths? This is the topic of the remainder of this paper. There are many conceivable ways of combining the two parsers, including more or less complex ensemble systems and voting schemes, which only perform the integration at parsing time. However, given that we are dealing with data-driven models, it should be possible to integrate at learning time, so that the two complementary models can learn from one another. In this paper, we propose to do this by letting one model generate features for the other. As explained in section 2, both models essentially learn a scoring function s : X —* R, where the domain X is different for the two models. For the graph-based model, X is the set of possible dependency arcs (i, j, l); for the transition-based model, X is the set of possible configuration-transition pairs (c, t). But in both cases, the input is represented by a k-dimensional feature vector f : X —* Rk. In the feature-based integration we simply extend the feature vector for one model, called the base model, with a certain number of features generated by the other model, which we call the guide model in this context. The additional features will be referred to as guide features, and the version of the base model trained with the extended feature vector will be called the guided model. The idea is that the guided model should be able to learn in which situations to trust the guide features, in order to exploit the complementary strength of the guide model, so that performance can be improved with respect to the base parser. This method of combining classifiers is sometimes referred to as classifier stacking. The exact form of the guide features depend on properties of the base model and will be discussed in sections 3.2–3.3 below, but the overall scheme for the feature-based integration can be described as follows. To train a guided version BC of base model B with guide model C and training set T, the guided model is trained, not on the original training set T, but on a version of T that has been parsed with the guide model C under a cross-validation scheme (to avoid overlap with training data for C). This means that, for every sentence x E T, BC has access at training time to both the gold standard dependency graph Gx and the graph GCx predicted by C, and it is the latter that forms the basis for the additional guide features. When parsing a new sentence x' with BC, x' is first parsed with model C (this time trained on the entire training set T) to derive GCx', so that the guide features can be extracted also at parsing time. The graph-based model, MSTParser, learns a scoring function s(i, j, l) E R over labeled dependencies. More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f(i, j, l) E Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006). The score of an arc is defined as a linear classifier s(i, j, l) = w · f(i, j, l), where w is a vector of feature weights to be learned by the model. For the guided graph-based model, which we call MSTMalt, this feature representation is modified to include an additional argument GMalt x , which is the dependency graph predicted by MaltParser on the input sentence x. Thus, the new feature representation will map an arc and the entire predicted MaltParser graph to a high dimensional feature representation, f(i, j,l, GMalt x ) E Rk+m. These m additional features account for the guide features over the MaltParser output. The specific features used by MSTMalt are given in table 1. All features are conjoined with the part-of-speech tags of the words involved in the dependency to allow the guided parser to learn weights relative to different surface syntactic environments. Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments. The transition-based model, MaltParser, learns a scoring function s(c, t) E R over configurations and transitions. The set of training instances for this learning problem is the set of pairs (c, t) such that t is the correct transition out of c in the transition sequence that derives the correct dependency graph Gx for some sentence x in the training set T. Each training instance (c, t) is represented by a feature vector f(c, t) E Rk, where features are defined in terms of arbitrary properties of the configuration c, including the state of the stack Qc, the input buffer ,(ic, and the partially built dependency graph Gc. In particular, many features involve properties of the two target tokens, the token on top of the stack Qc (Q0c) and the first token in the input buffer Qc (Q0c), which are the two tokens that may become connected by a dependency arc through the transition out of c. The full set of features used by the base model MaltParser is described in Nivre et al. (2006). For the guided transition-based model, which we call MaltMST, training instances are extended to triples (c, t, GMST), where GMST is the dependency xx graph predicted by the graph-based MSTParser for the sentence x to which the configuration c belongs. We define m additional guide features, based on properties of GMST x, and extend the feature vector accordingly to f(c, t, GMST x ) E Rk+,. The specific features used by MaltMST are given in table 1. Unlike MSTParser, features are not explicitly defined to conjoin guide features with part-of-speech features. These features are implicitly added through the polynomial kernel used to train the SVM. In this section, we present an experimental evaluation of the two guided models based on data from the CoNLL-X shared task, followed by a comparative error analysis including both the base models and the guided models. The data for the experiments are training and test sets for all thirteen languages from the CoNLL-X shared task on multilingual dependency parsing with training sets ranging in size from from 29,000 tokens (Slovene) to 1,249,000 tokens (Czech). The test sets are all standardized to about 5,000 tokens each. For more information on the data sets, see Buchholz and Marsi (2006). The guided models were trained according to the scheme explained in section 3, with two-fold crossvalidation when parsing the training data with the guide parsers. Preliminary experiments suggested that cross-validation with more folds had a negligible impact on the results. Models are evaluated by their labeled attachment score (LAS) on the test set, i.e., the percentage of tokens that are assigned both the correct head and the correct label, using the evaluation software from the CoNLL-X shared task with default settings.4 Statistical significance was assessed using Dan Bikel’s randomized parsing evaluation comparator with the default setting of 10,000 iterations.5 Table 2 shows the results, for each language and on average, for the two base models (MST, Malt) and for the two guided models (MSTMlt, MaltMST). First of all, we see that both guided models show a very consistent increase in accuracy compared to their base model, even though the extent of the improvement varies across languages from about half a percentage point (MaltMST on Chinese) up to almost four percentage points (MaltMST on Slovene).6 It is thus quite clear that both models have the capacity to learn from features generated by the other model. However, it is also clear that the graph-based MST model shows a somewhat larger improvement, both on average and for all languages except Czech, German, Portuguese and Slovene. Finally, given that the two base models had the previously best performance for these data sets, the guided models achieve a substantial improvement of the state of the art. While there is no statistically significant difference between the two base models, they are both outperformed by MaltMST (p < 0.0001), which in turn has significantly lower accuracy than MSTMalt (p < 0.0005). An extension to the models described so far would be to iteratively integrate the two parsers in the spirit of pipeline iteration (Hollingshead and Roark, 2007). For example, one could start with a Malt model, use it to train a guided MSTMalt model, then use that as the guide to train a MaltMSTM.,t model, etc. We ran such experiments, but found that accuracy did not increase significantly and in some cases decreased slightly. This was true regardless of which parser began the iterative process. In retrospect, this result is not surprising. Since the initial integration effectively incorporates knowledge from both parsing systems, there is little to be gained by adding additional parsers in the chain. The experimental results presented so far show that feature-based integration is a viable approach for improving the accuracy of both graph-based and transition-based models for dependency parsing, but they say very little about how the integration benefits the two models and what aspects of the parsing process are improved as a result. In order to get a better understanding of these matters, we replicate parts of the error analysis presented by McDonald and Nivre (2007), where parsing errors are related to different structural properties of sentences and their dependency graphs. For each of the four models evaluated, we compute error statistics for labeled attachment over all twelve languages together. Figure 2 shows accuracy in relation to sentence length, binned into ten-word intervals (1–10, 11-20, etc.). As expected, Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation (McDonald and Nivre, 2007). The guided models, MaltMST and MSTMalt, behave in a very similar fashion with respect to each other but both outperform their base parser over the entire range of sentence lengths. However, except for the two extreme data points (0–10 and 51–60) there is also a slight tendency for MaltMST to improve more for longer sentences and for MSTMalt to improve more for short sentences, which indicates that the feature-based integration allows one parser to exploit the strength of the other. Figure 3(a) plots precision (top) and recall (bottom) for dependency arcs of different lengths (predicted arcs for precision, gold standard arcs for recall). With respect to recall, the guided models appear to have a slight advantage over the base models for short and medium distance arcs. With respect to precision, however, there are two clear patterns. First, the graph-based models have better precision than the transition-based models when predicting long arcs, which is compatible with the results of McDonald and Nivre (2007). Secondly, both the guided models have better precision than their base model and, for the most part, also their guide model. In particular MSTMalt outperforms MST and is comparable to Malt for short arcs. More interestingly, MaltMST outperforms both Malt and MST for arcs up to length 9, which provides evidence that MaltMST has learned specifically to trust the guide features from MST for longer dependencies. The reason that accuracy does not improve for dependencies of length greater than 9 is probably that these dependencies are too rare for MaltMST to learn from the guide parser in these situations. Figure 3(b) shows precision (top) and recall (bottom) for dependency arcs at different distances from the root (predicted arcs for precision, gold standard arcs for recall). Again, we find the clearest patterns in the graphs for precision, where Malt has very low precision near the root but improves with increasing depth, while MST shows the opposite trend (McDonald and Nivre, 2007). Considering the guided models, it is clear that MaltMST improves in the direction of its guide model, with a 5-point increase in precision for dependents of the root and smaller improvements for longer distances. Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt. This again provides evidence that the guided parsers are learning from their guide models. Table 3 gives the accuracy for arcs relative to dependent part-of-speech. As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007). But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser. The general trend is that MST improves more than Malt, except for adjectives and conjunctions, where Malt has a greater disadvantage from the start and therefore benefits more from the guide features. Considering the results for parts of speech, as well as those for dependency length and root distance, it is interesting to note that the guided models often improve even in situations where their base parsers are more accurate than their guide models. This suggests that the improvement is not a simple function of the raw accuracy of the guide model but depends on the fact that labeled dependency decisions interact in inference algorithms for both graph-based and transition-based parsing systems. Thus, if a parser can improve its accuracy on one class of dependencies, e.g., longer ones, then we can expect to see improvements on all types of dependencies – as we do. The interaction between different decisions may also be part of the explanation why MST benefits more from the feature-based integration than Malt, with significantly higher accuracy for MSTMalt than for MaltMST as a result. Since inference is global (or practically global) in the graph-based model, an improvement in one type of dependency has a good chance of influencing the accuracy of other dependencies, whereas in the transition-based model, where inference is greedy, some of these additional benefits will be lost because of error propagation. This is reflected in the error analysis in the following recurrent pattern: Where Malt does well, MaltMST does only slightly better. But where MST is good, MSTMalt is often significantly better. Another part of the explanation may have to do with the learning algorithms used by the systems. Although both Malt and MST use discriminative algorithms, Malt uses a batch learning algorithm (SVM) and MST uses an online learning algorithm (MIRA). If the original rich feature representation of Malt is sufficient to separate the training data, regularization may force the weights of the guided features to be small (since they are not needed at training time). On the other hand, an online learning algorithm will recognize the guided features as strong indicators early in training and give them a high weight as a result. Features with high weight early in training tend to have the most impact on the final classifier due to both weight regularization and averaging. This is in fact observed when inspecting the weights of MSTMalt. Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006), who report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank. The combined parsing model is essentially an instance of the graph-based model, where arc scores are derived from the output of the different component parsers. Unlike the models presented here, integration takes place only at parsing time, not at learning time, and requires at least three different base parsers. The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task. Feature-based integration in the sense of letting a subset of the features for one model be derived from the output of a different model has been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank. In addition, feature-based integration has been used by Taskar et al. (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al. (2004), who trained classifiers on auxiliary data to guide named entity classifiers. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where guide features replace, rather than complement, the gold standard annotation during training. Feature-based integration is also similar to parse re-ranking (Collins, 2000), where one parser produces a set of candidate parses and a secondstage classifier chooses the most likely one. However, feature-based integration is not explicitly constrained to any parse decisions that the guide model might make and only the single most likely parse is used from the guide model, making it significantly more efficient than re-ranking. Finally, there are several recent developments in data-driven dependency parsing, which can be seen as targeting the specific weaknesses of graph-based and transition-based models, respectively, though without integrating the two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007).","Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006), who report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank. The combined parsing model is essentially an instance of the graph-based model, where arc scores are derived from the output of the different component parsers. Unlike the models presented here, integration takes place only at parsing time, not at learning time, and requires at least three different base parsers. The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task. Feature-based integration in the sense of letting a subset of the features for one model be derived from the output of a different model has been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank. In addition, feature-based integration has been used by Taskar et al. (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al. (2004), who trained classifiers on auxiliary data to guide named entity classifiers. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where guide features replace, rather than complement, the gold standard annotation during training. Feature-based integration is also similar to parse re-ranking (Collins, 2000), where one parser produces a set of candidate parses and a secondstage classifier chooses the most likely one. However, feature-based integration is not explicitly constrained to any parse decisions that the guide model might make and only the single most likely parse is used from the guide model, making it significantly more efficient than re-ranking. Finally, there are several recent developments in data-driven dependency parsing, which can be seen as targeting the specific weaknesses of graph-based and transition-based models, respectively, though without integrating the two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007)."
7,"This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a person is taking in an online debate. In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates. We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem. Our results show that our method is substantially better than challenging baseline methods.","This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a person is taking in an online debate. In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates. We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem. Our results show that our method is substantially better than challenging baseline methods. This paper presents a method for debate-side classification, i.e., recognizing which stance a person is taking in an online debate posting. In online debate forums, people debate issues, express their preferences, and argue why their viewpoint is right. In addition to expressing positive sentiments about one’s preference, a key strategy is also to express negative sentiments about the other side. For example, in the debate “which mobile phone is better: iPhone or Blackberry,” a participant on the iPhone side may explicitly assert and rationalize why the iPhone is better, and, alternatively, also argue why the Blackberry is worse. Thus, to recognize stances, we need to consider not only which opinions are positive and negative, but also what the opinions are about (their targets). Participants directly express their opinions, such as “The iPhone is cool,” but, more often, they mention associated aspects. Some aspects are particular to one topic (e.g., Active-X is part of IE but not Firefox), and so distinguish between them. But even an aspect the topics share may distinguish between them, because people who are positive toward one topic may value that aspect more. For example, both the iPhone and Blackberry have keyboards, but we observed in our corpus that positive opinions about the keyboard are associated with the pro Blackberry stance. Thus, we need to find distinguishing aspects, which the topics may or may not share. Complicating the picture further, participants may concede positive aspects of the opposing issue or topic, without coming out in favor of it, and they may concede negative aspects of the issue or topic they support. For example, in the following sentence, the speaker says positive things about the iPhone, even though he does not prefer it: “Yes, the iPhone may be cool to take it out and play with and show off, but past that, it offers nothing.” Thus, we need to consider discourse relations to sort out which sentiments in fact reveal the writer’s stance, and which are merely concessions. Many opinion mining approaches find negative and positive words in a document, and aggregate their counts to determine the final document polarity, ignoring the targets of the opinions. Some work in product review mining finds aspects of a central topic, and summarizes opinions with respect to these aspects. However, they do not find distinguishing factors associated with a preference for a stance. Finally, while other opinion analysis systems have considered discourse information, they have not distinguished between concessionary and non-concessionary opinions when determining the overall stance of a document. This work proposes an unsupervised opinion analysis method to address the challenges described above. First, for each debate side, we mine the web for opinion-target pairs that are associated with a preference for that side. This information is employed, in conjunction with discourse information, in an Integer Linear Programming (ILP) framework. This framework combines the individual pieces of information to arrive at debate-side classifications of posts in online debates. The remainder of this paper is organized as follows. We introduce our debate genre in Section 2 and describe our method in Section 3. We present the experiments in Section 4 and analyze the results in Section 5. Related work is in Section 6, and the conclusions are in Section 7. In this section, we describe our debate data, and elaborate on characteristic ways of expressing opinions in this genre. For our current work, we use the online debates from the website http://www.convinceme.net.1 In this work, we deal only with dual-sided, dual-topic debates about named entities, for example iPhone vs. Blackberry, where topics = iPhone, topic2 =Blackberry, sides = pro-iPhone, and side2=pro-Blackberry. Our test data consists of posts of 4 debates: Windows vs. Mac, Firefox vs. Internet Explorer, Firefox vs. Opera, and Sony Ps3 vs. Nintendo Wii. The iPhone vs. Blackberry debate and two other debates, were used as development data. Given below are examples of debate posts. Post 1 is taken from the iPhone vs. Blackberry debate, Post 2 is from the Firefox vs. Internet Explorer debate, and Post 3 is from the Windows vs. Mac debate: As described in Section 1, the debate genre poses significant challenges to opinion analysis. This subsection elaborates upon some of the complexities. Multiple polarities to argue for a side. Debate participants, in advocating their choice, switch back and forth between their opinions towards the sides. This makes it difficult for approaches that use only positive and negative word counts to decide which side the post is on. Posts 1 and 3 illustrate this phenomenon. Sentiments towards both sides (topics) within a single post. The above phenomenon gives rise to an additional problem: often, conflicting sides (and topics) are addressed within the same post, sometimes within the same sentence. The second sentence of Post 3 illustrates this, as it has opinions about both Windows and Mac. Differentiating aspects and personal preferences. People seldom repeatedly mention the topic/side; they show their evaluations indirectly, by evaluating aspects of each topic/side. Differentiating aspects determine the debate-post’s side. Some aspects are unique to one side/topic or the other, e.g., “3g” in Example 1 and “inline spell check” in Example 2. However, the debates are about topics that belong to the same domain and which therefore share many aspects. Hence, a purely ontological approach of finding “has-a” and “is-a” relations, or an approach looking only for product specifications, would not be sufficient for finding differentiating features. When the two topics do share an aspect (e.g., a keyboard in the iPhone vs. Blackberry debate), the writer may perceive it to be more positive for one than the other. And, if the writer values that aspect, it will influence his or her overall stance. For example, many people prefer the Blackberry keyboard over the iPhone keyboard; people to whom phone keyboards are important are more likely to prefer the Blackberry. Concessions. While debating, participants often refer to and acknowledge the viewpoints of the opposing side. However, they do not endorse this rival opinion. Uniform treatment of all opinions in a post would obviously cause errors in such cases. The first sentence of Example 1 is an instance of this phenomenon. The participant concedes that the iPhone appeals to young consumers, but this positive opinion is opposite to his overall stance. We propose an unsupervised approach to classifying the stance of a post in a dual-topic debate. For this, we first use a web corpus to learn preferences that are likely to be associated with a side. These learned preferences are then employed in conjunction with discourse constraints to identify the side for a given post. We need to find opinions and pair them with targets, both to mine the web for general preferences and to classify the stance of a debate post. We use straightforward methods, as these tasks are not the focus of this paper. To find opinions, we look up words in a subjectivity lexicon: all instances of those words are treated as opinions. An opinion is assigned the prior polarity that is listed for that word in the lexicon, except that, if the prior polarity is positive or negative, and the instance is modified by a negation word (e.g., “not”), then the polarity of that instance is reversed. We use the subjectivity lexicon of (Wilson et al., 2005),2 which contains approximately 8000 words which may be used to express opinions. Each entry consists of a subjective word, its prior polarity (positive (+), negative (−), neutral (*)), morphological information, and part of speech information. To pair opinions with targets, we built a rulebased system based on dependency parse information. The dependency parses are obtained using the Stanford parser.3 We developed the syntactic rules on separate data that is not used elsewhere in this paper. Table 1 illustrates some of these rules. Note that the rules are constructed (and explained in Table 1) with respect to the grammatical relation notations of the Stanford parser. As illustrated in the table, it is possible for an opinion to have more than one target. In such cases, the single opinion results in multiple opinion-target pairs, one for each target. Once these opinion-target pairs are created, we mask the identity of the opinion word, replacing the word with its polarity. Thus, the opiniontarget pair is converted to a polarity-target pair. For instance, “pleasing-interface” is converted to interface+. This abstraction is essential for handling the sparseness of the data. We observed in our development data that people highlight the aspects of topics that are the bases for their stances, both positive opinions toward aspects of the preferred topic, and negative opinions toward aspects of the dispreferred one. Thus, we decided to mine the web for aspects associated with a side in the debate, and then use that information to recognize the stances expressed in individual posts. Previous work mined web data for aspects associated with topics (Hu and Liu, 2004; Popescu et al., 2005). In our work, we search for aspects associated with a topic, but particularized to polarity. Not all aspects associated with a topic are discriminative with respect to stance; we hypothesized that, by including polarity, we would be more likely to find useful associations. An aspect may be associated with both of the debate topics, but not, by itself, be discriminative between stances toward the topics. However, opinions toward that aspect might discriminate between them. Thus, the basic unit in our web mining process is a polarity-target pair. Polarity-target pairs which explicitly mention one of the topics are used to anchor the mining process. Opinions about relevant aspects are gathered from the surrounding context. For each debate, we downloaded weblogs and forums that talk about the main topics (corresponding to the sides) of that debate. For example, for the iPhone vs. Blackberry debate, we search the web for pages containing “iPhone” and “Blackberry.” We used the Yahoo search API and imposed the search restriction that the pages should contain both topics in the http URL. This ensured that we downloaded relevant pages. An average of 3000 documents were downloaded per debate. We apply the method described in Section 3.1 to the downloaded web pages. That is, we find all instances of words in the lexicon, extract their targets, and mask the words with their polarities, yielding polarity-target pairs. For example, suppose the sentence “The interface is pleasing” is in the corpus. The system extracts the pair “pleasing-interface,” which is masked to “positive-interface,” which we notate as interface+. If the target in a polarity-target pair happens to be one of the topics, we select the polarity-target pairs in its vicinity for further processing (the rest are discarded). The intuition behind this is that, if someone expresses an opinion about a topic, he or she is likely to follow it up with reasons for that opinion. The sentiments in the surrounding context thus reveal factors that influence the preference or dislike towards the topic. We define the vicinity as the same sentence plus the following 5 sentences. Each unique target word targeti in the web corpus, i.e., each word used as the target of an opinion one or more times, is processed to generate the following conditional probabilities. where p = {+,− ,* } and q = {+,− ,* } denote the polarities of the target and the topic, respectively; j = {1, 2}; and i = {1...M}, where M is the number of unique targets in the corpus. For example, P(Mac+|interface+) is the probability that “interface” is the target of a positive opinion that is in the vicinity of a positive opinion toward “Mac.” Table 2 lists some of the probabilities learned by this approach. (Note that the neutral cases are not shown.) Table 2 contains examples of the learned probabilities. These probabilities align with what we qualitatively found in our development data. For example, the opinions towards “Storm” essentially follow the opinions towards “Blackberry;” that is, positive opinions toward “Storm” are usually found in the vicinity of positive opinions toward “Blackberry,” and negative opinions toward “Storm” are usually found in the vicinity of negative opinions toward “Blackberry” (for example, in the row for storm+, P(blackberry+|storm+) is much higher than the other probabilities). Thus, an opinion expressed about “Storm” is usually the opinion one has toward “Blackberry.” This is expected, as Storm is a type of Blackberry. A similar example is ipod+, which follows the opinion toward the iPhone. This is interesting because an iPod is not a phone; the association is due to preference for the brand. In contrast, the probability distribution for “phone” does not show a preference for any one side, even though both iPhone and Blackberry are phones. This indicates that opinions towards phones in general will not be able to distinguish between the debate sides. Another interesting case is illustrated by the probabilities for “e-mail.” People who like e-mail capability are more likely to praise the Blackberry, or even criticize the iPhone — they would thus belong to the pro-Blackberry camp. While we noted earlier that positive evaluations of keyboards are associated with positive evaluations of the Blackberry (by far the highest probability in that row), negative evaluations of keyboards, are, however, not a strong discriminating factor. For the other entries in the table, we see that criticisms of batteries and the phone network are more associated with negative sentiments towards the iPhones. The possibility of these various cases motivates our approach, in which opinions and their polarities are considered when searching for associations between debate topics and their aspects. Once we have the probabilities collected from the web, we can build our classifier to classify the debate posts. Here again, we use the process described in Section 3.1 to extract polarity-target pairs for each opinion expressed in the post. Let N be the number of instances of polarity-target pairs in the post. For each instance Ij (j = {1...N}), we look up the learned probabilities of Section 3.2 to create two scores, wj and uj: where target� � is the polarity-target type of which Ij is an instance. Score wj corresponds to side1 and uj corresponds to side2. A point to note is that, if a target word is repeated, and it occurs in different polarity-target instances, it is counted as a separate instance each time — that is, here we account for tokens, not types. Via Equations 2 and 3, we interpret the observed polarity-target instance Ij in terms of debate sides. We formulate the problem of finding the overall side of the post as an Integer Linear Programming (ILP) problem. The side that maximizes the overall side-score for the post, given all the N instances Ij, is chosen by maximizing the objective function Equations 5 and 6 implement binary constraints. Equation 7 enforces the constraint that each Ij can belong to exactly one side. Finally, Equations 8 and 9 ensure that a single side is chosen for the entire post. As described in Section 2, debate participants often acknowledge the opinions held by the opposing side. We recognize such discourse constructs using the Penn Discourse Treebank (Prasad et al., 2007) list of discourse connectives. In particular, we use the list of connectives from the Concession and Contra-expectation category. Examples of connectives in these categories are “while,” “nonetheless,” “however,” and “even if.” We use approximations to finding the arguments to the discourse connectives (ARG1 and ARG2 in Penn Discourse Treebank terms). If the connective is mid-sentence, the part of the sentence prior to the connective is considered conceded, and the part that follows the connective is considered nonconceded. An example is the second sentence of Example 3. If, on the other hand, the connective is sentence-initial, the sentence is split at the first comma that occurs mid sentence. The first part is considered conceded, and the second part is considered non-conceded. An example is the first sentence of Example 1. The opinions occurring in the conceded part are interpreted in reverse. That is, the weights corresponding to the sides wj and uj are interchanged in equation 4. Thus, conceded opinions are effectively made to count towards the opposing side. On http://www.convinceme.net, the html page for each debate contains side information for each post (side1 is blue in color and side2 is green). This gives us automatically labeled data for our evaluations. For each of the 4 debates in our test set, we use posts with at least 5 sentences for evaluation. We implemented two baselines: the OpTopic system that uses topic information only, and the OpPMI system that uses topic as well as related word (noun) information. All systems use the same lexicon, as well as exactly the same processes for opinion finding and opinion-target pairing. The OpTopic system This system considers only explicit mentions of the topic for the opinion analysis. Thus, for this system, the step of opinion-target pairing only finds all topic+1 , topic= , topic+2 , topic2 instances in the post (where, for example, an instance of topic+1 is a positive opinion whose target is explicitly topic1). The polarity-topic pairs are counted for each debate side according to the following equations. The post is assigned the side with the higher score. The OpPMI system This system finds opiniontarget pairs for not only the topics, but also for the words in the debate that are significantly related to either of the topics. We find semantic relatedness of each noun in the post with the two main topics of the debate by calculating the Pointwise Mutual Information (PMI) between the term and each topic over the entire web corpus. We use the API provided by the Measures of Semantic Relatedness (MSR)4 engine for this purpose. The MSR engine issues Google queries to retrieve documents and finds the PMI between any two given words. Table 3 lists PMIs between the topics and the words from Table 2. Each noun k is assigned to the topic with the higher PMI score. That is, if Next, the polarity-target pairs are found for the post, as before, and Equations 10 and 11 are used to assign a side to the post as in the OpTopic system, except that here, related nouns are also counted as instances of their associated topics. In our task, it is desirable to make a prediction for all the posts; hence #relevant = #Total posts. This results in Recall and Accuracy being the same. However, all of the systems do not classify a post if the post does not contain the information it needs. Thus, #guessed < #Total posts, and Precision is not the same as Accuracy. Table 4 reports the performance of four systems on the test data: the two baselines, our method using the preferences learned from the web corpus (OpPr) and the method additionally using discourse information to reverse conceded opinions. The OpTopic has low recall. This is expected, because it relies only on opinions explicitly toward the topics. The OpPMI has better recall than OpTopic; however, the precision drops for some debates. We believe this is due to the addition of noise. This result suggests that not all terms that are relevant to a topic are useful for determining the debate side. Finally, both of the OpPr systems are better than both baselines in Accuracy as well as F-measure for all four debates. The accuracy of the full OpPr system improves, on average, by 35 percentage points over the OpTopic system, and by 20 percentage points over the OpPMI system. The F-measure improves, on average, by 25 percentage points over the OpTopic system, and by 17 percentage points over the OpPMI system. Note that in 3 out of 4 of the debates, the full system is able to make a guess for all of the posts (hence, the metrics all have the same values). In three of the four debates, the system using concession handling described in Section 3.4 outperforms the system without it, providing evidence that our treatment of concessions is effective. On average, there is a 3 percentage point improvement in Accuracy, 5 percentage point improvement in Precision and 5 percentage point improvement in F-measure due to the added concession information. In this section, we discuss the results from the previous section and describe the sources of errors. As reported in the previous section, the OpPr system outperforms both the OpTopic and the OpPMI systems. In order to analyze why OpPr outperforms OpPMI, we need to compare Tables 2 and 3. Table 2 reports the conditional probabilities learned from the web corpus for polaritytarget pairs used in OpPr, and Table 3 reports the PMI of these same targets with the debate topics used in OpPMI. First, we observe that the PMI numbers are intuitive, in that all the words, except for “e-mail,” show a high PMI relatedness to both topics. All of them are indeed semantically related to the domain. Additionally, we see that some conclusions of the OpPMI system are similar to those of the OpPr system, for example, that “Storm” is more closely related to the Blackberry than the iPhone. However, notice two cases: the PMI values for “phone” and “e-mail” are intuitive, but they may cause errors in debate analysis. Because the iPhone and the Blackberry are both phones, the word “phone” does not have any distinguishing power in debates. On the other hand, the PMI measure of “e-mail” suggests that it is not closely related to the debate topics, though it is, in fact, a desirable feature for smart phone users, even more so with Blackberry users. The PMI measure does not reflect this. The “network” aspect shows a comparatively greater relatedness to the blackberry than to the iPhone. Thus, OpPMI uses it as a proxy for the Blackberry. This may be erroneous, however, because negative opinions towards “network” are more indicative of negative opinions towards iPhones, a fact revealed by Table 2. In general, even if the OpPMI system knows what topic the given word is more related to, it still does not know what the opinion towards that word means in the debate scenario. The OpPr system, on the other hand, is able to map it to a debate side. False lexicon hits. The lexicon is word based, but, as shown by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), many subjective words have both objective and subjective senses. Thus, one major source of errors is a false hit of a word in the lexicon. Opinion-target pairing. The syntactic rulebased opinion-target pairing system is a large source of errors in the OpPr as well as the baseline systems. Product review mining work has explored finding opinions with respect to, or in conjunction with, aspects (Hu and Liu, 2004; Popescu et al., 2005); however, in our work, we need to find information in the other direction – that is, given the opinion, what is the opinion about. Stoyanov and Cardie (2008) work on opinion co-reference; however, we need to identify the specific target. Pragmatic opinions. Some of the errors are due to the fact that the opinions expressed in the post are pragmatic. This becomes a problem especially when the debate post is small, and we have few other lexical clues in the post. The following post is an example: (4) The blackberry is something like $150 and the iPhone is $500. I don’t think it’s worth it. You could buy a iPod separate and have a boatload of extra money left over. In this example, the participant mentions the difference in the prices in the first sentence. This sentence implies a negative opinion towards the iPhone. However, recognizing this would require a system to have extensive world knowledge. In the second sentence, the lexicon does hit the word “worth,” and, using syntactic rules, we can determine it is negated. However, the opinion-target pairing system only tells us that the opinion is tied to the “it.” A co-reference system would be needed to tie the “it” to “iPhone” in the first sentence. Several researchers have worked on similar tasks. Kim and Hovy (2007) predict the results of an election by analyzing forums discussing the elections. Theirs is a supervised bag-of-words system using unigrams, bigrams, and trigrams as features. In contrast, our approach is unsupervised, and exploits different types of information. Bansal et al. (2008) predict the vote from congressional floor debates using agreement/disagreement features. We do not model inter-personal exchanges; instead, we model factors that influence stance taking. Lin at al (2006) identify opposing perspectives. Though apparently related at the task level, perspectives as they define them are not the same as opinions. Their approach does not involve any opinion analysis. Fujii and Ishikawa (2006) also work with arguments. However, their focus is on argument visualization rather than on recognizing stances. Other researchers have also mined data to learn associations among products and features. In their work on mining opinions in comparative sentences, Ganapathibhotla and Liu (2008) look for user preferences for one product’s features over another’s. We do not exploit comparative constructs, but rather probabilistic associations. Thus, our approach and theirs are complementary. A number of works in product review mining (Hu and Liu, 2004; Popescu et al., 2005; Kobayashi et al., 2005; Bloom et al., 2007) automatically find features of the reviewed products. However, our approach is novel in that it learns and exploits associations among opinion/polarity, topics, and aspects. Several researchers have recognized the important role discourse plays in opinion analysis (Polanyi and Zaenen, 2005; Snyder and Barzilay, 2007; Somasundaran et al., 2008; Asher et al., 2008; Sadamitsu et al., 2008). However, previous work did not account for concessions in determining whether an opinion supports one side or the other. More sophisticated approaches to identifying opinions and recognizing their contextual polarity have been published (e.g., (Wilson et al., 2005; Ikeda et al., 2008; Sadamitsu et al., 2008)). Those components are not the focus of our work. This paper addresses challenges faced by opinion analysis in the debate genre. In our method, factors that influence the choice of a debate side are learned by mining a web corpus for opinions. This knowledge is exploited in an unsupervised method for classifying the side taken by a post, which also accounts for concessionary opinions. Our results corroborate our hypothesis that finding relations between aspects associated with a topic, but particularized to polarity, is more effective than finding relations between topics and aspects alone. The system that implements this information, mined from the web, outperforms the web PMI-based baseline. Our hypothesis that addressing concessionary opinions is useful is also corroborated by improved performance.","This paper addresses challenges faced by opinion analysis in the debate genre. In our method, factors that influence the choice of a debate side are learned by mining a web corpus for opinions. This knowledge is exploited in an unsupervised method for classifying the side taken by a post, which also accounts for concessionary opinions. Our results corroborate our hypothesis that finding relations between aspects associated with a topic, but particularized to polarity, is more effective than finding relations between topics and aspects alone. The system that implements this information, mined from the web, outperforms the web PMI-based baseline. Our hypothesis that addressing concessionary opinions is useful is also corroborated by improved performance."
8,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 2008)). The sampler performs local edit operations to nodes in the synchronous trees, each of which is very fast, leading to a highly efficient inference technique. This allows us to train the model on large corpora without resort to punitive length limits, unlike previous approaches which were only applied to small data sets with short sentences. This paper is structured as follows: In Section 3 we argue for the use of efficient sampling techniques over SCFGs as an effective solution to the modelling and scaling problems of previous approaches. We describe our Bayesian SCFG model in Section 4 and a Gibbs sampler to explore its posterior. We apply this sampler to build phrase-based and hierarchical translation models and evaluate their performance on small and large corpora. A synchronous context free grammar (SCFG, (Lewis II and Stearns, 1968)) generalizes contextfree grammars to generate strings concurrently in two (or more) languages. A string pair is generated by applying a series of paired rewrite rules of the form, X → he, f, ai, where X is a nonterminal, e and f are strings of terminals and nonterminals and a specifies a one-to-one alignment between non-terminals in e and f. In the context of SMT, by assigning the source and target languages to the respective sides of a probabilistic SCFG it is possible to describe translation as the process of parsing the source sentence, which induces a parallel tree structure and translation in the target language (Chiang, 2007). Figure 1 shows an example derivation for Japanese to English translation using an SCFG. For efficiency reasons we only consider binary or ternary branching rules and don’t allow rules to mix terminals and nonterminals. This allows our sampler to more efficiently explore the space of grammars (Section 4.2), however more expressive grammars would be a straightforward extension of our model. Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such ⇒ hJohn-ga X4 X5, John X5 X4i ⇒ hJohn-ga ringo-o X5, John X5 an applei ⇒ hJohn-ga ringo-o tabeta, John ate an applei as GIZA++. Various heuristics are used to combine source-to-target and target-to-source alignments, after which a further heuristic is used to read off phrase pairs which are ‘consistent’ with the alignment. Although efficient, the sheer number of somewhat arbitrary heuristics makes this approach overly complicated. A number of authors have proposed alternative techniques for directly inducing phrase-based translation models from sentence aligned data. Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. Taking a different tack, DeNero et al. (2008) presented an interesting new model with inference courtesy of a Gibbs sampler, which was better able to explore the full space of phrase translations. However, the efficacy of this model is unclear due to the small-scale experiments and the short sampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine translation community. Our aim is to induce a grammar from a training set of sentence pairs. We use Bayes’ rule to reason under the posterior over grammars, P(g|x) a P(x|g)P(g), where g is a weighted SCFG grammar and x is our training corpus. The likelihood term, P(x|g), is the probability of the training sentence pairs under the grammar, while the prior term, P(g), describes our initial expectations about what consitutes a plausible grammar. Specifically we incorporate priors encoding our preference for a briefer and more succinct grammar, namely that: (a) the grammar should be small, with few rules rewriting each non-terminal; and (b) terminal rules which specify phrasal translation correspondence should be small, with few symbols on their right hand side. Further, Bayesian non-parametrics allow the capacity of the model to grow with the data. Thereby we avoid imposing hard limits on the grammar (and the thorny problem of model selection), but instead allow the model to find a grammar appropriately sized for its training data. Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008). Given a grammar, each sentence is generated as follows. Starting with a root non-terminal (z1), rewrite each frontier non-terminal (zi) using a rule chosen from our grammar expanding zi. Repeat until there are no remaining frontier non-terminals. This gives rise to the following derivation probability: where the derivation is a sequence of rules d = (r1, ... , rn), and zi denotes the root node of ri. We allow two types of rules: non-terminal and terminal expansions. The former rewrites a nonterminal symbol as a string of two or three nonterminals along with an alignment, specifying the corresponding ordering of the child trees in the source and target language. Terminal expansions rewrite a non-terminal as a pair of terminal n-grams, representing a phrasal translation pair, where either but not both may be empty. Each rule in the grammar, ri, is generated from its root symbol, zi, by first choosing a rule type ti E {TERM, NON-TERM} from a Bernoulli distribution, ri — Bernoulli(-y). We treat -y as a random variable with its own prior, -y — Beta(aR, aR) and integrate out the parameters, -y. This results in the following conditional probability for ti: where n−i ri,zi is the number of times ri has been used to rewrite zi in the set of all other rules, r−i, and n−i r,zi is the total count of rewriting zi. The Dirichlet (and thus Beta) distribution are exchangeable, meaning that any permutation of its events are equiprobable. This allows us to reason about each event given previous and subsequent events (i.e., treat each item as the ‘last’.) When ti = NON-TERM, we generate a binary or ternary non-terminal production. The nonterminal sequence and alignment are drawn from (z, a) — ON zi and, as before, we define a prior over the parameters, ON zi — Dirichlet(aT), and integrate out (Nzi. This results in the conditional probability: where nN,−i ri,zi is the count of rewriting zi with nonterminal rule ri, nN,−i ·,zi the total count over all nonterminal rules and |N |is the number of unique non-terminal rules. For terminal productions (ti = TERM) we first decide whether to generate a phrase in both languages or in one language only, according to a fixed probability pnull.3 Contingent on this decision, the terminal strings are then drawn from 3To discourage null alignments, we used Pnuu = 10−10 for this value in the experiments we report below. either φPzi for phrase pairs or φnull for single language phrases. We choose Dirichlet process (DP) priors for these parameters: zi where the base distributions, P1P and Pnull 1 , range over phrase pairs or monolingual phrases in either language, respectively. The most important choice for our model is the priors on the parameters of these terminal distributions. Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al., 2006). Therefore, careful consideration must be given when specifying the P1 distribution on terminals in order to counter this behavior. To construct a prior over string pairs, first we define the probability of a monolingual string (s): P0X (s) = PPoisson(|s|; 1) X where the PPoisson(k; 1) is the probability under a Poisson distribution of length k given an expected length of 1, while VX is the vocabulary size of language X. This distribution has a strong bias towards short strings. In particular note that generally a string of length k will be less probable than two of length k2, a property very useful for finding ‘minimal’ translation units. This contrasts with a geometric distribution in which a string of length k will be more probable than its segmentations. We define Pnull 1 as the string probability of the non-null part of the rule: The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 and φFz is defined analogously. This prior encourages frequent phrases to participate in many different translation pairs. Moreover, as longer strings are likely to be less frequent in the corpus this has a tendency to discourage long translation units. Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling a derivation for a given sentence pair from p(d|d−). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f|3|e|3) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but never need to reason over derivation forests. By integrating over (collapsing) the parameters we only store counts of rules used in the current sampled set of derivations, thereby avoiding explicitly representing the possibly infinite space of translation pairs. We define two operators for our Gibbs sampler, each of which re-samples local derivation structures. Figures 2 and 4 illustrate the permutations these operators make to derivation trees. The omitted tree structure in these figures denotes the Markov blanket of the operator: the structure which is held constant when enumerating the possible outcomes for an operator. The Split/Join operator iterates through the positions between each source word sampling whether a terminal boundary should exist at that position (Figure 2). If the source position adjacent nodes in a ternary rule can be re-parented as a binary rule, or vice-versa. falls between two existing terminals whose target phrases are adjacent, then any new target segmentation within those target phrases can be sampled, including null alignments. If the two existing terminals also share the same parent, then any possible re-ordering is also a valid outcome, as is removing the terminal boundary to form a single phrase pair. Otherwise, if the visited boundary point falls within an existing terminal, then all target split and re-orderings are possible outcomes. The probability for each of these configurations is evaluated (see Figure 3) from which the new configuration is sampled. While the first operator is theoretically capable of exploring the entire derivation forest (by flattening the tree into a single phrase and then splitting), the series of moves required would be highly improbable. To allow for faster mixing we employ the Insert/Delete operator which adds and deletes the parent non-terminal of a pair of adjacent nodes. This is illustrated in Figure 4. The update equations are analogous to those used for the Split/Join operator in Figure 3. In order for this operator to be effective we need to allow greater than binary branching nodes, otherwise deleting a nodes would require sampling from a much larger set of outcomes. Hence our adoption of a ternary branching grammar. Although such a grammar would be very inefficient for a dynamic programming algorithm, it allows our sampler to permute the internal structure of the trees more easily. Our model is parameterised by a vector of hyperparameters, α = which control the sparsity assumption over various model parameters. We could optimise each concentration parameter on the training corpus by hand, however this would be quite an onerous task. Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx — Gamma(10−4,104). This hyper-prior is relatively benign, allowing the model to consider a wide range of values for the hyperparameter. We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx|d, α−) using the MetropolisHastings algorithm. Unlike the Gibbs updates, this calculation cannot be distributed over a cluster (see Section 4.4) and thus is very costly. Therefore for small corpora we re-sample the hyperparameter after every pass through the corpus, for larger experiments we only re-sample every 20 passes. While employing a collapsed Gibbs sampler allows us to efficiently perform inference over the massive space of possible grammars, it induces dependencies between all the sentences in the training corpus. These dependencies make it difficult to scale our approach to larger corpora by distributing it across a number of processors. Recent work (Newman et al., 2007; Asuncion et al., 2008) suggests that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model and diagonal growing heuristic). We conduct experiments on both small and large corpora to allow a range of alignment qualities and also to verify the effectiveness of our distributed approximation of the Bayesian inference. The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al. (2008a) to first create initial trees. Where these factored trees contain nodes with mixed terminals and non-terminals, or more than three non-terminals, we discard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM BLEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. Firstly we evaluate models trained on a small Chinese-English corpus using a Gibbs sampler on a single CPU. This corpus consists of transcribed utterances made available for the IWSLT workshop (Eck and Hori, 2005). The sparse counts and high reordering for this corpus means the GIZA++ model produces very poor alignments. Table 2 shows the results for the benchmark Moses and Hiero systems on this corpus using both the heuristic phrase estimation, and our proposed Bayesian SCFG model. We can see that our model has a slight advantage. When we look at the grammars extracted by the two models we note that the SCFG model creates considerably more translation rules. Normally this would suggest the alignments of the SCFG model are a lot sparser (more unaligned tokens) than those of the heuristic, however this is not the case. The projected SCFG derivations actually produce more alignment points. However these alignments are much more locally consistent, containing fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules. We now test our model’s performance on a larger corpus, representing a realistic SMT experiment with millions of words and long sentences. The Chinese-English training data consists of the FBIS corpus (LDC2003E14) and the first 100k sentences from the Sinorama corpus (LDC2005E47). The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize BLEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty passes through the data. It is clear from the training curves that the distributed approximation tracks the corpus probability of the correct sampler sufficiently closely. This concurs with the findings of Newman et al. (2007) who also observed very little empirical difference between the sampler and its distributed approximation. Tables 3 and 4 show the result on the two NIST corpora when running the distributed sampler on a single 8-core machine.5 These scores tally with our initial hypothesis: that the hierarchical structure of our model suits languages that exhibit less monotone reordering. Figure 5 shows the projected alignment of a headline from the thousandth sample on the NIST Chinese data set. The effect of the grammar based alignment can clearly be seen. Where the combination of GIZA++ and the heuristics creates outlier alignments that impede rule extraction, the SCFG imposes a more rigid hierarchical structure on the alignments. We hypothesise that this property may be particularly useful for syntactic translation models which often have difficulty with inconsistent word alignments not corresponding to syntactic structure. The combined evidence of the ability of our Gibbs sampler to improve posterior likelihood (Figure 6) and our translation experiments demonstrate that we have developed a scalable and effective method for performing inference over phrasal SCFG, without compromising the strong theoretical underpinnings of our model. We have presented a Bayesian model of SCFG induction capable of capturing phrasal units of translational equivalence. Our novel Gibbs sampler over synchronous derivation trees can efficiently draw samples from the posterior, overcoming the limitations of previous models when dealing with long sentences. This avoids explicitly representing the full derivation forest required by dynamic programming approaches, and thus we are able to perform inference without resorting to heuristic restrictions on the model. Initial experiments suggest that this model performs well on languages for which the monotone bias of existing alignment and heuristic phrase extraction approaches fail. These results open the way for the development of more sophisticated models employing grammars capable of capturing a wide range of translation phenomena. In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004).","We have presented a Bayesian model of SCFG induction capable of capturing phrasal units of translational equivalence. Our novel Gibbs sampler over synchronous derivation trees can efficiently draw samples from the posterior, overcoming the limitations of previous models when dealing with long sentences. This avoids explicitly representing the full derivation forest required by dynamic programming approaches, and thus we are able to perform inference without resorting to heuristic restrictions on the model. Initial experiments suggest that this model performs well on languages for which the monotone bias of existing alignment and heuristic phrase extraction approaches fail. These results open the way for the development of more sophisticated models employing grammars capable of capturing a wide range of translation phenomena. In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004)."
9,"Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.","Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data. For English and a handful of other languages, there are large, well-annotated corpora with a variety of linguistic information ranging from named entity to discourse structure. Unfortunately, for the vast majority of languages very few linguistic resources are available. This situation is likely to persist because of the expense of creating annotated corpora that require linguistic expertise (Abeillé, 2003). On the other hand, parallel corpora between many resource-poor languages and resource-rich languages are ample, motivating recent interest in transferring linguistic resources from one language to another via parallel text. For example, several early works (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Merlo et al., 2002) demonstrate transfer of shallow processing tools such as part-of-speech taggers and noun-phrase chunkers by using word-level alignment models (Brown et al., 1994; Och and Ney, 2000). Alshawi et al. (2000) and Hwa et al. (2005) explore transfer of deeper syntactic structure: dependency grammars. Dependency and constituency grammar formalisms have long coexisted and competed in linguistics, especially beyond English (Mel’ˇcuk, 1988). Recently, dependency parsing has gained popularity as a simpler, computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (Eisner, 1996; Yamada and Matsumoto, 2003a; Nivre and Nilsson, 2005; McDonald et al., 2005) as well as unsupervised induction (Klein and Manning, 2004; Smith and Eisner, 2006). Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks. Dependency grammars are arguably more robust to transfer since syntactic relations between aligned words of parallel sentences are better conserved in translation than phrase structure (Fox, 2002; Hwa et al., 2005). Nevertheless, several challenges to accurate training and evaluation from aligned bitext remain: (1) partial word alignment due to non-literal or distant translation; (2) errors in word alignments and source language parses, (3) grammatical annotation choices that differ across languages and linguistic theories (e.g., how to analyze auxiliary verbs, conjunctions). In this paper, we present a flexible learning framework for transferring dependency grammars via bitext using the posterior regularization framework (Graça et al., 2008). In particular, we address challenges (1) and (2) by avoiding commitment to an entire projected parse tree in the target language during training. Instead, we explore formulations of both generative and discriminative probabilistic models where projected syntactic relations are constrained to hold approximately and only in expectation. Finally, we address challenge (3) by introducing a very small number of language-specific constraints that disambiguate arbitrary annotation choices. We evaluate our approach by transferring from an English parser trained on the Penn treebank to Bulgarian and Spanish. We evaluate our results on the Bulgarian and Spanish corpora from the CoNLL X shared task. We see that our transfer approach consistently outperforms unsupervised methods and, given just a few (2 to 7) languagespecific constraints, performs comparably to a supervised parser trained on a very limited corpus (30 - 140 training sentences). At a high level our approach is illustrated in Figure 1(a). A parallel corpus is word-level aligned using an alignment toolkit (Graça et al., 2009) and the source (English) is parsed using a dependency parser (McDonald et al., 2005). Figure 1(b) shows an aligned sentence pair example where dependencies are perfectly conserved across the alignment. An edge from English parent p to child c is called conserved if word p aligns to word p' in the second language, c aligns to c' in the second language, and p' is the parent of c'. Note that we are not restricting ourselves to one-to-one alignments here; p, c, p', and c' can all also align to other words. After filtering to identify well-behaved sentences and high confidence projected dependencies, we learn a probabilistic parsing model using the posterior regularization framework (Graça et al., 2008). We estimate both generative and discriminative models by constraining the posterior distribution over possible target parses to approximately respect projected dependencies and other rules which we describe below. In our experiments we evaluate the learned models on dependency treebanks (Nivre et al., 2007). Unfortunately the sentence in Figure 1(b) is highly unusual in its amount of dependency conservation. To get a feel for the typical case, we used off-the-shelf parsers (McDonald et al., 2005) for English, Spanish and Bulgarian on two bitexts (Koehn, 2005; Tiedemann, 2007) and compared several measures of dependency conservation. For the English-Bulgarian corpus, we observed that 71.9% of the edges we projected were edges in the corpus, and we projected on average 2.7 edges per sentence (out of 5.3 tokens on average). For Spanish, we saw conservation of 64.4% and an average of 5.9 projected edges per sentence (out of 11.5 tokens on average). As these numbers illustrate, directly transferring information one dependency edge at a time is unfortunately error prone for two reasons. First, parser and word alignment errors cause much of the transferred information to be wrong. We deal with this problem by constraining groups of edges rather than a single edge. For example, in some sentence pair we might find 10 edges that have both end points aligned and can be transferred. Rather than requiring our target language parse to contain each of the 10 edges, we require that the expected number of edges from this set is at least 10q, where q is a strength parameter. This gives the parser freedom to have some uncertainty about which edges to include, or alternatively to choose to exclude some of the transferred edges. A more serious problem for transferring parse information across languages are structural differences and grammar annotation choices between the two languages. For example dealing with auxiliary verbs and reflexive constructions. Hwa et al. (2005) also note these problems and solve them by introducing dozens of rules to transform the transferred parse trees. We discuss these differences in detail in the experimental section and use our framework introduce a very small number of rules to cover the most common structural differences. We explored two parsing models: a generative model used by several authors for unsupervised induction and a discriminative model used for fully supervised training. The discriminative parser is based on the edge-factored model and features of the MSTParser (McDonald et al., 2005). The parsing model defines a conditional distribution pg(z I x) over each projective parse tree z for a particular sentence x, parameterized by a vector 0. The probability of any particular parse is where z is a directed edge contained in the parse tree z and φ is a feature function. In the fully supervised experiments we run for comparison, parameter estimation is performed by stochastic gradient ascent on the conditional likelihood function, similar to maximum entropy models or conditional random fields. One needs to be able to compute expectations of the features φ(z, x) under the distribution pθ(z  |x). A version of the insideoutside algorithm (Lee and Choi, 1997) performs this computation. Viterbi decoding is done using Eisner’s algorithm (Eisner, 1996). We also used a generative model based on dependency model with valence (Klein and Manning, 2004). Under this model, the probability of a particular parse z and a sentence with part of speech tags x is given by where r(x) is the part of speech tag of the root of the parse tree z, z is an edge from parent zp to child zc in direction zd, either left or right, and vz indicates valency—false if zp has no other children further from it in direction zd than zc, true otherwise. The valencies vr/vl are marked as true if x has any children on the left/right in z, false otherwise. Graça et al. (2008) introduce an estimation framework that incorporates side-information into unsupervised problems in the form of linear constraints on posterior expectations. In grammar transfer, our basic constraint is of the form: the expected proportion of conserved edges in a sentence pair is at least η (the exact proportion we used was 0.9, which was determined using unlabeled data as described in Section 5). Specifically, let Cx be the set of directed edges projected from English for a given sentence x, then given a parse z, the proportion of conserved edges is f (x, z) = |CX |EzEz 1(z E Cx) and the expected proportion of conserved edges under distribution p(z  |x) is The posterior regularization framework (Graça et al., 2008) was originally defined for generative unsupervised learning. The standard objective is to minimize the negative marginal log-likelihood of the data : E[− log pθ(x)] = �E[− log Ez pθ(z, x)] over the parameters θ (we � use E to denote expectation over the sample sentences x). We typically also add standard regularization term on θ, resulting from a parameter prior − log p(θ) = R(θ), where p(θ) is Gaussian for the MST-Parser models and Dirichlet for the valence model. To introduce supervision into the model, we define a set 2x of distributions over the hidden variables z satisfying the desired posterior constraints in terms of linear equalities or inequalities on feature expectations (we use inequalities in this paper): In this paper, for example, we use the conservededge-proportion constraint as defined above. The marginal log-likelihood objective is then modified with a penalty for deviation from the desired set of distributions, measured by KLdivergence from the set Qx, KL(Qx||pθ(z|x)) = minqEQX KL(q(z)||pθ(z|x)). The generative learning objective is to minimize: For discriminative estimation (Ganchev et al., 2008), we do not attempt to model the marginal distribution of x, so we simply have the two regularization terms: Note that the idea of regularizing moments is related to generalized expectation criteria algorithm of Mann and McCallum (2007), as we discuss in the related work section below. In general, the objectives above are not convex in θ. To optimize these objectives, we follow an Expectation Maximization-like scheme. Recall that standard EM iterates two steps. An E-step computes a probability distribution over the model’s hidden variables (posterior probabilities) and an M-step that updates the model’s parameters based on that distribution. The posterior-regularized EM algorithm leaves the M-step unchanged, but involves projecting the posteriors onto a constraint set after they are computed for each sentence x: arg min KL(q(z) II pθ(z|x)) where pθ(z|x) are the posteriors. The new posteriors q(z) are used to compute sufficient statistics for this instance and hence to update the model’s parameters in the M-step for either the generative or discriminative setting. The optimization problem in Equation 3 can be efficiently solved in its dual formulation: Given λ, the primal solution is given by: q(z) = pθ(z  |x) exp{−λTf(x, z)}/Z, where Z is a normalization constant. There is one dual variable per expectation constraint, and we can optimize them by projected gradient descent, similar to log-linear model estimation. The gradient with respect to λ is given by: b − Eq[f(x, z)], so it involves computing expectations under the distribution q(z). This remains tractable as long as features factor by edge, f(x, z) = &;Ez f(x, z), because that ensures that q(z) will have the same form as pθ(z | x). Furthermore, since the constraints are per instance, we can use incremental or online version of EM (Neal and Hinton, 1998), where we update parameters θ after posterior-constrained E-step on each instance x. We conducted experiments on two languages: Bulgarian and Spanish, using each of the parsing models. The Bulgarian experiments transfer a parser from English to Bulgarian, using the OpenSubtitles corpus (Tiedemann, 2007). The Spanish experiments transfer from English to Spanish using the Spanish portion of the Europarl corpus (Koehn, 2005). For both corpora, we performed word alignments with the open source PostCAT (Graça et al., 2009) toolkit. We used the Tokyo tagger (Tsuruoka and Tsujii, 2005) to POS tag the English tokens, and generated parses using the first-order model of McDonald et al. (2005) with projective decoding, trained on sections 2-21 of the Penn treebank with dependencies extracted using the head rules of Yamada and Matsumoto (2003b). For Bulgarian we trained the Stanford POS tagger (Toutanova et al., 2003) on the Bulgtreebank corpus from CoNLL X. The Spanish Europarl data was POS tagged with the FreeLing language analyzer (Atserias et al., 2006). The discriminative model used the same features as MSTParser, summarized in Table 1. In order to evaluate our method, we a baseline inspired by Hwa et al. (2005). The baseline constructs a full parse tree from the incomplete and possibly conflicting transferred edges using a simple random process. We start with no edges and try to add edges one at a time verifying at each step that it is possible to complete the tree. We first try to add the transferred edges in random order, then for each orphan node we try all possible parents (both in random order). We then use this full labeling as supervision for a parser. Note that this baseline is very similar to the first iteration of our model, since for a large corpus the different random choices made in different sentences tend to smooth each other out. We also tried to create rules for the adoption of orphans, but the simple rules we tried added bias and performed worse than the baseline we report. Table 2 shows attachment accuracy of our method and the baseline for both language pairs under several conditions. By attachment accuracy we mean the fraction of words assigned the correct parent. The experimental details are described in this section. Link-left baselines for these corpora are much lower: 33.8% and 27.9% for Bulgarian and Spanish respectively. Preliminary experiments showed that our word alignments were not always appropriate for syntactic transfer, even when they were correct for translation. For example, the English “bike/V” could be translated in French as “aller/V en vélo/N”, where the word “bike” would be aligned with “vélo”. While this captures some of the semantic shared information in the two languages, we have no expectation that the noun “vélo” will have a similar syntactic behavior to the verb “bike”. To prevent such false transfer, we filter out alignments between incompatible POS tags. In both language pairs, filtering out noun-verb alignments gave the biggest improvement. Both corpora also contain sentence fragments, either because of question responses or fragmented speech in movie subtitles or because of voting announcements and similar formulaic sentences in the parliamentary proceedings. We overcome this problem by filtering out sentences that do not have a verb as the English root or for which the English root is not aligned to a verb in the target language. For the subtitles corpus we also remove sentences that end in an ellipsis or contain more than one comma. Finally, following (Klein and Manning, 2004) we strip out punctuation from the sentences. For the discriminative model this did not affect results significantly but improved them slightly in most cases. We found that the generative model gets confused by punctuation and tends to predict that periods at the end of sentences are the parents of words in the sentence. Our basic model uses constraints of the form: the expected proportion of conserved edges in a sentence pair is at least q = 90%.1 We call the generic model described above “norules” to distinguish it from the language-specific constraints we introduce in the sequel. The no rules columns of Table 2 summarize the performance in this basic setting. Discriminative models outperform the generative models in the majority of cases. The left panel of Table 3 shows the most common errors by child POS tag, as well as by true parent and guessed parent POS tag. Figure 2 shows that the discriminative model continues to improve with more transfer-type data 1We chose rl in the following way: we split the unlabeled parallel text into two portions. We trained a models with different rl on one portion and ran it on the other portion. We chose the model with the highest fraction of conserved constraints on the second portion. up to at least 40 thousand sentences. Using the straightforward approach outlined above is a dramatic improvement over the standard link-left baseline (and the unsupervised generative model as we discuss below), however it doesn’t have any information about the annotation guidelines used for the testing corpus. For example, the Bulgarian corpus has an unusual treatment of nonfinite clauses. Figure 4 shows an example. We see that the “,qa” is the parent of both the verb and its object, which is different than the treatment in the English corpus. We propose to deal with these annotation dissimilarities by creating very simple rules. For Spanish, we have three rules. The first rule sets main verbs to dominate auxiliary verbs. Specifically, whenever an auxiliary precedes a main verb the main verb becomes its parent and adopts its children; if there is only one main verb it becomes the root of the sentence; main verbs also become parents of pronouns, adverbs, and common nouns that directly preceed auxiliary verbs. By adopting children we mean that we change the parent of transferred edges to be the adopting node. The second Spanish rule states that the first element of an adjective-noun or noun-adjective pair dominates the second; the first element also adopts the children of the second element. The third and final Spanish rule sets all prepositions to be children of the first main verb in the sentence, unless the preposition is a “de” located between two noun phrases. In this later case, we set the closest noun in the first of the two noun phrases as the preposition’s parent. For Bulgarian the first rule is that “,qa” should dominate all words until the next verb and adopt their noun, preposition, particle and adverb children. The second rule is that auxiliary verbs should dominate main verbs and adopt their children. We have a list of 12 Bulgarian auxiliary verbs. The “seven rules” experiments add rules for 5 more words similar to the rule for “,qa”, specifically “zIe”, “JIH”, “KaKB♦”, “He”, “3a”. Table 3 compares the errors for different linguistic rules. When we train using the “,qa” rule and the rules for auxiliary verbs, the model learns that main verbs attach to auxiliary verbs and that “,qa” dominates its nonfinite clause. This causes an improvement in the attachment of verbs, and also drastically reduces words being attached to verbs instead of particles. The latter is expected because “,qa” is analyzed as a particle in the Bulgarian POS tagset. We see an improvement in root/verb confusions since “,qa” is sometimes errenously attached to a the following verb rather than being the root of the sentence. The rightmost panel of Table 3 shows similar analysis when we also use the rules for the five other closed-class words. We see an improvement in attachments in all categories, but no qualitative change is visible. The reason for this is probably that these words are relatively rare, but by encouraging the model to add an edge, it also rules out incorrect edges that would cross it. Consequently we are seeing improvements not only directly from the constraints we enforce but also indirectly as types of edges that tend to get ruled out. The generative model we use is a state of the art model for unsupervised parsing and is our only fully unsupervised baseline. As smoothing we add a very small backoff probability of 4.5 x 10−5 to each learned paramter. Unfortunately, we found generative model performance was disappointing overall. The maximum unsupervised accuracy it achieved on the Bulgarian data is 47.6% with initialization from Klein and Manning (2004) and this result is not stable. Changing the initialization parameters, training sample, or maximum sentence length used for training drastically affected the results, even for samples with several thousand sentences. When we use the transferred information to constrain the learning, EM stabilizes and achieves much better performance. Even setting all parameters equal at the outset does not prevent the model from learning the dependency structure of the aligned language. The top panels in Figure 5 show the results in this setting. We see that performance is still always below the accuracy achieved by supervised training on 20 annotated sentences. However, the improvement in stability makes the algorithm much more usable. As we shall see below, the discriminative parser performs even better than the generative model. u We trained our discriminative parser for 100 iterations of online EM with a Gaussian prior variance of 100. Results for the discriminative parser are shown in the bottom panels of Figure 5. The supervised experiments are given to provide context for the accuracies. For Bulgarian, we see that without any hints about the annotation guidelines, the transfer system performs better than an unsu% pervised parser, comparable to a supervised parser trained on 10 sentences. However, if we specify just the two rules for “da” and verb conjugations performance jumps to that of training on 6070 fully labeled sentences. If we have just a little more prior knowledge about how closed-class words are handled, performance jumps above 140 fully labeled sentence equivalent. We observed another desirable property of the discriminative model. While the generative model can get confused and perform poorly when the training data contains very long sentences, the discriminative parser does not appear to have this drawback. In fact we observed that as the maximum training sentence length increased, the parsing performance also improved. Our work most closely relates to Hwa et al. (2005), who proposed to learn generative dependency grammars using Collins’ parser (Collins, 1999) by constructing full target parses via projected dependencies and completion/transformation rules. Hwa et al. (2005) found that transferring dependencies directly was not sufficient to get a parser with reasonable performance, even when both the source language parses and the word alignments are performed by hand. They adjusted for this by introducing on the order of one or two dozen language-specific transformation rules to complete target parses for unaligned words and to account for diverging annotation rules. Transferring from English to Spanish in this way, they achieve 72.1% and transferring to Chinese they achieve 53.9%. Our learning method is very closely related to the work of (Mann and McCallum, 2007; Mann and McCallum, 2008) who concurrently developed the idea of using penalties based on posterior expectations of features not necessarily in the model in order to guide learning. They call their method generalized expectation constraints or alternatively expectation regularization. In this volume (Druck et al., 2009) use this framework to train a dependency parser based on constraints stated as corpus-wide expected values of linguistic rules. The rules select a class of edges (e.g. auxiliary verb to main verb) and require that the expectation of these be close to some value. The main difference between this work and theirs is the source of the information (a linguistic informant vs. cross-lingual projection). Also, we define our regularization with respect to inequality constraints (the model is not penalized for exceeding the required model expectations), while they require moments to be close to an estimated value. We suspect that the two learning methods could perform comparably when they exploit similar information. In this paper, we proposed a novel and effective learning scheme for transferring dependency parses across bitext. By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses. We show that discriminative training generally outperforms generative approaches even in this very weakly supervised setting. By adding easily specified languagespecific constraints, our models begin to rival strong supervised baselines for small amounts of data. Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.","In this paper, we proposed a novel and effective learning scheme for transferring dependency parses across bitext. By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses. We show that discriminative training generally outperforms generative approaches even in this very weakly supervised setting. By adding easily specified languagespecific constraints, our models begin to rival strong supervised baselines for small amounts of data. Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies."
10,"Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.","Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While ILP is of course also NP-hard, we show that, empirically, exact solutions are found very quickly for most problem instances. In an experiment intended to illustrate the practicality of the ILP approach, we show speed and search accuracy results for aligning phrases under a standard phrase translation model. Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. A sentence pair consists of two word sequences, e and f. A set of phrases {eij} contains all spans eij from between-word positions i to j of e. A link is an aligned pair of phrases, denoted (eij, fkl).' Let a weighted sentence pair additionally include a real-valued function 0 : {eij}x{fkl} —* R, which scores links. 0(eij, fkl) can be sentence-specific, for example encoding the product of a translation model and a distortion model for (eij, fkl). We impose no additional restrictions on 0 for our analysis. An alignment is a set of links. Given a weighted sentence pair, we will consider the space of bijective phrase alignments A: those a C {eij} x {fkl} that use each word token in exactly one link. We first define the notion of a partition: UiSi = T means Si are pairwise disjoint and cover T. Then, we can formally define the set of bijective phrase alignments: Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al. (2003). For a weighted sentence pair (e, f, φ), let the score of an alignment be the product of its link scores: Four related problems involving scored alignments arise when training phrase alignment models. OPTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a. DECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) > 1. O arises in the popular Viterbi approximation to EM (Hard EM) that assumes probability mass is concentrated at the mode of the posterior distribution over alignments. D is the corresponding decision problem for O, useful in analysis. EXPECTATION, £: Given a weighted sentence pair (e, f, φ) and indices i, j, k,l, compute Ea φ(a) over all a E A such that (eij, fkl) E a. SUM, S: Given (e, f, φ), compute EaEA φ(a). £ arises in computing sufficient statistics for re-estimating phrase translation probabilities (Estep) when training models. The existence of a polynomial time algorithm for £ implies a polynomial time algorithm for S, because A = U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}. For the space A of bijective alignments, problems £ and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). We give a novel proof that O is NP-hard, showing that D is NP-complete by reduction from SAT, the boolean satisfiability problem. This result holds despite the fact that the related problem of finding an optimal matching in a weighted bipartite graph (the ASSIGNMENT problem) is polynomialtime solvable using the Hungarian algorithm. A reduction proof of NP-completeness gives a construction by which a known NP-complete problem can be solved via a newly proposed problem. From a SAT instance, we construct a weighted sentence pair for which alignments with positive score correspond exactly to the SAT solutions. Since SAT is NPcomplete and our construction requires only polynomial time, we conclude that D is NP-complete.2 SAT: Given vectors of boolean variables v = (v) and propositional clauses3 C = (C), decide whether there exists an assignment to v that simultaneously satisfies each clause in C. For a SAT instance (v, C), we construct f to contain one word for each clause, and e to contain several copies of the literals that appear in those clauses. φ scores only alignments from clauses to literals that satisfy the clauses. The crux of the construction lies in ensuring that no variable is assigned both true and false. The details of constructing such a weighted sentence pair wsp(v, C) = (e, f, φ), described below, are also depicted in figure 1. Then, we set φ(·, ·) = 0 everywhere except: Proof. The score implies that f aligns using all oneword phrases and Vai E a, 0(ai) = 1. By condition 4, each fassign(v) aligns to all v� or all v in e. Then, assign each v to true if fassign(v) aligns to all v, and false otherwise. By condition 3, each C must align to a satisfying literal, while condition 4 assures that all available literals are consistent with this assignment to v, which therefore satisfies C. Claim 2. If (v, C) is satisfiable, then wsp(v, C) has an alignment a with 0(a) = 1. Proof. We construct such an alignment a from the satisfying assignment v. For each C, we choose a satisfying literal E consistent with the assignment. Align fC to the first available E token in e if the corresponding v is true, or the last if v is false. Align each fassign(v) to all remaining literals for v. Claims 1 and 2 together show that D is NPcomplete, and therefore that O is NP-hard. With another construction, we can show that S is #Phard, meaning that it is at least as hard as any #Pcomplete problem. #P is a class of counting problems related to NP, and #P-hard problems are NPhard as well. Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipartite graph G with edge set E = {(vj, vl)}, we construct e and f with n words each, and set 0(ej−1 j, fl−1 l) = 1 and 0 otherwise. The number of perfect matchings in G is the sum S for this weighted sentence pair. CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard.","Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipartite graph G with edge set E = {(vj, vl)}, we construct e and f with n words each, and set 0(ej−1 j, fl−1 l) = 1 and 0 otherwise. The number of perfect matchings in G is the sum S for this weighted sentence pair. CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard."
11,"mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.","mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation process. Therefore, it not only describes non-syntactic phrases with syntactic structure information, but also supports multi-level tree structure reordering in larger span. These give our model much more expressive power and flexibility than those previous models. Experiment results on the NIST MT-2005 ChineseEnglish translation task show that our method significantly outperforms Moses (Koehn et al., 2007), a state-of-the-art phrase-based SMT system, and other linguistically syntax-based methods, such as SCFG-based and STSG-based methods (Zhang et al., 2007). In addition, our study further demonstrates that 1) structure reordering rules in our model are very useful for performance improvement while discontinuous phrase rules have less contribution and 2) tree sequence rules are able to model non-syntactic phrases with syntactic structure information, and thus contribute much to the performance improvement, but those rules consisting of more than three sub-trees have almost no contribution. The rest of this paper is organized as follows: Section 2 reviews previous work. Section 3 elaborates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7. Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language model under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. the solution shows effective empirically, it only utilizes the source side syntactic phrases of the input parse tree during decoding. Furthermore, the translation probabilities of the bilingual phrases and other tree-to-string rules are not compatible since they are estimated independently, thus having different parameter spaces. To address the above problems, Liu et al. (2007) propose to use forest-to-string rules to enhance the expressive power of their tree-to-string model. As is inherent in a tree-to-string framework, Liu et al.’s method defines a kind of auxiliary rules to integrate forestto-string rules into tree-to-string models. One problem of this method is that the auxiliary rules are not described by probabilities since they are constructed during decoding, rather than learned from the training corpus. So, to balance the usage of different kinds of rules, they use a very simple feature counting the number of auxiliary rules used in a derivation for penalizing the use of forest-to-string and auxiliary rules. In this paper, an alternative solution is presented to combine the strengths of phrase-based and syntax-based methods. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al., 2006), nor depends on pseudo nodes (Marcu et al., 2006) or auxiliary rules (Liu et al., 2007). We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model. To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT. The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words). Given a pair of source and target parse trees (1 ) Fig. 1, Fig. 2 illustrates two examples of tree sequences derived from the two parse trees. A tree sequence translation rule r is a pair of aligned tree sequences r =< TS f j , two tree sequences, satisfying the following condition: `d (i, j) E A : i1 < i < i2 H j1 < j < j2 . Fig. 3 shows two rules extracted from the tree pair shown in Fig. 1, where r1 is a tree-to-tree rule and r2 is a tree sequence-to-tree sequence rule. Obviously, tree sequence rules are more powerful than phrases or tree rules as they can capture all phrases (including both syntactic and non-syntactic phrases) with syntactic structure information and allow any tree node operations in a longer span. We expect that these properties can well address the issues of non-isomorphic structure alignments, structure reordering, non-syntactic phrases and discontinuous phrases translations. Given the source and target sentences f1J and e; and their parse trees (1 ) sequence-to-tree sequence translation model is formulated as: In our implementation, we have: By Eq. (2), translation becomes a tree structure mapping issue. We model it using our tree sequence-based translation rules. Given the source parse tree (1 ) that could lead to the same target tree T(e;) , the mapping probability Pr (T (e;)  |T (f J )) is obtained by summing over the probabilities of all derivations. The probability of each derivationθ is given as the product of the probabilities of all the rules p(ri ) used in the derivation (here we assume that Eq. (3) formulates the tree sequence alignmentbased translation model. Figs. 1 and 3 show how the proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree. Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not distinguish between larger (taller) and shorter source side tree sequences, i.e. we let these rules compete directly with each other. Rules are extracted from word-aligned, bi-parsed sentence pairs < T (fJ ), T (e; ), A > , which are classified into two categories: 2) Extracting abstract rules from extracted initial rules with the help of sub initial rules. It is straightforward to extract initial rules. We first generate all fully lexicalized source and target tree sequences using a dynamic programming algorithm and then iterate over all generated source and target tree sequence pairs < TS f j TS e i > . If leaf nodes of TS(f jj2 ) and TS(e1) . We then derive abstract rules from initial rules by removing one or more of its sub initial rules. The abstract rule extraction algorithm presented next is implemented using dynamic programming. Due to space limitation, we skip the details here. In order to control the number of rules, we set three constraints for both finally extracted initial and abstract rules: 1) The depth of a tree in a rule is not greater than h . 2) The number of non-terminals as leaf nodes is not greater than c . 3) The tree number in a rule is not greater than d. In addition, we limit initial rules to have at most seven lexical words as leaf nodes on either side. However, in order to extract long-distance reordering rules, we also generate those initial rules with more than seven lexical words for abstract rules extraction only (not used in decoding). This makes our abstract rules more powerful in handling global structure reordering. Moreover, by configuring these parameters we can implement other translation models easily: 1) STSG-based model when d =1 ; 2) SCFG-based model when d =1 and h = 2 ; 3) phrase-based translation model only (no reordering model) when c = 0 and h =1. co-indexing the pairs of non-terminals that rooting the removed source and target parts 13: output the hypothesis with the highest score in h[1, J] as the final best translation The decoder is a span-based beam search together with a function for mapping the source derivations to the target ones. Algorithm 2 illustrates the decoding algorithm. It translates each span iteratively from small one to large one (lines 1-2). This strategy can guarantee that when translating the current span, all spans smaller than the current one have already been translated before if they are translatable (line 7). When translating a span, if the usable rule is an initial rule, then the tree sequence on the target side of the rule is a candidate translation (lines 4-5). Otherwise, we replace the nonterminal leaf nodes of the current abstract rule with their corresponding spans’ translations that are already translated in previous steps (line 7). To speed up the decoder, we use several thresholds to limit search beams for each span: It is worth noting that the decoder does not force a complete target parse tree to be generated. If no rules can be used to generate a complete target parse tree, the decoder just outputs whatever have phrase rules2. Finally, we investigate the impact of maximal sub-tree number and sub-tree depth in our model. All of the following discussions are held on the training and test data. been translated so far monotonically as one hypothesis. We conducted Chinese-to-English translation experiments. We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and development sets: c =3, α =20, β =-100 and y =100. We carried out a number of experiments to examine the proposed tree sequence alignment-based translation model. In this subsection, we first report the rule distributions and compare our model with the three baseline systems. Then we study the model’s expressive ability by comparing the contributions made by different kinds of rules, including strict tree sequence rules, non-syntactic phrase rules, structure reordering rules and discontinuous tured by the two syntax-based models through tree node operations. • Our model is much more effective in utilizing linguistic structures than STSG since it uses tree sequence as basic translation unit. This allows our model not only to handle structure reordering by tree node operations in a larger span, but also to capture non-syntactic phrases, which circumvents previous syntactic constraints, thus giving our model more expressive power. 3) The linguistically motivated SCFG shows much lower performance. This is largely because SCFG only allows sibling nodes reordering and fails to utilize both non-syntactic phrases and those syntactic phrases that cannot be covered by a single CFG rule. It thereby suggests that SCFG is less effective in modelling parse tree structure transfer between Chinese and English when using Penn Treebank style linguistic grammar and under wordalignment constraints. However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). As a result, it is more robust to the issue of non-syntactic phrase usage and non-isomorphic structure alignment. 26.07 further improves the performance. It suggests that they are complementary to each other since the lexicalized TSRs are used to model non-syntactic phrases while the other two kinds of TSRs can generalize the lexicalized rules to unseen phrases. 2) The lexicalized TSRs make the major contribution since they can capture non-syntactic phrases with syntactic structure features. refers to the structure reordering rules that have at least two non-terminal leaf nodes with inverted order in the source and target sides, which are usually not captured by phrase-based models. Note that the reordering between lexical words and non-terminal leaf nodes is not considered here) and Discontinuous Phrase Rules (DPR: refers to these rules having at least one non-terminal leaf node between two lexicalized leaf nodes) in our tree sequence-based model (d = 4 and h = 6 ) Table 3 shows the contributions of SRR and DPR. It clearly indicates that SRRs are very effective in reordering structures, which improve performance by 1.45 (26.07-24.62) BLEU score. However, DPRs have less impact on performance in our tree sequence-based model. This seems in contradiction to the previous observations3 in literature. However, it is not surprising simply because we use tree sequences as the basic translation units. Thereby, our model can capture all phrases. In this sense, our model behaves like a phrasebased model, less sensitive to discontinuous phrases (Wellington et al., 2006). Our additional experiments also verify that discontinuous phrase rules are complementary to syntactic phrase rules (Bod, 2007) while non-syntactic phrase rules may compromise the contribution of discontinuous phrase rules. Table 4 reports the numbers of these two kinds of rules. It shows that around 30% rules are shared by the two kinds of rule sets. These overlapped rules contain at least two non-terminal leaf nodes plus two terminal leaf nodes, which implies that longer rules do not affect performance too much. Fig. 5 studies the impact when setting different maximal tree depth ( h ) in a rule on the performance. It demonstrates that: 1) Significant performance improvement is achieved when the value of h is increased from 1 to 2. This can be easily explained by the fact that when h = 1, only monotonic search is conducted, while h =2 allows non-terminals to be leaf nodes, thus introducing preliminary structure features to the search and allowing non-monotonic search. 2) Internal structures and large span (due to h increasing) are also useful as attested by the gain of 0.86 (26.14-25.28) Blue score when the value of h increases from 2 to 4. Fig. 6 studies the impact on performance by setting different maximal tree number (d) in a rule. It further indicates that: 1) Tree sequence rules (d >1) are useful and even more helpful if we limit the tree depth to no more than two (lower line, h=2). However, tree sequence rules consisting of more than three subtrees have almost no contribution to the performance improvement. This is mainly due to data sparseness issue when d >3. 2) Even if only two-layer sub-trees (lower line) are allowed, our method still outperforms STSG and Moses when d>1. This further validates the effectiveness of our design philosophy of using multi-sub-trees as basic translation unit in SMT.","We conducted Chinese-to-English translation experiments. We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and development sets: c =3, α =20, β =-100 and y =100. We carried out a number of experiments to examine the proposed tree sequence alignment-based translation model. In this subsection, we first report the rule distributions and compare our model with the three baseline systems. Then we study the model’s expressive ability by comparing the contributions made by different kinds of rules, including strict tree sequence rules, non-syntactic phrase rules, structure reordering rules and discontinuous tured by the two syntax-based models through tree node operations. • Our model is much more effective in utilizing linguistic structures than STSG since it uses tree sequence as basic translation unit. This allows our model not only to handle structure reordering by tree node operations in a larger span, but also to capture non-syntactic phrases, which circumvents previous syntactic constraints, thus giving our model more expressive power. 3) The linguistically motivated SCFG shows much lower performance. This is largely because SCFG only allows sibling nodes reordering and fails to utilize both non-syntactic phrases and those syntactic phrases that cannot be covered by a single CFG rule. It thereby suggests that SCFG is less effective in modelling parse tree structure transfer between Chinese and English when using Penn Treebank style linguistic grammar and under wordalignment constraints. However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). As a result, it is more robust to the issue of non-syntactic phrase usage and non-isomorphic structure alignment. 26.07 further improves the performance. It suggests that they are complementary to each other since the lexicalized TSRs are used to model non-syntactic phrases while the other two kinds of TSRs can generalize the lexicalized rules to unseen phrases. 2) The lexicalized TSRs make the major contribution since they can capture non-syntactic phrases with syntactic structure features. refers to the structure reordering rules that have at least two non-terminal leaf nodes with inverted order in the source and target sides, which are usually not captured by phrase-based models. Note that the reordering between lexical words and non-terminal leaf nodes is not considered here) and Discontinuous Phrase Rules (DPR: refers to these rules having at least one non-terminal leaf node between two lexicalized leaf nodes) in our tree sequence-based model (d = 4 and h = 6 ) Table 3 shows the contributions of SRR and DPR. It clearly indicates that SRRs are very effective in reordering structures, which improve performance by 1.45 (26.07-24.62) BLEU score. However, DPRs have less impact on performance in our tree sequence-based model. This seems in contradiction to the previous observations3 in literature. However, it is not surprising simply because we use tree sequences as the basic translation units. Thereby, our model can capture all phrases. In this sense, our model behaves like a phrasebased model, less sensitive to discontinuous phrases (Wellington et al., 2006). Our additional experiments also verify that discontinuous phrase rules are complementary to syntactic phrase rules (Bod, 2007) while non-syntactic phrase rules may compromise the contribution of discontinuous phrase rules. Table 4 reports the numbers of these two kinds of rules. It shows that around 30% rules are shared by the two kinds of rule sets. These overlapped rules contain at least two non-terminal leaf nodes plus two terminal leaf nodes, which implies that longer rules do not affect performance too much. Fig. 5 studies the impact when setting different maximal tree depth ( h ) in a rule on the performance. It demonstrates that: 1) Significant performance improvement is achieved when the value of h is increased from 1 to 2. This can be easily explained by the fact that when h = 1, only monotonic search is conducted, while h =2 allows non-terminals to be leaf nodes, thus introducing preliminary structure features to the search and allowing non-monotonic search. 2) Internal structures and large span (due to h increasing) are also useful as attested by the gain of 0.86 (26.14-25.28) Blue score when the value of h increases from 2 to 4. Fig. 6 studies the impact on performance by setting different maximal tree number (d) in a rule. It further indicates that: 1) Tree sequence rules (d >1) are useful and even more helpful if we limit the tree depth to no more than two (lower line, h=2). However, tree sequence rules consisting of more than three subtrees have almost no contribution to the performance improvement. This is mainly due to data sparseness issue when d >3. 2) Even if only two-layer sub-trees (lower line) are allowed, our method still outperforms STSG and Moses when d>1. This further validates the effectiveness of our design philosophy of using multi-sub-trees as basic translation unit in SMT."
12,"Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.","Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline. Over the past decade, feature-based discriminative models have become the tool of choice for many natural language processing tasks. Although they take much longer to train than generative models, they typically produce higher performing systems, in large part due to the ability to incorporate arbitrary, potentially overlapping features. However, constituency parsing remains an area dominated by generative methods, due to the computational complexity of the problem. Previous work on discriminative parsing falls under one of three approaches. One approach does discriminative reranking of the n-best list of a generative parser, still usually depending highly on the generative parser score as a feature (Collins, 2000; Charniak and Johnson, 2005). A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences. However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effectively ignores the largest advantage of discriminative training. It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering. For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%. When they add in only a small set of orthographic features, their CRF error rate drops considerably more to 4.3%, and their out-of-vocabulary error rate drops by more than half. This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods. In this work, we provide just such a framework for training a feature-rich discriminative parser. Unlike previous work, we do not restrict ourselves to short sentences, but we do provide results both for training and testing on sentences of length < 15 (WSJ15) and for training and testing on sentences of length < 40, allowing previous WSJ15 results to be put in context with respect to most modern parsing literature. Our model is a conditional random field based model. For a rule application, we allow arbitrary features to be defined over the rule categories, span and split point indices, and the words of the sentence. It is well known that constituent length influences parse probability, but PCFGs cannot easily take this information into account. Another benefit of our feature based model is that it effortlessly allows smoothing over previously unseen rules. While the rule may be novel, it will likely contain features which are not. Practicality comes from three sources. We made use of stochastic optimization methods which allow us to find optimal model parameters with very few passes through the data. We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS. We also used limited parallelization, and prefiltering of the chart to avoid scoring rules which cannot tile into complete parses of the sentence. This speed-up does not come with a performance cost; we attain an F-score of 90.9%, a 14% relative reduction in errors over previous work on WSJ15. Our parsing model is based on a conditional random field model, however, unlike previous TreeCRF work, e.g., (Cohn and Blunsom, 2005; Jousse et al., 2006), we do not assume a particular tree structure, and instead find the most likely structure and labeling. This is similar to conventional probabilistic context-free grammar (PCFG) parsing, with two exceptions: (a) we maximize conditional likelihood of the parse tree, given the sentence, not joint likelihood of the tree and sentence; and (b) probabilities are normalized globally instead of locally – the graphical models depiction of our trees is undirected. Formally, we have a CFG G, which consists of (Manning and Sch¨utze, 1999): (i) a set of terminals {wk},k = 1,...,V; (ii) a set of nonterminals {Nk},k = 1,...,n; (iii) a designated start symbol ROOT; and (iv) a set of rules, {ρ = Ni —* ζ j}, where ζ j is a sequence of terminals and nonterminals. A PCFG additionally assigns probabilities to each rule ρ such that Vi∑j P(Ni —* ζ j) = 1. Our conditional random field CFG (CRF-CFG) instead defines local clique potentials φ(r|s;θ), where s is the sentence, and r contains a one-level subtree of a tree t, corresponding to a rule ρ, along with relevant information about the span of words which it encompasses, and, if applicable, the split position (see Figure 1). These potentials are relative to the sentence, unlike a PCFG where rule scores do not have access to words at the leaves of the tree, or even how many words they dominate. We then define a conditional probability distribution over entire trees, using the standard CRF distribution, shown in (1). There is, however, an important subtlety lurking in how we define the partition function. The partition function Zs, which makes the probability of all possible parses sum to unity, is defined over all structures as well as all labelings of those structures. We define τ(s) to be the set of all possible parse trees for the given sentence licensed by the grammar G. where The above model is not well-defined over all CFGs. Unary rules of the form Ni —* Nj can form cycles, leading to infinite unary chains with infinite mass. However, it is standard in the parsing literature to transform grammars into a restricted class of CFGs so as to permit efficient parsing. Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995). To address this issue, we define our model over a restricted class of CFGs which limits unary chains to not have any repeated states. This was done by collapsing all allowed unary chains to single unary rules, and disallowing multiple unary rule applications over the same span.1 We give the details of our binarization scheme in Section 5. Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG. Our clique potentials take an exponential form. We have a feature function, represented by f(r,s), which returns a vector with the value for each feature. We denote the value of feature fi by fi(r,s) and our model has a corresponding parameter θi for each feature. The clique potential function is then: The log conditional likelihood of the training data D, with an additional L2 regularization term, is then: And the partial derivatives of the log likelihood, with respect to the model weights are, as usual, the difference between the empirical counts and the model expectations: 1In our implementation of the inside-outside algorithm, we then need to keep two inside and outside scores for each span: one from before and one from after the application of unary rules. The partition function Zs and the partial derivatives can be efficiently computed with the help of the inside-outside algorithm.2 Zs is equal to the inside score of ROOT over the span of the entire sentence. To compute the partial derivatives, we walk through each rule, and span/split, and add the outside log-score of the parent, the inside log-score(s) of the child(ren), and the log-score for that rule and span/split. Zs is subtracted from this value to get the normalized log probability of that rule in that position. Using the probabilities of each rule application, over each span/split, we can compute the expected feature values (the second term in Equation 4), by multiplying this probability by the value of the feature corresponding to the weight for which we are computing the partial derivative. The process is analogous to the computation of partial derivatives in linear chain CRFs. The complexity of the algorithm for a particular sentence is O(n3), where n is the length of the sentence. Unlike (Taskar et al., 2004), our algorithm has the advantage of being easily parallelized (see footnote 7 in their paper). Because the computation of both the log likelihood and the partial derivatives involves summing over each tree individually, the computation can be parallelized by having many clients which each do the computation for one tree, and one central server which aggregates the information to compute the relevant information for a set of trees. Because we use a stochastic optimization method, as discussed in Section 3, we compute the objective for only a small portion of the training data at a time, typically between 15 and 30 sentences. In 2In our case the values in the chart are the clique potentials which are non-negative numbers, but not probabilities. this case the gains from adding additional clients decrease rapidly, because the computation time is dominated by the longest sentences in the batch. Training is also sped up by prefiltering the chart. On the inside pass of the algorithm one will see many rules which cannot actually be tiled into complete parses. In standard PCFG parsing it is not worth figuring out which rules are viable at a particular chart position and which are not. In our case however this can make a big difference.We are not just looking up a score for the rule, but must compute all the features, and dot product them with the feature weights, which is far more time consuming. We also have to do an outside pass as well as an inside one, which is sped up by not considering impossible rule applications. Lastly, we iterate through the data multiple times, so if we can compute this information just once, we will save time on all subsequent iterations on that sentence. We do this by doing an insideoutside pass that is just boolean valued to determine which rules are possible at which positions in the chart. We simultaneously compute the features for the possible rules and then save the entire data structure to disk. For all but the shortest of sentences, the disk I/O is easily worth the time compared to recomputation. The first time we see a sentence this method is still about one third faster than if we did not do the prefiltering, and on subsequent iterations the improvement is closer to tenfold. Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches. In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time. One early experiment on WSJ15 showed a seven time speed up. Utilization of stochastic optimization routines requires the implementation of a stochastic objective function. This function, Lˆ is designed to approximate the true function L based off a small subset of the training data represented by Db. Here b, the batch size, means that Db is created by drawing b training examples, with replacement, from the training set D. With this notation we can express the stochastic evaluation of the function as Lˆ (Db;e). This stochastic function must be designed to ensure that: Note that this property is satisfied, without scaling, for objective functions that sum over the training data, as it is in our case, but any priors must be scaled down by a factor of b/|D|. The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value. SGD was implemented using the standard update: ek+1 = ek − gk0L (D(k) And employed a gain schedule in the form where parameter τ was adjusted such that the gain is halved after five passes through the data. We found that an initial gain of η0 = 0.1 and batch size between 15 and 30 was optimal for this application. As discussed in Section 5 we performed experiments on both sentences of length < 15 and length < 40. All feature development was done on the length 15 corpus, due to the substantially faster train and test times. This has the unfortunate effect that our features are optimized for shorter sentences and less training data, but we found development on the longer sentences to be infeasible. Our features are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence). We ran two kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993). We used the standard splits, training on sections 2 to 21, testing on section 23 and doing development on section 22. Previous work on (non-reranking) discriminative parsing has given results on sentences of length < 15, but most parsing literature gives results on either sentences of length < 40, or all sentences. To properly situate this work with respect to both sets of literature we trained models on both length < 15 (WSJ15) and length < 40 (WSJ40), and we also tested on all sentences using the WSJ40 models. Our results also provide a context for interpreting previous work which used WSJ15 and not WSJ40. We used a relatively simple grammar with few additional annotations. Starting with the grammar read off of the training set, we added parent annotations onto each state, including the POS tags, resulting in rules such as S-ROOT —* NP-S VP-S. We also added head tag annotations to VPs, in the same manner as (Klein and Manning, 2003). Lastly, for the WSJ40 runs we used a simple, right branching binarization where each active state is annotated with its previous sibling and first child. This is equivalent to children of a state being produced by a second order Markov process. For the WSJ15 runs, each active state was annotated with only its first child, which is equivalent to a first order Markov process. See Table 5 for the number of states and rules produced. For both WSJ15 and WSJ40, we trained a generative model; a discriminative model, which used lexicon features, but no grammar features other than the rules themselves; and a feature-based model which had access to all features. For the length 15 data we also did experiments in which we relaxed the grammar. By this we mean that we added (previously unseen) rules to the grammar, as a means of smoothing. We chose which rules to add by taking existing rules and modifying the parent annotation on the parent of the rule. We used stochastic gradient descent for if some child is a verb tag, then rule, with that child replaced by the word Unaries which span one word: (r,w) (r,ds(w)) (b(p(r)),w) (b(p(r)),ds(w)) these experiments; the length 15 models had a batch size of 15 and we allowed twenty passes through the data.3 The length 40 models had a batch size of 30 and we allowed ten passes through the data. We used development data to decide when the models had converged. Additionally, we provide generative numbers for training on the entire PTB to give a sense of how much performance suffered from the reduced training data (generative-all in Table 4). The full results for WSJ15 are shown in Table 3 and for WSJ40 are shown in Table 4. The WSJ15 models were each trained on a single Dual-Core AMD OpteronTM using three gigabytes of RAM and no parallelization. The discriminatively trained generative model (discriminative in Table 3) took approximately 12 minutes per pass through the data, while the feature-based model (feature-based in Table 3) took 35 minutes per pass through the data. The feature-based model with the relaxed grammar (relaxed in Table 3) took about four times as long as the regular feature-based model. The discriminatively trained generative WSJ40 model (discriminative in Table 4) was trained using two of the same machines, with 16 gigabytes of RAM each for the clients.4 It took about one day per pass through the data. The feature-based WSJ40 model (featurebased in Table 4) was trained using four of these machines, also with 16 gigabytes of RAM each for the clients. It took about three days per pass through the data. The results clearly show that gains came from both the switch from generative to discriminative training, and from the extensive use of features. In Figure 3 we show for an example from section 22 the parse trees produced by our generative model and our feature-based discriminative model, and the correct parse. The parse from the feature-based model better exhibits the right branching tendencies of English. This is likely due to the heavy feature, which encourages long constituents at the end of the sentence. It is difficult for a standard PCFG to learn this aspect of the English language, because the score it assigns to a rule does not take its span into account. The most similar related work is (Johnson, 2001), which did discriminative training of a generative PCFG. The model was quite similar to ours, except that it did not incorporate any features and it required the parameters (which were just scores for rules) to be locally normalized, as with a generatively trained model. Due to training time, they used the ATIS treebank corpus , which is much smaller than even WSJ15, with only 1,088 training sentences, 294 testing sentences, and an average sentence length of around 11. They found no significant difference in performance between their generatively and discriminatively trained parsers. There are two probable reasons for this result. The training set is very small, and it is a known fact that generative models tend to work better for small datasets and discriminative models tend to work better for larger datasets (Ng and Jordan, 2002). Additionally, they made no use of features, one of the primary benefits of discriminative learning. Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains. We suspect that this is in part due to the grammar that they chose – the grammar of (Klein and Manning, 2003), which was hand annotated with the intent of optimizing performance of a PCFG. This grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few options to discriminate between. Starting with this grammar we found it difficult to achieve gains as well. Additionally, their long training time (several months for WSJ15, according to (Turian and Melamed, 2006)) made feature engineering difficult; they were unable to really explore the space of possible features. More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004). They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup is much more complex than ours and takes substantially longer to train – up to 5 days on WSJ15 – while achieving only small gains over (Taskar et al., 2004). The most recent similar research is (Petrov et al., 2007). They also do discriminative parsing of length 40 sentences, but with a substantially different setup. Following up on their previous work (Petrov et al., 2006) on grammar splitting, they do discriminative parsing with latent variables, which requires them to optimize a non-convex function. Instead of using a stochastic optimization technique, they use LBFGS, but do coarse-to-fine pruning to approximate their gradients and log likelihood. Because they were focusing on grammar splitting they, like (Johnson, 2001), did not employ any features, and, like (Taskar et al., 2004), they saw only small gains from switching from generative to discriminative training. We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length < 15 and the first results for such a parser trained and tested on sentences of length < 40. We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time. Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods. Looking at how other tasks, such as named entity recognition and part-of-speech tagging, have evolved over time, it is clear that greater gains are to be gotten from developing better features than from better models. We have provided just such a framework for improving parsing performance.","We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length < 15 and the first results for such a parser trained and tested on sentences of length < 40. We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time. Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods. Looking at how other tasks, such as named entity recognition and part-of-speech tagging, have evolved over time, it is clear that greater gains are to be gotten from developing better features than from better models. We have provided just such a framework for improving parsing performance."
13,"In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.","In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a system’s word dictionary1. The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based on the Margin Infused Relaxed Algorithm (MIRA) (Crammer, 2004; Crammer et al., 2005; McDonald, 2006). We describe k-best decoding for our hybrid model and design its loss function and the features appropriate for our task. In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance. Here, we describe our strategies that yield good balance for learning these two characteristics. We propose an errordriven policy that delivers this balance by acquiring examples of unknown words from particular errors in a training corpus. We conducted our experiments on Penn Chinese Treebank (Xia et al., 2000) and compared our approach with the best previous approaches reported in the literature. Experimental results indicate that our approach can achieve state-of-the-art performance. The paper proceeds as follows: Section 2 gives background on the word-character hybrid model, Section 3 describes our policies for correct path selection, Section 4 presents our training method based on MIRA, Section 5 shows our experimental results, Section 6 discusses related work, and Section 7 concludes the paper. In joint word segmentation and the POS tagging process, the task is to predict a path of word hypotheses y = (y1, ... , y#y) = ((w1, p1), ... , (w#y, p#y)) for a given character sequence x = (c1, ... , c#x), where w is a word, p is its POS tag, and a “#” symbol denotes the number of elements in each variable. The goal of our learning algorithm is to learn a mapping from inputs (unsegmented sentences) x E X to outputs (segmented paths) y E Y based on training samples of input-output pairs S = {(xt, yt)}t1. We represent the search space with a lattice based on the word-character hybrid model (Nakagawa and Uchimoto, 2007). In the hybrid model, given an input sentence, a lattice that consists of word-level and character-level nodes is constructed. Word-level nodes, which correspond to words found in the system’s word dictionary, have regular POS tags. Character-level nodes have special tags where position-of-character (POC) and POS tags are combined (Asahara, 2003; Nakagawa, 2004). POC tags indicate the word-internal positions of the characters, as described in Table 1. Figure 1 shows an example of a lattice for a Chinese sentence: “ ” (Chongming is China’s third largest island). Note that some nodes and state transitions are not allowed. For example, T and E nodes cannot occur at the beginning of the lattice (marked with dashed boxes), and the transitions from T to B nodes are also forbidden. These nodes and transitions are ignored during the lattice construction processing. In the training phase, since several paths (marked in bold) can correspond to the correct analysis in the annotated corpus, we need to select one correct path yt as a reference for training.2 The next section describes our strategies for dealing with this issue. With this search space representation, we can consistently handle unknown words with character-level nodes. In other words, we use word-level nodes to identify known words and character-level nodes to identify unknown words. In the testing phase, we can use a dynamic programming algorithm to search for the most likely path out of all candidate paths. In this section, we describe our strategies for selecting the correct path yt in the training phase. As shown in Figure 1, the paths marked in bold can represent the correct annotation of the segmented sentence. Ideally, we need to build a wordcharacter hybrid model that effectively learns the characteristics of unknown words (with characterlevel nodes) as well as those of known words (with word-level nodes). We can directly estimate the statistics of known words from an annotated corpus where a sentence is already segmented into words and assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path yt can contain both wordlevel and character-level nodes (marked with asterisks (*)). To discover more statistics of unknown words, one might consider just increasing the threshold value r to obtain more artificial unknown words. However, our experimental results indicate that our word-character hybrid model requires an appropriate balance between known and artificial unknown words to achieve optimal performance. We now describe our new approach to leverage additional examples of unknown words. Intuition suggests that even though the system can handle some unknown words, many unidentified unknown words remain that cannot be recovered by the system; we wish to learn the characteristics of such unidentified unknown words. We propose the following simple scheme: Several types of errors are produced by the baseline model, but we only focus on those caused by unidentified unknown words, which can be easily collected in the evaluation process. As described later in Section 5.2, we measure the recall on out-of-vocabulary (OOV) words. Here, we define unidentified unknown words as OOV words in each validation set that cannot be recovered by the system. After ten cross validation runs, we get a list of the unidentified unknown words derived from the whole training corpus. Note that the unidentified unknown words in the cross validation are not necessary to be infrequent words, but some overlap may exist. Finally, we obtain the artificial unknown words that combine the unidentified unknown words in cross validation and infrequent words for learning unknown words. We refer to this approach as the error-driven policy. Let Yt = {yt , ... , yKt � be a lattice consisting of candidate paths for a given sentence xt. In the word-character hybrid model, the lattice Yt can contain more than 1000 nodes, depending on the length of the sentence xt and the number of POS tags in the corpus. Therefore, we require a learning algorithm that can efficiently handle large and complex lattice structures. Online learning is an attractive method for the hybrid model since it quickly converges Algorithm 1 Generic Online Learning Algorithm Input: Training set S = {(xt, yt)}Tt�1 Output: Model weight vector w within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. We focus on an online learning algorithm called MIRA (Crammer, 2004), which has the desired accuracy and scalability properties. MIRA combines the advantages of margin-based and perceptron-style learning with an optimization scheme. In particular, we use a generalized version of MIRA (Crammer et al., 2005; McDonald, 2006) that can incorporate k-best decoding in the update procedure. To understand the concept of kbest MIRA, we begin with a linear score function: where w is a weight vector and f is a feature representation of an input x and an output y. Learning a mapping between an input-output pair corresponds to finding a weight vector w such that the best scoring path of a given sentence is the same as (or close to) the correct path. Given a training example (xt, yt), MIRA tries to establish a margin between the score of the correct path s(xt,yt; w) and the score of the best candidate path s(xt, y; w) based on the current weight vector w that is proportional to a loss function L(yt, y). In each iteration, MIRA updates the weight vector w by keeping the norm of the change in the weight vector as small as possible. With this framework, we can formulate the optimization problem as follows (McDonald, 2006): where bestk(xt; w(i)) E Yt represents a set of top k-best paths given the weight vector w(i). The above quadratic programming (QP) problem can be solved using Hildreth’s algorithm (Yair Censor, 1997). Replacing Eq. (2) into line 4 of Algorithm 1, we obtain k-best MIRA. The next question is how to efficiently generate bestk(xt; w(i)). In this paper, we apply a dynamic programming search (Nagata, 1994) to kbest MIRA. The algorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A*style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i). The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y* for a given sentence x where: In conventional sequence labeling where the observation sequence (word) boundaries are fixed, one can use the 0/1 loss to measure the errors of a predicted path with respect to the correct path. However, in our model, word boundaries vary based on the considered path, resulting in a different numbers of output tokens. As a result, we cannot directly use the 0/1 loss. We instead compute the loss function through false positives (FP) and false negatives (FN). Here, FP means the number of output nodes that are not in the correct path, and FN means the number of nodes in the correct path that cannot be recognized by the system. We define the loss function by: This loss function can reflect how bad the predicted path y� is compared to the correct path yt. A weighted loss function based on FP and FN can be found in (Ganchev et al., 2007). This section discusses the structure of f(x,y). We broadly classify features into two categories: unigram and bigram features. We design our feature templates to capture various levels of information about words and POS tags. Let us introduce some notation. We write w−1 and w0 for the surface forms of words, where subscripts −1 and 0 indicate the previous and current positions, respectively. POS tags p−1 and p0 can be interpreted in the same way. We denote the characters by cj. Unigram features: Table 2 shows our unigram features. Templates W0–W3 are basic word-level unigram features, where Length(w0) denotes the length of the word w0. Using just the surface forms can overfit the training data and lead to poor predictions on the test data. To alleviate this problem, we use two generalized features of the surface forms. The first is the beginning and end characters of the surface (A0–A7). For example, (AB(w0)) denotes the beginning character of the current word w0, and (AB(w0), AE(w0)) denotes the beginning and end characters in the word. The second is the types of beginning and end characters of the surface (T0–T7). We define a set of general character types, as shown in Table 4. Templates C0–C6 are basic character-level unigram features taken from (Nakagawa, 2004). These templates operate over a window of f2 characters. The features include characters (C0), pairs of characters (C1–C2), character types (C3), and pairs of character types (C4–C5). In addition, we add pairs of characters and character types (C6). Bigram features: Table 3 shows our bigram features. Templates B0-B9 are basic wordlevel bigram features. These features aim to capture all the possible combinations of word and POS bigrams. Templates TB0-TB6 are the types of characters for bigrams. For example, (TE(w−1), TB(w0)) captures the change of character types from the end character in the previous word to the beginning character in the current word. Note that if one of the adjacent nodes is a character-level node, we use the template CB0 that represents POS bigrams. In our preliminary experiments, we found that if we add more features to non-word-level bigrams, the number of features grows rapidly due to the dense connections between non-word-level nodes. However, these features only slightly improve performance over using simple POS bigrams. For Seg, a token is considered to be a correct one if the word boundary is correctly identified. For Seg & Tag, both the word boundary and its POS tag have to be correctly identified to be counted as a correct token. F1 = Previous studies on joint Chinese word segmentation and POS tagging have used Penn Chinese Treebank (CTB) (Xia et al., 2000) in experiments. However, versions of CTB and experimental settings vary across different studies. In this paper, we used CTB 5.0 (LDC2005T01) as our main corpus, defined the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b), and designed our experiments to explore the impact of the training corpus size on our approach. Table 5 provides the statistics of our experimental settings on the small and large training data. The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). Note that the development set was only used for evaluating the trained model to obtain the optimal values of tunable parameters. We evaluated both word segmentation (Seg) and joint word segmentation and POS tagging (Seg & Tag). We used recall (R), precision (P), and F1 as evaluation metrics. Following (Sproat and Emerson, 2003), we also measured the recall on OOV (ROOV) tokens and in-vocabulary (RIV) tokens. These performance measures can be calculated as follows: # of correct tokens # of tokens in test data Our model has three tunable parameters: the number of training iterations N; the number of top k-best paths; and the threshold r for infrequent words. Since we were interested in finding an optimal combination of word-level and characterlevel nodes for training, we focused on tuning r. We fixed N = 10 and k = 5 for all experiments. For the baseline policy, we varied r in the range of [1, 5] and found that setting r = 3 yielded the best performance on the development set for both the small and large training corpus experiments. For the error-driven policy, we collected unidentified unknown words using 10-fold cross validation on the training set, as previously described in Section 3. Table 6 shows the results of our word-character hybrid model using the error-driven and baseline policies. The third and fourth columns indicate the numbers of known and artificial unknown words in the training phase. The total number of words is the same, but the different policies yield different balances between the known and artificial unknown words for learning the hybrid model. Optimal balances were selected using the development set. The error-driven policy provides additional artificial unknown words in the training set. The error-driven policy can improve ROOV as well as maintain good RIV, resulting in overall F1 improvements. In this section, we attempt to make meaningful comparison with the best prior approaches reported in the literature. Although most previous studies used CTB, their versions of CTB and experimental settings are different, which complicates comparison. Ng and Low (2004) (N&L04) used CTB 3.0. However, they just showed POS tagging results on a per character basis, not on a per word basis. Zhang and Clark (2008) (Z&C08) generated CTB 3.0 from CTB 4.0. Jiang et al. (2008a; 2008b) (Jiang08a, Jiang08b) used CTB 5.0. Shi and Wang (2007) used CTB that was distributed in the SIGHAN Bakeoff. Besides CTB, they also used HowNet (Dong and Dong, 2006) to obtain semantic class features. Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. We decided to follow the experimental settings of Jiang et al. (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. The performance scores of previous studies are directly taken from their papers. We also conducted experiments using the system implemented by Nakagawa and Uchimoto (2007) (N&U07) for comparison. Our experiment on the large training corpus is identical to that of Jiang et al. (Jiang et al., 2008a; Jiang et al., 2008b). Table 7 compares the F1 results with previous studies on CTB 5.0. The result of our error-driven model is superior to previous reported results for both Seg and Seg & Tag, and the result of our baseline model compares favorably to the others. Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs 1–10364. We then divided CTB 3.0 into ten equal sets and conducted 10-fold cross validation. Unfortunately, Zhang and Clark’s experimental setting did not allow us to use our errordriven policy since performing 10-fold cross validation again on each main cross validation trial is computationally too expensive. Therefore, we used our baseline policy in this setting and fixed r = 3 for all cross validation runs. Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. Table 9 shows a summary of averaged F1 results on CTB 3.0. Our baseline model outperforms all prior approaches for both Seg and Seg & Tag, and we hope that our error-driven model can further improve performance. In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006). For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). Another potential algorithm is MIRA, which integrates the notion of the large-margin classifier (Crammer, 2004). In this paper, we first introduce MIRA to joint word segmentation and POS tagging and show very encouraging results. With regard to error-driven learning, Brill (1995) proposed a transformation-based approach that acquires a set of error-correcting rules by comparing the outputs of an initial tagger with the correct annotations on a training corpus. Our approach does not learn the error-correcting rules. We only aim to capture the characteristics of unknown words and augment their representatives. As for search space representation, Ng and Low (2004) found that for Chinese, the characterbased model yields better results than the wordbased model. Nakagawa and Uchimoto (2007) provided empirical evidence that the characterbased model is not always better than the wordbased model. They proposed a hybrid approach that exploits both the word-based and characterbased models. Our approach overcomes the limitation of the original hybrid model by a discriminative online learning algorithm for training. In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word segmentation and POS tagging. Our approach has two important advantages. The first is robust search space representation based on a hybrid model in which word-level and characterlevel nodes are used to identify known and unknown words, respectively. We introduced a simple scheme based on the error-driven concept to effectively learn the characteristics of known and unknown words from the training corpus. The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model. Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature. In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese.","In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word segmentation and POS tagging. Our approach has two important advantages. The first is robust search space representation based on a hybrid model in which word-level and characterlevel nodes are used to identify known and unknown words, respectively. We introduced a simple scheme based on the error-driven concept to effectively learn the characteristics of known and unknown words from the training corpus. The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model. Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature. In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese."
14,"In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training exam","In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training exam The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence (Winograd, 1972; Di Eugenio, 1992). Mapping instructions to a sequence of executable actions would enable the automation of tasks that currently require human participation. Examples include configuring software based on how-to guides and operating simulators using instruction manuals. In this paper, we present a reinforcement learning framework for inducing mappings from text to actions without the need for annotated training examples. For concreteness, consider instructions from a Windows troubleshooting guide on deleting temporary folders, shown in Figure 1. We aim to map this text to the corresponding low-level commands and parameters. For example, properly interpreting the third instruction requires clicking on a tab, finding the appropriate option in a tree control, and clearing its associated checkbox. In this and many other applications, the validity of a mapping can be verified by executing the induced actions in the corresponding environment and observing their effects. For instance, in the example above we can assess whether the goal described in the instructions is achieved, i.e., the folder is deleted. The key idea of our approach is to leverage the validation process as the main source of supervision to guide learning. This form of supervision allows us to learn interpretations of natural language instructions when standard supervised techniques are not applicable, due to the lack of human-created annotations. Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998). We assume that supervision is provided in the form of a reward function that defines the quality of executed actions. During training, the learner repeatedly constructs action sequences for a set of given documents, executes those actions, and observes the resulting reward. The learner’s goal is to estimate a policy — a distribution over actions given instruction text and environment state — that maximizes future expected reward. Our policy is modeled in a log-linear fashion, allowing us to incorporate features of both the instruction text and the environment. We employ a policy gradient algorithm to estimate the parameters of this model. We evaluate our method on two distinct applications: Windows troubleshooting guides and puzzle game tutorials. The key findings of our experiments are twofold. First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. In some respects, our task is more easily amenable to reinforcement learning. For instance, we are not interacting with a human user, so the cost of interaction is lower. However, while the state space can be designed to be relatively small in the dialogue management task, our state space is determined by the underlying environment and is typically quite large. We address this complexity by developing a policy gradient algorithm that learns efficiently while exploring a small subset of the states. Our task is to learn a mapping between documents and the sequence of actions they express. Figure 2 shows how one example sentence is mapped to three actions. Mapping Text to Actions As input, we are given a document d, comprising a sequence of sentences (u1, ... , ut), where each ui is a sequence of words. Our goal is to map d to a sequence of actions a� = (a0, ... , a,1). Actions are predicted and executed sequentially.2 An action a = (c, R, W') encompasses a command c, the command’s parameters R, and the words W' specifying c and R. Elements of R refer to objects available in the environment state, as described below. Some parameters can also refer to words in document d. Additionally, to account for words that do not describe any actions, c can be a null command. The Environment The environment state £ specifies the set of objects available for interaction, and their properties. In Figure 2, £ is shown on the right. The environment state £ changes in response to the execution of command c with parameters R according to a transition distribution p(£'J£, c, R). This distribution is a priori unknown to the learner. As we will see in Section 5, our approach avoids having to directly estimate this distribution. State To predict actions sequentially, we need to track the state of the document-to-actions mapping over time. A mapping state s is a tuple (£, d, j, W), where £ refers to the current environment state; j is the index of the sentence currently being interpreted in document d; and W contains words that were mapped by previous actions for the same sentence. The mapping state s is observed after each action. The initial mapping state s0 for document d is (£d, d, 0, 0); £d is the unique starting environment state for d. Performing action a in state s = (£, d, j, W) leads to a new state s' according to distribution p(s'|s, a), defined as follows: £ transitions according to p(£'|£, c, R), W is updated with a’s selected words, and j is incremented if all words of the sentence have been mapped. For the applications we consider in this work, environment state transitions, and consequently mapping state transitions, are deterministic. Training During training, we are provided with a set D of documents, the ability to sample from the transition distribution, and a reward function r(h). Here, h = (s0, a0, ... , sn−1, an−1, sn) is a history of states and actions visited while interpreting one document. r(h) outputs a realvalued score that correlates with correct action selection.3 We consider both immediate reward, which is available after each action, and delayed reward, which does not provide feedback until the last action. For example, task completion is a delayed reward that produces a positive value after the final action only if the task was completed successfully. We will also demonstrate how manually annotated action sequences can be incorporated into the reward. The goal of training is to estimate parameters 0 of the action selection distribution p(a|s, 0), called the policy. Since the reward correlates with action sequence correctness, the 0 that maximizes expected reward will yield the best actions. Our goal is to predict a sequence of actions. We construct this sequence by repeatedly choosing an action given the current mapping state, and applying that action to advance to a new state. Given a state s = (£, d, j, W), the space of possible next actions is defined by enumerating subspans of unused words in the current sentence (i.e., subspans of the jth sentence of d not in W), and the possible commands and parameters in environment state £.4 We model the policy distribution p(a|s; 0) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features. Under this representation, the policy distribution is: where 0(s, a) E Rn is an n-dimensional feature representation. During test, actions are selected according to the mode of this distribution. During training, our goal is to find the optimal policy p(a|s; θ). Since reward correlates with correct action selection, a natural objective is to maximize expected future reward — that is, the reward we expect while acting according to that policy from state s. Formally, we maximize the value function: where the history h is the sequence of states and actions encountered while interpreting a single document d E D. This expectation is averaged over all documents in D. The distribution p(h|θ) returns the probability of seeing history h when starting from state s and acting according to a policy with parameters θ. This distribution can be decomposed into a product over time steps: Input: A document set D, Feature representation φ, Reward function r(h), Number of iterations T Our reinforcement learning problem is to find the parameters θ that maximize Vθ from equation 2. Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters θ by performing stochastic gradient ascent. The gradient of Vθ is approximated by interacting with the environment, and the resulting reward is used to update the estimate of θ. Policy gradient algorithms optimize a non-convex objective and are only guaranteed to find a local optimum. However, as we will see, they scale to large state spaces and can perform well in practice. To find the parameters θ that maximize the objective, we first compute the derivative of Vθ. Expanding according to the product rule, we have: where the inner sum is over all time steps t in the current history h. Expanding the inner partial derivative we observe that: which is the derivative of a log-linear distribution. Equation 5 is easy to compute directly. However, the complete derivative of Vθ in equation 4 is intractable, because computing the expectation would require summing over all possible histories. Instead, policy gradient algorithms employ stochastic gradient ascent by computing a noisy estimate of the expectation using just a subset of the histories. Specifically, we draw samples from p(h|θ) by acting in the target environment, and use these samples to approximate the expectation in equation 4. In practice, it is often sufficient to sample a single history h for this approximation. Algorithm 1 details the complete policy gradient algorithm. It performs T iterations over the set of documents D. Step 3 samples a history that maps each document to actions. This is done by repeatedly selecting actions according to the current policy, and updating the state by executing the selected actions. Steps 4 and 5 compute the empirical gradient and update the parameters θ. In many domains, interacting with the environment is expensive. Therefore, we use two techniques that allow us to take maximum advantage of each environment interaction. First, a history h = (s0, a0, ... , sn) contains subsequences (si, ai,... sn) for i = 1 to n − 1, each with its own reward value given by the environment as a side effect of executing h. We apply the update from equation 5 for each subsequence. Second, for a sampled history h, we can propose alternative histories h' that result in the same commands and parameters with different word spans. We can again apply equation 5 for each h', weighted by its probability under the current policy, p(h�|θ) The algorithm we have presented belongs to a family of policy gradient algorithms that have been successfully used for complex tasks such as robot control (Ng et al., 2003). Our formulation is unique in how it represents natural language in the reinforcement learning framework. We can design a range of reward functions to guide learning, depending on the availability of annotated data and environment feedback. Consider the case when every training document d E D is annotated with its correct sequence of actions, and state transitions are deterministic. Given these examples, it is straightforward to construct a reward function that connects policy gradient to maximum likelihood. Specifically, define a reward function r(h) that returns one when h matches the annotation for the document being analyzed, and zero otherwise. Policy gradient performs stochastic gradient ascent on the objective from equation 2, performing one update per document. For document d, this objective becomes: where hd is the history corresponding to the annotated action sequence. Thus, with this reward policy gradient is equivalent to stochastic gradient ascent with a maximum likelihood objective. At the other extreme, when annotations are completely unavailable, learning is still possible given informative feedback from the environment. Crucially, this feedback only needs to correlate with action sequence quality. We detail environment-based reward functions in the next section. As our results will show, reward functions built using this kind of feedback can provide strong guidance for learning. We will also consider reward functions that combine annotated supervision with environment feedback. We study two applications of our model: following instructions to perform software tasks, and solving a puzzle game using tutorial guides. On its Help and Support website,5 Microsoft publishes a number of articles describing how to perform tasks and troubleshoot problems in the Windows operating systems. Examples of such tasks include installing patches and changing security settings. Figure 1 shows one such article. Our goal is to automatically execute these support articles in the Windows 2000 environment. Here, the environment state is the set of visible user interface (UI) objects, and object properties such as label, location, and parent window. Possible commands include left-click, right-click, double-click, and type-into, all of which take a UI object as a parameter; type-into additionally requires a parameter for the input text. Table 1 lists some of the features we use for this domain. These features capture various aspects of the action under consideration, the current Windows UI state, and the input instructions. For example, one lexical feature measures the similarity of a word in the sentence to the UI labels of objects in the environment. Environment-specific features, such as whether an object is currently in focus, are useful when selecting the object to manipulate. In total, there are 4,438 features. Reward Function Environment feedback can be used as a reward function in this domain. An obvious reward would be task completion (e.g., whether the stated computer problem was fixed). Unfortunately, verifying task completion is a challenging system issue in its own right. Instead, we rely on a noisy method of checking whether execution can proceed from one sentence to the next: at least one word in each sentence has to correspond to an object in the environment.6 For instance, in the sentence from Figure 2 the word “Run” matches the Run... menu item. If no words in a sentence match a current environment object, then one of the previous sentences was analyzed incorrectly. In this case, we assign the history a reward of -1. This reward is not guaranteed to penalize all incorrect histories, because there may be false positive matches between the sentence and the environment. When at least one word matches, we assign a positive reward that linearly increases with the percentage of words assigned to non-null commands, and linearly decreases with the number of output actions. This reward signal encourages analyses that interpret all of the words without producing spurious actions. Our second application is to a puzzle game called Crossblock, available online as a Flash game.7 Each of 50 puzzles is played on a grid, where some grid positions are filled with squares. The object of the game is to clear the grid by drawing vertical or horizontal line segments that remove groups of squares. Each segment must exactly cross a specific number of squares, ranging from two to seven depending on the puzzle. Humans players have found this game challenging and engaging enough to warrant posting textual tutorials.8 A sample puzzle and tutorial are shown in Figure 3. The environment is defined by the state of the grid. The only command is clear, which takes a parameter specifying the orientation (row or column) and grid location of the line segment to be removed. The challenge in this domain is to segment the text into the phrases describing each action, and then correctly identify the line segments from references such as “the bottom four from the second column from the left.” For this domain, we use two sets of binary features on state-action pairs (s, a). First, for each vocabulary word w, we define a feature that is one if w is the last word of a’s consumed words W'. These features help identify the proper text segmentation points between actions. Second, we introduce features for pairs of vocabulary word w and attributes of action a, e.g., the line orientation and grid locations of the squares that a would remove. This set of features enables us to match words (e.g., “row”) with objects in the environment (e.g., a move that removes a horizontal series of squares). In total, there are 8,094 features. Reward Function For Crossblock it is easy to directly verify task completion, which we use as the basis of our reward function. The reward r(h) is -1 if h ends in a state where the puzzle cannot be completed. For solved puzzles, the reward is a positive value proportional to the percentage of words assigned to non-null commands. Datasets For the Windows domain, our dataset consists of 128 documents, divided into 70 for training, 18 for development, and 40 for test. In the puzzle game domain, we use 50 tutorials, divided into 40 for training and 10 for test.9 Statistics for the datasets are shown below. The data exhibits certain qualities that make for a challenging learning problem. For instance, there are a surprising variety of linguistic constructs — as Figure 4 shows, in the Windows domain even a simple command is expressed in at least six different ways. Experimental Framework To apply our algorithm to the Windows domain, we use the Win32 application programming interface to simulate human interactions with the user interface, and to gather environment state information. The operating system environment is hosted within a virtual machine,10 allowing us to rapidly save and reset system state snapshots. For the puzzle game domain, we replicated the game with an implementation that facilitates automatic play. As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the policy distribution (Sutton and Barto, 1998), set to 0.1 in our experiments. For Windows, the development set is used to select the best parameters. For Crossblock, we choose the parameters that produce the highest reward during training. During evaluation, we use these parameters to predict mappings for the test documents. Evaluation Metrics For evaluation, we compare the results to manually constructed sequences of actions. We measure the number of correct actions, sentences, and documents. An action is correct if it matches the annotations in terms of command and parameters. A sentence is correct if all of its actions are correctly identified, and analogously for documents.11 Statistical significance is measured with the sign test. Additionally, we compute a word alignment score to investigate the extent to which the input text is used to construct correct analyses. This score measures the percentage of words that are aligned to the corresponding annotated actions in correctly analyzed documents. Baselines We consider the following baselines to characterize the performance of our approach. lems like ours are typically addressed using supervised techniques. We measure how a standard supervised approach would perform on this task by using a reward signal based on manual annotations of output action sequences, as defined in Section 5.2. As shown there, policy gradient with this reward is equivalent to stochastic gradient ascent with a maximum likelihood objective. when only a subset of training documents is annotated, and environment reward is used for the remainder. Our method seamlessly combines these two kinds of rewards. sider two naive baselines. Both scan through each sentence from left to right. A command c is executed on the object whose name is encountered first in the sentence. This command c is either selected randomly, or set to the majority command, which is leftclick. This procedure is repeated until no more words match environment objects. Table 2 presents evaluation results on the test sets. There are several indicators of the difficulty of this task. The random and majority baselines’ poor performance in both domains indicates that naive approaches are inadequate for these tasks. The performance of the fully supervised approach provides further evidence that the task is challenging. This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are 27.14 choices per action in the Windows domain, and 9.78 in the Crossblock domain. In both domains, the learners relying only on environment reward perform well. Although the fully supervised approach performs the best, adding just a few annotated training examples to the environment-based learner significantly reduces the performance gap. Figure 5 shows the overall tradeoff between annotation effort and system performance for the two domains. The ability to make this tradeoff is one of the advantages of our approach. The figure also shows that augmenting annotated documents with additional environment-reward documents invariably improves performance. The word alignment results from Table 2 indicate that the learners are mapping the correct words to actions for documents that are successfully completed. For example, the models that perform best in the Windows domain achieve nearly perfect word alignment scores. To further assess the contribution of the instruction text, we train a variant of our model without access to text features. This is possible in the game domain, where all of the puzzles share a single goal state that is independent of the instructions. This variant solves 34% of the puzzles, suggesting that access to the instructions significantly improves performance. In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions. This approach is able to use environment-based rewards, such as task completion, to learn to analyze text. We showed that having access to a suitable reward function can significantly reduce the need for annotations.","In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions. This approach is able to use environment-based rewards, such as task completion, to learn to analyze text. We showed that having access to a suitable reward function can significantly reduce the need for annotations."
15,"We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles.","We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. This paper describes a new approach to event semantics that jointly learns event relations and their participants from unlabeled corpora. The early years of natural language processing (NLP) took a “top-down” approach to language understanding, using representations like scripts (Schank and Abelson, 1977) (structured representations of events, their causal relationships, and their participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. This paper addresses two areas of work in event semantics, narrative event chains and semantic role labeling. We begin by highlighting areas in both that can mutually inform each other through a narrative schema model. Narrative Event Chains are partially ordered sets of events that all involve the same shared participant, the protagonist (Chambers and Jurafsky, 2008). A chain contains a set of verbs representing events, and for each verb, the grammatical role filled by the shared protagonist. An event is a verb together with its constellation of arguments. An event slot is a tuple of an event and a particular argument slot (grammatical relation), represented as a pair (v, d) where v is a verb and d E {subject, object, prep}. A chain is a tuple (L, O) where L is a set of event slots and O is a partial (temporal) ordering. We will write event slots in shorthand as (X pleads) or (pleads X) for (pleads, subject) and (pleads, object). Below is an example chain modeling criminal prosecution. In this example, the protagonist of the chain is the person being prosecuted and the other unspecified event slots remain unfilled and unconstrained. Chains in the Chambers and Jurafsky (2008) model are ordered; in this paper rather than address the ordering task we focus on event and argument induction, leaving ordering as future work. The Chambers and Jurafsky (2008) model learns chains completely unsupervised, (albeit after parsing and resolving coreference in the text) by counting pairs of verbs that share coreferring arguments within documents and computing the pointwise mutual information (PMI) between these verb-argument pairs. The algorithm creates chains by clustering event slots using their PMI scores, and we showed this use of co-referring arguments improves event relatedness. Our previous work, however, has two major limitations. First, the model did not express any information about the protagonist, such as its type or role. Role information (such as knowing whether a filler is a location, a person, a particular class of people, or even an inanimate object) could crucially inform learning and inference. Second, the model only represents one participant (the protagonist). Representing the other entities involved in all event slots in the narrative could potentially provide valuable information. We discuss both of these extensions next. The Chambers and Jurafsky (2008) narrative chains do not specify what type of argument fills the role of protagonist. Chain learning and clustering is based only on the frequency with which two verbs share arguments, ignoring any features of the arguments themselves. Take this example of an actual chain from an article in our training data. Given this chain of five events, we want to choose other events most likely to occur in this scenario. One of the top scoring event slots is (fly X). Narrative chains incorrectly favor (fly X) because it is observed during training with all five event slots, although not frequently with any one of them. An event slot like (charge X) is much more plausible, but is unfortunately scored lower by the model. Representing the types of the arguments can help solve this problem. Few types of arguments are shared between the chain and (fly X). However, (charge X) shares many arguments with (accuse X), (search X) and (suspect X) (e.g., criminal and suspect). Even more telling is that these arguments are jointly shared (the same or coreferent) across all three events. Chains represent coherent scenarios, not just a set of independent pairs, so we want to model argument overlap across all pairs. The second problem with narrative chains is that they make judgments only between protagonist arguments, one slot per event. All entities and slots in the space of events should be jointly considered when making event relatedness decisions. As an illustration, consider the verb arrest. Which verb is more related, convict or capture? A narrative chain might only look at the objects of these verbs and choose the one with the highest score, usually choosing convict. But in this case the subjects offer additional information; the subject of arrest (police) is different from that of convict (judge). A more informed decision prefers capture because both the objects (suspect) and subjects (police) are identical. This joint reasoning is absent from the narrative chain model. The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just described, Chambers and Jurafsky (2008) offers an unsupervised approach to event learning (goal 1), but lacks semantic role knowledge (goal 2). The following sections describe a model that addresses both goals. The next sections introduce typed narrative chains and chain merging, extensions that allow us to jointly learn argument roles with event structure. The first step in describing a narrative schema is to extend the definition of a narrative chain to include argument types. We now constrain the protagonist to be of a certain type or role. A Typed Narrative Chain is a partially ordered set of event slots that share an argument, but now the shared argument is a role defined by being a member of a set of types R. These types can be lexical units (such as observed head words), noun clusters, or other semantic representations. We use head words in the examples below, but we also evaluate with argument clustering by mapping head words to member clusters created with the CBC clustering algorithm (Pantel and Lin, 2002). We define a typed narrative chain as a tuple (L, P, O) with L and O the set of event slots and partial ordering as before. Let P be a set of argument types (head words) representing a single role. An example is given here: As mentioned above, narrative chains are learned by parsing the text, resolving coreference, and extracting chains of events that share participants. In our new model, argument types are learned simultaneously with narrative chains by finding salient words that represent coreferential arguments. We record counts of arguments that are observed with each pair of event slots, build the referential set for each word from its coreference chain, and then represent each observed argument by the most frequent head word in its referential set (ignoring pronouns and mapping entity mentions with person pronouns to a constant PERSON identifier). As an example, the following contains four worker mentions: But for a growing proportion of U.S. workers, the troubles really set in when they apply for unemployment benefits. Many workers find their benefits challenged. The four bolded terms are coreferential and (hopefully) identified by coreference. Our algorithm chooses the head word of each phrase and ignores the pronouns. It then chooses the most frequent head word as the most salient mention. In this example, the most salient term is workers. If any pair of event slots share arguments from this set, we count workers. In this example, the pair (X find) and (X apply) shares an argument (they and workers). The pair ((X find),(X apply)) is counted once for narrative chain induction, and ((X find), (X apply), workers) once for argument induction. Figure 1 shows the top occurring words across all event slot pairs in a criminal scenario chain. This chain will be part of a larger narrative schema, described in section 3.4. We now formalize event slot similarity with arguments. Narrative chains as defined in (Chambers and Jurafsky, 2008) score a new event slot (f, g) against a chain of size n by summing over the scores between all pairs: where C is a narrative chain, f is a verb with grammatical argument g, and sim(e, e') is the pointwise mutual information pmi(e, e'). Growing a chain by one adds the highest scoring event. We extend this function to include argument types by defining similarity in the context of a specific argument a: where A is a constant weighting factor and freq(b, b', a) is the corpus count of a filling the arguments of events b and b'. We then score the entire chain for a particular argument: sim((ei, di) , (ej, dj) , a) (3) Using this chain score, we finally extend chainsim to score a new event slot based on the argument that maximizes the entire chain’s score: The argument is now directly influencing event slot similarity scores. We will use this definition in the next section to build Narrative Schemas. Whereas a narrative chain is a set of event slots, a Narrative Schema is a set of typed narrative chains. A schema thus models all actors in a set of events. If (push X) is in one chain, (Y push) is in another. This allows us to model a document’s entire narrative, not just one main actor. A narrative schema is defined as a 2-tuple N = (E, C) with E a set of events (here defined as verbs) and C a set of typed chains over the event slots. We represent an event as a verb v and its grammatical argument positions D„ C_ {subject, object, prep}. Thus, each event slot (v, d) for all d E D„ belongs to a chain c E C in the schema. Further, each c must be unique for each slot of a single verb. Using the criminal prosecution domain as an example, a narrative schema in this domain is built as in figure 2. The three dotted boxes are graphical representations of the typed chains that are combined in this schema. The first represents the event slots in which the criminal is involved, the second the police, and the third is a court or judge. Although our representation uses a set of chains, it is equivalent to represent a schema as a constraint satisfaction problem between (e, d) event slots. The next section describes how to learn these schemas. Previous work on narrative chains focused on relatedness scores between pairs of verb arguments (event slots). The clustering step which built chains depended on these pairwise scores. Narrative schemas use a generalization of the entire verb with all of its arguments. A joint decision can be made such that a verb is added to a schema if both its subject and object are assigned to chains in the schema with high confidence. For instance, it may be the case that (Y pull over) scores well with the ‘police’ chain in figure 3. However, the object of (pull over A) is not present in any of the other chains. Police pull over cars, but this schema does not have a chain involving cars. In contrast, (Y search) scores well with the ‘police’ chain and (search X) scores well in the ‘defendant’ chain too. Thus, we want to favor search instead of pull over because the schema is already modeling both arguments. This intuition leads us to our event relatedness function for the entire narrative schema N, not just one chain. Instead of asking which event slot (v, d) is a best fit, we ask if v is best by considering all slots at once: where CN is the set of chains in our narrative N. If (v, d) does not have strong enough similarity with any chain, it creates a new one with base score Q. The Q parameter balances this decision of adding to an existing chain in N or creating a new one. We use equation 5 to build schemas from the set of events as opposed to the set of event slots that previous work on narrative chains used. In Chambers and Jurafsky (2008), narrative chains add the best (e, d) based on the following: where m is the number of seen event slots in the corpus and (vj, gj) is the jth such possible event slot. Schemas are now learned by adding events that maximize equation 5: where |v |is the number of observed verbs and vj is the jth such verb. Verbs are incrementally added to a narrative schema by strength of similarity. Figures 3 and 4 show two criminal schemas learned completely automatically from the NYT portion of the Gigaword Corpus (Graff, 2002). We parse the text into dependency graphs and resolve coreferences. The figures result from learning over the event slot counts. In addition, figure 5 shows six of the top 20 scoring narrative schemas learned by our system. We artificially required the clustering procedure to stop (and sometimes continue) at six events per schema. Six was chosen as the size to enable us to compare to FrameNet in the next section; the mean number of verbs in FrameNet frames is between five and six. A low Q was chosen to limit chain splitting. We built a new schema starting from each verb that occurs in more than 3000 and less than 50,000 documents in the NYT section. This amounted to approximately 1800 verbs from which we show the top 20. Not surprisingly, most of the top schemas concern business, politics, crime, or food. Most previous work on unsupervised semantic role labeling assumes that the set of possible automatically built from the verb ‘convict’. Each node shape is a chain in the schema. classes is very small (i.e, PropBank roles ARG0 and ARG1) and is known in advance. By contrast, our approach induces sets of entities that appear in the argument positions of verbs in a narrative schema. Our model thus does not assume the set of roles is known in advance, and it learns the roles at the same time as clustering verbs into frame-like schemas. The resulting sets of entities (such as {police, agent, authorities, government} or {court, judge, justice}) can be viewed as a kind of schema-specific semantic role. How can this unsupervised method of learning roles be evaluated? In Section 6 we evaluate the schemas together with their arguments in a cloze task. In this section we perform a more qualitative evalation by comparing our schema to FrameNet. FrameNet (Baker et al., 1998) is a database of frames, structures that characterize particular situations. A frame consists of a set of events (the verbs and nouns that describe them) and a set of frame-specific semantic roles called frame elements that can be arguments of the lexical units in the frame. FrameNet frames share commonalities with narrative schemas; both represent aspects of situations in the world, and both link semantically related words into frame-like sets in which each predicate draws its argument roles from a frame-specific set. They differ in that schemas focus on events in a narrative, while frames focus on events that share core participants. Nonetheless, the fact that FrameNet defines frame-specific argument roles suggests that comparing our schemas and roles to FrameNet would be elucidating. We took the 20 learned narrative schemas described in the previous section and used FrameNet to perform qualitative evaluations on three aspects of schema: verb groupings, linking structure (the mapping of each argument role to syntactic subject or object), and the roles themselves (the set of entities that constitutes the schema roles). Verb groupings To compare a schema’s event selection to a frame’s lexical units, we first map the top 20 schemas to the FrameNet frames that have the largest overlap with each schema’s six verbs. We were able to map 13 of our 20 narratives to FrameNet (for the remaining 7, no frame contained more than one of the six verbs). The remaining 13 schemas contained 6 verbs each for a total of 78 verbs. 26 of these verbs, however, did not occur in FrameNet, either at all, or with the correct sense. Of the remaining 52 verb mappings, 35 (67%) occurred in the closest FrameNet frame or in a frame one link away. 17 verbs (33%) deliberate deadlocked found convict acquit occurred in a different frame than the one chosen. We examined the 33% of verbs that occurred in a different frame. Most occurred in related frames, but did not have FrameNet links between them. For instance, one schema includes the causal verb trade with unaccusative verbs of change like rise and fall. FrameNet separates these classes of verbs into distinct frames, distinguishing motion frames from caused-motion frames. Even though trade and rise are in different FrameNet frames, they do in fact have the narrative relation that our system discovered. Of the 17 misaligned events, we judged all but one to be correct in a narrative sense. Thus although not exactly aligned with FrameNet’s notion of event clusters, our induction algorithm seems to do very well. Linking structure Next, we compare a schema’s linking structure, the grammatical relation chosen for each verb event. We thus decide, e.g., if the object of the verb arrest (arrest B) plays the same role as the object of detain (detain B), or if the subject of detain (B detain) would have been more appropriate. We evaluated the clustering decisions of the 13 schemas (78 verbs) that mapped to frames. For each chain in a schema, we identified the frame element that could correctly fill the most verb arguments in the chain. The remaining arguments were considered incorrect. Because we assumed all verbs to be transitive, there were 156 arguments (subjects and objects) in the 13 schema. Of these 156 arguments, 151 were correctly clustered together, achieving 96.8% accuracy. The schema in figure 5 with events detain, seize, arrest, etc. shows some of these errors. The object of all of these verbs is an animate theme, but confiscate B and raid B are incorrect; people cannot be confiscated/raided. They should have been split into their own chain within the schema. Argument Roles Finally, we evaluate the learned sets of entities that fill the argument slots. As with the above linking evaluation, we first identify the best frame element for each argument. For example, the events in the top left schema of figure 5 map to the Manufacturing frame. Argument B was identified as the Product frame element. We then evaluate the top 10 arguments in the argument set, judging whether each is a reasonable filler of the role. In our example, drug and product are correct Product arguments. An incorrect argument is test, as it was judged that a test is not a product. We evaluated all 20 schemas. The 13 mapped schemas used their assigned frames, and we created frame element definitions for the remaining 7 that were consistent with the syntactic positions. There were 400 possible arguments (20 schemas, 2 chains each), and 289 were judged correct for a precision of 72%. This number includes Person and Organization names as correct fillers. A more conservative metric removing these classes results in 259 (65%) correct. Most of the errors appear to be from parsing mistakes. Several resulted from confusing objects with adjuncts. Others misattached modifiers, such as including most as an argument. The cooking schema appears to have attached verbal arguments learned from instruction lists (wash, heat, boil). Two schemas require situations as arguments, but the dependency graphs chose as arguments the subjects of the embedded clauses, resulting in 20 incorrect arguments in these schema. The previous section compared our learned knowledge to current work in event and role semantics. We now provide a more formal evaluation against untyped narrative chains. The two main contributions of schema are (1) adding typed arguments and (2) considering joint chains in one model. We evaluate each using the narrative cloze test as in (Chambers and Jurafsky, 2008). The cloze task (Taylor, 1953) evaluates human understanding of lexical units by removing a random word from a sentence and asking the subject to guess what is missing. The narrative cloze is a variation on this idea that removes an event slot from a known narrative chain.Performance is measured by the position of the missing event slot in a system’s ranked guess list. This task is particularly attractive for narrative schemas (and chains) because it aligns with one of the original ideas behind Schankian scripts, namely that scripts help humans ‘fill in the blanks’ when language is underspecified. We count verb pairs and shared arguments over the NYT portion of the Gigaword Corpus (years 1994-2004), approximately one million articles. We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepositional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and argument head words. We use the OpenNLP1 coreference engine to resolve entity mentions. The test set is the same as in (Chambers and Jurafsky, 2008). 100 random news articles were selected from the 2001 NYT section of the Gigaword Corpus. Articles that did not contain a protagonist with five or more events were ignored, leaving a test set of 69 articles. We used a smaller development set of size 17 to tune parameters. The first evaluation compares untyped against typed narrative event chains. The typed model uses equation 4 for chain clustering. The dotted line ‘Chain’ and solid ‘Typed Chain’ in figure 6 shows the average ranked position over the test set. The untyped chains plateau and begin to worsen as the amount of training data increases, but the typed model is able to improve for some time after. We see a 6.9% gain at 2004 when both lines trend upwards. The second evaluation compares the performance of the narrative schema model against single narrative chains. We ignore argument types and use untyped chains in both (using equation 1 instead of 4). The dotted line ‘Chain’ and solid ‘Schema’ show performance results in figure 6. Narrative Schemas have better ranked scores in all data sizes and follow the previous experiment in improving results as more data is added even though untyped chains trend upward. We see a 3.3% gain at 2004. The final evaluation combines schemas with argument types to measure overall gain. We evaluated with both head words and CBC clusters as argument representations. Not only do typed chains and schemas outperform untyped chains, combining the two gives a further performance boost. Clustered arguments improve the results further, helping with sparse argument counts (‘Typed Schema’ in figure 6 uses CBC arguments). Overall, using all the data (by year 2004) shows a 10.1% improvement over untyped narrative chains. Our significant improvement in the cloze evaluation shows that even though narrative cloze does not evaluate argument types, jointly modeling the arguments with events improves event clustering. Likewise, the FrameNet comparison suggests that modeling related events helps argument learning. The tasks mutually inform each other. Our argument learning algorithm not only performs unsupervised induction of situation-specific role classes, but the resulting roles and linking structures may also offer the possibility of (unsupervised) FrameNet-style semantic role labeling. Finding the best argument representation is an important future direction. The performance of our noun clusters in figure 6 showed that while the other approaches leveled off, clusters continually improved with more data. The exact balance between lexical units, clusters, or more general (traditional) semantic roles remains to be solved, and may be application specific. We hope in the future to show that a range of NLU applications can benefit from the rich inferential structures that narrative schemas provide.","Our significant improvement in the cloze evaluation shows that even though narrative cloze does not evaluate argument types, jointly modeling the arguments with events improves event clustering. Likewise, the FrameNet comparison suggests that modeling related events helps argument learning. The tasks mutually inform each other. Our argument learning algorithm not only performs unsupervised induction of situation-specific role classes, but the resulting roles and linking structures may also offer the possibility of (unsupervised) FrameNet-style semantic role labeling. Finding the best argument representation is an important future direction. The performance of our noun clusters in figure 6 showed that while the other approaches leveled off, clusters continually improved with more data. The exact balance between lexical units, clusters, or more general (traditional) semantic roles remains to be solved, and may be application specific. We hope in the future to show that a range of NLU applications can benefit from the rich inferential structures that narrative schemas provide."
16,"A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including imof up to 3.6% using the and up to 16.5% using cluster f-measure.","A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including imof up to 3.6% using the and up to 16.5% using cluster f-measure. Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on Soon et al. (2001). They built a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity. The goal of the present work is simply to show that transitivity constraints are a useful source of information, which can and should be incorporated into an ILP-based coreference system. For this goal, we put aside the anaphoricity classifier and focus on the pairwise classifier and transitivity constraints. We build a pairwise logistic classifier, trained on all pairs of mentions, and then at test time we use an ILP solver equipped with transitivity constraints to find the most likely legal assignment to the variables which represent the pairwise decisions.1 Our results show a significant improvement compared to the naive use of the pairwise classifier. Other work on global models of coreference (as opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and Baldridge (2007). Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the standard logistic form: where mi and mj correspond to mentions i and 3 respectively; f(mi, mj) is a feature function over a pair of mentions; 0 are the feature weights we wish to learn; and x(i j) is a boolean variable which takes value 1 if mi and mj are coreferent, and 0 if they are not. The log likelihood of a document is the sum of the log likelihoods of all pairs of mentions: (2) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x(i,j). Note that this model is degenerate, because it assigns probability mass to nonsensical clusterings. Specifically, it will allow Prior work (Soon et al., 2001; Denis and Baldridge, 2007) has generated training data for pairwise classifiers in the following manner. For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search. The COREF-ILP model of Denis and Baldridge (2007) took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent. This is equivalent to finding the most likely assignment to each x(i,j) in Equation 2. As noted, these assignments may not be a legal clustering because there is no guarantee of transitivity. The transitive closure happens in an ad-hoc manner after this assignment is found: any two mentions linked through other mentions are determined to be coreferent. Our SOON-STYLE baseline used the same training and testing regimen as Soon et al. (2001). Our D&B-STYLE baseline used the same test time method as Denis and Baldridge (2007), however at training time we created data for all mention pairs. Because of the ad-hoc manner in which transitivity is enforced in our baseline systems, we do not necessarily find the most probable legal clustering. This is exactly the kind of task at which integer linear programming excels. We need to first formulate the objective function which we wish the ILP solver to maximize at test time.2 Let p(i j) = log P(x(i ,j)  |mi, mj; 0), which is the log probability that mi and mj are coreferent according to the pairwise logistic classifier discussed in the previous section, and let p(i,j) = log(1 − p(i,j)), be the log probability that they are not coreferent. Our objective function is then the log probability of a particular (possibly illegal) variable assignment: We add binary constraints on each of the variables: x(i,j) E 10, 11. We also add constraints, over each triple of mentions, to enforce transitivity: This constraint ensures that whenever x(zj) = x(j�k) = 1 it must also be the case that x(z�k) = 1. We used lp solve3 to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER, but we found that several documents where too long for lp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. The MUC scorer (Vilain et al., 1995) is a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measure (Ghosh, 2003), which is the standard f-measure computed over true/false coreference decisions for pairs of mentions; the Rand index (Rand, 1971), which is pairwise accuracy of the clustering; and variation of information (Meila, 2003), which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better). Our results are summarized in Table 1. We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREFILP system of Denis and Baldridge (2007), which was also based on a naive pairwise classifier. They used an ILP solver to find an assignment for the variables, but as they note at the end of Section 5.1, it is equivalent to taking all links for which the classifier returns a probability > 0.5, and so the ILP solver is not really necessary. We also include their JOINTILP numbers, however that system makes use of an additional anaphoricity classifier. For all three corpora, the ILP model beat both baselines for the cluster f-score, Rand index, and variation of information metrics. Using the V metric, the ILP system and the D&B-STYLE baseline performed about the same on the MUC-6 corpus, though for both ACE corpora, the ILP system was the clear winner. When using the MUC scorer, the ILP system always did worse than the D&B-STYLE baseline. However, this is precisely because the transitivity constraints tend to yield smaller clusters (which increase precision while decreasing recall). Remember that going in the opposite direction and simply putting all mentions in one cluster produces a MUC score which is higher than any in the table, even though this clustering is clearly not useful in applications. Hence, we are skeptical of this measure’s utility and provide it primarily for comparison with previous work. The improvements from the ILP system are most clearly shown on the ACE NWIRE corpus, where the V f-score improved 3.6%, and the cluster f-score improved 16.5%. We showed how to use integer linear programming to encode transitivity constraints in a coreference classifier which models pairwise decisions over mentions. We also demonstrated that enforcing such constraints at test time can significantly improve performance, using a variety of evaluation metrics.","We showed how to use integer linear programming to encode transitivity constraints in a coreference classifier which models pairwise decisions over mentions. We also demonstrated that enforcing such constraints at test time can significantly improve performance, using a variety of evaluation metrics."
17,"We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.","We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging. The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods. In this paper, we develop new methods for unsupervised part-of-speech tagging. We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type. The goal is to tag each word token so as to maximize accuracy against a gold tag sequence. Whether this is a realistic problem set-up is arguable, but an interesting collection of methods and results has accumulated around it, and these can be clearly compared with one another. We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which a gold tag sequence is available. There are 5,878 word types in this test set. We use the standard tag dictionary, consisting of 57,388 word/tag pairs derived from the entire Penn Treebank.1 8,910 dictionary entries are relevant to the 5,878 word types in the test set. Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data. There are 45 distinct grammatical tags. In this set-up, there are no unknown words. Figure 1 shows prior results for this problem. While the methods are quite different, they all make use of two common model elements. One is a probabilistic n-gram tag model P(ti|ti−n+1...ti−1), which we call the grammar. The other is a probabilistic word-given-tag model P(wi|ti), which we call the dictionary. The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication. They improve this to 83.9% by employing a fully Bayesian approach which integrates over all possible parameter values, rather than estimating a single distribution. They further improve this to 86.8% by using priors that favor sparse distributions. Smith and Eisner (2005) employ a contrastive estimation technique, in which they automatically generate negative examples and use CRF training. In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes. They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank. Goldberg et al. (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence “V V” is disallowed). Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for “the” from six to just one). In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments. The literature omits one other baseline, which is EM with a 2-gram tag model. Here we obtain 81.7% accuracy, which is better than the 3-gram model. It seems that EM with a 3-gram tag model runs amok with its freedom. For the rest of this paper, we will limit ourselves to a 2-gram tag model. We analyze the tag sequence output produced by EM and try to see where EM goes wrong. The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. The Bayesian methods overcome this effect by using priors which favor sparser distributions. But it is not easy to model such priors into EM learning. As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.). We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence). The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus. We see how the rare tag labels (like FW, SYM, etc.) are abused by EM. As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output. We also look at things more globally. We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence. We call this the observed grammar size, and it is 915. That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence. Here, we show a sample word sequence and the corresponding IP network generated for that sequence. can compare it with the gold tagging’s observed grammar size, which is 760. So we can safely say that EM is learning a grammar that is too big, still abusing its freedom. Bayesian sparse priors aim to create small models. We take a different tack in the paper and directly ask: What is the smallest model that explains the text? Our approach is related to minimum description length (MDL). We formulate our question precisely by asking which tag sequence (of the 106425 available) has the smallest observed grammar size. The answer is 459. That is, there exists a tag sequence that contains 459 distinct tag bigrams, and no other tag sequence contains fewer. We obtain this answer by formulating the problem in an integer programming (IP) framework. Figure 2 illustrates this with a small sample word sequence. We create a network of possible taggings, and we assign a binary variable to each link in the network. We create constraints to ensure that those link variables receiving a value of 1 form a left-to-right path through the tagging network, and that all other link variables receive a value of 0. We accomplish this by requiring the sum of the links entering each node to equal to the sum of the links leaving each node. We also create variables for every possible tag bigram and word/tag dictionary entry. We constrain link variable assignments to respect those grammar and dictionary variables. For example, we do not allow a link variable to “activate” unless the corresponding grammar variable is also “activated”. Finally, we add an objective function that minimizes the number of grammar variables that are assigned a value of 1. Figure 3 shows the IP solution for the example word sequence from Figure 2. Of course, a small grammar size does not necessarily correlate with higher tagging accuracy. For the small toy example shown in Figure 3, the correct tagging is “PRO AUX V . PRO V” (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead. For solving the integer program, we use CPLEX software (a commercial IP solver package). Alternatively, there are other programs such as lp solve, which are free and publicly available for use. Once we create an integer program for the full test corpus, and pass it to CPLEX, the solver returns an sponding grammar sizes for the sample word sequence from Figure 2 using the given dictionary and grammar. The IP solver finds the smallest grammar set that can explain the given word sequence. In this example, there exist two solutions that each contain only 4 tag pair entries, and IP returns one of them. objective function value of 459.3 CPLEX also returns a tag sequence via assignments to the link variables. However, there are actually 104378 tag sequences compatible with the 459-sized grammar, and our IP solver just selects one at random. We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%. We also note that CPLEX takes 320 seconds to return the optimal solution for the integer program corresponding to this particular test data (24,115 tokens with the 45-tag set). It might be interesting to see how the performance of the IP method (in terms of time complexity) is affected when scaling up to larger data and bigger tagsets. We leave this as part of future work. But we do note that it is possible to obtain less than optimal solutions faster by interrupting the CPLEX solver. Our IP formulation can find us a small model, but it does not attempt to fit the model to the data. Fortunately, we can use EM for that. We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP. Starting with uniform probabilities, EM finds a tagging that is 84.5% accurate, substantially better than the 81.7% originally obtained with the fully-connected grammar. So we see a benefit to our explicit small-model approach. While EM does not find the most accurate 3Note that the grammar identified by IP is not uniquely minimal. For the same word sequence, there exist other minimal grammars having the same size (459 entries). In our experiments, we choose the first solution returned by CPLEX. sequence consistent with the IP grammar (90.3%), it finds a relatively good one. The IP+EM tagging (with 84.5% accuracy) has some interesting properties. First, the dictionary we observe from the tagging is of higher quality (with fewer spurious tagging assignments) than the one we observe from the original EM tagging. Figure 4 shows some examples. We also measure the quality of the two observed grammars/dictionaries by computing their precision and recall against the grammar/dictionary we observe in the gold tagging.4 We find that precision of the observed grammar increases from 0.73 (EM) to 0.94 (IP+EM). In addition to removing many bad tag bigrams from the grammar, IP minimization also removes some of the good ones, leading to lower recall (EM = 0.87, IP+EM = 0.57). In the case of the observed dictionary, using a smaller grammar model does not affect the precision (EM = 0.91, IP+EM = 0.89) or recall (EM = 0.89, IP+EM = 0.89). During EM training, the smaller grammar with fewer bad tag bigrams helps to restrict the dictionary model from making too many bad choices that EM made earlier. Here are a few examples of bad dictionary entries that get removed when we use the minimized grammar for EM training: in FW a SYM of RP In RBR During EM training, the minimized grammar helps to eliminate many incorrect entries (i.e., zero out model parameters) from the dictionary, thereby yielding an improved dictionary model. So using the minimized grammar (which has higher precision) helps to improve the quality of the chosen dictionary (examples shown in Figure 4). This in turn helps improve the tagging accuracy from 81.7% to 84.5%. It is clear that the IP-constrained grammar is a better choice to run EM on than the full grammar. Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training. In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar). Next, we proceed to recover some good tag bigrams and expand the grammar in a restricted fashion by making use of the higher-quality dictionary produced by the IP+EM method. We now run EM again on the full grammar (all possible tag bigrams) in combination with this good dictionary (containing fewer entries than the full dictionary). Unlike the original training with full grammar, where EM could choose any tag bigram, now the choice of grammar entries is constrained by the good dictionary model that we provide EM with. This allows EM to recover some of the good tag pairs, and results in a good grammardictionary combination that yields better tagging performance. With these improvements in mind, we embark on an alternating scheme to find better models and taggings. We run EM for multiple passes, and in each pass we alternately constrain either the grammar model or the dictionary model. The procedure is simple and proceeds as follows: We notice significant gains in tagging performance when applying this technique. The tagging accuracy increases at each step and finally settles at a high of 91.6%, which outperforms the existing state-of-the-art systems for the 45-tag set. The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al. (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. Figure 5 shows the tagging performance achieved at each step. We found that it is the elimination of incorrect entries from the dictionary (and grammar) and not necessarily the initialization weights from previous EM training, that results in the tagging improvements. Initializing the last trained dictionary or grammar at each step with uniform weights also yields the same tagging improvements as shown in Figure 5. We find that the observed grammar also improves, growing from 459 entries to 603 entries, with precision increasing from 0.94 to 0.96, and recall increasing from 0.57 to 0.76. The figure also shows the model’s internal grammar and dictionary sizes. Figure 6 and 7 show how the precision/recall of the observed grammar and dictionary varies for different models from Figure 5. In the case of the observed grammar (Figure 6), precision increases at each step, whereas recall drops initially (owing to the grammar minimization) but then picks up again. The precision/recall of the observed dictionary on the other hand, is not affected by much. Multiple random restarts for EM, while not often emphasized in the literature, are key in this domain. Recall that our original EM tagging with a fully-connected 2-gram tag model was 81.7% accurate. When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy. Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8). As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models. Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al. (2008). But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data. Because EM is efficient, we can extend our word-sequence trainModel 1 Model 2 Model 3 Model 4 Model 5 ing data from the 24,115-token set to the entire Penn Treebank (973k tokens). We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%. This is our final result on the 45-tagset, and we note that it is higher than previously reported results. Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008). Their systems were shown to obtain considerable improvements in accuracy when using a 17-tagset (a coarsergrained version of the tag labels from the Penn Treebank) instead of the 45-tagset. When tagging the same standard test corpus with the smaller 17-tagset, our method is able to achieve a substantially high accuracy of 96.8%, which is the best result reported so far on this task. The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case (Goldberg et al., 2008). The first row in the table compares tagging results when using a full dictionary (i.e., a lexicon containing entries for 49,206 word types). The InitEM-HMM system from Goldberg et al. (2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008). In comparison, the Bayesian HMM (BHMM) model from Goldwater et al. (2007) and the CE+spl model (Contrastive Estimation with a spelling model) from Smith and Eisner (2005) report lower accuracies (87.3% and 88.7%, respectively). Our system (IP+EM) which uses integer programming and EM, gets the highest accuracy (96.8%). The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank. The IP+EM models used in the 17-tagset experiments reported here were not trained on the entire Penn Treebank, but instead used a smaller section containing 77,963 tokens for estimating model parameters. We also include the accuracies for our IP+EM model when using only the 24,115 token test corpus for EM estimation (shown within parenthesis in second column of the table in Figure 9). We find that our performance does not degrade when the parameter estimation is done using less data, and our model still achieves a high accuracy of 96.8%. The literature also includes results reported in a different setting for the tagging problem. In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types. In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities). Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries. We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word. We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary. Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the tag given a particular suffix (e.g., P(VBG I ing) = 0.97, P(N I ing) = 0.0001, ...). Next, for every unknown word “w”, the trained P(tag I suffix) model is used to predict the top 3 tag possibilities for “w” (using only its suffix information), and subsequently this word along with its 3 tags are added as a new entry to the lexicon. We do this for every unknown word, and eventually we have a dictionary containing entries for all the words. Once the completed lexicon (containing both correct entries for words in the lexicon and the predicted entries for unknown words) is available, we follow the same methodology from Sections 3 and 4 using integer programming to minimize the size of the grammar and then applying EM to estimate parameter values. Figure 9 shows comparative results for the 17tagset case when the dictionary is incomplete. The second and third rows in the table shows tagging accuracies for different systems when a cutoff of 2 (i.e., all word types that occur with frequency counts < 2 in the test corpus are removed) and a cutoff of 3 (i.e., all word types occurring with frequency counts < 3 in the test corpus are removed) is applied to the dictionary. This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary. As the results in Figure 9 illustrate, the IP+EM method clearly does better than all the other systems except for the LDA+AC model. The LDA+AC model from Toutanova and Johnson (2008) has a strong ambiguity class component and uses more features to handle the unknown words better, and this contributes to the slightly higher performance in the incomplete dictionary cases, when compared to the IP+EM model. The method proposed in this paper is simple— once an integer program is produced, there are solvers available which directly give us the solution. In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task. While some previous methods introduced for the same task have achieved big tagging improvements using additional linguistic knowledge or manual supervision, our models are not provided with any additional information. Figure 10 illustrates for the 45-tag set some of the common mistakes that our best tagging model (92.3%) makes. In some cases, the model actually gets a reasonable tagging but is penalized perhaps unfairly. For example, “to” is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO. The model also gets penalized for tagging the word “U.S.” as an adjective (JJ), which might be considered valid in some cases such as “the U.S. State Department”. In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS). Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998). In MDL, there is a single objective function to (1) maximize the likelihood of observing the data, and at the same time (2) minimize the length of the model description (which depends on the model size). However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results. In the past, only a few approaches utilizing MDL have been shown to work for natural language applications. These approaches employ heuristic search methods with MDL for the task of unsupervised learning of morphology of natural languages (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2005). The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques. We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007).","The method proposed in this paper is simple— once an integer program is produced, there are solvers available which directly give us the solution. In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task. While some previous methods introduced for the same task have achieved big tagging improvements using additional linguistic knowledge or manual supervision, our models are not provided with any additional information. Figure 10 illustrates for the 45-tag set some of the common mistakes that our best tagging model (92.3%) makes. In some cases, the model actually gets a reasonable tagging but is penalized perhaps unfairly. For example, “to” is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO. The model also gets penalized for tagging the word “U.S.” as an adjective (JJ), which might be considered valid in some cases such as “the U.S. State Department”. In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS). Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998). In MDL, there is a single objective function to (1) maximize the likelihood of observing the data, and at the same time (2) minimize the length of the model description (which depends on the model size). However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results. In the past, only a few approaches utilizing MDL have been shown to work for natural language applications. These approaches employ heuristic search methods with MDL for the task of unsupervised learning of morphology of natural languages (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2005). The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques. We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007)."
18,"In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the effects of applying such a technique to highermodels trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.","In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the effects of applying such a technique to highermodels trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score. A statistical language model assigns a probability P(w) to any given string of words wm1 = w1, ..., wm. In the case of n-gram language models this is done by factoring the probability: do not differ in the last n − 1 words, one problem ngram language models suffer from is that the training data is too sparse to reliably estimate all conditional probabilities P(wi|wi−1 Class-based n-gram models are intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model (Brown et al., 1990). They have often been shown to improve the performance of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001). However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006). Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-gram order. When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification. Most often, variants of the exchange algorithm (Kneser and Ney, 1993; Martinet al., 1998) or the agglomerative clustering algorithm presented in (Brown et al., 1990) are used, both of which have prohibitive runtimes when clustering large vocabularies on the basis of large training corpora with a sufficiently high number of classes. In this paper we introduce a modification of the exchange algorithm with improved efficiency and then present a distributed version of the modified algorithm, which makes it feasible to obtain word classifications using billions of tokens of training data. We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used. By partitioning all N„ words of the vocabulary into Nc sets, with c(w) mapping a word onto its equivalence class and c(wi) mapping a sequence of words onto the sequence of their respective equivalence classes, a typical class-based n-gram model approximates P(wi|wi−1 thus greatly reducing the number of parameters in the model, since usually Nc is much smaller than N„. In the following, we will call this type of model a two-sided class-based model, as both the history of each n-gram, the sequence of words conditioned on, as well as the predicted word are replaced by their class. Once a partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure. In addition, the size of the model is usually greatly reduced. Two-sided class-based models received most attention in the literature. However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001). In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: which we will call predictive class-based models in the following sections. One of the frequently used algorithms for automatically obtaining partitions of the vocabulary is the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998). Beginning with an initial clustering, the algorithm greedily maximizes the log likelihood of a two-sided class bigram or trigram model as described in Eq. (1) on the training data. Let V be the set of words in the vocabulary and C the set of classes. This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data: The algorithm iterates over all words in the vocabulary and tentatively moves each word to each cluster. The change in the optimization criterion is computed for each of these tentative moves and the exchange leading to the highest increase in the optimization criterion (3) is performed. This procedure is then repeated until the algorithm reaches a local optimum. To be able to efficiently calculate the changes in the optimization criterion when exchanging a word, the counts in Eq. (3) are computed once for the initial clustering, stored, and afterwards updated when a word is exchanged. Often only a limited number of iterations are performed, as letting the algorithm terminate in a local optimum can be computationally impractical. The implementation described in (Martin et al., 1998) uses a memory saving technique introducing a binary search into the complexity estimation. For the sake of simplicity, we omit this detail in the following complexity analysis. We also do not employ this optimization in our implementation. The worst case complexity of the exchange algorithm is quadratic in the number of classes. However, Input: The fixed number of clusters Nc Compute initial clustering while clustering changed in last iteration do forall w ∈ V do forall c ∈ C do move word w tentatively to cluster c compute updated optimization criterion move word w to cluster maximizing optimization criterion the number of occurrences of the word v followed by some word in class c. Then the following optimization criterion can be derived, with F(C) being the log likelihood function of the predictive class bigram model given a clustering C: the average case complexity can be reduced by updating only the counts which are actually affected by moving a word from one cluster to another. This can be done by considering only those sets of clusters for which N(w, c) > 0 or N(c, w) > 0 for a word w about to be exchanged, both of which can be calculated efficiently when exchanging a word. The algorithm scales linearly in the size of the vocabulary. With Npre c and Nsuc cdenoting the average number of clusters preceding and succeeding another cluster, B denoting the number of distinct bigrams in the training corpus, and I denoting the number of iterations, the worst case complexity of the algorithm is in: When using large corpora with large numbers of bigrams the number of required updates can increase towards the quadratic upper bound as Npre c and Nsuc c approach Nc. For a more detailed description and further analysis of the complexity, the reader is referred to (Martin et al., 1998). Modifying the exchange algorithm in order to optimize the log likelihood of a predictive class bigram model, leads to substantial performance improvements, similar to those previously reported for another type of one-sided class model in (Whittaker and Woodland, 2001). We use a predictive class bigram model as given in Eq. (2), for which the maximum-likelihood probability estimates for the n-grams are given by their relative frequencies: where N(w) again denotes the number of occurrences of the word w in the training corpus and N(v, c) The very last summation of Eq. (8) now effectively sums over all occurrences of all words and thus cancels out with the first summation of (8) which leads to: In the first summation of Eq. (9), for a given word v only the set of classes which contain at least one word w for which N(v, w) > 0 must be considered, denoted by suc(v). The second summation is equivalent to When exchanging a word w between two classes c and c0, only two summands of the second summation of Eq. (10) are affected. The first summation can be updated by iterating over all bigrams ending in the exchanged word. Throughout one iteration of the algorithm, in which for each word in the vocabulary each possible move to another class is evaluated, this amounts to the number of distinct bigrams in the training corpus B, times the number of clusters N,. Thus the worst case complexity using the modified optimization criterion is in: Using this optimization criterion has two effects on the complexity of the algorithm. The first difference is that in contrast to the exchange algorithm using a two sided class-based bigram model in its optimization criterion, only two clusters are affected by moving a word. Thus the algorithm scales linearly in the number of classes. The second difference is that B dominates the term B + N„ for most corpora and scales far less than linearly with the vocabulary size, providing a significant performance advantage over the other optimization criterion, especially when large vocabularies are used (Whittaker and Woodland, 2001). For efficiency reasons, an exchange of a word between two clusters is separated into a remove and a move procedure. In each iteration the remove procedure only has to be called once for each word, while for a given word move is called once for every cluster to compute the consequences of the tentative exchanges. An outline of the move procedure is given below. The remove procedure is similar. Procedure MoveWord When training on large corpora, even the modified exchange algorithm would still require several days if not weeks of CPU time for a sufficient number of iterations. To overcome this we introduce a novel distributed exchange algorithm, based on the modified exchange algorithm described in the previous section. The vocabulary is randomly partitioned into sets of roughly equal size. With each word w in one of these sets, all words v preceding w in the corpus are stored with the respective bigram count N(v, w). The clusterings generated in each iteration as well as the initial clustering are stored as the set of words in each cluster, the total number of occurrences of each cluster in the training corpus, and the list of words preceeding each cluster. For each word w in the predecessor list of a given cluster c, the number of times w occurs in the training corpus before any word in c, N(w, c), is also stored. Together with the counts stored with the vocabulary partitions, this allows for efficient updating of the terms in Eq. (10). The initial clustering together with all the required counts is created in an initial iteration by assigning the n-th most frequent word to cluster n mod N,. While (Martinet al., 1998) and (Emami and Jelinek, 2005) observe that the initial clustering does not seem to have a noticeable effect on the quality of the resulting clustering or the convergence rate, the intuition behind this method of initialization is that it is unlikely for the most frequent words to be clustered together due to their high numbers of occurrences. In each subsequent iteration each one of a number of workers is assigned one of the partitions of the words in the vocabulary. After loading the current clustering, it then randomly chooses a subset of these words of a fixed size. For each of the selected words the worker then determines to which cluster the word is to be moved in order to maximize the increase in log likelihood, using the count updating procedures described in the previous section. All changes a worker makes to the clustering are accumulated locally in delta data structures. At the end of the iteration all deltas are merged and applied to the previous clustering, resulting in the complete clustering loaded in the next iteration. This algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2004) that we used for our implementation. While the greedy non-distributed exchange algorithm is guaranteed to converge as each exchange increases the log likelihood of the assumed bigram model, this is not necessarily true for the distributed exchange algorithm. This stems from the fact that the change in log likelihood is calculated by each worker under the assumption that no other changes to the clustering are performed by other workers in this iteration. However, if in each iteration only a rather small and randomly chosen subset of all words are considered for exchange, the intuition is that the remaining words still define the parameters of each cluster well enough for the algorithm to converge. In (Emami and Jelinek, 2005) the authors observe that only considering a subset of the vocabulary of half the size of the complete vocabulary in each iteration does not affect the time required by the exchange algorithm to converge. Yet each iteration is sped up by approximately a factor of two. The quality of class-based models trained using the resulting clusterings did not differ noticeably from those trained using clusterings for which the full vocabulary was considered in each iteration. Our experiments showed that this also seems to be the case for the distributed exchange algorithm. While considering very large subsets of the vocabulary in each iteration can cause the algorithm to not converge at all, considering only a very small fraction of the words for exchange will increase the number of iterations required to converge. In experiments we empirically determined that choosing a subset of roughly a third of the size of the full vocabulary is a good balance in this trade-off. We did not observe the algorithm to not converge unless we used fractions above half of the vocabulary size. We typically ran the clustering for 20 to 30 iterations after which the number of words exchanged in each iteration starts to stabilize at less than 5 percent of the vocabulary size. Figure 1 shows the number of words exchanged in each of 34 iterations when clustering the approximately 300,000 word vocabulary of the Arabic side of the English-Arabic parallel training data into 512 and 2,048 clusters. Despite a steady reduction in the number of words exchanged per iteration, we observed the convergence in regards to log-likelihood to be far from monotone. In our experiments we were able to achieve significantly more monotone and faster convergence by employing the following heuristic. As described in Section 5, we start out the first iteration with a random partition of the vocabulary into subsets each assigned to a specific worker. However, instead of keeping this assignment constant throughout all iterations, after each iteration the vocabulary is partitioned anew so that all words from any given cluster are considered by the same worker in the next iteration. The intuition behind this heuristic is that as the clustering becomes more coherent, the information each worker has about groups of similar words is becoming increasingly accurate. In our experiments this heuristic lead to almost monotone convergence in log-likelihood. It also reduced the number of iterations required to converge by up to a factor of three. The runtime of the distributed exchange algorithm depends highly on the number of distinct bigrams in the training corpus. When clustering the approximately 1.5 million word vocabulary of a 405 million token English corpus into 1,000 clusters, one iteration takes approximately 5 minutes using 50 workers based on standard hardware running the Linux operating system. When clustering the 0.5 million most frequent words in the vocabulary of an English corpus with 31 billion tokens into 1,000 clusters, one iteration takes approximately 30 minutes on 200 workers. When scaling up the vocabulary and corpus sizes, the current bottleneck of our implementation is loading the current clustering into memory. While the memory requirements decrease with each iteration, during the first few iterations a worker typically still needs approximately 2 GB of memory to load the clustering generated in the previous iteration when training 1,000 clusters on the 31 billion token corpus. We trained a number of predictive class-based language models on different Arabic and English corpora using clusterings trained on the complete data of the same corpus. We use the distributed training and application infrastructure described in (Brants et al., 2007) with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system. For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (Brants et al., 2007). Models with Stupid Backoff return scores rather than normalized probabilities, thus perplexities cannot be calculated for these models. Instead we report BLEU scores (Papineni et al., 2002) of the machine translation system using different combinations of word- and classbased models for translation tasks from English to Arabic and Arabic to English. For English we used three different training data sets: en target: The English side of Arabic-English and Chinese-English parallel data provided by LDC (405 million tokens). en ldcnews: Consists of several English news data sets provided by LDC (5 billion tokens). en webnews: Consists of data collected up to December 2005 from web pages containing primarily English news articles (31 billion tokens). A fourth data set, en web, was used together with the other three data sets to train the large wordbased model used in the second machine translation experiment. This set consists of general web data collected in January 2006 (2 trillion tokens). For Arabic we used the following two different training data sets: ar gigaword: Consists of several Arabic news data sets provided by LDC (629 million tokens). ar webnews: Consists of data collected up to December 2005 from web pages containing primarily Arabic news articles (approximately 600 million tokens). Given a sentence f in the source language, the machine translation problem is to automatically produce a translation e� in the target language. In the subsequent experiments, we use a phrase-based statistical machine translation system based on the loglinear formulation of the problem described in (Och and Ney, 2002): where {hm(e, f)} is a set of M feature functions and {am} a set of weights. We use each predictive classbased language model as well as a word-based model as separate feature functions in the log-linear combination in Eq. (11). The weights are trained using minimum error rate training (Och, 2003) with BLEU score as the objective function. The dev and test data sets contain parts of the 2003, 2004 and 2005 Arabic NIST MT evaluation sets among other parallel data. The blind test data used is the “NIST” part of the 2006 Arabic-English NIST MT evaluation set, and is not included in the training data. For the first experiment we trained predictive class-based 5-gram models using clusterings with 64, 128, 256 and 512 clusters1 on the en target data. We then added these models as additional features to the log linear model of the Arabic-English machine translation system. The word-based language model used by the system in these experiments is a 5-gram model also trained on the en target data set. Table 1 shows the BLEU scores reached by the translation system when combining the different class-based models with the word-based model in comparison to the BLEU scores by a system using only the word-based model on the Arabic-English translation task. Adding the class-based models leads to small improvements in BLEU score, with the highest improvements for both dev and nist06 being statistically significant 2. In the next experiment we used two predictive class-based models, a 5-gram model with 512 clusters trained on the en target data set and a 6-gram model also using 512 clusters trained on the en ldcnews data set. We used these models in addition to a word-based 6-gram model created by combining models trained on all four English data sets. Table 2 shows the BLEU scores of the machine translation system using only this word-based model, the scores after adding the class-based model trained on the en target data set and when using all three models. For our experiment with the English Arabic translation task we trained two 5-gram predictive classbased models with 512 clusters on the Arabic ar gigaword and ar webnews data sets. The wordbased Arabic 5-gram model we used was created by combining models trained on the Arabic side of the parallel training data (347 million tokens), the ar gigaword and ar webnews data sets, and additional Arabic web data. As shown in Table 3, adding the predictive classbased model trained on the ar webnews data set leads to small improvements in dev and nist06 scores but causes the test score to decrease. However, adding the class-based model trained on the ar gigaword data set to the other class-based and the word-based model results in further improvement of the dev score, but also in large improvements of the test and nist06 scores. We performed experiments to eliminate the possibility of data overlap between the training data and the machine translation test data as cause for the large improvements. In addition, our experiments showed that when there is overlap between the training and test data, the class-based models lead to lower scores as long as they are trained only on data also used for training the word-based model. One explanation could be that the domain of the ar gigaword corpus is much closer to the domain of the test data than that of other training data sets used. However, further investigation is required to explain the improvements. The clusters produced by the distributed algorithm vary in their size and number of occurrences. In a clustering of the en target data set with 1,024 clusters, the cluster sizes follow a typical longtailed distribution with the smallest cluster containing 13 words and the largest cluster containing 20,396 words. Table 4 shows some examples of the generated clusters. For each cluster we list all words occurring more than 1,000 times in the corpus.","We trained a number of predictive class-based language models on different Arabic and English corpora using clusterings trained on the complete data of the same corpus. We use the distributed training and application infrastructure described in (Brants et al., 2007) with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system. For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (Brants et al., 2007). Models with Stupid Backoff return scores rather than normalized probabilities, thus perplexities cannot be calculated for these models. Instead we report BLEU scores (Papineni et al., 2002) of the machine translation system using different combinations of word- and classbased models for translation tasks from English to Arabic and Arabic to English. For English we used three different training data sets: en target: The English side of Arabic-English and Chinese-English parallel data provided by LDC (405 million tokens). en ldcnews: Consists of several English news data sets provided by LDC (5 billion tokens). en webnews: Consists of data collected up to December 2005 from web pages containing primarily English news articles (31 billion tokens). A fourth data set, en web, was used together with the other three data sets to train the large wordbased model used in the second machine translation experiment. This set consists of general web data collected in January 2006 (2 trillion tokens). For Arabic we used the following two different training data sets: ar gigaword: Consists of several Arabic news data sets provided by LDC (629 million tokens). ar webnews: Consists of data collected up to December 2005 from web pages containing primarily Arabic news articles (approximately 600 million tokens). Given a sentence f in the source language, the machine translation problem is to automatically produce a translation e� in the target language. In the subsequent experiments, we use a phrase-based statistical machine translation system based on the loglinear formulation of the problem described in (Och and Ney, 2002): where {hm(e, f)} is a set of M feature functions and {am} a set of weights. We use each predictive classbased language model as well as a word-based model as separate feature functions in the log-linear combination in Eq. (11). The weights are trained using minimum error rate training (Och, 2003) with BLEU score as the objective function. The dev and test data sets contain parts of the 2003, 2004 and 2005 Arabic NIST MT evaluation sets among other parallel data. The blind test data used is the “NIST” part of the 2006 Arabic-English NIST MT evaluation set, and is not included in the training data. For the first experiment we trained predictive class-based 5-gram models using clusterings with 64, 128, 256 and 512 clusters1 on the en target data. We then added these models as additional features to the log linear model of the Arabic-English machine translation system. The word-based language model used by the system in these experiments is a 5-gram model also trained on the en target data set. Table 1 shows the BLEU scores reached by the translation system when combining the different class-based models with the word-based model in comparison to the BLEU scores by a system using only the word-based model on the Arabic-English translation task. Adding the class-based models leads to small improvements in BLEU score, with the highest improvements for both dev and nist06 being statistically significant 2. In the next experiment we used two predictive class-based models, a 5-gram model with 512 clusters trained on the en target data set and a 6-gram model also using 512 clusters trained on the en ldcnews data set. We used these models in addition to a word-based 6-gram model created by combining models trained on all four English data sets. Table 2 shows the BLEU scores of the machine translation system using only this word-based model, the scores after adding the class-based model trained on the en target data set and when using all three models. For our experiment with the English Arabic translation task we trained two 5-gram predictive classbased models with 512 clusters on the Arabic ar gigaword and ar webnews data sets. The wordbased Arabic 5-gram model we used was created by combining models trained on the Arabic side of the parallel training data (347 million tokens), the ar gigaword and ar webnews data sets, and additional Arabic web data. As shown in Table 3, adding the predictive classbased model trained on the ar webnews data set leads to small improvements in dev and nist06 scores but causes the test score to decrease. However, adding the class-based model trained on the ar gigaword data set to the other class-based and the word-based model results in further improvement of the dev score, but also in large improvements of the test and nist06 scores. We performed experiments to eliminate the possibility of data overlap between the training data and the machine translation test data as cause for the large improvements. In addition, our experiments showed that when there is overlap between the training and test data, the class-based models lead to lower scores as long as they are trained only on data also used for training the word-based model. One explanation could be that the domain of the ar gigaword corpus is much closer to the domain of the test data than that of other training data sets used. However, further investigation is required to explain the improvements. The clusters produced by the distributed algorithm vary in their size and number of occurrences. In a clustering of the en target data set with 1,024 clusters, the cluster sizes follow a typical longtailed distribution with the smallest cluster containing 13 words and the largest cluster containing 20,396 words. Table 4 shows some examples of the generated clusters. For each cluster we list all words occurring more than 1,000 times in the corpus."
19,"We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.","We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance. In natural language parsing, lexical information is seen as crucial to resolving ambiguous relationships, yet lexicalized statistics are sparse and difficult to estimate directly. It is therefore attractive to consider intermediate entities which exist at a coarser level than the words themselves, yet capture the information necessary to resolve the relevant ambiguities. In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach. First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner. We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features. To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research—e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The idea of combining word clusters with discriminative learning has been previously explored by Miller et al. (2004), in the context of namedentity recognition, and their work directly inspired our research. However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure. Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications. In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled data to facilitate the utilization of the supervised corpus, increasing the performance of the model in absolute terms. Second, given a fixed target performance level, we might wish to use unlabeled data to reduce the amount of annotated data necessary to reach this target. We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3). By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4). The remainder of this paper is divided as follows: Section 2 gives background on dependency parsing and clustering, Section 3 describes the cluster-based features, Section 4 presents our experimental results, Section 5 discusses related work, and Section 6 concludes with ideas for future research. Recent work (Buchholz and Marsi, 2006; Nivre et al., 2007) has focused on dependency parsing. Dependency syntax represents syntactic information as a network of head-modifier dependency arcs, typically restricted to be a directed tree (see Figure 1 for an example). Dependency parsing depends critically on predicting head-modifier relationships, which can be difficult due to the statistical sparsity of these word-to-word interactions. Bilexical dependencies are thus ideal candidates for the application of coarse word proxies such as word clusters. In this paper, we take a part-factored structured classification approach to dependency parsing. For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y E Y(x) decomposes into a set of “parts” r E y. In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model. In higher-order parsing models, the parts can consist of interactions between more than two words. For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio “plays”, “Elianti”, and “.” in Figure 1. The Carreras (2007) parser has parts for both sibling interactions and grandparent interactions, such as the trio “*”, “plays”, and “Haag” in Figure 1. These kinds of higher-order factorizations allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the folAbove, we have assumed that each part is scored by a linear model with parameters w and featuremapping f(·). For many different part factorizations and structure domains Y(·), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007). In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words. Initially, each word in the vocabulary is considered to be in its own distinct cluster. The algorithm then repeatedly merges the pair of clusters which causes the smallest decrease in the likelihood of the text corpus, according to a class-based bigram language model defined on the word clusters. By tracing the pairwise merge operations, one obtains a hierarchical clustering of the words, which can be represented as a binary tree as in Figure 2. Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2. In order to obtain a clustering of the words, we select all nodes at a certain depth from the root of the hierarchy. For example, in Figure 2 we might select the four nodes at depth 2 from the root, yielding the clusters {apple,pear}, {Apple,IBM}, {bought,run}, and {of,in}. Note that the same clustering can be obtained by truncating each word’s bit-string to a 2-bit prefix. By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al., 2004). For all of the experiments in this paper, we used the Liang (2005) implementation of the Brown algorithm to obtain the necessary word clusters. Key to the success of our approach is the use of features which allow word-cluster-based information to assist the parser. The feature sets we used are similar to other feature sets in the literature (McDonald et al., 2005a; Carreras, 2007), so we will not attempt to give a exhaustive description of the features in this section. Rather, we describe our features at a high level and concentrate on our methodology and motivations. In our experiments, we employed two different feature sets: a baseline feature set which draws upon “normal” information sources such as word forms and parts of speech, and a cluster-based feature set that also uses information derived from the Brown cluster hierarchy. Our first-order baseline feature set is similar to the feature set of McDonald et al. (2005a), and consists of indicator functions for combinations of words and parts of speech for the head and modifier of each dependency, as well as certain contextual tokens.1 Our second-order baseline features are the same as those of Carreras (2007) and include indicators for triples of part of speech tags for sibling interactions and grandparent interactions, as well as additional bigram features based on pairs of words involved these higher-order interactions. Examples of baseline features are provided in Table 1. tag. Abbreviations: ht = head POS, hw = head word, hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head, hc* = full bit string of head; mt,mw,mc4,mc6,mc* = likewise for modifier; st,gt,sc4,gc4,... = likewise for sibling and grandchild. The first- and second-order cluster-based feature sets are supersets of the baseline feature sets: they include all of the baseline feature templates, and add an additional layer of features that incorporate word clusters. Following Miller et al. (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al. (2004) work (between 12 and 20 bits) performed poorly in dependency parsing.2 After experimenting with many different feature configurations, we eventually settled on a simple but effective methodology. First, we found that it was helpful to employ two different types of word clusters: 2. Full bit strings,3 which we used as substitutes for word forms. Using these two types of clusters, we generated new features by mimicking the template structure of the original baseline features. For example, the baseline feature set includes indicators for word-to-word and tag-to-tag interactions between the head and modifier of a dependency. In the cluster-based feature set, we correspondingly introduce new indicators for interactions between pairs of short bit-string prefixes and pairs of full bit strings. Some examples of cluster-based features are given in Table 1. Second, we found it useful to concentrate on “hybrid” features involving, e.g., one bit-string and one part of speech. In our initial attempts, we focused on features that used cluster information exclusively. While these cluster-only features provided some benefit, we found that adding hybrid features resulted in even greater improvements. One possible explanation is that the clusterings generated by the Brown algorithm can be noisy or only weakly relevant to syntax; thus, the clusters are best exploited when “anchored” to words or parts of speech. Finally, we found it useful to impose a form of vocabulary restriction on the cluster-based features. Specifically, for any feature that is predicated on a word form, we eliminate this feature if the word in question is not one of the top-N most frequent words in the corpus. When N is between roughly 100 and 1,000, there is little effect on the performance of the cluster-based feature sets.4 In addition, the vocabulary restriction reduces the size of the feature sets to managable proportions. In order to evaluate the effectiveness of the clusterbased feature sets, we conducted dependency parsing experiments in English and Czech. We test the features in a wide range of parsing configurations, including first-order and second-order parsers, and labeled and unlabeled parsers.5 3As in Brown et al. (1992), we limit the clustering algorithm so that it recovers at most 1,000 distinct bit-strings; thus full bit strings are not equivalent to word forms. The English experiments were performed on the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2–21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24). The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures. To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.10 Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.11 We trained the parsers using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which represents a balance between strong performance and fast training times. To select the number of iterations of perceptron training, we performed up to 30 iterations and chose the iteration which optimized accuracy on the development set. Our feature mappings are quite high-dimensional, so we eliminated all features which occur only once in the training data. The resulting models still had very high dimensionality, ranging from tens of millions to as many as a billion features.12 All results presented in this section are given in terms of parent-prediction accuracy, which measures the percentage of tokens that are attached to the correct head token. For labeled dependency structures, both the head token and dependency label must be correctly predicted. In addition, in English parsing we ignore the parent-predictions of punctuation tokens,13 and in Czech parsing we retain the punctuation tokens; this matches previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). In our English experiments, we tested eight different parsing configurations, representing all possible choices between baseline or cluster-based feature sets, first-order (Eisner, 2000) or second-order (Carreras, 2007) factorizations, and labeled or unlabeled parsing. Table 2 compiles our final test results and also includes two results from previous work by McDonald et al. (2005a) and McDonald and Pereira (2006), for the purposes of comparison. We note a few small differences between our parsers and the 12Due to the sparsity of the perceptron updates, however, only a small fraction of the possible features were active in our trained models. 13A punctuation token is any token whose gold-standard part of speech tag is one of {‘‘ ’’ .1. parsers evaluated in this previous work. First, the MD1 and MD2 parsers were trained via the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2004), while we use the averaged perceptron. In addition, the MD2 model uses only sibling interactions, whereas the dep2/dep2c parsers include both sibling and grandparent interactions. There are some clear trends in the results of Table 2. First, performance increases with the order of the parser: edge-factored models (dep1 and MD1) have the lowest performance, adding sibling relationships (MD2) increases performance, and adding grandparent relationships (dep2) yields even better accuracies. Similar observations regarding the effect of model order have also been made by Carreras (2007). Second, note that the parsers using cluster-based feature sets consistently outperform the models using the baseline features, regardless of model order or label usage. Some of these improvements can be quite large; for example, a first-order model using cluster-based features generally performs as well as a second-order model using baseline features. Moreover, the benefits of cluster-based feature sets combine additively with the gains of increasing model order. For example, consider the unlabeled parsers in Table 2: on Section 23, increasing the model order from dep1 to dep2 results in a relative reduction in error of roughly 13%, while introducing clusterbased features from dep2 to dep2c yields an additional relative error reduction of roughly 14%. As a final note, all 16 comparisons between cluster-based features and baseline features shown in Table 2 are statistically significant.14 We performed additional experiments to evaluate the effect of the cluster-based features as the amount of training data is varied. Note that the dependency parsers we use require the input to be tagged with parts of speech; thus the quality of the part-ofspeech tagger can have a strong effect on the performance of the parser. In these experiments, we consider two possible scenarios: for training both tagger and parser. Table 3 displays the accuracy of first- and secondorder models when trained on smaller portions of the Treebank, in both scenarios described above. Note that the cluster-based features obtain consistent gains regardless of the size of the training set. When the tagger is trained on the reduced-size datasets, the gains of cluster-based features are more pronounced, but substantial improvements are obtained even when the tagger is accurate. It is interesting to consider the amount by which cluster-based features reduce the need for supervised data, given a desired level of accuracy. Based on Table 3, we can extrapolate that cluster-based features reduce the need for supervised data by roughly a factor of 2. For example, the performance of the dep1c and dep2c models trained on 1k sentences is roughly the same as the performance of the dep1 and dep2 models, respectively, trained on 2k sentences. This approximate data-halving effect can be observed throughout the results in Table 3. When combining the effects of model order and cluster-based features, the reductions in the amount of supervised data required are even larger. For example, in scenario 1 the dep2c model trained on 1k sentences is close in performance to the dep1 model trained on 4k sentences, and the dep2c model trained on 4k sentences is close to the dep1 model trained on the entire training set (roughly 40k sentences). In our Czech experiments, we considered only unlabeled parsing,15 leaving four different parsing configurations: baseline or cluster-based features and first-order or second-order parsing. Note that our feature sets were originally tuned for English parsing, and except for the use of Czech clusters, we made no attempt to retune our features for Czech. Czech dependency structures may contain nonprojective edges, so we employ a maximum directed spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b) as our firstorder parser for Czech. For the second-order parsing experiments, we used the Carreras (2007) parser. Since this parser only considers projective dependency structures, we “projectivized” the PDT 1.0 training set by finding, for each sentence, the projective tree which retains the most correct dependencies; our second-order parsers were then trained with respect to these projective trees. The development and test sets were not projectivized, so our secondorder parser is guaranteed to make errors in test sentences containing non-projective dependencies. To overcome this, McDonald and Pereira (2006) use a two-stage approximate decoding process in which the output of their second-order parser is “deprojectivized” via greedy search. For simplicity, we did not implement a deprojectivization stage on top of our second-order parser, but we conjecture that such techniques may yield some additional performance gains; we leave this to future work. Table 4 gives accuracy results on the PDT 1.0 test set for our unlabeled parsers. As in the English experiments, there are clear trends in the results: parsers using cluster-based features outperform parsers using baseline features, and secondorder parsers outperform first-order parsers. Both of the comparisons between cluster-based and baseline features in Table 4 are statistically significant.16 Table 5 compares accuracy results on the PDT 1.0 test set for our parsers and several other recent papers. As in our English experiments, we performed additional experiments on reduced sections of the PDT; the results are shown in Table 6. For simplicity, we did not retrain a tagger for each reduced dataset, so we always use the (automatically-assigned) part of speech tags provided in the corpus. Note that the cluster-based features obtain improvements at all training set sizes, with data-reduction factors similar to those observed in English. For example, the dep1c model trained on 4k sentences is roughly as good as the dep1 model trained on 8k sentences. Here, we present two additional results which further explore the behavior of the cluster-based feature sets. In Table 7, we show the development-set performance of second-order parsers as the threshold for lexical feature elimination (see Section 3.2) is varied. Note that the performance of cluster-based features is fairly insensitive to the threshold value, whereas the performance of baseline features clearly degrades as the vocabulary size is reduced. In Table 8, we show the development-set performance of the first- and second-order parsers when features containing part-of-speech-based information are eliminated. Note that the performance obtained by using clusters without parts of speech is close to the performance of the baseline features. As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of “backed-off” version of the baseline features. However, our work is focused on discriminative learning as opposed to generative models. Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice. However, their approach depends on the usage of a high-quality parse reranker, whereas the method described here simply augments the features of an existing parser. Note that our two approaches are compatible in that we could also design a reranker and apply self-training techniques on top of the clusterbased features. In this paper, we have presented a simple but effective semi-supervised learning approach and demonstrated that it achieves substantial improvement over a competitive baseline in two broad-coverage dependency parsing tasks. Despite this success, there are several ways in which our approach might be improved. To begin, recall that the Brown clustering algorithm is based on a bigram language model. Intuitively, there is a “mismatch” between the kind of lexical information that is captured by the Brown clusters and the kind of lexical information that is modeled in dependency parsing. A natural avenue for further research would be the development of clustering algorithms that reflect the syntactic behavior of words; e.g., an algorithm that attempts to maximize the likelihood of a treebank, according to a probabilistic dependency model. Alternately, one could design clustering algorithms that cluster entire head-modifier arcs rather than individual words. Another idea would be to integrate the clustering algorithm into the training algorithm in a limited fashion. For example, after training an initial parser, one could parse a large amount of unlabeled text and use those parses to improve the quality of the clusters. These improved clusters can then be used to retrain an improved parser, resulting in an overall algorithm similar to that of McClosky et al. (2006). Setting aside the development of new clustering algorithms, a final area for future work is the extension of our method to new domains, such as conversational text or other languages, and new NLP problems, such as machine translation.","In this paper, we have presented a simple but effective semi-supervised learning approach and demonstrated that it achieves substantial improvement over a competitive baseline in two broad-coverage dependency parsing tasks. Despite this success, there are several ways in which our approach might be improved. To begin, recall that the Brown clustering algorithm is based on a bigram language model. Intuitively, there is a “mismatch” between the kind of lexical information that is captured by the Brown clusters and the kind of lexical information that is modeled in dependency parsing. A natural avenue for further research would be the development of clustering algorithms that reflect the syntactic behavior of words; e.g., an algorithm that attempts to maximize the likelihood of a treebank, according to a probabilistic dependency model. Alternately, one could design clustering algorithms that cluster entire head-modifier arcs rather than individual words. Another idea would be to integrate the clustering algorithm into the training algorithm in a limited fashion. For example, after training an initial parser, one could parse a large amount of unlabeled text and use those parses to improve the quality of the clusters. These improved clusters can then be used to retrain an improved parser, resulting in an overall algorithm similar to that of McClosky et al. (2006). Setting aside the development of new clustering algorithms, a final area for future work is the extension of our method to new domains, such as conversational text or other languages, and new NLP problems, such as machine translation."
20,"We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.","We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods. The task of unsupervised (or semi-supervised) partof-speech (POS) tagging is the following: given a dictionary mapping words in a language to their possible POS, and large quantities of unlabeled text data, learn to predict the correct part of speech for a given word in context. The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged. Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm. However, as recently noted 'This work is supported in part by the Lynn and William Frankel Center for Computer Science. by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved. This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have been traditionally used to perform the filtering. Furthermore, these methods require rather comprehensive dictionaries in order to perform well. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on “diluted dictionaries” – in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). All the work mentioned above focuses on unsupervised English POS tagging. The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus). As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available. The problem is rather approached as a workbench for exploring new learning methods. The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised CRF training using contrastive estimation (SE), a fully-bayesian HMM model that jointly performs clustering and sequence learning (GG), and a Bayesian LDA-based model using only observed context features to predict tag words (TJ). These sophisticated learning algorithms all outperform the traditional baseline of EM-HMM based methods, while relying on similar knowledge: the lexical context of the words to be tagged and their letter structure (e.g., presence of suffixes, capitalization and hyphenation).1 Our motivation for tackling unsupervised POS tagging is different: we are interested in developing a Hebrew POS tagger. We have access to a good Hebrew lexicon (and a morphological analyzer), and a fair amount of unlabeled training data, but hardly any annotated corpora. We actually report results on full morphological disambiguation for Hebrew, a task similar but more challenging than POS tagging: we deal with a tagset much larger than English (over 3,561 distinct tags) and an ambiguity level of about 2.7 per token as opposed to 1.4 for English. Instead of inventing a new learning framework, we go back to the traditional EM trained HMMs. We argue that the key challenge to learning an effective model is to define good enough initial conditions. Given sufficiently good initial conditions, EM trained models can yield highly competitive results. Such models have other benefits as well: they are simple, robust, and computationally more attractive. In this paper, we concentrate on methods for deriving sufficiently good initial conditions for EMHMM learning. Our method for learning initial conditions for the p(tjw) distributions relies on a mixture of language specific models: a paradigmatic model of similar words (where similar words are words with similar inflection patterns), simple syntagmatic constraints (e.g., the sequence V-V is extremely rare in English). These are complemented by a linear lexical context model. Such models are simple to build and test. We present results for unsupervised PoS tagging of Hebrew text and for the common WSJ English test sets. We show that our method achieves state-ofthe-art results for the English setting, even with a relatively small dictionary. Furthermore, while recent work report results on a reduced English tagset of 17 PoS tags, we also present results for the complete 45 tags tagset of the WSJ corpus. This considerably raises the bar of the EM-HMM baseline. We also report state-of-the-art results for Hebrew full morphological disambiguation. Our primary conclusion is that the problem of learning effective stochastic classifiers remains primarily a search task. Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations. A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to capture complex linguistic intuition without fear that over-generalization could lead a learner to diverge. The tagging accuracy of supervised stochastic taggers is around 96%–97% (Manning and Schutze, 1999). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised token-based EMestimated HMM, trained on a corpus of about 1M words, over a tagset of 159 tags. Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept. Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%. This unsupervised tagger relied on an initial step in which the most probable tag for each word is chosen. Optimized lexicons and Brill’s most-probable-tag Oracle are not available in realistic unsupervised settings, yet, they show that good initial conditions greatly facilitate learning. Recent work on unsupervised POS tagging for English has significantly improved the results on this task: GG, SE and most recently TJ report the best results so far on the task of unsupervised POS tagging of the WSJ with diluted dictionaries. With dictionaries as small as 1249 lexical entries the LDA-based method with a strong ambiguity-class model reaches POS accuracy as high as 89.7% on a reduced tagset of 17 tags. While these 3 methods rely on the same feature set (lexical context, spelling features) for the learning stage, the LDA approach bases its predictions entirely on observable features, and excludes the traditional hidden states sequence. In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t|w) from unlabeled data, which we describe below. Our method uses this algorithm as a first step, and refines the approximation by introducing additional linguistic constraints and an iterative refinement step. The most common model for unsupervised learning of stochastic processes is Hidden Markov Models (HMM). For the case of tagging, the states correspond to the tags ti, and words wi are emitted each time a state is visited. The parameters of the model can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2. In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t_1, t+1) distribution. General syntagmatic constraints We set linguistically motivated constraints on the p(t|t_1, t+1) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token דלי can be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections םידליה,םידלי,דליה (the boy, boys, the boys), while the verb SW set is composed of gender and tense inflections ודלי,הדלי (she/they gave birth). The approximated probability of each analysis is based on the corpus frequency of its SW set. For the complete details, refer to the original paper. Cucerzan and Yarowsky (2000) proposed a similar method for the unsupervised estimation of p(t|w) in English, relying on simple spelling features to characterize similar word classes. Linear-Context-based p(t|w) approximation The method of Levinger et al. makes use of Hebrew inflection patterns in order to estimate context free approximation of p(t|w) by relating a word to its different inflections. However, the context in which a word occurs can also be very informative with respect to its POS-analysis (Sch¨utze, 1995). We propose a novel algorithm for estimating p(t|w) based on the contexts in which a word occurs.3 The algorithm starts with an initial p(t|w) estimate, and iteratively re-estimates: 3While we rely on the same intuition, our use of context differs from earlier works on distributional POS-tagging like (Sch¨utze, 1995), in which the purpose is to directly assign the possible POS for an unknown word. In contrast, our algorithm aims to improve the estimate for the whole distribution p(tIw), to be further disambiguated by the EM-HMM learner. where Z is a normalization factor, W is the set of all words in the corpus, C is the set of all contexts, and RELc ⊆ C is a set of reliable contexts, defined below. allow(t, w) is a binary function indicating whether t is a valid tag for w. p(c|w) and p(w|c) are estimated via raw corpus counts. Intuitively, we estimate the probability of a tag given a context as the average probability of a tag given any of the words appearing in that context, and similarly the probability of a tag given a word is the averaged probability of that tag in all the (reliable) contexts in which the word appears. At each round, we define RELc, the set of reliable contexts, to be the set of all contexts in which p(t|c) > 0 for at most X different ts. The method is general, and can be applied to different languages. The parameters to specify for each language are: the initial estimation p(t|w), the estimation of the allow relation for known and OOV words, and the types of contexts to consider. In Hebrew, several words combine into a single token in both agglutinative and fusional ways. This results in a potentially high number of tags for each token. On average, in our corpus, the number of possible analyses per known word reached 2.7, with the ambiguity level of the extended POS tagset in corpus for English (1.41) (Dermatas and Kokkinakis, 1995). In this work, we use the morphological analyzer of MILA – Knowledge Center for Processing Hebrew (KC analyzer). In contrast to English tagsets, the number of tags for Hebrew, based on all combinations of the morphological attributes, can grow theoretically to about 300,000 tags. In practice, we found ‘only’ about 3,560 tags in a corpus of 40M tokens training corpus taken from Hebrew news material and Knesset transcripts. For testing, we manually tagged the text which is used in the Hebrew Treebank (Sima’an et al., 2001) (about 90K tokens), according to our tagging guidelines. General syntagmatic constraints We define 4 syntagmatic constraints over p(t|t_1, t+1): (1) a construct state form cannot be followed by a verb, preposition, punctuation, existential, modal, or copula; (2) a verb cannot be followed by the preposition * ˇsel (of), (3) copula and existential cannot be followed by a verb, and (4) a verb cannot be followed by another verb, unless one of them has a prefix, or the second verb is an infinitive, or the first verb is imperative and the second verb is in future tense.4 Morphology-Based p(t|w) approximation We extended the set of rules used in Levinger et al. , in order to support the wider tagset used by the KC analyzer: (1) The SW set for adjectives, copulas, existentials, personal pronouns, verbs and participles, is composed of all gender-number inflections; (2) The SW set for common nouns is composed of all number inflections, with definite article variation for absolute noun; (3) Prefix variations for proper nouns; (4) Gender variation for numerals; and (5) Gendernumber variation for all suffixes (possessive, nominative and accusative). Linear-Context-based p(t|w) approximation For the initial p(t|w) we use either a uniform distribution based on the tags allowed in the dictionary, or the estimate obtained by using the modified Levinger et al. algorithm. We use contexts of the form LR=w_1, w+1 (the neighbouring words). We estimate p(w|c) and p(c|w) via relative frequency over all the events w1, w2, w3 occurring at least 10 times in the corpus. allow(t, w) follows the dictionary. Because of the wide coverage of the Hebrew lexicon, we take RELc to be C (all available contexts). We run a series of experiments with 8 distinct initial conditions, as shown in Table 1: our baseline (Uniform) is the uniform distribution over all tags provided by the KC analyzer for each word. The Syntagmatic initial conditions add the p(t|t_1, t+1) constraints described above to the uniform baseline. The Morphology-Based and Linear-Context initial conditions are computed as described above, while the Morph+Linear is the result of applying the linear-context algorithm over initial values computed by the Morphology-based method. We repeat these last 3 models with the addition of the syntagmatic constraints (Synt+Morph). For each of these, we first compare the computed p(tjw) against a gold standard distribution, taken from the test corpus (90K tokens), according to the measure used by (Levinger et al., 1995) (Dist). On this measure, we confirm that our improved morpholexical approximation improves the results reported by Levinger et al. from 74% to about 80% on a richer tagset, and on a much larger test set (90K vs. 3,400 tokens). We then report on the effectiveness of p(tjw) as a context-free tagger that assigns to each word the most likely tag, both for full morphological analysis (3,561 tags) (Full) and for the simpler task of token segmentation and POS tag selection (36 tags) (Seg+Pos). The best results on this task are 80.8% and 87.5% resp. achieved on the Morph+Linear initial conditions. Finally, we test effectiveness of the initial conditions with EM-HMM learning. We reach 88% accuracy on full morphological and 92% accuracy for POS tagging and word segmentation, for the Morph+Linear initial conditions. As expected, EM-HMM improves results (from 80% to 88%). Strikingly, EM-HMM improves the uniform initial conditions from 64% to above 85%. However, better initial conditions bring us much over this particular local maximum – with an error reduction of 20%. In all cases, the main improvement over the uniform baseline is brought by the morphology-based initial conditions. When applied on its own, the linear context brings modest improvement. But the combination of the paradigmatic morphology-based method with the linear context improves all measures. A most interesting observation is the detrimental contribution of the syntagmatic constraints we introduced. We found that 113,453 sentences of the corpus (about 5%) contradict these basic and apparently simple constraints. As an alternative to these common-sense constraints, we tried to use a small seed of randomly selected sentences (10K annotated tokens) in order to skew the initial uniform distribution of the state transitions. We initialize the p(tjt_1, t+1) distribution with smoothed ML estimates based on tag trigram and bigram counts (ignoring the tag-word annotations). This small seed initialization (InitTrans) has a great impact on accuracy. Overall, we reach 89.4% accuracy on full morphological and 92.4% accuracy for POS tagging and word segmentation, for the Morph+Linear conditions – an error reduction of more than 25% from the uniform distribution baseline. We now apply the same technique to English semisupervised POS tagging. Recent investigations of this task use dictionaries derived from the Penn WSJ corpus, with a reduced tag set of 17 tags5 instead of the original 45-tags tagset. They experiment with full dictionaries (containing complete POS information for all the words in the text) as well as “diluted” dictionaries, from which large portions of the vocabulary are missing. These settings are very different from those used for Hebrew: the tagset is much smaller (17 vs. 3,560) and the dictionaries are either complete or extremely crippled. However, for the sake of comparison, we have reproduced the same experimental settings. We derive dictionaries from the complete WSJ corpus6, and the exact same diluted dictionaries used in SE, TJ and GG. many of the stop words get wrong analyses stemming from tagging mistakes (for instance, the word the has 6 possible analyses in the data-derived dictionary, which we checked manually and found all but DT erroneous). Such noise is not expected in a real world dictionary, and our algorithm is not designed to accomodate it. We corrected the entries for the 20 most frequent words in the corpus. This step could probably be done automatically, but we consider it to be a non-issue in any realistic setting. Syntagmatic Constraints We indirectly incorporated syntagmatic constraints through a small change to the tagset. The 17-tags English tagset allows for V-V transitions. Such a construction is generally unlikely in English. By separating modals from the rest of the verbs, and creating an additional class for the 5 be verbs (am,is,are,was,were), we made such transition much less probable. The new 19-tags tagset reflects the “verb can not follow a verb” constraint. Morphology-Based p(t|w) approximation English morphology is much simpler compared to that of Hebrew, making direct use of the Levinger context free approximation impossible. However, some morphological cues exist in English as well, in particular common suffixation patterns. We implemented our morphology-based context-free p(t|w) approximation for English as a special case of the linear context-based algorithm described in Sect.3. Instead of generating contexts based on neighboring words, we generate them using the following 5 morphological templates: suff=S The word has suffix 5 (suff=ing). L+suff=W,S The word appears just after word W, with suffix 5 (L+suff=have,ed). R+suff=S,W The word appears just before word W, with suffix 5 (R+suff=ing,to) wsuf=S1,S2 The word suffix is 51, the same stem is seen with suffix 52 (wsuf=E,s). suffs=SG The word stem appears with the 5G group of suffixes (suffs=ed,ing,s). We consider a word to have a suffix only if the word stem appears with a different suffix somewhere in the text. We implemented a primitive stemmer for extracting the suffixes while preserving a usable stem by taking care of few English orthography rules (handling, e.g., , bigger → big er, nicer → nice er, happily → happy ly, picnicking → picnic ing). For the immediate context W in the templates L+suff,R+suff, we consider only the 20 most frequent tokens in the corpus. Linear-Context-based p(t|w) approximation We expect the context based approximation to be particularly useful in English. We use the following 3 context templates: LL=w_2,w_1, LR=w_1,w+1 and RR=w+1,w+2. We estimate p(w|c) and p(c|w) by relative frequency over word triplets occurring at least twice in the unannotated training corpus. Combined p(t|w) approximation This approximation combines the morphological and linear context approximations by using all the abovementioned context templates together in the iterative process. For all three p(t|w) approximations, we take RELC to be contexts containing at most 4 tags. allow(t, w) follows the dictionary for known words, and is the set of all open-class POS for unknown words. We take the initial p(t|w) for each w to be uniform over all the dictionary specified tags for w. Accordingly, the initial p(t|w) = 0 for w not in the dictionary. We run the process for 8 iterations.7 Diluted Dictionaries and Unknown Words Some of the missing dictionary elements are assigned a set of possible POS-tags and corresponding probabilities in the p(t|w) estimation process. Other unknown tokens remain with no analysis at the end of the initial process computation. For these missing elements, we assign an ambiguity class by a simple ambiguity-class guesser, and set p(t|w) to be uniform over all the tags in the ambiguity class. Our ambiguity-class guesser assigns for each word the set of all open-class tags that appeared with the word suffix in the dictionary. The word suffix is the longest (up to 3 characters) suffix of the word that also appears in the top-100 suffixes in the dictionary. Taggers We test the resulting p(t|w) approximation by training 2 taggers: CF-Tag, a context-free tagger assigning for each word its most probable POS according to p(t|w), with a fallback to the most probable tag in case the word does not appear in the dictionary or if ∀t, p(t|w) = 0. EM-HMM, a second-order EM-HMM initialized with the estimated p(t|w). Baselines As baseline, we use two EM-trained HMM taggers, initialized with a uniform p(t|w) for every word, based on the allowed tags in the dictionary. For words not in the dictionary, we take the allowed tags to be either all the open-class POS 7This is the first value we tried, and it seems to work fine. We haven’t experimented with other values. The same applies for the choice of 4 as the RELC threshold. (uniform(oc)) or the allowed tags according to our simple ambiguity-class guesser (uniform(suf)). All the p(tjw) estimates and HMM models are trained on the entire WSJ corpus. We use the same 24K word test-set as used in SE, TJ and GG, as well as the same diluted dictionaries. We report the results on the same reduced tagsets for comparison, but also include the results on the full 46 tags tagset. Table 2 summarizes the results of our experiments. Uniform initialization based on the simple suffixbased ambiguity class guesser yields big improvements over the uniform all-open-class initialization. However, our refined initial conditions always improve the results (by as much as 40% error reduction). As expected, the linear context is much more effective than the morphological one, especially with richer dictionaries. This seem to indicate that in English the linear context is better at refining the estimations when the ambiguity classes are known, while the morphological context is in charge of adding possible tags when the ambiguity classes are not known. Furthermore, the benefit of the morphology-context is bigger for the complete tagset setting, indicating that, while the coarsegrained POS-tags are indicated by word distribution, the finer distinctions are indicated by inflections and orthography. The combination of linear and morphology contexts is always beneficial. Syntagmatic constraints (e.g., separating be verbs and modals from the rest of the verbs) constantly improve results by about 1%. Note that the context-free tagger based on our p(tjw) estimates is quite accurate. As with the EM trained models, combining linear and morphological contexts is always beneficial. To put these numbers in context, Table 3 lists current state-of-the art results for the same task. CE+spl is the Contrastive-Estimation CRF method of SE. BHMM is the completely Bayesian-HMM of GG. PLSA+AC, LDA, LDA+AC are the models presented in TJ, LDA+AC is a Bayesian model with a strong ambiguity class (AC) component, and is the current state-of-the-art of this task. The other models are variations excluding the Bayesian components (PLSA+AC) or the ambiguity class. While our models are trained on the unannotated text of the entire WSJ Treebank, CE and BHMM use much less training data (only the 24k words of the test-set). However, as noted by TJ, there is no reason one should limit the amount of unlabeled data used, and in addition other results reported in GG,SE show that accuracy does not seem to improve as more unlabeled data are used with the models. We also report results for training our EM-HMM tagger on the smaller dataset (the p(tjw) estimation is still based on the entire unlabeled WSJ). All the abovementioned models follow the assumption that all 17 tags are valid for the unknown words. In contrast, we restrict the set of allowed tags for an unknown word to open-class tags. Closed class words are expected to be included in a dictionary, even a small one. The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al., 1993), and proved highly beneficial also in our case. Notice that even our simplest models, in which the initial p(tjw) distribution for each w is uniform, already outperform most of the other models, and, in the case of the diluted dictionaries, by a wide margin. Similarly, given the p(tjw) estimate, EMHMM training on the smaller dataset (24k) is still very competitive (yet results improve with more unlabeled data). When we use our refined p(tjw) distribution as the basis of EM-HMM training, we get the best results for the complete dictionary case. With the diluted dictionaries, we are outperformed only by LDA+AC. As we outperform this model in the complete dictionary case, it seems that the advantage of this model is due to its much stronger ambiguity class model, and not its Bayesian components. Also note that while we outperform this model when using the 19-tags tagset, it is slightly better in the original 17-tags setting. It could be that the reliance of the LDA models on observed surface features instead of hidden state features is beneficial avoiding the misleading V-V transitions. We also list the performance of our best models with a slightly more realistic dictionary setting: we take our dictionary to include information for all words occurring in section 0-18 of the WSJ corpus (43208 words). We then train on the entire unannotated corpus, and test on sections 22-24 – the standard train/test split for supervised English POS tagging. We achieve accuracy of 92.85% for the 19tags set, and 91.3% for the complete 46-tags tagset.","We now apply the same technique to English semisupervised POS tagging. Recent investigations of this task use dictionaries derived from the Penn WSJ corpus, with a reduced tag set of 17 tags5 instead of the original 45-tags tagset. They experiment with full dictionaries (containing complete POS information for all the words in the text) as well as “diluted” dictionaries, from which large portions of the vocabulary are missing. These settings are very different from those used for Hebrew: the tagset is much smaller (17 vs. 3,560) and the dictionaries are either complete or extremely crippled. However, for the sake of comparison, we have reproduced the same experimental settings. We derive dictionaries from the complete WSJ corpus6, and the exact same diluted dictionaries used in SE, TJ and GG. many of the stop words get wrong analyses stemming from tagging mistakes (for instance, the word the has 6 possible analyses in the data-derived dictionary, which we checked manually and found all but DT erroneous). Such noise is not expected in a real world dictionary, and our algorithm is not designed to accomodate it. We corrected the entries for the 20 most frequent words in the corpus. This step could probably be done automatically, but we consider it to be a non-issue in any realistic setting. Syntagmatic Constraints We indirectly incorporated syntagmatic constraints through a small change to the tagset. The 17-tags English tagset allows for V-V transitions. Such a construction is generally unlikely in English. By separating modals from the rest of the verbs, and creating an additional class for the 5 be verbs (am,is,are,was,were), we made such transition much less probable. The new 19-tags tagset reflects the “verb can not follow a verb” constraint. Morphology-Based p(t|w) approximation English morphology is much simpler compared to that of Hebrew, making direct use of the Levinger context free approximation impossible. However, some morphological cues exist in English as well, in particular common suffixation patterns. We implemented our morphology-based context-free p(t|w) approximation for English as a special case of the linear context-based algorithm described in Sect.3. Instead of generating contexts based on neighboring words, we generate them using the following 5 morphological templates: suff=S The word has suffix 5 (suff=ing). L+suff=W,S The word appears just after word W, with suffix 5 (L+suff=have,ed). R+suff=S,W The word appears just before word W, with suffix 5 (R+suff=ing,to) wsuf=S1,S2 The word suffix is 51, the same stem is seen with suffix 52 (wsuf=E,s). suffs=SG The word stem appears with the 5G group of suffixes (suffs=ed,ing,s). We consider a word to have a suffix only if the word stem appears with a different suffix somewhere in the text. We implemented a primitive stemmer for extracting the suffixes while preserving a usable stem by taking care of few English orthography rules (handling, e.g., , bigger → big er, nicer → nice er, happily → happy ly, picnicking → picnic ing). For the immediate context W in the templates L+suff,R+suff, we consider only the 20 most frequent tokens in the corpus. Linear-Context-based p(t|w) approximation We expect the context based approximation to be particularly useful in English. We use the following 3 context templates: LL=w_2,w_1, LR=w_1,w+1 and RR=w+1,w+2. We estimate p(w|c) and p(c|w) by relative frequency over word triplets occurring at least twice in the unannotated training corpus. Combined p(t|w) approximation This approximation combines the morphological and linear context approximations by using all the abovementioned context templates together in the iterative process. For all three p(t|w) approximations, we take RELC to be contexts containing at most 4 tags. allow(t, w) follows the dictionary for known words, and is the set of all open-class POS for unknown words. We take the initial p(t|w) for each w to be uniform over all the dictionary specified tags for w. Accordingly, the initial p(t|w) = 0 for w not in the dictionary. We run the process for 8 iterations.7 Diluted Dictionaries and Unknown Words Some of the missing dictionary elements are assigned a set of possible POS-tags and corresponding probabilities in the p(t|w) estimation process. Other unknown tokens remain with no analysis at the end of the initial process computation. For these missing elements, we assign an ambiguity class by a simple ambiguity-class guesser, and set p(t|w) to be uniform over all the tags in the ambiguity class. Our ambiguity-class guesser assigns for each word the set of all open-class tags that appeared with the word suffix in the dictionary. The word suffix is the longest (up to 3 characters) suffix of the word that also appears in the top-100 suffixes in the dictionary. Taggers We test the resulting p(t|w) approximation by training 2 taggers: CF-Tag, a context-free tagger assigning for each word its most probable POS according to p(t|w), with a fallback to the most probable tag in case the word does not appear in the dictionary or if ∀t, p(t|w) = 0. EM-HMM, a second-order EM-HMM initialized with the estimated p(t|w). Baselines As baseline, we use two EM-trained HMM taggers, initialized with a uniform p(t|w) for every word, based on the allowed tags in the dictionary. For words not in the dictionary, we take the allowed tags to be either all the open-class POS 7This is the first value we tried, and it seems to work fine. We haven’t experimented with other values. The same applies for the choice of 4 as the RELC threshold. (uniform(oc)) or the allowed tags according to our simple ambiguity-class guesser (uniform(suf)). All the p(tjw) estimates and HMM models are trained on the entire WSJ corpus. We use the same 24K word test-set as used in SE, TJ and GG, as well as the same diluted dictionaries. We report the results on the same reduced tagsets for comparison, but also include the results on the full 46 tags tagset. Table 2 summarizes the results of our experiments. Uniform initialization based on the simple suffixbased ambiguity class guesser yields big improvements over the uniform all-open-class initialization. However, our refined initial conditions always improve the results (by as much as 40% error reduction). As expected, the linear context is much more effective than the morphological one, especially with richer dictionaries. This seem to indicate that in English the linear context is better at refining the estimations when the ambiguity classes are known, while the morphological context is in charge of adding possible tags when the ambiguity classes are not known. Furthermore, the benefit of the morphology-context is bigger for the complete tagset setting, indicating that, while the coarsegrained POS-tags are indicated by word distribution, the finer distinctions are indicated by inflections and orthography. The combination of linear and morphology contexts is always beneficial. Syntagmatic constraints (e.g., separating be verbs and modals from the rest of the verbs) constantly improve results by about 1%. Note that the context-free tagger based on our p(tjw) estimates is quite accurate. As with the EM trained models, combining linear and morphological contexts is always beneficial. To put these numbers in context, Table 3 lists current state-of-the art results for the same task. CE+spl is the Contrastive-Estimation CRF method of SE. BHMM is the completely Bayesian-HMM of GG. PLSA+AC, LDA, LDA+AC are the models presented in TJ, LDA+AC is a Bayesian model with a strong ambiguity class (AC) component, and is the current state-of-the-art of this task. The other models are variations excluding the Bayesian components (PLSA+AC) or the ambiguity class. While our models are trained on the unannotated text of the entire WSJ Treebank, CE and BHMM use much less training data (only the 24k words of the test-set). However, as noted by TJ, there is no reason one should limit the amount of unlabeled data used, and in addition other results reported in GG,SE show that accuracy does not seem to improve as more unlabeled data are used with the models. We also report results for training our EM-HMM tagger on the smaller dataset (the p(tjw) estimation is still based on the entire unlabeled WSJ). All the abovementioned models follow the assumption that all 17 tags are valid for the unknown words. In contrast, we restrict the set of allowed tags for an unknown word to open-class tags. Closed class words are expected to be included in a dictionary, even a small one. The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al., 1993), and proved highly beneficial also in our case. Notice that even our simplest models, in which the initial p(tjw) distribution for each w is uniform, already outperform most of the other models, and, in the case of the diluted dictionaries, by a wide margin. Similarly, given the p(tjw) estimate, EMHMM training on the smaller dataset (24k) is still very competitive (yet results improve with more unlabeled data). When we use our refined p(tjw) distribution as the basis of EM-HMM training, we get the best results for the complete dictionary case. With the diluted dictionaries, we are outperformed only by LDA+AC. As we outperform this model in the complete dictionary case, it seems that the advantage of this model is due to its much stronger ambiguity class model, and not its Bayesian components. Also note that while we outperform this model when using the 19-tags tagset, it is slightly better in the original 17-tags setting. It could be that the reliance of the LDA models on observed surface features instead of hidden state features is beneficial avoiding the misleading V-V transitions. We also list the performance of our best models with a slightly more realistic dictionary setting: we take our dictionary to include information for all words occurring in section 0-18 of the WSJ corpus (43208 words). We then train on the entire unannotated corpus, and test on sections 22-24 – the standard train/test split for supervised English POS tagging. We achieve accuracy of 92.85% for the 19tags set, and 91.3% for the complete 46-tags tagset."
21,"In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English.","In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English. The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.' The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language analysis, and find it useful to modify it in order to accommodate useful “phrasal” chunks that are present in parallel training data but not licensed by linguistically motivated parses of the target language. Similarly, Cowan et al. (2006) focus on using syntactically rich representations of source and target parse trees, but they resort to phrase-based translation for modifiers within clauses. Finding the right way to balance linguistic analysis with unconstrained data-driven modeling is clearly a key challenge. In this paper we address this challenge from a less explored direction. Rather than starting with a system based on linguistically motivated parse trees, we begin with a model that is syntactic only in the formal sense. We then introduce soft constraints that take source-language parses into account to a limited extent. Introducing syntactic constraints in this restricted way allows us to take maximal advantage of what can be learned from parallel training data, while effectively factoring in key aspects of linguistically motivated analysis. As a result, we obtain substantial improvements in performance for both Chinese-English and Arabic-English translation. In Section 2, we briefly review the Hiero statistical MT framework (Chiang, 2005, 2007), upon which this work builds, and we discuss Chiang’s initial effort to incorporate soft source-language constituency constraints for Chinese-English translation. In Section 3, we suggest that an insufficiently fine-grained view of constituency constraints was responsible for Chiang’s lack of strong results, and introduce finer grained constraints into the model. Section 4 demonstrates the the value of these constraints via substantial improvements in ChineseEnglish translation performance, and extends the approach to Arabic-English. Section 5 discusses the results, and Section 6 considers related work. Finally we conclude in Section 7 with a summary and potential directions for future work. Hiero (Chiang, 2005; Chiang, 2007) is a hierarchical phrase-based statistical MT framework that generalizes phrase-based models by permitting phrases with gaps. Formally, Hiero’s translation model is a weighted synchronous contextfree grammar. Hiero employs a generalization of the standard non-hierarchical phrase extraction approach in order to acquire the synchronous rules of the grammar directly from word-aligned parallel text Rules have the form X → he, 1i, where e and f are phrases containing terminal symbols (words) and possibly co-indexed instances of the nonterminal symbol X.2 Associated with each rule is a set of translation model features, Oi(�f, e); for example, one intuitively natural feature of a rule is the phrase translation (log-)probability O( f, e) _ log p(e |f) , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh (Koehn et al., 2003). In addition to this phrase translation probability feature, Hiero’s feature set includes the inverse phrase translation probability log p(�f|e), lexical weights lexwt(�f|e) and lexwt(e |�f), which are estimates of translation quality based on word-level correspondences (Koehn et al., 2003), and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see (Chiang, 2007) for details. These features are combined using a log-linear model, with each synchronous rule contributing to the total log-probability of a derived hypothesis. Each Ai is a weight associated with feature Oi, and these weights are typically optimized using minimum error rate training (Och, 2003). When looking at Hiero rules, which are acquired automatically by the model from parallel text, it is easy to find many cases that seem to respect linguistically motivated boundaries. For example, seems to capture the use of jingtian/this year as a temporal modifier when building linguistic constituents such as noun phrases (the election this year) or verb phrases (voted in the primary this year). However, it is important to observe that nothing in the Hiero framework actually requires nonterminal symbols to cover linguistically sensible constituents, and in practice they frequently do not.3 Chiang (2005) conjectured that there might be value in allowing the Hiero model to favor hypotheses for which the synchronous derivation respects linguistically motivated source-language constituency boundaries, as identified using a parser. He tested this conjecture by adding a soft constraint in the form of a “constituency feature”: if a synchronous rule X —* (e, f) is used in a derivation, and the span of f is a constituent in the sourcelanguage parse, then a term a, is added to the model score in expression (1).4 Unlike a hard constraint, which would simply prevent the application of rules violating syntactic boundaries, using the feature to introduce a soft constraint allows the model to boost the “goodness” for a rule if it is constitent with the source language constituency analysis, and to leave its score unchanged otherwise. The weight a,, like all other aZ, is set via minimum error rate training, and that optimization process determines empirically the extent to which the constituency feature should be trusted. Figure 1 illustrates the way the constituency feature worked, treating English as the source language for the sake of readability. In this example, a, would be added to the hypothesis score for any rule used in the hypothesis whose source side spanned the minister, a speech, yesterday, gave a speech yesterday, or the minister gave a speech yesterday. A rule translating, say, minister gave a as a unit would receive no such boost. Chiang tested the constituency feature for Chinese-English translation, and obtained no significant improvement on the test set. The idea then seems essentially to have been abandoned; it does not appear in later discussions (Chiang, 2007). On the face of it, there are any number of possible reasons Chiang’s (2005) soft constraint did not work – including, for example, practical issues like the quality of the Chinese parses.5 However, we focus here on two conceptual issues underlying his use of source language syntactic constituents. First, the constituency feature treats all syntactic constituent types equally, making no distinction among them. For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003). Moreover, a parser may tend to be more accurate for some constituents than for others. Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent. Logically, however, it might make sense not just to give a rule X —* (e, f) extra credit when f matches a constituent, but to incur a cost when f violates a constituent boundary. Using the example in Figure 1, we might want to penalize hypotheses containing rules where f is the minister gave a (and other cases, such as minister gave, minister gave a, and so forth).6 These observations suggest a finer-grained approach to the constituency feature idea, retaining the idea of soft constraints, but applying them using various soft-constraint constituency features. Our first observation argues for distinguishing among constituent types (NP, VP, etc.). Our second observation argues for distinguishing the benefit of match6This accomplishes coverage of the logically complete set of possibilities, which include not only f matching a constituent exactly or crossing its boundaries, but also f being properly contained within the constituent span, properly containing it, or being outside it entirely. Whenever these latter possibilities occur, f will exactly match or cross the boundaries of some other constituent. ing constituents from the cost of crossing constituent boundaries. We therefore define a space of new features as the cross product {CP, IP, NP, VP, ...} x {_, +}. where = and + signify matching and crossing boundaries, respectively. For example, ONP= would denote a binary feature that matches whenever the span of f exactly covers an NP in the source-side parse tree, resulting in ANP= being added to the hypothesis score (expression (1)). Similarly, oVP+ would denote a binary feature that matches whenever the span of f crosses a VP boundary in the parse tree, resulting in AVP+ being subtracted from the hypothesis score.7 For readability from this point forward, we will omit 0 from the notation and refer to features such as NP= (which one could read as “NP match”), VP+ (which one could read as “VP crossing”), etc. In addition to these individual features, we define three more variants: • For each constituent type, e.g. NP, we define a feature NP_ that ties the weights of NP= and NP+. If NP= matches a rule, the model score is incremented by ANP_, and if NP+ matches, the model score is decremented by the same quantity. • For each constituent type, e.g. NP, we define a version of the model, NP2, in which NP= and NP+ are both included as features, with separate weights ANP= and ANP+. • We define a set of “standard” linguistic labels containing {CP, IP, NP, VP, PP, ADJP, ADVP, QP, LCP, DNP} and excluding other labels such as PRN (parentheses), FRAG (fragment), etc.8 We define feature XP= as the disjunction of {CP=, IP=, ..., DNP=}; i.e. its value equals 1 for a rule if the span of f exactly covers a constituent having any of the standard labels. The 7Formally, AVP+ simply contributes to the sum in expression (1), as with all features in the model, but weight optimization using minimum error rate training should, and does, automatically assign this feature a negative weight. 8We map SBAR and S labels in Arabic parses to CP and IP, respectively, consistent with the Chinese parses. We map Chinese DP labels to NP. DNP and LCP appear only in Chinese. We ran no ADJP experiment in Chinese, because this label virtually aways spans only one token in the Chinese parses. definitions of XP+, XP_, and XP2 are analogous. feature can be viewed as a disjunctive “alllabels=” feature, we also defined “all-labels+”, “all-labels2”, and “all-labels_” analogously. We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 For the Chinese-English translation experiments, we trained the translation model on the corpora in Table 1, totalling approximately 2.1 million sentence pairs after GIZA++ filtering for length ratio. Chinese text was segmented using the Stanford segmenter (Tseng et al., 2005). We trained a 5-gram language model using the English (target) side of the training set, pruning 4gram and 5-gram singletons. For minimum error rate training and development we used the NIST MTeval MT03 set. Table 2 presents our results. We first evaluated translation performance using the NIST MT06 (nisttext) set. Like Chiang (2005), we find that the original, undifferentiated constituency feature (Chiang05) introduces a negligible, statistically insignificant improvement over the baseline. However, we find that several of the finer-grained constraints (IP=, VP=, VP+, QP+, and NP=) achieve statistically significant improvements over baseline (up to .74 BLEU), and the latter three also improve significantly on the undifferentiated constituency feature. By combining multiple finer-grained syntactic features, we obtain significant improvements of up to 1.65 BLEU points (NP_, VP2, IP2, all-labels_, and XP+). We also obtained further gains using combinations of features that had performed well; e.g., condition IP2.VP2.NP_ augments the baseline features with IP2 and VP2 (i.e. IP=, IP+, VP= and VP+), and NP_ (tying weights of NP= and NP+; see Section 3). Since component features in those combinations were informed by individual-feature performance on the test set, we tested the best performing conditions from MT06 on a new test set, NIST MT08. NP= and VP+ yielded significant improvements of up to 1.53 BLEU. Combination conditions replicated the pattern of results from MT06, including the same increasing order of gains, with improvements up to 1.11 BLEU. For Arabic-English translation, we used the training corpora in Table 3, approximately 100,000 sentence pairs after GIZA++ length-ratio filtering. We trained a trigram language model using the English side of this training set, plus the English Gigaword v2 AFP and Gigaword v1 Xinhua corpora. Development and minimum error rate training were done using the NIST MT02 set. Table 4 presents our results. We first tested on on the NIST MT03 and MT06 (nist-text) sets. On MT03, the original, undifferentiated constituency feature did not improve over baseline. Two individual finer-grained features (PP+ and AdvP=) yielded statistically significant gains up to .42 BLEU points, and feature combinations AP2, XP2 and all-labels2 yielded significant gains up to 1.03 BLEU points. XP2 and all-labels2 also improved significantly on the undifferentiated constituency feature, by .72 and 1.11 BLEU points, respectively. For MT06, Chiang’s original feature improved the baseline significantly — this is a new result using his feature, since he did not experiment with Arabic — as did our our IP=, PP=, and VP= conditions. Adding individual features PP+ and AdvP= yielded significant improvements up to 1.4 BLEU points over baseline, and in fact the improvement for individual feature AdvP= over Chiang’s undifferentiated constituency feature approaches significance (p < .075). More important, several conditions combining features achieved statistically significant improvements over baseline of up 1.94 BLEU points: XP2, IP2, IP, VP=.PP+.AdvP=, AP2, PP+.AdvP=, and AdvP2. Of these, AdvP2 is also a significant improvement over the undifferentiated constituency feature (Chiang-05), with p < .01. As we did for Chinese, we tested the best-performing models on a new test set, NIST MT08. Consistent patterns reappeared: improvements over the baseline up to 1.69 BLEU (p < .01), with AdvP2 again in the lead (also outperforming the undifferentiated constituency feature, p < .05). (p < .05). **: Better than baseline (p < .01). +: Better than Chiang-05 (p < .05). ++: Better than Chiang-05 (p < .01). -: Almost significantly better than Chiang-05 (p < .075) The results in Section 4 demonstrate, to our knowledge for the first time, that significant and sometimes substantial gains over baseline can be obtained by incorporating soft syntactic constraints into Hiero’s translation model. Within language, we also see considerable consistency across multiple test sets, in terms of which constraints tend to help most. Furthermore, our results provide some insight into why the original approach may have failed to yield a positive outcome. For Chinese, we found that when we defined finer-grained versions of the exact-match features, there was value for some constituency types in biasing the model to favor matching the source language parse. Moreover, we found that there was significant value in allowing the model to be sensitive to violations (crossing boundaries) of source parses. These results confirm that parser quality was not the limitation in the original work (or at least not the only limitation), since in our experiments the parser was held constant. Looking at combinations of new features, some “double-feature” combinations (VP2, IP2) achieved large gains, although note that more is not necessarily better: combinations of more features did not yield better scores, and some did not yield any gain at all. No conflated feature reached significance, but it is not the case that all conflated features are worse than their same-constituent “double-feature” counterparts. We found no simple correlation between finer-grained feature scores (and/or boundary condition type) and combination or conflation scores. Since some combinations seem to cancel individual contributions, we can conclude that the higher the number of participant features (of the kinds described here), the more likely a cancellation effect is; therefore, a “double-feature” combination is more likely to yield higher gains than a combination containing more features. We also investigated whether non-canonical linguistic constituency labels such as PRN, FRAG, UCP and VSB introduce “noise”, by means of the XP features — the XP= feature is, in fact, simply the undifferentiated constituency feature, but sensitive only to “standard” XPs. Although performance of XP=, XP2 and all-labels+ were similar to that of the undifferentiated constituency feature, XP+ achieved the highest gain. Intuitively, this seems plausible: the feature says, at least for Chinese, that a translation hypothesis should incur a penalty if it is translating a substring as a unit when that substring is not a canonical source constituent. Having obtained positive results with Chinese, we explored the extent to which the approach might improve translation using a very different source language. The approach on Arabic-English translation yielded large BLEU gains over baseline, as well as significant improvements over the undifferentiated constituency feature. Comparing the two sets of experiments, we see that there are definitely language-specific variations in the value of syntactic constraints; for example, AdvP, the top performer in Arabic, cannot possibly perform well for Chinese, since in our parses the AdvP constituents rarely include more than a single word. At the same time, some IP and VP variants seem to do generally well in both languages. This makes sense, since — at least for these language pairs and perhaps more generally — clauses and verb phrases seem to correspond often on the source and target side. We found it more surprising that no NP variant yielded much gain in Arabic; this question will be taken up in future work. Space limitations preclude a thorough review of work attempting to navigate the tradeoff between using language analyzers and exploiting unconstrained data-driven modeling, although the recent literature is full of variety and promising approaches. We limit ourselves here to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal coverage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide constraints on the extraction of hierarchical phrase pairs. Unlike Hiero, their translation model uses a full range of named nonterminal symbols in the synchronous grammar. As an alternative way to relax strict parser-based constituency requirements, they explore the use of phrases spanning generalized, categorial-style constituents in the parse tree, e.g. type NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgments for in-coverage cases. When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new and successful way to incorporate syntax into statistical MT, allowing the model to exploit non-local dependencies and lexically sensitive reordering without requiring linguistically motivated parsing of either the source or target language. An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results. In this paper, we returned to the idea of linguistically motivated soft constraints, and we demonstrated that they can, in fact, lead to substantial improvements in translation performance when integrated into the Hiero framework. We accomplished this using constraints that not only distinguish among constituent types, but which also distinguish between the benefit of matching the source parse bracketing, versus the cost of using phrases that cross relevant bracketing boundaries. We demonstrated improvements for ChineseEnglish translation, and succeed in obtaining substantial gains for Arabic-English translation, as well. Our results contribute to a growing body of work on combining monolingually based, linguistically motivated syntactic analysis with translation models that are closely tied to observable parallel training data. Consistent with other researchers, we find that “syntactic constituency” may be too coarse a notion by itself; rather, there is value in taking a finergrained approach, and in allowing the model to decide how far to trust each element of the syntactic analysis as part of the system’s optimization process.","When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new and successful way to incorporate syntax into statistical MT, allowing the model to exploit non-local dependencies and lexically sensitive reordering without requiring linguistically motivated parsing of either the source or target language. An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results. In this paper, we returned to the idea of linguistically motivated soft constraints, and we demonstrated that they can, in fact, lead to substantial improvements in translation performance when integrated into the Hiero framework. We accomplished this using constraints that not only distinguish among constituent types, but which also distinguish between the benefit of matching the source parse bracketing, versus the cost of using phrases that cross relevant bracketing boundaries. We demonstrated improvements for ChineseEnglish translation, and succeed in obtaining substantial gains for Arabic-English translation, as well. Our results contribute to a growing body of work on combining monolingually based, linguistically motivated syntactic analysis with translation models that are closely tied to observable parallel training data. Consistent with other researchers, we find that “syntactic constituency” may be too coarse a notion by itself; rather, there is value in taking a finergrained approach, and in allowing the model to decide how far to trust each element of the syntactic analysis as part of the system’s optimization process."
22,"This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections.","This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections. Today, we can easily find a large amount of unlabeled data for many supervised learning applications in Natural Language Processing (NLP). Therefore, to improve performance, the development of an effective framework for semi-supervised learning (SSL) that uses both labeled and unlabeled data is attractive for both the machine learning and NLP communities. We expect that such SSL will replace most supervised learning in real world applications. In this paper, we focus on traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER). These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS tagging, CoNLL’00 shared task data for syntactic chunking, and CoNLL’03 shared task data for NER. We used up to 1G-words (one billion tokens) of unlabeled data to explore the performance improvement with respect to the unlabeled data size. In addition, we investigate the performance improvement for ‘unseen data’ from the viewpoint of unlabeled data coverage. Finally, we compare our results with those provided by the best current systems. The contributions of this paper are threefold. First, we present a simple, scalable, but powerful task-independent model for semi-supervised sequential labeling and segmentation. Second, we report the best current results for the widely used test collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements. We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) (Lafferty et al., 2001). As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007). Let x ∈ X and y ∈ Y be an input and output, where X and Y represent the set of possible inputs and outputs, respectively. C stands for the set of cliques in an undirected graphical model G(x, y), which indicates the interdependency of a given x and y. yc denotes the output from the corresponding clique c. Each clique c∈C has a potential function IFc. Then, the CRFs define the conditional probability p(y|x) as a product of IFcs. In addition, let f = (f1, ..., fI) be a feature vector, and A = (A1, ..., AI) be a parameter vector, whose lengths are I. p(y|x; A) on a CRF is defined as follows: where Z(x) = Py∈Y Qc∈C 'Fc(yc, x; A) is the partition function. We generally assume that the potential function is a non-negative real value function. Therefore, the exponentiated weighted sum over the features of a clique is widely used, so that, Suppose we have J kinds of probability models (PMs). The j-th joint PM is represented by pj(xj, y; 0j) where 0j is a model parameter. xj = Tj(x) is simply an input x transformed by a predefined function Tj. We assume xj has the same graph structure as x. This means pj(xj, y) can be factorized by the cliques c in G(x, y). That is, pj(xj, y; 0j)=Qc pj(xjc, yc; 0j). Thus, we can incorporate generative models such as Bayesian networks including (1D and 2D) hidden Markov models (HMMs) as these joint PMs. Actually, there is a difference in that generative models are directed graphical models while our conditional PM is an undirected. However, this difference causes no violations when we construct our approach. Let us introduce A0=(A1, ..., AI, AI+1, . . ., AI+J), and h = (f1, ..., fI, log p1, ..., log pJ), which is the concatenation of feature vector f and the loglikelihood of J-joint PMs. Then, we can define a new potential function by embedding the joint PMs; where Θ = {0j}Jj=1, and hc(yc, x) is h obtained from the corresponding clique c in G(x, y). Since each pj(xjc, yc) has range [0, 1], which is nonnegative, IF0c can also be used as a potential function. Thus, the conditional model for our SSL can be written as: where Z0(x) = Py∈YQc∈C V (yc, x; A0, Θ). Hereafter in this paper, we refer to this conditional model as a ‘Joint probability model Embedding style SemiSupervised Conditional Model’, or JESS-CM for short. Given labeled data, Dl={(xn, yn)}Nn=1, the MAP estimation of A0 under a fixed Θ can be written as: where p(A0) is a prior probability distribution of A0. Clearly, JESS-CM shown in Equation 2 has exactly the same form as Equation 1. With a fixed Θ, the log-likelihood, log pj, can be seen simply as the feature functions of JESS-CM as with fi. Therefore, embedded joint PMs do not violate the global convergence conditions. As a result, as with supervised CRFs, it is guaranteed that A0 has a value that achieves the global maximum of L1(A0|Θ). Moreover, we can obtain the same form of gradient as that of supervised CRFs (Sha and Pereira, 2003), that is, Thus, we can easily optimize L1 by using the forward-backward algorithm since this paper solely focuses on a sequence model and a gradient-based optimization algorithm in the same manner as those used in supervised CRF parameter estimation. We cannot naturally incorporate unlabeled data into standard discriminative learning methods since the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is to use maximum marginal likelihood (MML) parameter estimation, i.e., (Nigam et al., 2000). Given unlabeled data Du = {xm}Mm=1, MML estimation in our setting maximizes the marginal distribution of a joint PM over a missing (hidden) variable y, namely, it maximizes Em log Ey∈Y p(xm, y; θ). Following this idea, there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data (Suzuki et al., 2007). Here, we refer to it as ‘Maximum Discriminant Functions sum’ (MDF) parameter estimation. MDF estimation substitutes p(x, y) with discriminant functions g(x, y). Therefore, to estimate the parameter Θ of JESS-CM by using MDF estimation, the following objective function is maximized with a fixed λ0: where p(Θ) is a prior probability distribution of Θ. Since the normalization factor does not affect the determination of y, the discriminant function of JESS-CM shown in Equation 2 is defined as g(x, y; λ0, Θ) = Hc∈C Ψ0 c(yc, x; λ0, Θ). With a fixed λ0, the local maximum of L2(Θ|λ0) around the initialized value of Θ can be estimated by an iterative computation such as the EM algorithm (Dempster et al., 1977). A parameter estimation algorithm of λ0 and Θ can be obtained by maximizing the objective functions L1(λ0|Θ) and L2(Θ|λ0) iteratively and alternately. Figure 1 summarizes an algorithm for estimating λ0 and Θ for JESS-CM. This paper considers a situation where there are many more unlabeled data M than labeled data N, that is, N << M. This means that the calculation cost for unlabeled data is dominant. Thus, in order to make the overall parameter estimation procedure Input: training data D = {Dl, Du} where labeled data Dl = {(xn, yn)}Nn=1, scalable for handling large scale unlabeled data, we only perform one step of MDF estimation for each t as explained on 3. in Figure 1. In addition, the calculation cost for estimating parameters of embedded joint PMs (HMMs) is independent of the number of HMMs, J, that we used (Suzuki et al., 2007). As a result, the cost for calculating the JESS-CM parameters, λ0 and Θ, is essentially the same as executing T iterations of the MML estimation for a single HMM using the EM algorithm plus T + 1 time optimizations of the MAP estimation for a conventional supervised CRF if it converged when t = T. In addition, our parameter estimation algorithm can be easily performed in parallel computation. SSL based on a hybrid generative/discriminative approach proposed in (Suzuki et al., 2007) has been defined as a log-linear model that discriminatively combines several discriminative models, pDi , and generative models, pGj , such that: where Λ={λi}Ii=1, and Γ={{γi}Ii=1, {γj}I+J j=I+1}. With the hybrid model, if we use the same labeled training data to estimate both Λ and Γ, γjs will become negligible (zero or nearly zero) since pDi is already fitted to the labeled training data while pGj are trained by using unlabeled data. As a solution, a given amount of labeled training data is divided into two distinct sets, i.e., 4/5 for estimating Λ, and the remaining 1/5 for estimating F (Suzuki et al., 2007). Moreover, it is necessary to split features into several sets, and then train several corresponding discriminative models separately and preliminarily. In contrast, JESS-CM is free from this kind of additional process, and the entire parameter estimation procedure can be performed in a single pass. Surprisingly, although JESS-CM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure, JESS-CM provides F-scores of 94.45 and 88.03 for CoNLL’00 and ’03 data, respectively, which are 0.15 and 0.83 points higher than those reported in (Suzuki et al., 2007) for the same configurations. This performance improvement is basically derived from the full benefit of using labeled training data for estimating the parameter of the conditional model while the combination weights, F, of the hybrid model are estimated solely by using 1/5 of the labeled training data. These facts indicate that JESS-CM has several advantageous characteristics compared with the hybrid model. In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data. To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIPSTER corpus, we extracted all the Wall Street Journal articles published between 1990 and 1992. With the English Gigaword corpus, we extracted articles from five news sources published between 1994 and 1996. The unlabeled data used in this paper is detailed in Table 2. Note that the total size of the unlabeled data reaches 1G-words (one billion tokens). We used the same graph structure as the linear chain CRF for JESS-CM. As regards the design of the feature functions fi, Table 3 shows the feature templates used in our experiments. In the table, s indicates a focused token position. Xs_1.s represents the bi-gram of feature X obtained from s − 1 and s positions. {Xu}Bu�A indicates that u ranges from A to B. For example, {Xu}s+2 u�s_2 is equal to five feature templates, {Xs_2i Xs_1i Xsi Xs+1i Xs+2}. ‘word type’ or wtp represents features of a word such as capitalization, the existence of digits, and punctuation as shown in (Sutton et al., 2006) without regular expressions. Although it is common to use external resources such as gazetteers for NER, we used none. All our features can be automatically extracted from the given training data. We used first order HMMs for embedded joint PMs since we assume that they have the same graph structure as JESS-CM as described in Section 2.2. To reduce the required human effort, we simply used the feature templates shown in Table 3 to generate the features of the HMMs. With our design, one feature template corresponded to one HMM. This design preserves the feature whereby each HMM emits a single symbol from a single state (or transition). We can easily ignore overlapping features that appear in a single HMM. As a result, 47, 39 and 79 distinct HMMs are embedded in the potential functions of JESS-CM for POS tagging, chunking and NER experiments, respectively. In our experiments, we selected Gaussian and Dirichlet priors as the prior distributions in G1 and G2, respectively. This means that JESS-CM has two tunable parameters, Q2 and q, in the Gaussian and Dirichlet priors, respectively. The values of these tunable parameters are chosen by employing a binary line search. We used the value for the best performance with the development set2. However, it may be computationally unrealistic to retrain the entire procedure several times using 1G-words of unlabeled data. Therefore, these tunable parameter values are selected using a relatively small amount of unlabeled data (17M-words), and we used the selected values in all our experiments. The left graph in Figure 2 shows typical q behavior. The left end is equivalent to optimizing G2 without a prior, and the right end is almost equivalent to considering pj(xj, y) for all j to be a uniform distribution. This is why it appears to be bounded by the performance obtained from supervised CRF. We omitted the influence of Q2 because of space constraints, but its behavior is nearly the same as that of supervised CRF. Unfortunately, G2(O|A0) may have two or more local maxima. Our parameter estimation procedure does not guarantee to provide either the global optimum or a convergence solution in O and A0 space. An example of non-convergence is the oscillation of the estimated O. That is, O traverses two or more local maxima. Therefore, we examined its convergence property experimentally. The right graph in Figure 2 shows a typical convergence property. Fortunately, in all our experiments, JESS-CM converged in a small number of iterations. No oscillation is observed here. Table 4 shows the performance of JESS-CM using 1G-words of unlabeled data and the performance gain compared with supervised CRF, which is trained under the same conditions as JESS-CM except that joint PMs are not incorporated. We emphasize that our model achieved these large improvements solely using unlabeled data as additional resources, without introducing a sophisticated model, deep feature engineering, handling external handcrafted resources, or task dependent human knowledge (except for the feature design). Our method can greatly reduce the human effort needed to obtain a high performance tagger or chunker. Figure 3 shows the learning curves of JESS-CM with respect to the size of the unlabeled data, where the x-axis is on the logarithmic scale of the unlabeled data size (Mega-word). The scale at the top of the graph shows the ratio of the unlabeled data size to the labeled data size. We observe that a small amount of unlabeled data hardly improved the performance since the supervised CRF results are competitive. It seems that we require at least dozens of times more unlabeled data than labeled training data to provide a significant performance improvement. The most important and interesting behavior is that the performance improvements against the unlabeled data size are almost linear on a logarithmic scale within the size of the unlabeled data used in our experiments. Moreover, there is a possibility that the performance is still unsaturated at the 1G-word unlabeled data point. This suggests that increasing the unlabeled data in JESS-CM may further improve the performance. Suppose J=1, the discriminant function of JESSCM is g(x, y) = A(x, y)p1(x1, y; 01)λI+1 where A(x, y) = exp(A · & fc(yc, x)). Note that both A(x, y) and AI+j are given and fixed during the MDF estimation of joint PM parameters O. Therefore, the MDF estimation in JESS-CM can be regarded as a variant of the MML estimation (see Section 2.2), namely, it is MML estimation with a bias, A(x, y), and smooth factors, AI+j. MML estimation can be seen as modeling p(x) since it is equivalent to maximizing Em log p(xm) with marginalized hidden variables y, where EYEY p(x, y) = p(x). Generally, more data will lead to a more accurate model of p(x). With our method, as with modeling p(x) in MML estimation, more unlabeled data is preferable since it may provide more accurate modeling. This also means that it provides better ‘clusters’ over the output space since Y is used as hidden states in HMMs. These are intuitive explanations as to why more unlabeled data in JESS-CM produces better performance. We try to investigate the impact of unlabeled data on the performance of unseen data. We divide the test set (or the development set) into two disjoint sets: L.app and L.neg app. L.app is a set of sentences constructed by words that all appeared in the Labeled training data. L.-,app is a set of sentences that have at least one word that does not appear in the Labeled training data. Table 5 shows the performance with these two sets obtained from both supervised CRF and JESSCM with 1G-word unlabeled data. As the supervised CRF results, the performance of the L.-,app sets is consistently much lower than that of the corresponding L.app sets. Moreover, we can observe that the ratios of L.¬app are not so small; nearly half (46.1% and 40.4%) in the PTB III data, and more than half (70.7%, 54.3% and 64.3%) in CoNLL’00 and ’03 data, respectively. This indicates that words not appearing in the labeled training data are really harmful for supervised learning. Although the performance with L.¬app sets is still poorer than with L.app sets, the JESS-CM results indicate that the introduction of unlabeled data effectively improves the performance of L.¬app sets, even more than that of L.app sets. These improvements are essentially very important; when a tagger and chunker are actually used, input data can be obtained from anywhere and this may mostly include words that do not appear in the given labeled training data since the labeled training data is limited and difficult to increase. This means that the improved performance of L.¬app can link directly to actual use. Table 5 also shows the ratios of sentences that are constructed from words that all appeared in the 1G-word Unlabeled data used in our experiments (U.app) in the L.¬app and L.app. This indicates that most of the words in the development or test sets are covered by the 1G-word unlabeled data. This may be the main reason for JESS-CM providing large performance gains for both the overall and L.¬app set performance of all three tasks. Table 6 shows the relation between JESS-CM performance and U.app in the NER experiments. The development data and test data were obtained from 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles, respectively. We find that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance. The evidence provided by the above experiments implies that increasing the coverage of unlabeled data offers the strong possibility of increasing the expected performance of unseen data. Thus, it strongly encourages us to use an SSL approach that includes JESS-CM to construct a general tagger and chunker for actual use. In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized in Table 7. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. As shown in Tables 8 and 9, the previous best performance for syntactic chunking and NER was reported by (Ando and Zhang, 2005), and is referred to as ‘ASO-semi’. ASO-semi also incorporates unlabeled data solely as additional information in the same way as JESS-CM. ASO-semi uses unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. As regards syntactic chunking, JESS-CM significantly outperformed ASO-semi for the same 15M-word unlabeled data size obtained from the Wall Street Journal in 1991 as described in (Ando and Zhang, 2005). Unfortunately with NER, JESS-CM is slightly inferior to ASO-semi for the same 27M-word unlabeled data size extracted from the Reuters corpus. In fact, JESS-CM using 37M-words of unlabeled data provided a comparable result. We observed that ASOsemi prefers ‘nugget extraction’ tasks to ’field segmentation’ tasks (Grenager et al., 2005). We cannot provide details here owing to the space limitation. Intuitively, their word prediction auxiliary problems can capture only a limited number of characteristic behaviors because the auxiliary problems are constructed by a limited number of ‘binary’ classifiers. Moreover, we should remember that ASOsemi used the human knowledge that ‘named entities mostly consist of nouns or adjectives’ during the auxiliary problem construction in their NER experiments. In contrast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary problems as well as calculating SVD are too costly when the size of the unlabeled data increases. These facts imply that our SSL framework is rather appropriate for handling large scale unlabeled data. On the other hand, ASO-semi and JESS-CM have an important common feature. That is, both methods discriminatively combine models trained by using unlabeled data in order to create informative feature representation for discriminative learning. Unlike self/co-training approaches (Blum and Mitchell, 1998), which use estimated labels as ‘correct labels’, this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training. Ando and Zhang (2007) have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications. There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005). There is an essential difference between this method and JESSCM. We categorize their approach as an ‘indirect approach’ since the outputs of the target task, y, are not considered during the unlabeled data incorporation. Note that ASO-semi is also an ‘indirect approach’. On the other hand, our approach is a ‘direct approach’ because the distribution of y obtained from JESS-CM is used as ‘seeds’ of hidden states during MDF estimation for join PM parameters (see Section 4.1). In addition, MDF estimation over unlabeled data can effectively incorporate the ‘labeled’ training data information via a ‘bias’ since A included in A(x, y) is estimated from labeled training data.","In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized in Table 7. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. As shown in Tables 8 and 9, the previous best performance for syntactic chunking and NER was reported by (Ando and Zhang, 2005), and is referred to as ‘ASO-semi’. ASO-semi also incorporates unlabeled data solely as additional information in the same way as JESS-CM. ASO-semi uses unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. As regards syntactic chunking, JESS-CM significantly outperformed ASO-semi for the same 15M-word unlabeled data size obtained from the Wall Street Journal in 1991 as described in (Ando and Zhang, 2005). Unfortunately with NER, JESS-CM is slightly inferior to ASO-semi for the same 27M-word unlabeled data size extracted from the Reuters corpus. In fact, JESS-CM using 37M-words of unlabeled data provided a comparable result. We observed that ASOsemi prefers ‘nugget extraction’ tasks to ’field segmentation’ tasks (Grenager et al., 2005). We cannot provide details here owing to the space limitation. Intuitively, their word prediction auxiliary problems can capture only a limited number of characteristic behaviors because the auxiliary problems are constructed by a limited number of ‘binary’ classifiers. Moreover, we should remember that ASOsemi used the human knowledge that ‘named entities mostly consist of nouns or adjectives’ during the auxiliary problem construction in their NER experiments. In contrast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary problems as well as calculating SVD are too costly when the size of the unlabeled data increases. These facts imply that our SSL framework is rather appropriate for handling large scale unlabeled data. On the other hand, ASO-semi and JESS-CM have an important common feature. That is, both methods discriminatively combine models trained by using unlabeled data in order to create informative feature representation for discriminative learning. Unlike self/co-training approaches (Blum and Mitchell, 1998), which use estimated labels as ‘correct labels’, this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training. Ando and Zhang (2007) have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications. There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005). There is an essential difference between this method and JESSCM. We categorize their approach as an ‘indirect approach’ since the outputs of the target task, y, are not considered during the unlabeled data incorporation. Note that ASO-semi is also an ‘indirect approach’. On the other hand, our approach is a ‘direct approach’ because the distribution of y obtained from JESS-CM is used as ‘seeds’ of hidden states during MDF estimation for join PM parameters (see Section 4.1). In addition, MDF estimation over unlabeled data can effectively incorporate the ‘labeled’ training data information via a ‘bias’ since A included in A(x, y) is estimated from labeled training data."
23,"The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.","The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. Sentiment classification is the task of identifying the sentiment polarity of a given text. The sentiment polarity is usually positive or negative and the text genre is usually product review. In recent years, sentiment classification has drawn much attention in the NLP field and it has many useful applications, such as opinion mining and summarization (Liu et al., 2005; Ku et al., 2006; Titov and McDonald, 2008). To date, a variety of corpus-based methods have been developed for sentiment classification. The methods usually rely heavily on an annotated corpus for training the sentiment classifier. The sentiment corpora are considered as the most valuable resources for the sentiment classification task. However, such resources in different languages are very imbalanced. Because most previous work focuses on English sentiment classification, many annotated corpora for English sentiment classification are freely available on the Web. However, the annotated corpora for Chinese sentiment classification are scarce and it is not a trivial task to manually label reliable Chinese sentiment corpora. The challenge before us is how to leverage rich English corpora for Chinese sentiment classification. In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources. Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages. Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g. SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language. In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews. Unlabeled Chinese reviews can be fully leveraged in the proposed approach. First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews. Then, we can view the classification problem in two independent views: Chinese view with only Chinese features and English view with only English features. We then use the co-training approach to making full use of the two redundant views of features. The SVM classifier is adopted as the basic classifier in the proposed approach. Experimental results show that the proposed approach can outperform the baseline inductive classifiers and the more advanced transductive classifiers. The rest of this paper is organized as follows: Section 2 introduces related work. The proposed co-training approach is described in detail in Section 3. Section 4 shows the experimental results. Lastly we conclude this paper in Section 5. Sentiment classification can be performed on words, sentences or documents. In this paper we focus on document sentiment classification. The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based. Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification. To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages. Standard Naïve Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods. In this study, we focus on improving the corpus-based method for crosslingual sentiment classification of Chinese product reviews by developing novel approaches. Cross-domain text classification can be considered as a more general task than cross-lingual sentiment classification. In the problem of crossdomain text classification, the labeled and unlabeled data come from different domains, and their underlying distributions are often different from each other, which violates the basic assumption of traditional classification learning. To date, many semi-supervised learning algorithms have been developed for addressing the cross-domain text classification problem by transferring knowledge across domains, including Transductive SVM (Joachims, 1999), EM(Nigam et al., 2000), EM-based Naïve Bayes classifier (Dai et al., 2007a), Topic-bridged PLSA (Xue et al., 2008), Co-Clustering based classification (Dai et al., 2007b), two-stage approach (Jiang and Zhai, 2007). DauméIII and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model. In particular, several previous studies focus on the problem of cross-lingual text classification, which can be considered as a special case of general cross-domain text classification. Bel et al. (2003) present practical and cost-effective solutions. A few novel models have been proposed to address the problem, e.g. the EM-based algorithm (Rigutini et al., 2005), the information bottleneck approach (Ling et al., 2008), the multilingual domain models (Gliozzo and Strapparava, 2005), etc. To the best of our knowledge, cotraining has not yet been investigated for crossdomain or cross-lingual text classification. The purpose of our approach is to make use of the annotated English corpus for sentiment polarity identification of Chinese reviews in a supervised framework, without using any Chinese resources. Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chinese reviews into English reviews. Lastly, we use the classifier to classify the translated English reviews. 2) We first translate the labeled English reviews into Chinese reviews, and then learn a classifier based on the translated Chinese reviews with labels. Lastly, we use the classifier to classify the unlabeled Chinese reviews. The above two methods have been used in (Banea et al., 2008) for Romanian subjectivity analysis, but the experimental results are not very promising. As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different. In order to address the above problem, we propose to use the co-training approach to make use of some amounts of unlabeled Chinese reviews to improve the classification accuracy. The co-training approach can make full use of both the English features and the Chinese features in a unified framework. The framework of the proposed approach is illustrated in Figure 1. The framework consists of a training phase and a classification phase. In the training phase, the input is the labeled English reviews and some amounts of unlabeled Chinese reviews1. The labeled English reviews are translated into labeled Chinese reviews, and the unlabeled Chinese reviews are translated into unlabeled English reviews, by using machine translation services. Therefore, each review is associated with an English version and a Chinese version. The English features and the Chinese features for each review are considered two independent and redundant views of the review. The co-training algorithm is then applied to learn two classifiers and finally the two classifiers are combined into a single sentiment classifier. In the classification phase, each unlabeled Chinese review for testing is first translated into English review, and then the learned classifier is applied to classify the review into either positive or negative. The steps of review translation and the cotraining algorithm are described in details in the next sections, respectively. In order to overcome the language gap, we must translate one language into another language. Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory. A few commercial machine translation services can be publicly accessed, e.g. Google Translate2, Yahoo Babel Fish3 and Windows Live Translate4. In this study, we adopt Google Translate for both English-to-Chinese Translation and Chinese-toEnglish Translation, because it is one of the state-of-the-art commercial machine translation systems used today. Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages. The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way. One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features. Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise. The co-training algorithm is illustrated in Figure 2. In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration. The intuition of the co-training algorithm is that if one classifier can confidently predict the class of an example, which is very similar to some of labeled ones, it can provide one more training example for the other classifier. But, of course, if this example happens to be easy to be classified by the first classifier, it does not mean that this example will be easy to be classified by the second classifier, so the second classifier will get useful information to improve itself and vice versa (Kiritchenko and Matwin, 2001). In the co-training algorithm, a basic classification algorithm is required to construct Cen and C,n. Typical text classifiers include Support Vector Machine (SVM), Naïve Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN) , etc. In this study, we adopt the widely-used SVM classifier (Joachims, 2002). Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets. The English or Chinese features used in this study include both unigrams and bigrams5 and the feature weight is simply set to term frequency6. Feature selection methods (e.g. Document Frequency (DF), Information Gain (IG), and Mutual Information (MI)) can be used for dimension reduction. But we use all the features in the experiments for comparative analysis, because there is no significant performance improvement after applying the feature selection techniques in our empirical study. The output value of the SVM classifier for a review indicates the confidence level of the review’s classification. Usually, the sentiment polarity of a review is indicated by the sign of the prediction value. Given: In the training phase, the co-training algorithm learns two separate classifiers: Cen and C,n. Therefore, in the classification phase, we can obtain two prediction values for a test review. We normalize the prediction values into [-1, 1] by dividing the maximum absolute value. Finally, the average of the normalized values is used as the overall prediction value of the review. The following three datasets were collected and used in the experiments: Test Set (Labeled Chinese Reviews): In order to assess the performance of the proposed approach, we collected and labeled 886 product reviews (451 positive reviews + 435 negative reviews) from a popular Chinese IT product web site-IT1688. The reviews focused on such products as mp3 players, mobile phones, digital camera and laptop computers. Training Set (Labeled English Reviews): There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007)9, because the corpus was large-scale and it was within similar domains as the test set. The dataset consisted of 8000 Amazon product reviews (4000 positive reviews + 4000 negative reviews) for four different product types: books, DVDs, electronics and kitchen appliances. Unlabeled Set (Unlabeled Chinese Reviews): We downloaded additional 1000 Chinese product reviews from IT168 and used the reviews as the unlabeled set. Therefore, the unlabeled set and the test set were in the same domain and had similar underlying feature distributions. Each Chinese review was translated into English review, and each English review was translated into Chinese review. Therefore, each review has two independent views: English view and Chinese view. A review is represented by both its English view and its Chinese view. Note that the training set and the unlabeled set are used in the training phase, while the test set is blind to the training phase. We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the accuracy metric to measure the overall performance of the system. The metrics are defined the same as in general text categorization. In the experiments, the proposed co-training approach (CoTrain) is compared with the following baseline methods: SVM(CN): This method applies the inductive SVM with only Chinese features for sentiment classification in the Chinese view. Only Englishto-Chinese translation is needed. And the unlabeled set is not used. SVM(EN): This method applies the inductive SVM with only English features for sentiment classification in the English view. Only Chineseto-English translation is needed. And the unlabeled set is not used. SVM(ENCN1): This method applies the inductive SVM with both English and Chinese features for sentiment classification in the two views. Both English-to-Chinese and Chinese-toEnglish translations are required. And the unlabeled set is not used. SVM(ENCN2): This method combines the results of SVM(EN) and SVM(CN) by averaging the prediction values in the same way with the co-training approach. TSVM(CN): This method applies the transductive SVM with only Chinese features for sentiment classification in the Chinese view. Only English-to-Chinese translation is needed. And the unlabeled set is used. TSVM(EN): This method applies the transductive SVM with only English features for sentiment classification in the English view. Only Chinese-to-English translation is needed. And the unlabeled set is used. TSVM(ENCN1): This method applies the transductive SVM with both English and Chinese features for sentiment classification in the two views. Both English-to-Chinese and Chinese-toEnglish translations are required. And the unlabeled set is used. TSVM(ENCN2): This method combines the results of TSVM(EN) and TSVM(CN) by averaging the prediction values. Note that the first four methods are straightforward methods used in previous work, while the latter four methods are strong baselines because the transductive SVM has been widely used for improving the classification accuracy by leveraging additional unlabeled examples. In the experiments, we first compare the proposed co-training approach (I=40 and p=n=5) with the eight baseline methods. The three parameters in the co-training approach are empirically set by considering the total number (i.e. 1000) of the unlabeled Chinese reviews. In our empirical study, the proposed approach can perform well with a wide range of parameter values, which will be shown later. Table 1 shows the comparison results. Seen from the table, the proposed co-training approach outperforms all eight baseline methods over all metrics. Among the eight baselines, the best one is TSVM(ENCN2), which combines the results of two transductive SVM classifiers. Actually, TSVM(ENCN2) is similar to CoTrain because CoTrain also combines the results of two classifiers in the same way. However, the co-training approach can train two more effective classifiers, and the accuracy values of the component English and Chinese classifiers are 0.775 and 0.790, respectively, which are higher than the corresponding TSVM classifiers. Overall, the use of transductive learning and the combination of English and Chinese views are beneficial to the final classification accuracy, and the cotraining approach is more suitable for making use of the unlabeled Chinese reviews than the transductive SVM. Figure 3 shows the accuracy curve of the cotraining approach (Combined Classifier) with different numbers of iterations. The iteration number I is varied from 1 to 80. When I is set to 1, the co-training approach is degenerated into SVM(ENCN2). The accuracy curves of the component English and Chinese classifiers learned in the co-training approach are also shown in the figure. We can see that the proposed co-training approach can outperform the best baselineTSVM(ENCN2) after 20 iterations. After a large number of iterations, the performance of the cotraining approach decreases because noisy training examples may be selected from the remaining unlabeled set. Finally, the performance of the approach does not change any more, because the algorithm runs out of all possible examples in the unlabeled set. Fortunately, the proposed approach performs well with a wide range of iteration numbers. We can also see that the two component classifier has similar trends with the cotraining approach. It is encouraging that the component Chinese classifier alone can perform better than the best baseline when the iteration number is set between 40 and 70. Figure 4 shows how the growth size at each iteration (p positive and n negative confident examples) influences the accuracy of the proposed co-training approach. In the above experiments, we set p=n, which is considered as a balanced growth. When p differs very much from n, the growth is considered as an imbalanced growth. Balanced growth of (2, 2), (5, 5), (10, 10) and (15, 15) examples and imbalanced growth of (1, 5), (5, 1) examples are compared in the figure. We can see that the performance of the cotraining approach with the balanced growth can be improved after a few iterations. And the performance of the co-training approach with large p and n will more quickly become unchanged, because the approach runs out of the limited examples in the unlabeled set more quickly. However, the performance of the co-training approaches with the two imbalanced growths is always going down quite rapidly, because the labeled unbalanced examples hurt the performance badly at each iteration. In the above experiments, all features (unigram + bigram) are used. As mentioned earlier, feature selection techniques are widely used for dimension reduction. In this section, we further conduct experiments to investigate the influences of feature selection techniques on the classification results. We use the simple but effective document frequency (DF) for feature selection. Figures 6 show the comparison results of different feature sizes for the co-training approach and two strong baselines. The feature size is measured as the proportion of the selected features against the total features (i.e. 100%). We can see from the figure that the feature selection technique has very slight influences on the classification accuracy of the methods. It can be seen that the co-training approach can always outperform the two baselines with different feature sizes. The results further demonstrate the effectiveness and robustness of the proposed cotraining approach. In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification. The experimental results show the effectiveness of the proposed approach. In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification. The approach has the effect of “smoothing” the learning curves. During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations. 2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service. We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text.","In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification. The experimental results show the effectiveness of the proposed approach. In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification. The approach has the effect of “smoothing” the learning curves. During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations. 2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service. We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text."
24,"Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems. The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.","Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems. The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (Tromble et al., 2008). A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair 7-1 = (V, £) consisting of a vertex set V and a set of hyperedges £ C_ V* x V. Each hyperedge e E £ connects a head vertex h(e) with a sequence of tail vertices T(e) = {v1, ..., v,,,}. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice). Each hyperedge is labeled with a rule re from the SCFG. The number of nonterminals on the right-hand side of re corresponds with the arity of e. An example without scores is shown in Figure 1. A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1, r2, ..., rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E. Given a set of source sentences F1� with corresponding reference translations RS , the objective of MERT is to find a parameter set �λm which minimizes an automated evaluation criterion under a linear model: In the context of statistical machine translation, the optimization procedure was first described in Och (2003) for N-best lists and later extended to phrase-lattices in Macherey et al. (2008). The algorithm is based on the insight that, under a loglinear model, the cost function of any candidate translation can be represented as a line in the plane if the initial parameter set λM is shifted along a direction dM . Let C = {E1, ..., EK} denote a set of candidate translations, then computing the best scoring translation hypothesis E� out of C results in the following optimization problem: as the independent variable. For any particuthe decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment. If to +oo, other translation hypotheses may at some point constitute the topmost line segments and thus change the decision made by the decoder. The entire sequence of topmost line segments is called upper envelope and provides an exhaustive representation of all possible outcomes that the decoder may yield if is shifted along the chosen direction. Both the translations and their corresponding line segments can efficiently be computed without incorporating any error criterion. Once the envelope has been determined, the translation candidates of its constituent line segments are projected onto their corresponding error counts, thus yielding the exact and unsmoothed error surface for all candidate translations encoded in C. The error surface can now easily be traversed in order to find that under which the new paramM minimizes the global error. In this section, we present an extension of the algorithm described in Macherey et al. (2008) that allows us to efficiently compute and represent upper envelopes over all candidate translations encoded in hypergraphs. Conceptually, the algorithm works by propagating (initially empty) envelopes from the source nodes bottom-up to its unique root node, thereby expanding the envelopes by applying SCFG rules to the partial candidate translations that are associated with the constituent line segments. To recombine envelopes, we need two operators: the sum and the maximum over convex polygons. To illustrate which operator is applied when, we transform 7-1 = (V, £) into a regular graph with typed nodes by (1) marking all vertices v E V with the symbol V and (2) replacing each hyperedge e E £, 1, with a small subgraph consisting of a new vertex whose incoming and outgoing edges connect the same head an input: associative map a: V --+ Env(V), hyperarc e output: Minkowski sum of envelopes over T(e) in the transformed graph as were connected by e in the original graph. The unique outgoing edge of v∧(e) is associated with the rule re; incoming edges are not linked to any rule. Figure 2 illustrates the transformation for a hyperedge with arity 3. The graph transformation is isomorphic. The rules associated with every hyperedge specify how line segments in the envelopes of a hyperedge’s tail nodes can be combined. Suppose we have a hyperedge e with rule re : X —* aX1bX2c and T(e) = {v1, v2}. Then we substitute X1 and X2 in the rule with candidate translations associated with line segments in envelopes Env(v1) and Env(v2) respectively. To derive the algorithm, we consider the general case of a hyperedge e with rule re : X —* w1X1w2...wnXnwn+1. Because the right-hand side of re has n nonterminals, the arity of e is |e |= n. Let T(e) = {v1, ..., vn} denote the tail nodes of e. We now assume that each tail node vi E T(e) is associated with the upper envelope over all candidate translations that are induced by derivations of the corresponding nonterminal symbol Xi. These envelopes shall be deAlgorithm 2 V-operation (Max) input: array L[0..K-1] containing line objects output: upper envelope of L noted by Env(vi). To decompose the problem of computing and propagating the tail envelopes over the hyperedge e to its head node, we now define two operations, one for either node type, to specify how envelopes associated with the tail vertices are propagated to the head vertex. Nodes of Type “∧”: For a type n node, the resulting envelope is the Minkowski sum over the envelopes of the incoming edges (Berg et al., 2008). Since the envelopes of the incoming edges are convex hulls, the Minkowski sum provides an upper bound to the number of line segments that constitute the resulting envelope: the bound is the sum over the number of line segments in the envelopes of the incoming edges, i.e. : Algorithm 1 shows the pseudo code for computing the Minkowski sum over multiple envelopes. The line objects E used in this algorithm are encoded as 4-tuples, each consisting of the xintercept with E’s left-adjacent line stored as E.x, the slope E.m, the y-intercept E.y, and the (partial) derivation tree E.D. At the beginning, the leftmost line segment of each envelope is inserted into a priority queue pq. The priority is defined in terms of a line’s x-intercept such that lower values imply higher priority. Hence, the priority queue enumerates all line segments from left to right in ascending order of their x-intercepts, which is the order needed to compute the Minkowski sum. Nodes of Type “V”: The operation performed at nodes of type “V” computes the convex hull over the union of the envelopes propagated over the incoming edges. This operation is a “max” operation and it is identical to the algorithm described in (Macherey et al., 2008) for phrase lattices. Algorithm 2 contains the pseudo code. The complete algorithm then works as follows: Traversing all nodes in x bottom-up in topological order, we proceed for each node v E V over its incoming hyperedges and combine in each such hyperedge e the envelopes associated with the tail nodes T(e) by computing their sum according to Algorithm 1 (n-operation). For each incoming hyperedge e, the resulting envelope is then expanded by applying the rule re to its constituent line segments. The envelopes associated with different incoming hyperedges of node v are then combined and reduced according to Algorithm 2 (V-operation). By construction, the envelope at the root node is the convex hull over the line segments of all candidate translations that can be derived from the hypergraph. The suggested algorithm has similar properties as the algorithm presented in (Macherey et al., 2008). In particular, it has the same upper bound on the number of line segments that constitute the envelope at the root node, i.e, the size of this envelope is guaranteed to be no larger than the number of edges in the transformed hypergraph. We first review Minimum Bayes-Risk (MBR) decoding for statistical MT. An MBR decoder seeks the hypothesis with the least expected loss under a probability model (Bickel and Doksum, 1977). If we think of statistical MT as a classifier that maps a source sentence F to a target sentence E, the MBR decoder can be expressed as follows: where L(E, E') is the loss between any two hypotheses E and E', P(E|F) is the probability model, and 9 is the space of translations (N-best list, lattice, or a hypergraph). MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004). This reranking can be done for any sentencelevel loss function such as BLEU (Papineni et al., 2001), Word Error Rate, or Position-independent Error Rate. Recently, Tromble et al. (2008) extended MBR decoding to translation lattices under an approximate BLEU score. They approximated log(BLEU) score by a linear function of n-gram matches and candidate length. If E and E' are the reference and the candidate translations respectively, this linear function is given by: where w is an n-gram present in either E or E', and θ0,θ1,..., θN are weights which are determined empirically, where N is the maximum ngram order. Under such a linear decomposition, the MBR decoder (Equation 1) can be written as Tromble et al. (2008) implement the MBR decoder using Weighted Finite State Automata (WFSA) operations. First, the set of n-grams is extracted from the lattice. Next, the posterior probability of each n-gram is computed. A new automaton is then created by intersecting each ngram with weight (from Equation 2) to an unweighted lattice. Finally, the MBR hypothesis is extracted as the best path in the automaton. We will refer to this procedure as FSAMBR. The above steps are carried out one n-gram at a time. For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive. We now present an alternate approximate procedure which can avoid this where the posterior probability of an n-gram in the lattice is given by enumeration making the resulting algorithm much faster than FSAMBR. The key idea behind this new algorithm is to rewrite the n-gram posterior probability (Equation 4) as follows: where f(e, w, E) is a score assigned to edge e on path E containing n-gram w: { 1 w ∈ e,p(e|G) > p(e'|G), e' precedes e on E 0 otherwise In other words, for each path E, we count the edge that contributes n-gram w and has the highest edge posterior probability relative to its predecessors on the path E; there is exactly one such edge on each lattice path E. We note that f(e, w, E) relies on the full path E which means that it cannot be computed based on local statistics. We therefore approximate the quantity f(e, w, E) with f*(e, w,G) that counts the edge e with n-gram w that has the highest arc posterior probability relative to predecessors in the entire lattice G. f*(e, w,G) can be computed locally, and the n-gram posterior probability based on f* can be determined as follows: Algorithm 3 MBR Decoding on Lattices (Algorithm 3). However, there are important differences when computing the n-gram posterior probabilities (Step 3). In this inside pass, we now maintain both n-gram prefixes and suffixes (up to the maximum order −1) on each hypergraph node. This is necessary because unlike a lattice, new ngrams may be created at subsequent nodes by concatenating words both to the left and the right side of the n-gram. When the arity of the edge is 2, a rule has the general form aX1bX2c, where X1 and X2 are sequences from tail nodes. As a result, we need to consider all new sequences which can be created by the cross-product of the n-grams on the two tail nodes. E.g. if X1 = {c, cd, d} and X2 = {f, g}, then a total of six sequences will result. In practice, such a cross-product is not prowhere P(e|G) is the posterior probability of a lattice edge. The algorithm to perform Lattice MBR is given in Algorithm 3. For each node t in the lattice, we maintain a quantity Score(w, t) for each n-gram w that lies on a path from the source node to t. Score(w, t) is the highest posterior probability among all edges on the paths that terminate on t and contain n-gram w. The forward pass requires computing the n-grams introduced by each edge; to do this, we propagate n-grams (up to maximum order −1) terminating on each node. We next extend the Lattice MBR decoding algorithm (Algorithm 3) to rescore hypergraphs produced by a SCFG based MT system. Algorithm 4 is an extension to the MBR decoder on lattices Algorithm 4 MBR Decoding on Hypergraphs hibitive when the maximum n-gram order in MBR does not exceed the order of the n-gram language model used in creating the hypergraph. In the latter case, we will have a small set of unique prefixes and suffixes on the tail nodes. Lattice MBR Decoding (Equation 3) assumes a linear form for the gain function (Equation 2). This linear function contains n + 1 parameters B0, B1, ..., BN, where N is the maximum order of the n-grams involved. Tromble et al. (2008) obtained these factors as a function of n-gram precisions derived from multiple training runs. However, this does not guarantee that the resulting linear score (Equation 2) is close to the corpus BLEU. We now describe how MERT can be used to estimate these factors to achieve a better approximation to the corpus BLEU. We recall that MERT selects weights in a linear model to optimize an error criterion (e.g. corpus BLEU) on a training set. The lattice MBR decoder (Equation 3) can be written as a linear model: E� = argmaxE,Eg �Ni=0 Bigi(E', F), where g0(E',F) = IE'I and gi(E', F) = The linear approximation to BLEU may not hold in practice for unseen test sets or languagepairs. Therefore, we would like to allow the decoder to backoff to the MAP translation in such cases. To do that, we introduce an additional feature function gN+1(E, F) equal to the original decoder cost for this sentence. A weight assignment of 1.0 for this feature function and zeros for the other feature functions would imply that the MAP translation is chosen. We now have a total of N+2 feature functions which we optimize using MERT to obtain highest BLEU score on a training set. We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show how MERT can be used to tune MBR parameters. We report results on two tasks. The first one is the constrained data track of the NIST Arabicto-English (aren) and Chinese-to-English (zhen) translation task1. On this task, the parallel and the monolingual data included all the allowed training sets for the constrained track. Table 1 reports statistics computed over these data sets. Our development set (dev) consists of the NIST 2005 eval set; we use this set for optimizing MBR parameters. We report results on NIST 2002 and NIST 2003 evaluation sets. The second task consists of systems for 39 language-pairs with English as the target language and trained on at most 300M word tokens mined from the web and other published sources. The development and test sets for this task are randomly selected sentences from the web, and contain 5000 and 1000 sentences respectively. Our phrase-based statistical MT system is similar to the alignment template system described in (Och and Ney, 2004; Tromble et al., 2008). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list. MBR decoding is performed in the second pass. We also train two SCFG-based MT systems: a hierarchical phrase-based SMT (Chiang, 2007) system and a syntax augmented machine translation (SAMT) system using the approach described in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hypergraph or an N-best list, which are then rescored using MBR decoding. Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system. The first two columns show the average amount of time in msecs that either algorithm requires to compute the upper envelope when applied to phrase lattices. Compared to the algorithm described in (Macherey et al., 2008) which is optimized for phrase lattices, the hypergraph implementation causes a small increase in running time. This increase is mainly due to the representation of line segments; while the phraselattice implementation stores a single backpointer, the hypergraph version stores a vector of backpointers. The last two columns show the average amount of time that is required to compute the upper envelope on hypergraphs. For comparison, we prune hypergraphs to the same density (# of edges per edge on the best path) and achieve identical running times for computing the error surface. We first compare the new lattice MBR (Algorithm 3) with MBR decoding on 1000-best lists and FSAMBR (Tromble et al., 2008) on lattices generated by the phrase-based systems; evaluation is done using both BLEU and average run-time per sentence (Table 3). Note that N-best MBR uses a sentence BLEU loss function. The new lattice MBR algorithm gives about the same performance as FSAMBR while yielding a 20X speedup. We next report the performance of MBR on hypergraphs generated by Hiero/SAMT systems. Table 4 compares Hypergraph MBR (HGMBR) with MAP and MBR decoding on 1000 best lists. On some systems such as the Arabic-English SAMT, the gains from Hypergraph MBR over 1000-best MBR are significant. In other cases, Hypergraph MBR performs at least as well as N-best MBR. In all cases, we observe a 7X speedup in runtime. This shows the usefulness of Hypergraph MBR decoding as an efficient alternative to Nbest MBR. We now describe the results by tuning MBR ngram parameters (Equation 2) using MERT. We first compute N + 1 MBR feature functions on each edge of the lattice/hypergraph. We also include the total decoder cost on the edge as as additional feature function. MERT is then performed to optimize the BLEU score on a development set; For MERT, we use 40 random initial parameters as well as parameters computed using corpus based statistics (Tromble et al., 2008). Table 5 shows results for NIST systems. We report results on nist03 set and present three systems for each language pair: phrase-based (pb), hierarchical (hier), and SAMT; Lattice MBR is done for the phrase-based system while HGMBR is used for the other two. We select the MBR scaling factor (Tromble et al., 2008) based on the development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5 and 1.0 for the aren-phrase, aren-hier, aren-samt, zhen-phrase zhen-hier and zhen-samt systems respectively. For the multi-language case, we train phrase-based systems and perform lattice MBR for all language pairs. We use a scaling factor of 0.7 for all pairs. Additional gains can be obtained by tuning this factor; however, we do not explore that dimension in this paper. In all cases, we prune the lattices/hypergraphs to a density of 30 using forward-backward pruning (Sixtus and Ortmanns, 1999). We consider a BLEU score difference to be a) gain if is at least 0.2 points, b) drop if it is at most -0.2 points, and c) no change otherwise. The results are shown in Table 6. In both tables, the following results are reported: Lattice/HGMBR with default parameters (−5,1.5, 2, 3, 4) computed using corpus statistics (Tromble et al., 2008), Lattice/HGMBR with parameters derived from MERT both without/with the baseline model cost feature (mert−b, mert+b). For multi-language systems, we only show the # of language-pairs with gains/no-changes/drops for each MBR variant with respect to the MAP translation. We observed in the NIST systems that MERT resulted in short translations relative to MAP on the unseen test set. To prevent this behavior, we modify the MERT error criterion to include a sentence-level brevity scorer with parameter α: BLEU+brevity(α). This brevity scorer penalizes each candidate translation that is shorter than the average length over its reference translations, using a penalty term which is linear in the difference between either length. We tune α on the development set so that the brevity score of MBR translation is close to that of the MAP translation. In the NIST systems, MERT yields small improvements on top of MBR with default parameters. This is the case for Arabic-English Hiero/SAMT. In all other cases, we see no change or even a slight degradation due to MERT. We hypothesize that the default MBR parameters (Tromble et al., 2008) are well tuned. Therefore there is little gain by additional tuning using MERT. In the multi-language systems, the results show a different trend. We observe that MBR with default parameters results in gains on 18 pairs, no differences on 9 pairs, and losses on 12 pairs. When we optimize MBR features with MERT, the number of language pairs with gains/no changes/drops is 22/5/12. Thus, MERT has a bigger impact here than in the NIST systems. We hypothesize that the default MBR parameters are sub-optimal for some language pairs and that MERT helps to find better parameter settings. In particular, MERT avoids the need for manually tuning these parameters by language pair. Finally, when baseline model costs are added as an extra feature (mert+b), the number of pairs with gains/no changes/drops is 26/8/5. This shows that this feature can allow MBR decoding to backoff to the MAP translation. When MBR does not produce a higher BLEU score relative to MAP on the development set, MERT assigns a higher weight to this feature function. We see such an effect for 4 systems.","We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show how MERT can be used to tune MBR parameters. We report results on two tasks. The first one is the constrained data track of the NIST Arabicto-English (aren) and Chinese-to-English (zhen) translation task1. On this task, the parallel and the monolingual data included all the allowed training sets for the constrained track. Table 1 reports statistics computed over these data sets. Our development set (dev) consists of the NIST 2005 eval set; we use this set for optimizing MBR parameters. We report results on NIST 2002 and NIST 2003 evaluation sets. The second task consists of systems for 39 language-pairs with English as the target language and trained on at most 300M word tokens mined from the web and other published sources. The development and test sets for this task are randomly selected sentences from the web, and contain 5000 and 1000 sentences respectively. Our phrase-based statistical MT system is similar to the alignment template system described in (Och and Ney, 2004; Tromble et al., 2008). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list. MBR decoding is performed in the second pass. We also train two SCFG-based MT systems: a hierarchical phrase-based SMT (Chiang, 2007) system and a syntax augmented machine translation (SAMT) system using the approach described in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hypergraph or an N-best list, which are then rescored using MBR decoding. Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system. The first two columns show the average amount of time in msecs that either algorithm requires to compute the upper envelope when applied to phrase lattices. Compared to the algorithm described in (Macherey et al., 2008) which is optimized for phrase lattices, the hypergraph implementation causes a small increase in running time. This increase is mainly due to the representation of line segments; while the phraselattice implementation stores a single backpointer, the hypergraph version stores a vector of backpointers. The last two columns show the average amount of time that is required to compute the upper envelope on hypergraphs. For comparison, we prune hypergraphs to the same density (# of edges per edge on the best path) and achieve identical running times for computing the error surface. We first compare the new lattice MBR (Algorithm 3) with MBR decoding on 1000-best lists and FSAMBR (Tromble et al., 2008) on lattices generated by the phrase-based systems; evaluation is done using both BLEU and average run-time per sentence (Table 3). Note that N-best MBR uses a sentence BLEU loss function. The new lattice MBR algorithm gives about the same performance as FSAMBR while yielding a 20X speedup. We next report the performance of MBR on hypergraphs generated by Hiero/SAMT systems. Table 4 compares Hypergraph MBR (HGMBR) with MAP and MBR decoding on 1000 best lists. On some systems such as the Arabic-English SAMT, the gains from Hypergraph MBR over 1000-best MBR are significant. In other cases, Hypergraph MBR performs at least as well as N-best MBR. In all cases, we observe a 7X speedup in runtime. This shows the usefulness of Hypergraph MBR decoding as an efficient alternative to Nbest MBR. We now describe the results by tuning MBR ngram parameters (Equation 2) using MERT. We first compute N + 1 MBR feature functions on each edge of the lattice/hypergraph. We also include the total decoder cost on the edge as as additional feature function. MERT is then performed to optimize the BLEU score on a development set; For MERT, we use 40 random initial parameters as well as parameters computed using corpus based statistics (Tromble et al., 2008). Table 5 shows results for NIST systems. We report results on nist03 set and present three systems for each language pair: phrase-based (pb), hierarchical (hier), and SAMT; Lattice MBR is done for the phrase-based system while HGMBR is used for the other two. We select the MBR scaling factor (Tromble et al., 2008) based on the development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5 and 1.0 for the aren-phrase, aren-hier, aren-samt, zhen-phrase zhen-hier and zhen-samt systems respectively. For the multi-language case, we train phrase-based systems and perform lattice MBR for all language pairs. We use a scaling factor of 0.7 for all pairs. Additional gains can be obtained by tuning this factor; however, we do not explore that dimension in this paper. In all cases, we prune the lattices/hypergraphs to a density of 30 using forward-backward pruning (Sixtus and Ortmanns, 1999). We consider a BLEU score difference to be a) gain if is at least 0.2 points, b) drop if it is at most -0.2 points, and c) no change otherwise. The results are shown in Table 6. In both tables, the following results are reported: Lattice/HGMBR with default parameters (−5,1.5, 2, 3, 4) computed using corpus statistics (Tromble et al., 2008), Lattice/HGMBR with parameters derived from MERT both without/with the baseline model cost feature (mert−b, mert+b). For multi-language systems, we only show the # of language-pairs with gains/no-changes/drops for each MBR variant with respect to the MAP translation. We observed in the NIST systems that MERT resulted in short translations relative to MAP on the unseen test set. To prevent this behavior, we modify the MERT error criterion to include a sentence-level brevity scorer with parameter α: BLEU+brevity(α). This brevity scorer penalizes each candidate translation that is shorter than the average length over its reference translations, using a penalty term which is linear in the difference between either length. We tune α on the development set so that the brevity score of MBR translation is close to that of the MAP translation. In the NIST systems, MERT yields small improvements on top of MBR with default parameters. This is the case for Arabic-English Hiero/SAMT. In all other cases, we see no change or even a slight degradation due to MERT. We hypothesize that the default MBR parameters (Tromble et al., 2008) are well tuned. Therefore there is little gain by additional tuning using MERT. In the multi-language systems, the results show a different trend. We observe that MBR with default parameters results in gains on 18 pairs, no differences on 9 pairs, and losses on 12 pairs. When we optimize MBR features with MERT, the number of language pairs with gains/no changes/drops is 22/5/12. Thus, MERT has a bigger impact here than in the NIST systems. We hypothesize that the default MBR parameters are sub-optimal for some language pairs and that MERT helps to find better parameter settings. In particular, MERT avoids the need for manually tuning these parameters by language pair. Finally, when baseline model costs are added as an extra feature (mert+b), the number of pairs with gains/no changes/drops is 26/8/5. This shows that this feature can allow MBR decoding to backoff to the MAP translation. When MBR does not produce a higher BLEU score relative to MAP on the development set, MERT assigns a higher weight to this feature function. We see such an effect for 4 systems."
25,"used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised of similar schemata called chains raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. introduce two evaluations: the evaluate event relatedness, and an orcoherence to evaluate narrative order. show a over baseline narrative prediction and temporal coherence. tate learning, and thus this paper addresses the three of chain induction: event ordering of events selection (pruning the event space into discrete sets). Learning these prototypical schematic sequences of events is important for rich understanding of text. Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering. For example, Schank and Abelson (1977) proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer, Waiter, Cook, Tables, etc. ), the events constituting the script (entering, sitting down, asking for menus, etc. ), and the various preconditions, ordering, and results of each of the constituent actions. Consider these two distinct narrative chains.","used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised of similar schemata called chains raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. introduce two evaluations: the evaluate event relatedness, and an orcoherence to evaluate narrative order. show a over baseline narrative prediction and temporal coherence. tate learning, and thus this paper addresses the three of chain induction: event ordering of events selection (pruning the event space into discrete sets). Learning these prototypical schematic sequences of events is important for rich understanding of text. Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering. For example, Schank and Abelson (1977) proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer, Waiter, Cook, Tables, etc. ), the events constituting the script (entering, sitting down, asking for menus, etc. ), and the various preconditions, ordering, and results of each of the constituent actions. Consider these two distinct narrative chains. This paper induces a new representation of structured knowledge called narrative event chains (or narrative chains). Narrative chains are partially ordered sets of events centered around a common protagonist. They are related to structured sequences of participants and events that have been called scripts (Schank and Abelson, 1977) or Fillmorean frames. These participants and events can be filled in and instantiated in a particular text situation to draw inferences. Chains focus on a single actor to faciliIt would be useful for question answering or textual entailment to know that ‘X denied ’ is also a likely event in the left chain, while ‘ replaces W’ temporally follows the right. Narrative chains (such as Firing of Employee or Executive Resigns) offer the structure and power to directly infer these new subevents by providing critical background knowledge. In part due to its complexity, automatic induction has not been addressed since the early nonstatistical work of Mooney and DeJong (1985). The first step to narrative induction uses an entitybased model for learning narrative relations by following a protagonist. As a narrative progresses through a series of events, each event is characterized by the grammatical role played by the protagonist, and by the protagonist’s shared connection to surrounding events. Our algorithm is an unsupervised distributional learning approach that uses coreferring arguments as evidence of a narrative relation. We show, using a new evaluation task called narrative cloze, that our protagonist-based method leads to better induction than a verb-only approach. The next step is to order events in the same narrative chain. We apply work in the area of temporal classification to create partial orders of our learned events. We show, using a coherence-based evaluation of temporal ordering, that our partial orders lead to better coherence judgements of real narrative instances extracted from documents. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. While previous work hasn’t focused specifically on learning narratives1, our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. <patient> kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (<patient> kidnapped and <patient> abducted) or related events (<patient> kidnapped and <patient> released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefore, is to learn events that constitute narrative chains. Formally, a narrative chain is a partially ordered set of narrative events that share a common actor. A narrative event is a tuple of an event (most simply a verb) and its participants, represented as typed dependencies. Since we are focusing on a single actor in this study, a narrative event is thus a tuple of the event and the typed dependency of the protagonist: (event, dependency). A narrative chain is a set of narrative events {e1, e2, ..., en}, where n is the size of the chain, and a relation B(ei, ej) that is true if narrative event ei occurs strictly before ej in time. The notion of a protagonist motivates our approach to narrative learning. We make the following assumption of narrative coherence: verbs sharing coreferring arguments are semantically connected by virtue of narrative discourse structure. A single document may contain more than one narrative (or topic), but the narrative assumption states that a series of argument-sharing verbs is more likely to participate in a narrative chain than those not sharing. In addition, the narrative approach captures grammatical constraints on narrative coherence. Simple distributional learning might discover that the verb push is related to the verb fall, but narrative learning can capture additional facts about the participants, specifically, that the object or patient of the push is the subject or agent of the fall. Each focused protagonist chain offers one perspective on a narrative, similar to the multiple perspectives on a commercial transaction event offered by buy and sell. A narrative chain, by definition, includes a partial ordering of events. Early work on scripts included ordering constraints with more complex preconditions and side effects on the sequence of events. This paper presents work toward a partial ordering and leaves logical constraints as future work. We focus on the before relation, but the model does not preclude advanced theories of temporal order. Our first model learns basic information about a narrative chain: the protagonist and the constituent subevents, although not their ordering. For this we need a metric for the relation between an event and a narrative chain. Pairwise relations between events are first extracted unsupervised. A distributional score based on how often two events share grammatical arguments (using pointwise mutual information) is used to create this pairwise relation. Finally, a global narrative score is built such that all events in the chain provide feedback on the event in question (whether for inclusion or for decisions of inference). Given a list of observed verb/dependency counts, we approximate the pointwise mutual information (PMI) by: where e(w, d) is the verb/dependency pair w and d (e.g. e(push,subject)). The numerator is defined by: where C(e(x, d), e(y, f)) is the number of times the two events e(x, d) and e(y, f) had a coreferring entity filling the values of the dependencies d and f. We also adopt the ‘discount score’ to penalize low occuring words (Pantel and Ravichandran, 2004). Given the debate over appropriate metrics for distributional learning, we also experimented with the t-test. Our experiments found that PMI outperforms the t-test on this task by itself and when interpolated together using various mixture weights. Once pairwise relation scores are calculated, a global narrative score can then be built such that all events provide feedback on the event in question. For instance, given all narrative events in a document, we can find the next most likely event to occur by maximizing: where n is the number of events in our chain and ei is the ith event. m is the number of events f in our training corpus. A ranked list of guesses can be built from this summation and we hypothesize that the more events in our chain, the more informed our ranked output. An example of a chain with 3 events and the top 6 ranked guesses is given in figure 1. The cloze task (Taylor, 1953) is used to evaluate a system (or human) for language proficiency by removing a random word from a sentence and having the system attempt to fill in the blank (e.g. I forgot to the waitress for the good service). Depending on the type of word removed, the test can evaluate syntactic knowledge as well as semantic. Deyes (1984) proposed an extended task, discourse cloze, to evaluate discourse knowledge (removing phrases that are recoverable from knowledge of discourse relations like contrast and consequence). We present a new cloze task that requires narrative knowledge to solve, the narrative cloze. The narrative cloze is a sequence of narrative events in a document from which one event has been removed. The task is to predict the missing verb and typed dependency. Take this example text about American football with McCann as the protagonist: These clauses are represented in the narrative model as five events: (threw subject), (pulled object), (told object), (start subject), (completed subject). These verb/dependency events make up a narrative cloze model. We could remove (threw subject) and use the remaining four events to rank this missing event. Removing a single such pair to be filled in automatically allows us to evaluate a system’s knowledge of narrative relations and coherence. We do not claim this cloze task to be solvable even by humans, but rather assert it as a comparative measure to evaluate narrative knowledge. We use years 1994-2004 (1,007,227 documents) of the Gigaword Corpus (Graff, 2002) for training2. We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006)3, recording all verbs with subject, object, or prepositional typed dependencies. We use the OpenNLP4 coreference engine to resolve the entity mentions. For each document, the verb pairs that share coreferring entities are recorded with their dependency types. Particles are included with the verb. We used 10 news stories from the 1994 section of the corpus for development. The stories were hand chosen to represent a range of topics such as business, sports, politics, and obituaries. We used 69 news stories from the 2001 (year selected randomly) section of the corpus for testing (also removed from training). The test set documents were randomly chosen and not preselected for a range of topics. From each document, the entity involved in the most events was selected as the protagonist. For this evaluation, we only look at verbs. All verb clauses involving the protagonist are manually extracted and translated into the narrative events (verb,dependency). Exceptions that are not included are verbs in headlines, quotations (typically not part of a narrative), “be” properties (e.g. john is happy), modifying verbs (e.g. hurried to leave, only leave is used), and multiple instances of one event. The original test set included 100 documents, but those without a narrative chain at least five events in length were removed, leaving 69 documents. Most of the removed documents were not stories, but genres such as interviews and cooking recipes. An example of an extracted chain is shown in figure 2. We evalute with Narrative Cloze using leave-oneout cross validation, removing one event and using the rest to generate a ranked list of guesses. The test dataset produces 740 cloze tests (69 narratives with 740 events). After generating our ranked guesses, the position of the correct event is averaged over all 740 tests for the final score. We penalize unseen events by setting their ranked position to the length of the guess list (ranging from 2k to 15k). Figure 1 is an example of a ranked guess list for a short chain of three events. If the original document contained (fired obj), this cloze test would score 3. We want to measure the utility of the protagonist and the narrative coherence assumption, so our baseline learns relatedness strictly based upon verb co-occurence. The PMI is then defined as between all occurrences of two verbs in the same document. This baseline evaluation is verb only, as dependencies require a protagonist to fill them. After initial evaluations, the baseline was performing very poorly due to the huge amount of data involved in counting all possible verb pairs (using a protagonist vastly reduces the number). We experimented with various count cutoffs to remove rare occurring pairs of verbs. The final results use a baseline where all pairs occurring less than 10 times in the training data are removed. Since the verb-only baseline does not use typed dependencies, our narrative model cannot directly compare to this abstracted approach. We thus modified the narrative model to ignore typed dependencies, but still count events with shared arguments. Thus, we calculate the PMI across verbs that share arguments. This approach is called Protagonist. The full narrative model that includes the grammatical dependencies is called Typed Deps. Experiments with varying sizes of training data are presented in figure 3. Each ranked list of candidate verbs for the missing event in Baseline/Protagonist contained approximately 9 thousand candidates. Of the 740 cloze tests, 714 of the removed events were present in their respective list of guesses. This is encouraging as only 3.5% of the events are unseen (or do not meet cutoff thresholds). When all training data is used (1994-2004), the average ranked position is 1826 for Baseline and 1160 for Protagonist (1 being most confident). The Baseline performs better at first (years 1994-5), but as more data is seen, the Baseline worsens while the Protagonist improves. This verb-only narrative model shows a 36.5% improvement over the baseline trained on all years. Results from the full Typed Deps model, not comparable to the baseline, parallel the Protagonist results, improving as more data is seen (average ranked position of 1908 with all the training data). We also ran the experiment without OpenNLP coreference, and instead used exact and substring matching for coreference resolution. This showed a 5.7% decrease in the verb-only results. These results show that a protagonist greatly assists in narrative judgements. The model proposed in the previous section is designed to learn the major subevents in a narrative chain, but not how these events are ordered. In this section we extend the model to learn a partial temporal ordering of the events. There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships. The currently highest performing of these on raw data is the model of temporal labeling described in our previous work (Chambers et al., 2007). Other approaches have depended on hand tagged features. Chambers et al. (2007) shows 59.4% accuracy on the classification task for six possible relations between pairs of events: before, immediately-before, included-by, simultaneous, begins and ends. We focus on the before relation because the others are less relevant to our immediate task. We combine immediately-before with before, and merge the other four relations into an other category. At the binary task of determining if one event is before or other, we achieve 72.1% accuracy on Timebank. The above approach is a two-stage machine learning architecture. In the first stage, the model uses supervised machine learning to label temporal attributes of events, including tense, grammatical aspect, and aspectual class. This first stage classifier relies on features such as neighboring part of speech tags, neighboring auxiliaries and modals, and WordNet synsets. We use SVMs (Chambers et al. (2007) uses Naive Bayes) and see minor performance boosts on Timebank. These imperfect classifications, combined with other linguistic features, are then used in a second stage to classify the temporal relationship between two events. Other features include event-event syntactic properties such as the syntactic dominance relations between the two events, as well as new bigram features of tense, aspect and class (e.g. “present past” if the first event is in the present, and the second past), and whether the events occur in the same or different sentences. We use the entire Timebank Corpus as supervised training data, condensing the before and immediately-before relations into one before relation. The remaining relations are merged into other. The vast majority of potential event pairs in Timebank are unlabeled. These are often none relations (events that have no explicit relation) or as is often the case, overlap relations where the two events have no Timebank-defined ordering but overlap in time. Even worse, many events do have an ordering, but they were not tagged by the human annotators. This could be due to the overwhelming task of temporal annotation, or simply because some event orderings are deemed more important than others in understanding the document. We consider all untagged relations as other, and experiment with including none, half, and all of them in training. Taking a cue from Mani et al. (2006), we also increased Timebank’s size by applying transitivity rules to the hand labeled data. The following is an example of the applied transitive rule: if run BEFORE fall and fall BEFORE injured then run BEFORE injured This increases the number of relations from 37519 to 45619. Perhaps more importantly for our task, of all the added relations, the before relation is added the most. We experimented with original vs. expanded Timebank and found the expanded performed slightly worse. The decline may be due to poor transitivity additions, as several Timebank documents contain inconsistent labelings. All reported results are from training without transitivity. We classify the Gigaword Corpus in two stages, once for the temporal features on each event (tense, grammatical aspect, aspectual class), and once between all pairs of events that share arguments. This allows us to classify the before/other relations between all potential narrative events. The first stage is trained on Timebank, and the second is trained using the approach described above, varying the size of the none training relations. Each pair of events in a gigaword document that share a coreferring argument is treated as a separate ordering classification task. We count the resulting number of labeled before relations between each verb/dependency pair. Processing the entire corpus produces a database of event pair counts where confidence of two generic events A and B can be measured by comparing how many before labels have been seen versus their inverted order B and A5. We want to evaluate temporal order at the narrative level, across all events within a chain. We envision narrative chains being used for tasks of coherence, among other things, and so it is desired to evaluate temporal decisions within a coherence framework. Along these lines, our test set uses actual narrative chains from documents, hand labeled for a partial ordering. We evaluate coherence of these true chains against a random ordering. The task is thus deciding which of the two chains is most coherent, the original or the random (baseline 50%)? We generated up to 300 random orderings for each test document, averaging the accuracy across all. Our evaluation data is the same 69 documents used in the test set for learning narrative relations. The chain from each document is hand identified and labeled for a partial ordering using only the before relation. Ordering was done by the authors and all attempts were made to include every before relation that exists in the document, or that could be deduced through transitivity rules. Figure 4 shows an example and its full reversal, although the evaluation uses random orderings. Each edge is a distinct before relation and is used in the judgement score. The coherence score for a partially ordered narrative chain is the sum of all the relations that our classified corpus agrees with, weighted by how certain we are. If the gigaword classifications disagree, a weighted negative score is given. Confidence is based on a logarithm scale of the difference between the counts of before and after classifications. Formally, the score is calculated as the following: where E is the set of all event pairs, B(i, j) is how many times we classified events i and j as before in Gigaword, and D(i, j) _ |B(i, j) − B(j,i)|. The relation i0j indicates that i is temporally before j. Out approach gives higher scores to orders that coincide with the pairwise orderings classified in our gigaword training data. The results are shown in figure 5. Of the 69 chains, 6 did not have any ordered events and were removed from the evaluation. We generated (up to) 300 random orderings for each of the remaining 63. We report 75.2% accuracy, but 22 of the 63 had 5 or fewer pairs of ordered events. Figure 5 therefore shows results from chains with more than 5 pairs, and also 10 or more. As we would hope, the accuracy improves the larger the ordered narrative chain. We achieve 89.0% accuracy on the 24 documents whose chains most progress through time, rather than chains that are difficult to order with just the before relation. Training without none relations resulted in high recall for before decisions. Perhaps due to data sparsity, this produces our best results as reported above. Up till this point, we have learned narrative relations across all possible events, including their temporal order. However, the discrete lists of events for which Schank scripts are most famous have not yet been constructed. We intentionally did not set out to reproduce explicit self-contained scripts in the sense that the ‘restaurant script’ is complete and cannot include other events. The name narrative was chosen to imply a likely order of events that is common in spoken and written retelling of world events. Discrete sets have the drawback of shutting out unseen and unlikely events from consideration. It is advantageous to consider a space of possible narrative events and the ordering within, not a closed list. However, it is worthwhile to construct discrete narrative chains, if only to see whether the combination of event learning and ordering produce scriptlike structures. This is easily achievable by using the PMI scores from section 4 in an agglomerative clustering algorithm, and then applying the ordering relations from section 5 to produce a directed graph. Figures 6 and 7 show two learned chains after clustering and ordering. Each arrow indicates a before relation. Duplicate arrows implied by rules of transitivity are removed. Figure 6 is remarkably accurate, and figure 7 addresses one of the chains from our introduction, the employment narrative. The core employment events are accurate, but clustering included life events (born, died, graduated) from obituaries of which some temporal information is incorrect. The Timebank corpus does not include obituaries, thus we suffer from sparsity in training data.","Up till this point, we have learned narrative relations across all possible events, including their temporal order. However, the discrete lists of events for which Schank scripts are most famous have not yet been constructed. We intentionally did not set out to reproduce explicit self-contained scripts in the sense that the ‘restaurant script’ is complete and cannot include other events. The name narrative was chosen to imply a likely order of events that is common in spoken and written retelling of world events. Discrete sets have the drawback of shutting out unseen and unlikely events from consideration. It is advantageous to consider a space of possible narrative events and the ordering within, not a closed list. However, it is worthwhile to construct discrete narrative chains, if only to see whether the combination of event learning and ordering produce scriptlike structures. This is easily achievable by using the PMI scores from section 4 in an agglomerative clustering algorithm, and then applying the ordering relations from section 5 to produce a directed graph. Figures 6 and 7 show two learned chains after clustering and ordering. Each arrow indicates a before relation. Duplicate arrows implied by rules of transitivity are removed. Figure 6 is remarkably accurate, and figure 7 addresses one of the chains from our introduction, the employment narrative. The core employment events are accurate, but clustering included life events (born, died, graduated) from obituaries of which some temporal information is incorrect. The Timebank corpus does not include obituaries, thus we suffer from sparsity in training data."
26,"A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.","A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP community has also moved in this direction by relaxing the amount of supervision to the setting where each sentence is paired with a small set of candidate meanings (Kate and Mooney, 2007; Chen and Mooney, 2008). The goal of this paper is to reduce the amount of supervision even further. We assume that we are given a world state represented by a set of records along with a text, an unsegmented sequence of words. For example, in the weather forecast domain (Section 2.2), the text is the weather report, and the records provide a structured representation of the temperature, sky conditions, etc. In this less restricted data setting, we must resolve multiple ambiguities: (1) the segmentation of the text into utterances; (2) the identification of relevant facts, i.e., the choice of records and aspects of those records; and (3) the alignment of utterances to facts (facts are the meaning representations of the utterances). Furthermore, in some of our examples, much of the world state is not referenced at all in the text, and, conversely, the text references things which are not represented in our world state. This increased amount of ambiguity and noise presents serious challenges for learning. To cope with these challenges, we propose a probabilistic generative model that treats text segmentation, fact identification, and alignment in a single unified framework. The parameters of this hierarchical hidden semi-Markov model can be estimated efficiently using EM. We tested our model on the task of aligning text to records in three different domains. The first domain is Robocup sportscasting (Chen and Mooney, 2008). Their best approach (KRISPER) obtains 67% F1; our method achieves 76.5%. This domain is simplified in that the segmentation is known. The second domain is weather forecasts, for which we created a new dataset. Here, the full complexity of joint segmentation and alignment arises. Nonetheless, we were able to obtain reasonable results on this task. The third domain we considered is NFL recaps (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007). The language used in this domain is richer by orders of magnitude, and much of it does not reference the world state. Nonetheless, taking the first unsupervised approach to this problem, we were able to make substantial progress: We achieve an F1 of 53.2%, which closes over half of the gap between a heuristic baseline (26%) and supervised systems (68%–80%). Our goal is to learn the correspondence between a text w and the world state s it describes. We use the term scenario to refer to such a (w, s) pair. The text is simply a sequence of words w = (wi, ... , w|w|). We represent the world state s as a set of records, where each record r E s is described by a record type r.t E T and a tuple of field values r.v = (r.vi, ... , r.vm).1 For example, temperature is a record type in the weather domain, and it has four fields: time, min, mean, and max. The record type r.t E T specifies the field type r.tf E {INT, STR, CAT} of each field value r.vf, f = 1, ... , m. There are three possible field types—integer (INT), string (STR), and categorical (CAT)—which are assumed to be known and fixed. Integer fields represent numeric properties of the world such as temperature, string fields represent surface-level identifiers such as names of people, and categorical fields represent discrete concepts such as score types in football (touchdown, field goal, and safety). The field type determines the way we expect the field value to be rendered in words: integer fields can be numerically perturbed, string fields can be spliced, and categorical fields are represented by open-ended word distributions, which are to be learned. See Section 3.3 for details. In this domain, a Robocup simulator generates the state of a soccer game, which is represented by a set of event records. For example, the record pass(arg1=pink1,arg2=pink5) denotes a passing event; this type of record has two fields: arg1 (the actor) and arg2 (the recipient). As the game is progressing, humans interject commentaries about notable events in the game, e.g., pink] passes back to pink5 near the middle of the field. All of the fields in this domain are categorical, which means there is no a priori association between the field value pink1 and the word pink]. This degree of flexibility is desirable because pink] is sometimes referred to as pink goalie, a mapping which does not arise from string operations but must instead be learned. We used the dataset created by Chen and Mooney (2008), which contains 1919 scenarios from the 2001–2004 Robocup finals. Each scenario consists of a single sentence representing a fragment of a commentary on the game, paired with a set of candidate records. In the annotation, each sentence corresponds to at most one record (possibly one not in the candidate set, in which case we automatically get that sentence wrong). See Figure 1(a) for an example and Table 1 for summary statistics on the dataset. In this domain, the world state contains detailed information about a local weather forecast and the text is a short forecast report (see Figure 1(b) for an example). To create the dataset, we collected local weather forecasts for 3,753 cities in the US (those with population at least 10,000) over three days (February 7–9, 2009) from www.weather.gov. For each city and date, we created two scenarios, one for the day forecast and one for the night forecast. The forecasts consist of hour-by-hour measurements of temperature, wind speed, sky cover, chance of rain, etc., which represent the underlying world state. This world state is summarized by records which aggregate measurements over selected time intervals. For example, one of the records states the minimum, average, and maximum temperature from 5pm to 6am. This aggregation process produced 22,146 scenarios, each containing |s |= 36 multi-field records. There are 12 record types, each consisting of only integer and categorical fields. To annotate the data, we split the text by punctuation into lines and labeled each line with the records to which the line refers. These lines are used only for evaluation and are not part of the model (see Section 5.1 for further discussion). The weather domain is more complex than the Robocup domain in several ways: The text w is longer, there are more candidate records, and most notably, w references multiple records (5.8 on average), so the segmentation of w is unknown. See Table 1 for a comparison of the two datasets. In this domain, each scenario represents a single NFL football game (see Figure 1(c) for an example). The world state (the things that happened during the game) is represented by database tables, e.g., scoring summary, team comparison, drive chart, play-by-play, etc. Each record is a database entry, for instance, the receiving statistics for a certain player. The text is the recap of the game— an article summarizing the game highlights. The dataset we used was collected by Barzilay and Lapata (2005). The data includes 466 games during the 2003–2004 NFL season. 78 of these games were annotated by Snyder and Barzilay (2007), who aligned each sentence to a set of records. This domain is by far the most complicated of the three. Many records corresponding to inconsequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent using our simple model. Fortunately, most of the fields are integer fields or string fields (generally names or brief descriptions), which provide important anchor points for learning the correspondences. Nonetheless, the same names and numbers occur in multiple records, so there is still uncertainty about which record is referenced by a given sentence. To learn the correspondence between a text w and a world state s, we propose a generative model p(w I s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, which proceeds in three stages (Figure 2 shows the corresponding graphical model): 1. Record choice: choose a sequence of records r = (r1, ... , r|r|) to describe, where each ri E s. 2. Field choice: for each chosen record ri, select a sequence of fields fi = (fi1, . . . , fi|fi|), where each fij E 11, ... , m}. 3. Word choice: for each chosen field fij, choose a number cij > 0 and generate a sequence of cij words. The observed text w is the terminal yield formed by concatenating the sequences of words of all fields generated; note that the segmentation of w provided by c = 1cij} is latent. Think of the words spanned by a record as constituting an utterance with a meaning representation given by the record and subset of fields chosen. Formally, our probabilistic model places a distribution over (r, f, c, w) and factorizes according to the three stages as follows: p(r, f, c, w  |s) = p(r  |s)p(f  |r)p(c, w  |r, f, s) The following three sections describe each of these stages in more detail. The record choice model specifies a distribution over an ordered sequence of records r = (r1, ... , r|r|), where each record ri E s. This model is intended to capture two types of regularities in the discourse structure of language. The first is salience, that is, some record types are simply more prominent than others. For example, in the NFL domain, 70% of scoring records are mentioned whereas only 1% of punting records are mentioned. The second is the idea of local coherence, that is, the order in which one mentions records tend to follow certain patterns. For example, in the weather domain, the sky conditions are generally mentioned first, followed by temperature, and then wind speed. To capture these two phenomena, we define a Markov model on the record types (and given the record type, a record is chosen uniformly from the set of records with that type): where s(t) def = 1r E s : r.t = t} and r0.t is a dedicated START record type.2 We also model the transition of the final record type to a designated STOP record type in order to capture regularities about the types of records which are described last. More sophisticated models of coherence could also be employed here (Barzilay and Lapata, 2008). We assume that s includes a special null record whose type is NULL, responsible for generating parts of our text which do not refer to any real records. Each record type t E T has a separate field choice model, which specifies a distribution over a sequence of fields. We want to capture salience and coherence at the field level like we did at the record level. For instance, in the weather domain, the minimum and maximum fields of a temperature record are mentioned whereas the average is not. In the Robocup domain, the actor typically precedes the recipient in passing event records. Formally, we have a Markov model over the fields:3 Each record type has a dedicated null field with its own multinomial distribution over words, intended to model words which refer to that record type in general (e.g., the word passes for passing records). We also model transitions into the first field and transitions out of the final field with special START and STOP fields. This Markov structure allows us to capture a few elements of rudimentary syntax. We arrive at the final component of our model, which governs how the information about a particular field of a record is rendered into words. For each field fij, we generate the number of words cij from a uniform distribution over 11, 2,... , Cmax}, where Cmax is set larger than the length of the longest text we expect to see. Conditioned on the fields f, the words w are generated independently:4 where r(k) and f(k) are the record and field responsible for generating word wk, as determined by the segmentation c. The word choice model pw(w I t, v) specifies a distribution over words given the field type t and field value v. This distribution is a mixture of a global backoff distribution over words and a field-specific distribution which depends on the field type t. Although we designed our word choice model to be relatively general, it is undoubtedly influenced by the three domains. However, we can readily extend or replace it with an alternative if desired; this modularity is one principal benefit of probabilistic modeling. Integer Fields (t = INT) For integer fields, we want to capture the intuition that a numeric quantity v is rendered in the text as a word which is possibly some other numerical value w due to stylistic factors. Sometimes the exact value v is used (e.g., in reporting football statistics). Other times, it might be customary to round v (e.g., wind speeds are typically rounded to a multiple of 5). In other cases, there might just be some unexplained error, where w deviates from v by some noise c+ = w − v > 0 or c− = v − w > 0. We model c+ and c− as geometric distributions.5 In summary, we allow six possible ways of generating the word w given v: v rv15 LvI5 round5(v) v − c− v + c+ Separate probabilities for choosing among these possibilities are learned for each field type (see Figure 3 for an example). String Fields (t = STR) Strings fields are intended to represent values which we expect to be realized in the text via a simple surface-level transformation. For example, a name field with value v = Moe Williams is sometimes referenced in the text by just Williams. We used a simple generic model of rendering string fields: Let w be a word chosen uniformly from those in v. Categorical Fields (t = CAT) Unlike string fields, categorical fields are not tied down to any lexical representation; in fact, the identities of the categorical field values are irrelevant. For each categorical field f and possible value v, we have a , clear mostly sunny partly, cloudy increasing mostly cloudy, partly of inch an possible new a rainfall herence structure at both the record and field levels. To quantify the benefits of incorporating these two aspects, we compare our full model with two simpler variants. skyCover.mode in the weather domain. It is interesting to note that skyCover=75-100 is so highly correlated with rain that the model learns to connect an overcast sky in the world to the indication of rain in the text. separate multinomial distribution over words from which w is drawn. An example of a categorical field is skyCover.mode in the weather domain, which has four values: 0-25, 25-50, 50-75, and 75-100. Table 2 shows the top words for each of these field values learned by our model. Our learning and inference methodology is a fairly conventional application of Expectation Maximization (EM) and dynamic programming. The input is a set of scenarios D, each of which is a text w paired with a world state s. We maximize the marginal likelihood of our data, summing out the latent variables (r, f, c): where 0 are the parameters of the model (all the multinomial probabilities). We use the EM algorithm to maximize (3), which alternates between the E-step and the M-step. In the E-step, we compute expected counts according to the posterior p(r, f, c  |w, s; 0). In the M-step, we optimize the parameters 0 by normalizing the expected counts computed in the E-step. In our experiments, we initialized EM with a uniform distribution for each multinomial and applied add-0.1 smoothing to each multinomial in the M-step. As with most complex discrete models, the bulk of the work is in computing expected counts under p(r, f, c  |w, s; 0). Formally, our model is a hierarchical hidden semi-Markov model conditioned on s. Inference in the E-step can be done using a dynamic program similar to the inside-outside algorithm. Two important aspects of our model are the segmentation of the text and the modeling of the coIn the annotated data, each text w has been divided into a set of lines. These lines correspond to clauses in the weather domain and sentences in the Robocup and NFL domains. Each line is annotated with a (possibly empty) set of records. Let A be the gold set of these line-record alignment pairs. To evaluate a learned model, we compute the Viterbi segmentation and alignment (argmaxr,f,c p(r, f, c  |w, s)). We produce a predicted set of line-record pairs A' by aligning a line to a record ri if the span of (the utterance corresponding to) ri overlaps the line. The reason we evaluate indirectly using lines rather than using utterances is that it is difficult to annotate the segmentation of text into utterances in a simple and consistent manner. We compute standard precision, recall, and F1 of A' with respect to A. Unless otherwise specified, performance is reported on all scenarios, which were also used for training. However, we did not tune any hyperparameters, but rather used generic values which worked well enough across all three domains. We ran 10 iterations of EM on Models 1–3. Table 3 shows that performance improves with increased model sophistication. We also compare our model to the results of Chen and Mooney (2008) in Table 4. Figure 4 provides a closer look at the predictions made by each of our three models for a particular example. Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word. Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this). Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors. Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006). For example, the ballstopped record occurs frequently but is never mentioned in the text. At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation. As a result, our model incorrectly chooses to align the two. For the weather domain, staged training was necessary to get good results. For Model 1, we ran 15 iterations of EM. For Model 2, we ran 5 iterations of EM on Model 1, followed by 10 iterations on Model 2. For Model 3, we ran 5 iterations of Model 1, 5 iterations of a simplified variant of Model 3 where records were chosen independently, and finally, 5 iterations of Model 3. When going from one model to another, we used the final posterior distributions of the former to initialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference. Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain. Figure 5 shows the predictions of the three models on an example. Model 1 is only able to form isolated (but not completely inaccurate) associations. By modeling segmentation, Model 2 accounts for the intermediate words, but errors are still made due to the lack of Markov structure. Model 3 remedies this. However, unexpected structures are sometimes learned. For example, the temperature.time=6-21 field indicates daytime, which happens to be perfectly correlated with the word high, although high intuitively should be associated with the temperature.max field. In these cases of high correlation (Table 2 provides another example), it is very difficult to recover the proper alignment without additional supervision. In order to scale up our models to the NFL domain, we first pruned for each sentence the records which have either no numerical values (e.g., 23, 23-10, 2/4) nor name-like words (e.g., those that appear only capitalized in the text) in common. This eliminated all but 1.5% of the record candidates per sentence, while maintaining an oracle alignment F1 score of 88.7. Guessing a single random record for each sentence yields an F1 of 12.0. A reasonable heuristic which uses weighted number- and string-matching achieves 26.7. Due to the much greater complexity of this domain, Model 2 was easily misled as it tried without success to find a coherent segmentation of the fields. We therefore created a variant, Model 2’, where we constrained each field to generate exactly one word. To train Model 2’, we ran 5 iterations of EM where each sentence is assumed to have exactly one record, followed by 5 iterations where the constraint was relaxed to also allow record boundaries at punctuation and the word and. We did not experiment with Model 3 since the discourse structure on records in this domain is not at all governed by a simple Markov model on record types—indeed, most regions do not refer to any records at all. We also fixed the backoff probability to 0.1 instead of learning it and enforced zero numerical deviation on integer field values. Model 2’ achieved an F1 of 39.9, an improvement over Model 1, which attained 32.8. Inspection of the errors revealed the following problem: The alignment task requires us to sometimes align a sentence to multiple redundant records (e.g., play and score) referenced by the same part of the text. However, our model generates each part of text from only one record, and thus it can only allow an alignment to one record.7 To cope with this incompatibility between the data and our notion of semantics, we used the following solution: We divided the records into three groups by type: play, score, and other. Each group has a copy of the model, but we enforce that they share the same segmentation. We also introduce a potential that couples the presence or absence of records across groups on the same segment to capture regular cooccurrences between redundant records. Table 6 shows our results. With groups, we achieve an F1 of 53.2. Though we still trail supervised techniques, which attain numbers in the 68–80 range, we have made substantial progress over our baseline using an unsupervised method. Furthermore, our model provides a more detailed analysis of the correspondence between the world state and text, rather than just producing a single alignment decision. Most of the remaining errors made by our model are due to a lack of calibration. Sometimes, our false positives are close calls where a sentence indirectly references a record, and our model predicts the alignment whereas the annotation standard does not. We believe that further progress is possible with a richer model.","Two important aspects of our model are the segmentation of the text and the modeling of the coIn the annotated data, each text w has been divided into a set of lines. These lines correspond to clauses in the weather domain and sentences in the Robocup and NFL domains. Each line is annotated with a (possibly empty) set of records. Let A be the gold set of these line-record alignment pairs. To evaluate a learned model, we compute the Viterbi segmentation and alignment (argmaxr,f,c p(r, f, c  |w, s)). We produce a predicted set of line-record pairs A' by aligning a line to a record ri if the span of (the utterance corresponding to) ri overlaps the line. The reason we evaluate indirectly using lines rather than using utterances is that it is difficult to annotate the segmentation of text into utterances in a simple and consistent manner. We compute standard precision, recall, and F1 of A' with respect to A. Unless otherwise specified, performance is reported on all scenarios, which were also used for training. However, we did not tune any hyperparameters, but rather used generic values which worked well enough across all three domains. We ran 10 iterations of EM on Models 1–3. Table 3 shows that performance improves with increased model sophistication. We also compare our model to the results of Chen and Mooney (2008) in Table 4. Figure 4 provides a closer look at the predictions made by each of our three models for a particular example. Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word. Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this). Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors. Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006). For example, the ballstopped record occurs frequently but is never mentioned in the text. At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation. As a result, our model incorrectly chooses to align the two. For the weather domain, staged training was necessary to get good results. For Model 1, we ran 15 iterations of EM. For Model 2, we ran 5 iterations of EM on Model 1, followed by 10 iterations on Model 2. For Model 3, we ran 5 iterations of Model 1, 5 iterations of a simplified variant of Model 3 where records were chosen independently, and finally, 5 iterations of Model 3. When going from one model to another, we used the final posterior distributions of the former to initialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference. Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain. Figure 5 shows the predictions of the three models on an example. Model 1 is only able to form isolated (but not completely inaccurate) associations. By modeling segmentation, Model 2 accounts for the intermediate words, but errors are still made due to the lack of Markov structure. Model 3 remedies this. However, unexpected structures are sometimes learned. For example, the temperature.time=6-21 field indicates daytime, which happens to be perfectly correlated with the word high, although high intuitively should be associated with the temperature.max field. In these cases of high correlation (Table 2 provides another example), it is very difficult to recover the proper alignment without additional supervision. In order to scale up our models to the NFL domain, we first pruned for each sentence the records which have either no numerical values (e.g., 23, 23-10, 2/4) nor name-like words (e.g., those that appear only capitalized in the text) in common. This eliminated all but 1.5% of the record candidates per sentence, while maintaining an oracle alignment F1 score of 88.7. Guessing a single random record for each sentence yields an F1 of 12.0. A reasonable heuristic which uses weighted number- and string-matching achieves 26.7. Due to the much greater complexity of this domain, Model 2 was easily misled as it tried without success to find a coherent segmentation of the fields. We therefore created a variant, Model 2’, where we constrained each field to generate exactly one word. To train Model 2’, we ran 5 iterations of EM where each sentence is assumed to have exactly one record, followed by 5 iterations where the constraint was relaxed to also allow record boundaries at punctuation and the word and. We did not experiment with Model 3 since the discourse structure on records in this domain is not at all governed by a simple Markov model on record types—indeed, most regions do not refer to any records at all. We also fixed the backoff probability to 0.1 instead of learning it and enforced zero numerical deviation on integer field values. Model 2’ achieved an F1 of 39.9, an improvement over Model 1, which attained 32.8. Inspection of the errors revealed the following problem: The alignment task requires us to sometimes align a sentence to multiple redundant records (e.g., play and score) referenced by the same part of the text. However, our model generates each part of text from only one record, and thus it can only allow an alignment to one record.7 To cope with this incompatibility between the data and our notion of semantics, we used the following solution: We divided the records into three groups by type: play, score, and other. Each group has a copy of the model, but we enforce that they share the same segmentation. We also introduce a potential that couples the presence or absence of records across groups on the same segment to capture regular cooccurrences between redundant records. Table 6 shows our results. With groups, we achieve an F1 of 53.2. Though we still trail supervised techniques, which attain numbers in the 68–80 range, we have made substantial progress over our baseline using an unsupervised method. Furthermore, our model provides a more detailed analysis of the correspondence between the world state and text, rather than just producing a single alignment decision. Most of the remaining errors made by our model are due to a lack of calibration. Sometimes, our false positives are close calls where a sentence indirectly references a record, and our model predicts the alignment whereas the annotation standard does not. We believe that further progress is possible with a richer model."
27,"We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.","We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets. As is common for many natural language processing problems, the state-of-the-art in noun phrase (NP) coreference resolution is typically quantified based on system performance on manually annotated text corpora. In spite of the availability of several benchmark data sets (e.g. MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution. In particular, it remains difficult to assess the effectiveness of different coreference resolution approaches, even in relative terms. For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems. In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver. It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation. Results vary widely across data sets. Coreference resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 6070% on the MUC 6 and 7 data sets (e.g. Soon et al. (2001) and Yang et al. (2003)). What accounts for these differences? Are they due to properties of the documents or domains? Or do differences in the coreference task definitions account for the differences in performance? Given a new text collection and domain, what level of performance should we expect? We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by state-of-the-art systems. Except for some fairly general statements, for example that proper names are easier to resolve than pronouns, which are easier than common nouns, there has been little analysis of which aspects of the problem have achieved success and which remain elusive. The goal of this paper is to take initial steps toward making sense of the disparate performance results reported for NP coreference resolution. For our investigations, we employ a state-of-the-art classification-based NP coreference resolver and focus on the widely used MUC and ACE coreference resolution data sets. We hypothesize that performance variation within and across coreference resolvers is, at least in part, a function of (1) the (sometimes unstated) assumptions in evaluation methodologies, and (2) the relative difficulty of the benchmark text corpora. With these in mind, Section 3 first examines three subproblems that play an important role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We quantitatively measure the impact of each of these subproblems on coreference resolution performance as a whole. Our results suggest that the availability of accurate detectors for anaphoricity or coreference elements could substantially improve the performance of state-ofthe-art resolvers, while improvements to named entity recognition likely offer little gains. Our results also confirm that the assumptions adopted in some evaluations dramatically simplify the resolution task, rendering it an unrealistic surrogate for the original problem. In Section 4, we quantify the difficulty of a text corpus with respect to coreference resolution by analyzing performance on different resolution classes. Our goals are twofold: to measure the level of performance of state-of-the-art coreference resolvers on different types of anaphora, and to develop a quantitative measure for estimating coreference resolution performance on new data sets. We introduce a coreference performance prediction (CPP) measure and show that it accurately predicts the performance of our coreference resolver. As a side effect of our research, we provide a new set of much-needed benchmark results for coreference resolution under common sets of fully-specified evaluation assumptions. This paper studies the six most commonly used coreference resolution data sets. Two of those are from the MUC conferences (MUC-6, 1995; MUC7, 1997) and four are from the Automatic Content Evaluation (ACE) Program (NIST, 2004). In this section, we outline the differences between the MUC and ACE coreference resolution tasks, and define terminology for the rest of the paper. Noun phrase coreference resolution is the process of determining whether two noun phrases (NPs) refer to the same real-world entity or concept. It is related to anaphora resolution: a NP is said to be anaphoric if it depends on another NP for interpretation. Consider the following: John Hall is the new CEO. He starts on Monday. Here, he is anaphoric because it depends on its antecedent, John Hall, for interpretation. The two NPs also corefer because each refers to the same person, JOHN HALL. As discussed in depth elsewhere (e.g. van Deemter and Kibble (2000)), the notions of coreference and anaphora are difficult to define precisely and to operationalize consistently. Furthermore, the connections between them are extremely complex and go beyond the scope of this paper. Given these complexities, it is not surprising that the annotation instructions for the MUC and ACE data sets reflect different interpretations and simplifications of the general coreference relation. We outline some of these differences below. Syntactic Types. To avoid ambiguity, we will use the term coreference element (CE) to refer to the set of linguistic expressions that participate in the coreference relation, as defined for each of the MUC and ACE tasks.1 At times, it will be important to distinguish between the CEs that are included in the gold standard — the annotated CEs — from those that are generated by the coreference resolution system — the extracted CEs. At a high level, both the MUC and ACE evaluations define CEs as nouns, pronouns, and noun phrases. However, the MUC definition excludes (1) “nested” named entities (NEs) (e.g. “America” in “Bank of America”), (2) relative pronouns, and (3) gerunds, but allows (4) nested nouns (e.g. “union” in “union members”). The ACE definition, on the other hand, includes relative pronouns and gerunds, excludes all nested nouns that are not themselves NPs, and allows premodifier NE mentions of geo-political entities and locations, such as “Russian” in “Russian politicians”. Semantic Types. ACE restricts CEs to entities that belong to one of seven semantic classes: person, organization, geo-political entity, location, facility, vehicle, and weapon. MUC has no semantic restrictions. Singletons. The MUC data sets include annotations only for CEs that are coreferent with at least one other CE. ACE, on the other hand, permits “singleton” CEs, which are not coreferent with any other CE in the document. These substantial differences in the task definitions (summarized in Table 1) make it extremely difficult to compare performance across the MUC and ACE data sets. In the next section, we take a closer look at the coreference resolution task, analyzing the impact of various subtasks irrespective of the data set differences. Coreference resolution is a complex task that requires solving numerous non-trivial subtasks such as syntactic analysis, semantic class tagging, pleonastic pronoun identification and antecedent identification to name a few. This section examines the role of three such subtasks — named entity recognition, anaphoricity determination, and coreference element detection — in the performance of an end-to-end coreference resolution system. First, however, we describe the coreference resolver that we use for our study. We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions). This system is a classification-based coreference resolver, modeled after the systems of Ng and Cardie (2002b) and Bengtson and Roth (2008). First it classifies pairs of CEs as coreferent or not coreferent, pairing each identified CE with all preceding CEs. The CEs are then clustered into coreference chains2 based on the pairwise decisions. RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extraction, classification, and clustering. We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09. Preprocessing. The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to the source texts. The OpenNLP toolkit (Baldridge, J., 2005) performs tokenization, sentence splitting, and part-of-speech tagging. The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations. We employ the Stanford CRF-based Named Entity Recognizer (Finkel et al., 2004) for named entity tagging. With these preprocessing components, RECONCILEACL09 uses heuristics to correctly extract approximately 90% of the annotated CEs for the MUC and ACE data sets. Feature Set. To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details. Lexical (9): String-based comparisons of the two CEs, such as exact string matching and head noun matching. Proximity (5): Sentence and paragraph-based measures of the distance between two CEs. Grammatical (28): A wide variety of syntactic properties of the CEs, either individually or as a pair. These features are based on part-of-speech tags, parse trees, or dependency relations. For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs are in an appositive construction. Semantic (19): Capture semantic information about one or both NPs such as tests for gender and animacy, semantic compatibility based on WordNet, and semantic comparisons of NE types. Classification and Clustering. We configure RECONCILEACL09 to use the Averaged Perceptron learning algorithm (Freund and Schapire, 1999) and to employ single-link clustering (i.e. transitive closure) to generate the final partitioning.3 Our experiments rely on the MUC and ACE corpora. For ACE, we use only the newswire portion because it is closest in composition to the MUC corpora. Statistics for each of the data sets are shown in Table 2. When available, we use the standard test/train split. Otherwise, we randomly split the data into a training and test set following a 70/30 ratio. Scoring Algorithms. We evaluate using two common scoring algorithms4 — MUC and B3. The MUC scoring algorithm (Vilain et al., 1995) computes the F1 score (harmonic mean) of precision and recall based on the identifcation of unique coreference links. We use the official MUC scorer implementation for the two MUC corpora and an equivalent implementation for ACE. The B3 algorithm (Bagga and Baldwin, 1998) computes a precision and recall score for each CE: where RCe is the coreference chain to which ce is assigned in the response (i.e. the system-generated output) and KCe is the coreference chain that contains ce in the key (i.e. the gold standard). Precision and recall for a set of documents are computed as the mean over all CEs in the documents and the F1 score of precision and recall is reported. B3 Complications. Unlike the MUC score, which counts links between CEs, B3 presumes that the gold standard and the system response are clusterings over the same set of CEs. This, of course, is not the case when the system automatically identifies the CEs, so the scoring algorithm requires a mapping between extracted and annotated CEs. We will use the term twin(ce) to refer to the unique annotated/extracted CE to which the extracted/annotated CE is matched. We say that a CE is twinless (has no twin) if no corresponding CE is identified. A twinless extracted CE signals that the resolver extracted a spurious CE, while an annotated CE is twinless when the resolver fails to extract it. Unfortunately, it is unclear how the B3 score should be computed for twinless CEs. Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient — it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident. We propose two different ways to deal with twinless CEs for B3. One option, B3all, retains all twinless extracted CEs. It computes the preci4We also experimented with the CEAF score (Luo, 2005), but excluded it due to difficulties dealing with the extracted, rather than annotated, CEs. CEAF assigns a zero score to each twinless extracted CE and weights all coreference chains equally, irrespective of their size. As a result, runs with extracted CEs exhibit very low CEAF precision, leading to unreliable scores. sion as above when ce has a twin, and computes the precision as 1/|RCe |if ce is twinless. (Similarly, recall(ce) = 1/|KC  |if ce is twinless.) The second option, B30, discards twinless extracted CEs, but penalizes recall by setting recall(ce) = 0 for all twinless annotated CEs. Thus, B30 presumes that all twinless extracted CEs are spurious. Results. Table 3, box 1 shows the performance of RECONCILEACL09 using a default (0.5) coreference classifier threshold. The MUC score is highest for the MUC6 data set, while the four ACE data sets show much higher B3 scores as compared to the two MUC data sets. The latter occurs because the ACE data sets include singletons. The classification threshold, however, can be gainfully employed to control the trade-off between precision and recall. This has not traditionally been done in learning-based coreference resolution research — possibly because there is not much training data available to sacrifice as a validation set. Nonetheless, we hypothesized that estimating a threshold from just the training data might be effective. Our results (BASELINE box in Table 3) indicate that this indeed works well.5 With the exception of MUC6, results on all data sets and for all scoring algorithms improve; moreover, the scores approach those for runs using an optimal threshold (box 3) for the experiment as determined by using the test set. In all remaining experiments, we learn the threshold from the training set as in the BASELINE system. Below, we resume our investigation of the role of three coreference resolution subtasks and measure the impact of each on overall performance. Previous work has shown that resolving coreference between proper names is relatively easy (e.g. Kameyama (1997)) because string matching functions specialized to the type of proper name (e.g. person vs. location) are quite accurate. Thus, we would expect a coreference resolution system to depend critically on its Named Entity (NE) extractor. On the other hand, state-of-the-art NE taggers are already quite good, so improving this component may not provide much additional gain. To study the influence of NE recognition, we replace the system-generated NEs of RECONCILEACL09 with gold-standard NEs and retrain the coreference classifier. Results for each of the data sets are shown in box 4 of Table 3. (No gold standard NEs are available for MUC7.) Comparison to the BASELINE system (box 2) shows that using gold standard NEs leads to improvements on all data sets with the exception of ACE2 and ACE05, on which performance is virtually unchanged. The improvements tend to be small, however, between 0.5 to 3 performance points. We attribute this to two factors. First, as noted above, although far from perfect, NE taggers generally perform reasonably well. Second, only 20 to 25% of the coreference element resolutions required for these data sets involve a proper name (see Section 4). Conclusion #1: Improving the performance of NE taggers is not likely to have a large impact on the performance of state-of-the-art coreference resolution systems. We expect CE detection to be an important subproblem for an end-to-end coreference system. Results for a system that assumes perfect CEs are shown in box 5 of Table 3. For these runs, RECONCILEACL09 uses only the annotated CEs for both training and testing. Using perfect CEs solves a large part of the coreference resolution task: the annotated CEs divulge anaphoricity information, perfect NP boundaries, and perfect information regarding the coreference relation defined for the data set. We see that focusing attention on all and only the annotated CEs leads to (often substantial) improvements in performance on all metrics over all data sets, especially when measured using the MUC score. Conclusion #2: Improving the ability of coreference resolvers to identify coreference elements would likely improve the state-of-the-art immensely — by 10-20 points in MUC F1 score and from 2-12 F1 points for B3. This finding explains previously published results that exhibit striking variability when run with annotated CEs vs. system-extracted CEs. On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al., 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)). The goal of the module is to determine whether or not an NP is anaphoric. For example, pleonastic pronouns (e.g. it is raining) are special cases that do not require coreference resolution. Unfortunately, neither the MUC nor the ACE data sets include anaphoricity information for all NPs. Rather, they encode anaphoricity information implicitly for annotated CEs: a CE is considered anaphoric if is not a singleton.6 To study the utility of anaphoricity information, we train and test only on the “anaphoric” extracted CEs, i.e. the extracted CEs that have an annotated twin that is not a singleton. Note that for the MUC datasets all extracted CEs that have twins are considered anaphoric. Results for this experiment (box 6 in Table 3) are similar to the previous experiment using perfect CEs: we observe big improvements across the board. This should not be surprising since the experimental setting is quite close to that for perfect CEs: this experiment also presumes knowledge of when a CE is part of an annotated coreference chain. Nevertheless, we see that anaphoricity infomation is important. First, good anaphoricity identification should reduce the set of extracted CEs making it closer to the set of annotated CEs. Second, further improvements in MUC score for the ACE data sets over the runs using perfect CEs (box 5) reveal that accurately determining anaphoricity can lead to substantial improvements in MUC score. ACE data includes annotations for singleton CEs, so knowling whether an annotated CE is anaphoric divulges additional information. Conclusion #4: An accurate anaphoricity determination component can lead to substantial improvement in coreference resolution performance. Different types of anaphora that have to be handled by coreference resolution systems exhibit different properties. In linguistic theory, binding mechanisms vary for different kinds of syntactic constituents and structures. And in practice, empirical results have confirmed intuitions that different types of anaphora benefit from different classifier features and exhibit varying degrees of difficulty (Kameyama, 1997). However, performance 6Also, the first element of a coreference chain is usually non-anaphoric, but we do not consider that issue here. evaluations rarely include analysis of where stateof-the-art coreference resolvers perform best and worst, aside from general conclusions. In this section, we analyze the behavior of our coreference resolver on different types of anaphoric expressions with two goals in mind. First, we want to deduce the strengths and weaknesses of state-of-the-art systems to help direct future research. Second, we aim to understand why current coreference resolvers behave so inconsistently across data sets. Our hypothesis is that the distribution of different types of anaphoric expressions in a corpus is a major factor for coreference resolution performance. Our experiments confirm this hypothesis and we use our empirical results to create a coreference performance prediction (CPP) measure that successfully estimates the expected level of performance on novel data sets. We study the resolution complexity of a text corpus by defining resolution classes. Resolution classes partition the set of anaphoric CEs according to properties of the anaphor and (in some cases) the antecedent. Previous work has studied performance differences between pronominal anaphora, proper names, and common nouns, but we aim to dig deeper into subclasses of each of these groups. In particular, we distinguish between proper and common nouns that can be resolved via string matching, versus those that have no antecedent with a matching string. Intuitively, we expect that it is easier to resolve the cases that involve string matching. Similarly, we partition pronominal anaphora into several subcategories that we expect may behave differently. We define the following nine resolution classes: Proper Names: Three resolution classes cover CEs that are named entities (e.g. the PERSON, LOCATION, ORGANIZATION and DATE classes for MUC and ACE) and have a prior referent7 in the text. These three classes are distinguished by the type of antecedent that can be resolved against the proper name. Common NPs: Three analogous string match classes cover CEs that have a common noun as a head: (4) CN-e (5) CN-p (6) CN-n. As noted above, resolution classes are defined for annotated CEs. We use the twin relationship to match extracted CEs to annotated CEs and to evaluate performance on each resolution class. To score each resolution class separately, we define a new variant of the MUC scorer. We compute a MUC-RC score (for MUC Resolution Class) for class C as follows: we assume that all CEs that do not belong to class C are resolved correctly by taking the correct clustering for them from the gold standard. Starting with this correct partial clustering, we run our classifier on all ordered pairs of CEs for which the second CE is of class C, essentially asking our coreference resolver to determine whether each member of class C is coreferent with each of its preceding CEs. We then count the number of unique correct/incorrect links that the system introduced on top of the correct partial clustering and compute precision, recall, and F1 score. This scoring function directly measures the impact of each resolution class on the overall MUC score. Table 4 shows the results of our resolution class analysis on the test portions of the six data sets. The # columns show the frequency counts for each resolution class, and the % columns show the distributions of the classes in each corpus (i.e. 17% of all resolutions in the MUC6 corpus were in the PN-e class). The scr columns show the MUCRC score for each resolution class. The right-hand side of Table 4 shows the average distribution and scores across all data sets. These scores confirm our expectations about the relative difficulty of different types of resolutions. For example, it appears that proper names are easier to resolve than common nouns; gendered pronouns are easier than 1st and 2nd person pronouns, which, in turn, are easier than ungendered 3rd person pronouns. Similarly, our intuition is confirmed that many CEs can be accurately resolved based on exact string matching, whereas resolving against antecedents that do not have overlapping strings is much more difficult. The average scores in Table 4 show that performance varies dramatically across the resolution classes, but, on the surface, appears to be relatively consistent across data sets. None of the data sets performs exactly the same, of course, so we statistically analyze whether the behavior of each resolution class is similar across the data sets. For each data set, we compute the correlation between the vector of MUC-RC scores over the resolution classes and the average vector of MUC-RC scores for the remaining five data sets. Table 5 contains the results, which show high correlations (over .90) for all six data sets. These results indicate that the relative performance of the resolution classes is consistent across corpora. Next, we hypothesize that the distribution of resolution classes in a corpus explains (at least partially) why performance varies so much from corpus to corpus. To explore this issue, we create a sure to predict the performance on new data sets. The CPP measure uses the empirical performance of each resolution class observed on previous data sets and forms a predicton based on the make-up of resolution classes in a new corpus. The distribution of resolution classes for a new corpus can be easily determined because the classes can be recognized superficially by looking only at the strings that represent each NP. We compute the CPP score for each of our six data sets based on the average resolution class performance measured on the other five data sets. The predicted score for each class is computed as a weighted sum of the observed scores for each resolution class (i.e. the mean for the class measured on the other five data sets) weighted by the proportion of CEs that belong to the class. The predicted scores are shown in Table 6 and compared with the MUC scores that are produced by RECONCILEACL09.8 Our results show that the CPP measure is a good predictor of coreference resolution performance on unseen data sets, with the exception of one outlier – the MUC6 data set. In fact, the correlation between predicted and observed scores is 0.731 for all data sets and 0.913 excluding MUC6. RECONCILEACL09’s performance on MUC6 is better than predicted due to the higher than average scores for the common noun classes. We attribute this to the fact that MUC6 includes annotations for nested nouns, which almost always fall in the CN-e and CN-p classes. In addition, many of the features were first created for the MUC6 data set, so the feature extractors are likely more accurate than for other data sets. Overall, results indicate that coreference performance is substantially influenced by the mix of resolution classes found in the data set. Our CPP measure can be used to produce a good estimate of the level of performance on a new corpus. 8Observed scores for MUC6 and 7 differ slightly from Table 3 because this part of the work did not use the OPTIONAL field of the key, employed by the official MUC scorer. The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers.","The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers."
28,"For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.","For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family. For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995). The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals. The connection between languages should be a powerful source of information for automatic linguistic analysis as well. In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning? (ii) Will this joint analysis provide more or less benefit when the languages belong to the same family? We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning). For example, the English word misunderstanding would be segmented into mis understand - ing. This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004). The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows: This example illustrates the potential benefits of unsupervised multilingual learning. The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land. They also use an identical suffix (-y) to represent the first person possessive pronoun (my). These similarities in form should guide the model by constraining the space of joint segmentations. The corresponding English phrase lacks this resemblance to its Semitic counterparts. However, in this as in many cases, no segmentation is required for English as all the morphemes are expressed as individual words. For this reason, English should provide a strong source of disambiguation for highly inflected languages, such as Arabic and Hebrew. In general, we pose the following question. In which scenario will multilingual learning be most effective? Will it be for related languages, which share a common core of linguistic features, or for distant languages, whose linguistic divergence can provide strong sources of disambiguation? As a first step towards answering this question, we propose a model which can take advantage of both similarities and differences across languages. This joint bilingual model identifies optimal morphemes for two languages and at the same time finds compact multilingual representations. For each language in the pair, the model favors segmentations which yield high frequency morphemes. Moreover, bilingual morpheme pairs which consistently share a common semantic or syntactic function are treated as abstract morphemes, generated by a single language-independent process. These abstract morphemes are induced automatically by the model from recurring bilingual patterns. For example, in the case above, the tuple (in, fy, b-, b-) would constitute one of three abstract morphemes in the phrase. When a morpheme occurs in one language without a direct counterpart in the other language, our model can explain away the stray morpheme as arising through a language-specific process. To achieve this effect in a probabilistic framework, we formulate a hierarchical Bayesian model with Dirichlet Process priors. This framework allows us to define priors over the infinite set of possible morphemes in each language. In addition, we define a prior over abstract morphemes. This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood. The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages. We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations. The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977). Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation. When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%. Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning. However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit. Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of this research assumes that one language has annotations for the task of interest. Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language. In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does assume the availability of annotations in one language. Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language. The key assumption here is that certain distributional properties are invariant across languages from the same language families. An example of such a property is the distribution of part-of-speech bigrams. Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon. A locally optimal segmentation is identified using a task-specific greedy search. In contrast to previous approaches, our model induces morphological segmentation for multiple related languages simultaneously. By representing morphemes abstractly through the simultaneous alignment and segmentation of data in two languages, our algorithm capitalizes on deep connections between morpheme usage across different languages. The underlying assumption of our work is that structural commonality across different languages is a powerful source of information for morphological analysis. In this section, we provide several examples that motivate this assumption. The main benefit of joint multilingual analysis is that morphological structure ambiguous in one language is sometimes explicitly marked in another language. For example, in Hebrew, the preposition meaning “in”, b-, is always prefixed to its nominal argument. On the other hand, in Arabic, the most common corresponding particle is fy, which appears as a separate word. By modeling crosslingual morpheme alignments while simultaneously segmenting, the model effectively propagates information between languages and in this case would be encouraged to segment the Hebrew prefix b-. Cognates are another important means of disambiguation in the multilingual setting. Consider translations of the phrase “...and they wrote it...”: In both languages, the triliteral root ktb is used to express the act of writing. By considering the two phrases simultaneously, the model can be encouraged to split off the respective Hebrew and Arabic prefixes w- and f- in order to properly align the cognate root ktb. In the following section, we describe a model that can model both generic cross-lingual patterns (fy and b-), as well as cognates between related languages (ktb for Hebrew and Arabic). Overview In order to simultaneously model probabilistic dependencies across languages as well as morpheme distributions within each language, we employ a hierarchical Bayesian model.2 Our segmentation model is based on the notion that stable recurring string patterns within words are indicative of morphemes. In addition to learning independent morpheme patterns for each language, the model will prefer, when possible, to join together frequently occurring bilingual morpheme pairs into single abstract morphemes. The model is fully unsupervised and is driven by a preference for stable and high frequency cross-lingual morpheme patterns. In addition the model can incorporate character-to-character phonetic correspondences between alphabets as prior information, thus allowing the implicit modeling of cognates. Our aim is to induce a model which concentrates probability on highly frequent patterns while still allowing for the possibility of those previously unseen. Dirichlet processes are particularly suitable for such conditions. In this framework, we can encode prior knowledge over the infinite sets of possible morpheme strings as well as abstract morphemes. Distributions drawn from a Dirichlet process nevertheless produce sparse representations with most probability mass concentrated on a small number of observed and predicted patterns. Our model utilizes a Dirichlet process prior for each language, as well as for the cross-lingual links (abstract morphemes). Thus, a distribution over morphemes and morpheme alignments is first drawn from the set of Dirichlet processes and then produces the observed data. In practice, we never deal with such distributions directly, but rather integrate over them during Gibbs sampling. In the next section we describe our model’s “generative story” for producing the data we observe. We formalize our model in the context of two languages £ and F. However, the formulation can be extended to accommodate evidence from multiple languages as well. We provide an example of parallel phrase generation in Figure 1. High-level Generative Story We have a parallel corpus of several thousand short phrases in the two languages £ and F. Our model provides a generative story explaining how these parallel phrases were probabilistically created. The core of the model consists of three components: a distribution A over bilingual morpheme pairs (abstract morphemes), a distribution E over stray morphemes in language £ occurring without a counterpart in language F, and a similar distribution F for stray morphemes in language F. As usual for hierarchical Bayesian models, the generative story begins by drawing the model parameters themselves – in our case the three distributions A, E, and F. These three distributions are drawn from three separate Dirichlet processes, each with appropriately defined base distributions. The Dirichlet processes ensure that the resulting distributions concentrate their probability mass on a small number of morphemes while holding out reasonable probability for unseen possibilities. Once A, E, and F have been drawn, we model our parallel corpus of short phrases as a series of independent draws from a phrase-pair generation model. For each new phrase-pair, the model first chooses the number and type of morphemes to be generated. In particular, it must choose how many unaligned stray morphemes from language £, unaligned stray morphemes from language F, and abstract morphemes are to compose the parallel phrases. These three numbers, respectively denoted as m, n, and k, are drawn from a Poisson distribution. This step is illustrated in Figure 1 part (a). The model then proceeds to independently draw m language £ morphemes from distribution E, n language-F morphemes from distribution F, and k abstract morphemes from distribution A. This step is illustrated in part (b) of Figure 1. The m + k resulting language-£ morphemes are then ordered and fused to form a phrase in language £, and likewise for the n + k resulting languageF morphemes. The ordering and fusing decisions are modeled as draws from a uniform distribution over the set of all possible orderings and fusings for sizes m, n, and k. These final steps are illustrated in parts (c)-(d) of Figure 1. Now we describe the model more formally. Stray Morpheme Distributions Sometimes a morpheme occurs in a phrase in one language without a corresponding foreign language morpheme in the parallel phrase. We call these “stray morphemes,” and we employ language-specific morpheme distributions to model their generation. For each language, we draw a distribution over all possible morphemes (finite-length strings composed of characters in the appropriate alphabet) from a Dirichlet process with concentration parameter α and base distribution Pe or Pf respectively: The base distributions Pe and Pf can encode prior knowledge about the properties of morphemes in each of the two languages, such as length and character n-grams. For simplicity, we use a geometric distribution over the length of the string with a final end-morpheme character. The distributions E and F which result from the respective Dirichlet processes place most of their probability mass on a small number of morphemes with the degree of concentration As before, the resulting distribution A will give non-zero probability to all abstract morphemes (e, f). The base distribution acts as a prior on such pairs. To define we can simply use a mixture of geometric distributions in the lengths of the component morphemes. However, if the languages £ and are related and the regular phonetic correspondences between the letter in the two alphabets are known, then we can use to assign higher likelihood to potential cognates. In particular we define the prior f) to be the probabilistic string-edit distance (Ristad and Yianilos, 1998) between and the known phonetic correspondences to parameterize the string-edit model. In particular, insertion and deletion probabilities are held constant for all characters, and substitution probabilities are determined based on the known sound correspondences. We report results for both the simple geometric prior as well as the string-edit prior. Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution. These three integers represent the number and type of the morphemes d the number of coupled bilingual morpheme pairs, respectively. controlled by the prior Nevertheless, some nonzero probability is reserved for every possible string. We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006). Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes. This model assigns probabilities to all pairs of morphemes is, all pairs of finite strings from the respective alphabets (e, f). Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g. (fy, b-) for in Arabic and Hebrew). These morpheme pairs can thus be viewed as representing abstract morphemes. As with the stray morpheme models, we wish to define a distribution which concentrates probability mass on a small number of highly co-occurring morpheme pairs while still holding out some probability for all other pairs. We define this abstract morpheme model A as a draw from an that compose the parallel phrase, giving the number of stray morphemes in each language £ and an m, n, k ti Given these values, we now draw the appropriate number of stray an d abstract morphemes from the corresponding distributions: e1, ..., em ∼ E f1, ..., fn ∼ F (ei, fi), ..., (ek, f�k) ∼ A The sets of morphemes drawn for each language are then ordered: ˜e1, ..., ˜em+k ∼ ORDER|e1, ..., em, ei, ..., ek ˜f1, ..., ˜fn+k ∼ ORDER|f1, ..., fn, f1, ..., fk Finally the ordered morphemes are fused into the words that form the parallel phrases: To keep the model as simple as possible, we employ uniform distributions over the sets of orderings and fusings. In other words, given a set of r morphemes (for each language), we define the distribution over permutations of the morphemes to simply be ORDER(·|r) = 1��. Then, given a fixed morpheme order, we consider fusing each adjacent morpheme into a single word. Again, we simply model the distribution over the r − 1 fusing decisions uniformly as FUSE(·|r) = 1 2''−1 . Implicit Alignments Note that nowhere do we explicitly assign probabilities to morpheme alignments between parallel phrases. However, our model allows morphemes to be generated in precisely one of two ways: as a lone stray morpheme or as part of a bilingual abstract morpheme pair. Thus, our model implicitly assumes that each morpheme is either unaligned, or aligned to exactly one morpheme in the opposing language. If we are given a parallel phrase with already segmented morphemes we can easily induce the distribution over alignments implied by our model. As we will describe in the next section, drawing from these induced alignment distributions plays a crucial role in our inference procedure. Inference Given our corpus of short parallel bilingual phrases, we wish to make segmentation decisions which yield a set of morphemes with high joint probability. To assess the probability of a potential morpheme set, we need to marginalize over all possible alignments (i.e. possible abstract morpheme pairings and stray morpheme assignments). We also need to marginalize over all possible draws of the distributions A, E, and F from their respective Dirichlet process priors. We achieve these aims by performing Gibbs sampling. Sampling We follow (Neal, 1998) in the derivation of our blocked and collapsed Gibbs sampler. Gibbs sampling starts by initializing all random variables to arbitrary starting values. At each iteration, the sampler selects a random variable Xi, and draws a new value for Xi from the conditional distribution of Xi given the current value of the other variables: P(Xi|X_i). The stationary distribution of variables derived through this procedure is guaranteed to converge to the true joint distribution of the random variables. However, if some variables can be jointly sampled, then it may be beneficial to perform block sampling of these variables to speed convergence. In addition, if a random variable is not of direct interest, we can avoid sampling it directly by marginalizing it out, yielding a collapsed sampler. We utilize variable blocking by jointly sampling multiple segmentation and alignment decisions. We also collapse our Gibbs sampler in the standard way, by using predictive posteriors marginalized over all possible draws from the Dirichlet processes (resulting in Chinese Restaurant Processes). Resampling For each bilingual phrase, we resample each word in the phrase in turn. For word w in language E, we consider at once all possible segmentations, and for each segmentation all possible alignments. We keep fixed the previously sampled segmentation decisions for all other words in the phrase as well as sampled alignments involving morphemes in other words. We are thus considering at once: all possible segmentations of w along with all possible alignments involving morphemes in w with some subset of previously sampled languageF morphemes.3 The sampling formulas are easily derived as products of the relevant Chinese Restaurant Processes (with a minor adjustment to take into account the number of stray and abstract morphemes resulting from each decision). See (Neal, 1998) for general formulas for Gibbs sampling from distributions with Dirichlet process priors. All results reported are averaged over five runs using simulated annealing. Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots. We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation. Dataset As a source of parallel data, we use the Hebrew Bible and translations. For the Hebrew version, we use an edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006). This Bible edition is augmented by gold standard morphological analysis (including segmentation) performed by biblical scholars. For the Arabic, Aramaic, and English versions, fied by augmenting the model with a pair of “morphemeidentity” variables deterministically drawn from each abstract morpheme. Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism. we use the Van Dyke Arabic translation,4 Targum Onkelos,5 and the Revised Standard Version (Nelson, 1952), respectively. We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data (Habash and Rambow, 2005). The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included. We don’t have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic. To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph. This process allows us to group together phrases like fy s.bah. in Arabic and bbqr in Hebrew while being reasonably certain that all the relevant morphemes are contained in the short extracted phrases. The number of words in such phrases ranges from one to four words in the Semitic languages and up to six words in English. Before performing any experiments, a manual inspection of the generated parallel phrases revealed that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment. Therefore, we eliminated all parallel phrases that occur fewer than five times. As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English. The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic. For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of (Thackston, 1999). All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero. Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score. This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points. The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase. During testing, we do not allow the models to consider any multilingual evidence. This restriction allows us to simulate future performance on purely monolingual data. Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart. However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR (Creutz and Lagus, 2007), a state-of-the-art unsupervised system for morphological segmentation. While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English. The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task. We use the publicly available implementation of this system. To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decisions based on a coin weighted with the true segmentation frequency. Table 1 shows the performance of the various automatic segmentation methods. The first three rows provide baselines, as mentioned in the previous section. Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don’t use character-tocharacter phonetic correspondences to capture cognate information. We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL. We notice that in general, adding English – which has comparatively little morphological ambiguity – is about as useful as adding a more closely related Semitic language. However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair.","Table 1 shows the performance of the various automatic segmentation methods. The first three rows provide baselines, as mentioned in the previous section. Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don’t use character-tocharacter phonetic correspondences to capture cognate information. We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL. We notice that in general, adding English – which has comparatively little morphological ambiguity – is about as useful as adding a more closely related Semitic language. However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair."
29,"We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.","We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types. Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994). Although parallel text is plentiful for some language pairs such as English-Chinese or EnglishArabic, it is scarce or even non-existent for most others, such as English-Hindi or French-Japanese. Moreover, parallel text could be scarce for a language pair even if monolingual data is readily available for both languages. In this paper, we consider the problem of learning translations from monolingual sources alone. This task, though clearly more difficult than the standard parallel text approach, can operate on language pairs and in domains where standard approaches cannot. We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations. Precision and recall are then measured over these bilingual lexicons. This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. In our method, we represent each language as a monolingual lexicon (see figure 2): a list of word types characterized by monolingual feature vectors, such as context counts, orthographic substrings, and so on (section 5). We define a generative model over (1) a source lexicon, (2) a target lexicon, and (3) a matching between them (section 2). Our model is based on canonical correlation analysis (CCA)1 and explains matched word pairs via vectors in a common latent space. Inference in the model is done using an EM-style algorithm (section 3). Somewhat surprisingly, we show that it is possible to learn or extend a translation lexicon using monolingual corpora alone, in a variety of languages and using a variety of corpora, even in the absence of orthographic features. As might be expected, the task is harder when no seed lexicon is provided, when the languages are strongly divergent, or when the monolingual corpora are from different domains. Nonetheless, even in the more difficult cases, a sizable set of high-precision translations can be extracted. As an example of the performance of the system, in English-Spanish induction with our best feature set, using corpora derived from topically similar but non-parallel sources, the system obtains 89.0% precision at 33% recall. As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language. Let s = (s1, ... , snS) denote nS word types appearing in the source language, and t = (t1, ... , tnT) denote word types in the target language. Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj. We propose the following generative model over matchings m and word types (s, t), which we call matching canonical correlation analysis (MCCA). First, we generate a matching m E M, where M is the set of matchings in which each word type is matched to at most one other word type.2 We take MATCHING-PRIOR to be uniform over M.3 Then, for each matched pair of word types (i, j) E m, we need to generate the observed feature vectors of the source and target word types, fS(si) E RdS and fT (tj) E RdT . The feature vector of each word type is computed from the appropriate monolingual corpus and summarizes the word’s monolingual characteristics; see section 5 for details and figure 2 for an illustration. Since si and tj are translations of each other, we expect fS(si) and fT(tj) to be connected somehow by the generative process. In our model, they are related through a vector zi,j E Rd representing the shared, language-independent concept. Specifically, to generate the feature vectors, we first generate a random concept zi,j — N(0, Id), where Id is the d x d identity matrix. The source feature vector fS(si) is drawn from a multivariate Gaussian with mean WSzi,j and covariance FS, where WS is a dS x d matrix which transforms the language-independent concept zi,j into a languagedependent vector in the source space. The arbitrary covariance parameter FS �: 0 explains the sourcespecific variations which are not captured by WS; it does not play an explicit role in inference. The target fT (tj) is generated analogously using WT and FT, conditionally independent of the source given zi,j (see figure 2). For each of the remaining unmatched source word types si which have not yet been generated, we draw the word type features from a baseline normal distribution with variance σ2IdS, with hyperparameter σ2 » 0; unmatched target words are similarly generated. If two word types are truly translations, it will be better to relate their feature vectors through the latent space than to explain them independently via the baseline distribution. However, if a source word type is not a translation of any of the target word types, we can just generate it independently without requiring it to participate in the matching. Given our probabilistic model, we would like to maximize the log-likelihood of the observed data with respect to the model parameters 0 = (WS, WT, `pS, &T). We use the hard (Viterbi) EM algorithm as a starting point, but due to modeling and computational considerations, we make several important modifications, which we describe later. The general form of our algorithm is as follows: E-step: Find the maximum weighted (partial) bipartite matching m E ✓Vl M-step: Find the best parameters 0 by performing canonical correlation analysis (CCA) M-step Given a matching m, the M-step optimizes logp(m, s, t; 0) with respect to 0, which can be rewritten as This objective corresponds exactly to maximizing the likelihood of the probabilistic CCA model presented in Bach and Jordan (2006), which proved that the maximum likelihood estimate can be computed by canonical correlation analysis (CCA). Intuitively, CCA finds d-dimensional subspaces US E RdS×d of the source and UT E RdT ×d of the target such that the components of the projections U>S fS(si) and U>T fT(tj) are maximally correlated.4 US and UT can be found by solving an eigenvalue problem (see Hardoon et al. (2003) for details). Then the maximum likelihood estimates are as follows: WS = CSSUSP1/2, WT = CTTUTP1/2, q'S =CSS − WSWS> , and 'PT = CTT − WTWT> , where P is a d x d diagonal matrix of the canonical correlations, CSS = |m |E(i,j)∈m fS(si)fS(si)> is the empirical covariance matrix in the source domain, and CTT is defined analogously. E-step To perform a conventional E-step, we would need to compute the posterior over all matchings, which is #P-complete (Valiant, 1979). On the other hand, hard EM only requires us to compute the best matching under the current model:5 We cast this optimization as a maximum weighted bipartite matching problem as follows. Define the edge weight between source word type i and target word type j to be are presented for other languages in section 6. In this section, we describe the data and experimental methodology used throughout this work. which can be loosely viewed as a pointwise mutual information quantity. We can check that the objective logp(m, s, t; B) is equal to the weight of a matching plus some constant C: To find the optimal partial matching, edges with weight wz,7 < 0 are set to zero in the graph and the optimal full matching is computed in 0((nS+nT)3) time using the Hungarian algorithm (Kuhn, 1955). If a zero edge is present in the solution, we remove the involved word types from the matching.6 Bootstrapping Recall that the E-step produces a partial matching of the word types. If too few word types are matched, learning will not progress quickly; if too many are matched, the model will be swamped with noise. We found that it was helpful to explicitly control the number of edges. Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, the distances between the words’ mean latent concepts: where A is a thresholding constant, zz = E(zi,j �fS(si)) = P1/2Us fS(si), and zj* is defined analogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual corpus.11 For all languages pairs except English-Arabic, we extract evaluation lexicons from the Wiktionary online dictionary. As we discuss in section 7, our extracted lexicons have low coverage, particularly for proper nouns, and thus all performance measures are (sometimes substantially) pessimistic. For EnglishArabic, we extract a lexicon from 100k parallel sentences of UN parallel corpora by running the HMM intersected alignment model (Liang et al., 2008), adding (s, t) to the lexicon if s was aligned to t at least three times and more than any other word. Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. We used two methods to derive a seed lexicon. The first is to use the evaluation lexicon Le and select the hundred most common noun word types in the source corpus which have translations in Le. The second method is to heuristically induce, where applicable, a seed lexicon using edit distance, as is done in Koehn and Knight (2002). Section 6.2 compares the performance of these two methods. We evaluate a proposed lexicon Lp against the evaluation lexicon Le using the Fl measure in the standard fashion; precision is given by the number of proposed translations contained in the evaluation lexicon, and recall is given by the fraction of possible translation pairs proposed.12 Since our model naturally produces lexicons in which each entry is associated with a weight based on the model, we can give a full precision/recall curve (see figure 3). We summarize these curves with both the best Fl over all possible thresholds and various precisions px at recalls x. All reported numbers exclude evaluation on the seed lexicon entries, regardless of how those seeds are derived or whether they are correct. In all experiments, unless noted otherwise, we used a seed of size 100 obtained from Le and considered lexicons between the top n = 2,000 most frequent source and target noun word types which were not in the seed lexicon; each system proposed an already-ranked one-to-one translation lexicon amongst these n words. Where applicable, we compare against the EDITDIST baseline, which solves a maximum bipartite matching problem where edge weights are normalized edit distances. We will use MCCA (for matching CCA) to denote our model using the optimal feature set (see section 5.3). In this section, we explore feature representations of word types in our model. Recall that f�(·) and fT (·) map source and target word types to vectors in RdS and RdT, respectively (see section 2). The features used in each representation are defined identically and derived only from the appropriate monolingual corpora. For a concrete example of a word type to feature vector mapping, see figure 2. For closely related languages, such as English and Spanish, translation pairs often share many orthographic features. One direct way to capture orthographic similarity between word pairs is edit distance. Running EDITDIST (see section 4.3) on ENES-W yielded 61.1 p0.33, but precision quickly degrades for higher recall levels (see EDITDIST in table 1). Nevertheless, when available, orthographic clues are strong indicators of translation pairs. We can represent orthographic features of a word type w by assigning a feature to each substring of length G 3. Note that MCCA can learn regular orthographic correspondences between source and target words, which is something edit distance cannot capture (see table 5). Indeed, running our MCCA model with only orthographic features on EN-ESW, labeled ORTHO in table 1, yielded 80.1 p0.33, a 31% error-reduction over EDITDIST in p0.33. While orthographic features are clearly effective for historically related language pairs, they are more limited for other language pairs, where we need to appeal to other clues. One non-orthographic clue that word types s and t form a translation pair is that there is a strong correlation between the source words used with s and the target words used with t. To capture this information, we define context features for each word type w, consisting of counts of nouns which occur within a window of size 4 around w. Consider the translation pair (time, tiempo) illustrated in figure 2. As we become more confident about other translation pairs which have active period and periodico context features, we learn that translation pairs tend to jointly generate these features, which leads us to believe that time and tiempo might be generated by a common underlying concept vector (see section 2).13 Using context features alone on EN-ES-W, our MCCA model (labeled CONTEXT in table 1) yielded a 80.2 p0.33. It is perhaps surprising that context features alone, without orthographic information, can yield a best-F1comparable to EDITDIST. We can of course combine context and orthographic features. Doing so yielded 89.03 p0.33 (labeled MCCA in table 1); this represents a 46.4% error reduction in p0.33 over the EDITDIST baseline. For the remainder of this work, we will use MCCA to refer to our model using both orthographic and context features. In this section we examine how system performance varies when crucial elements are altered. There are many sources from which we can derive monolingual corpora, and MCCA performance depends on the degree of similarity between corpora. We explored the following levels of relationships between corpora, roughly in order of closest to most distant: Our results for all conditions are presented in table 2(a). The predominant trend is that system performance degraded when the corpora diverged in content, presumably due to context features becoming less informative. However, it is notable that even in the most extreme case of disjoint corpora from different time periods and topics (e.g. EN-ES-G), we are still able to recover lexicons of reasonable accuracy. All of our experiments so far have exploited a small seed lexicon which has been derived from the evaluation lexicon (see section 4.3). In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. Using this automatically derived seed lexicon, we ran our system on EN-ESD as before, evaluating on the top 2,000 noun word types not included in the automatic lexicon.14 Using the automated seed lexicon, and still evaluating against our Wiktionary lexicon, MCCA-AUTO yielded 91.8 p0.33 (see table 2(b)), indicating that our system can produce lexicons of comparable accuracy with a heuristically chosen seed. We should note that this performance represents no knowledge given to the system in the form of gold seed lexicon entries. We also explored how system performance varies for language pairs other than English-Spanish. On English-French, for the disjoint EN-FR-D corpus (described in section 4.1), MCCA yielded 88.3 p0.33 (see table 2(c) for more performance measures). This verified that our model can work for another closely related language-pair on which no model development was performed. One concern is how our system performs on language pairs where orthographic features are less applicable. Results on disjoint English-Chinese and English-Arabic are given as EN-CH-D and EN-AR in table 2(c), both using only context features. In these cases, MCCA yielded much lower precisions of 26.8 and 31.0 p0.33, respectively. For both languages, performance degraded compared to EN-ESfidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1, MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl. In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). We have presented a novel generative model for bilingual lexicon induction and presented results under a variety of data conditions (section 6.1) and languages (section 6.3) showing that our system can produce accurate lexicons even in highly adverse conditions. In this section, we broadly characterize and analyze the behavior of our system. We manually examined the top 100 errors in the English-Spanish lexicon produced by our system on EN-ES-W. Of the top 100 errors: 21 were correct translations not contained in the Wiktionary lexicon (e.g. pintura to painting), 4 were purely morphological errors (e.g. airport to aeropuertos), 30 were semantically related (e.g. basketball to b´eisbol), 15 were words with strong orthographic similarities (e.g. coast to costas), and 30 were difficult to categorize and fell into none of these categories. Since many of our ‘errors’ actually represent valid translation pairs not contained in our extracted dictionary, we supplemented our evaluation lexicon with one automatically derived from 100k sentences of parallel Europarl data. We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word. Evaluating against the union of these lexicons yielded 98.0 p0.33, a significant improvement over the 92.3 using only the Wiktionary lexicon. Of the true errors, the most common arose from semantically related words which had strong context feature correlations (see table 4(b)). We also explored the relationships our model learns between features of different languages. We projected each source and target feature into the shared canonical space, and for each projected source feature we examined the closest projected target features. In table 5(a), we present some of the orthographic feature relationships learned by our system. Many of these relationships correspond to phonological and morphological regularities such as the English suffix ing mapping to the Spanish suffix gia. In table 5(b), we present context feature correspondences. Here, the broad trend is for words which are either translations or semantically related across languages to be close in canonical space.","We have presented a novel generative model for bilingual lexicon induction and presented results under a variety of data conditions (section 6.1) and languages (section 6.3) showing that our system can produce accurate lexicons even in highly adverse conditions. In this section, we broadly characterize and analyze the behavior of our system. We manually examined the top 100 errors in the English-Spanish lexicon produced by our system on EN-ES-W. Of the top 100 errors: 21 were correct translations not contained in the Wiktionary lexicon (e.g. pintura to painting), 4 were purely morphological errors (e.g. airport to aeropuertos), 30 were semantically related (e.g. basketball to b´eisbol), 15 were words with strong orthographic similarities (e.g. coast to costas), and 30 were difficult to categorize and fell into none of these categories. Since many of our ‘errors’ actually represent valid translation pairs not contained in our extracted dictionary, we supplemented our evaluation lexicon with one automatically derived from 100k sentences of parallel Europarl data. We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word. Evaluating against the union of these lexicons yielded 98.0 p0.33, a significant improvement over the 92.3 using only the Wiktionary lexicon. Of the true errors, the most common arose from semantically related words which had strong context feature correlations (see table 4(b)). We also explored the relationships our model learns between features of different languages. We projected each source and target feature into the shared canonical space, and for each projected source feature we examined the closest projected target features. In table 5(a), we present some of the orthographic feature relationships learned by our system. Many of these relationships correspond to phonological and morphological regularities such as the English suffix ing mapping to the Spanish suffix gia. In table 5(b), we present context feature correspondences. Here, the broad trend is for words which are either translations or semantically related across languages to be close in canonical space."
30,"Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space. In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of an error reduction in tagging acof compared to the traditional pipeline approach.","Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space. In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of an error reduction in tagging acof compared to the traditional pipeline approach. Since Chinese sentences do not contain explicitly marked word boundaries, word segmentation is a necessary step before POS tagging can be performed. Typically, a Chinese POS tagger takes segmented inputs, which are produced by a separate word segmentor. This two-step approach, however, has an obvious flaw of error propagation, since word segmentation errors cannot be corrected by the POS tagger. A better approach would be to utilize POS information to improve word segmentation. For example, the POS-word pattern “number word” + “^ (a common measure word)” can help in segmenting the character sequence “�^A” into the word sequence “� (one) ^ (measure word) A (person)” instead of “� (one) ^A (personal; adj)”. Moreover, the comparatively rare POS pattern “number word” + “number word” can help to prevent segmenting a long number word into two words. In order to avoid error propagation and make use of POS information for word segmentation, segmentation and POS tagging can be viewed as a single task: given a raw Chinese input sentence, the joint POS tagger considers all possible segmented and tagged sequences, and chooses the overall best output. A major challenge for such a joint system is the large search space faced by the decoder. For a sentence with n characters, the number of possible output sequences is O(2n−1 · Tn), where T is the size of the tag set. Due to the nature of the combined candidate items, decoding can be inefficient even with dynamic programming. Recent research on Chinese POS tagging has started to investigate joint segmentation and tagging, reporting accuracy improvements over the pipeline approach. Various decoding approaches have been used to reduce the combined search space. Ng and Low (2004) mapped the joint segmentation and POS tagging task into a single character sequence tagging problem. Two types of tags are assigned to each character to represent its segmentation and POS. For example, the tag “b NN” indicates a character at the beginning of a noun. Using this method, POS features are allowed to interact with segmentation. Since tagging is restricted to characters, the search space is reduced to O((4T)'), and beam search decoding is effective with a small beam size. However, the disadvantage of this model is the difficulty in incorporating whole word information into POS tagging. For example, the standard “word + POS tag” feature is not explicitly applicable. Shi and Wang (2007) introduced POS information to segmentation by reranking. N-best segmentation outputs are passed to a separately-trained POS tagger, and the best output is selected using the overall POSsegmentation probability score. In this system, the decoding for word segmentation and POS tagging are still performed separately, and exact inference for both is possible. However, the interaction between POS and segmentation is restricted by reranking: POS information is used to improve segmentation only for the N segmentor outputs. In this paper, we propose a novel joint model for Chinese word segmentation and POS tagging, which does not limiting the interaction between segmentation and POS information in reducing the combined search space. Instead, a novel multiple beam search algorithm is used to do decoding efficiently. Candidate ranking is based on a discriminative joint model, with features being extracted from segmented words and POS tags simultaneously. The training is performed by a single generalized perceptron (Collins, 2002). In experiments with the Chinese Treebank data, the joint model gave an error reduction of 14.6% in segmentation accuracy and 12.2% in the overall segmentation and tagging accuracy, compared to the traditional pipeline approach. In addition, the overall results are comparable to the best systems in the literature, which exploit knowledge outside the training data, even though our system is fully data-driven. Different methods have been proposed to reduce error propagation between pipelined tasks, both in general (Sutton et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for specific problems such as language modeling and utterance classification (Saraclar and Roark, 2005) and labeling and chunking (Shimizu and Haas, 2006). Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks. We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2. The word segmentation features are extracted from word bigrams, capturing word, word length and character information in the context. The word length features are normalized, with those more than 15 being treated as 15. The POS tagging features are based on contextual information from the tag trigram, as well as the neighboring three-word window. To reduce overfitting and increase the decoding speed, templates 4, 5, 6 and 7 only include words with less than 3 characters. Like the baseline segmentor, the baseline tagger also normalizes word length features. Templates 15 and 16 in Table 2 are inspired by the CTBMorph feature templates in Tseng et al. (2005), which gave the most accuracy improvement in their experiments. Here the category of a character is the set of tags seen on the character during training. Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data. During training, the baseline POS tagger stores special word-tag pairs into a tag dictionary (Ratnaparkhi, 1996). Such information is used by the decoder to prune unlikely tags. For each word occurring more than N times in the training data, the decoder can only assign a tag the word has been seen with in the training data. This method led to improvement in the decoding speed as well as the output accuracy for English POS tagging (Ratnaparkhi, 1996). Besides tags for frequent words, our baseline POS tagger also uses the tag dictionary to store closed-set tags (Xia, 2000) – those associated only with a limited number of Chinese words. In this section, we build a joint word segmentation and POS tagging model that uses exactly the same source of information as the baseline system, by applying the feature templates from the baseline word segmentor and POS tagger. No extra knowledge is used by the joint model. However, because word segmentation and POS tagging are performed simultaneously, POS information participates in word segmentation. We formulate joint word segmentation and POS tagging as a single problem, which maps a raw Chinese sentence to a segmented and POS tagged output. Given an input sentence x, the output F(x) satisfies: where GEN(x) represents the set of possible outputs for x. Score(y) is computed by a feature-based linear model. Denoting the global feature vector for the tagged sentence y with 4b(y), we have: where w� is the parameter vector in the model. Each element in w� gives a weight to its corresponding element in 4b(y), which is the count of a particular feature over the whole sentence y. We calculate the w� value by supervised learning, using the averaged perceptron algorithm (Collins, 2002), given in Figure 1. 1 We take the union of feature templates from the baseline segmentor (Table 1) and POS tagger (Table 2) as the feature templates for the joint system. All features are treated equally and processed together according to the linear model, regardless of whether they are from the baseline segmentor or tagger. In fact, most features from the baseline POS tagger, when used in the joint model, represent segmentation patterns as well. For example, the aforementioned pattern “number word” + “^”, which is Inputs: training examples (xi, yi) Initialization: set w� = 0 Algorithm: for t = 1..T, i = 1..N calculate zi = arg maxyEGEN(xi) Φ(y) - w� if zi =� yi useful only for the POS “number word” in the baseline tagger, is also an effective indicator of the segmentation of the two words (especially “^”) in the joint model. One of the main challenges for the joint segmentation and POS tagging system is the decoding algorithm. The speed and accuracy of the decoder is important for the perceptron learning algorithm, but the system faces a very large search space of combined candidates. Given the linear model and feature templates, exact inference is very hard even with dynamic programming. Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy. This beam search algorithm processes an input sentence incrementally. At each stage, the incoming character is combined with existing partial candidates in all possible ways to generate new partial candidates. An agenda is used to control the search space, keeping only the B best partial candidates ending with the current character. The algorithm is simple and efficient, with a linear time complexity of O(BTn), where n is the size of input sentence, and T is the size of the tag set (T = 1 for pure word segmentation). It worked well for word segmentation alone (Zhang and Clark, 2007), even with an agenda size as small as 8, and a simple beam search algorithm also works well for POS tagging (Ratnaparkhi, 1996). However, when applied to the joint model, it resulted in a reduction in segmentation accuracy (compared to the baseline segmentor) even with B as large as 1024. One possible cause of the poor performance of the standard beam search method is the combined nature of the candidates in the search space. In the baseInput: raw sentence sent – a list of characters Variables: candidate sentence item – a list of (word, tag) pairs; maximum word-length record maxlen for each tag; the agenda list agendas; the tag dictionary tagdict; start index for current word; end index for current word Initialization: agendas[0] = [“”], agendas[i] = [] (i! = 0) Algorithm: for end index = 1 to sent.length: foreach tag: for start index = max(1, end index − maxlen[tag] + 1) to end index: word = sent[start index..end index] if (word, tag) consistent with tagdict: for item E agendas[start index − 1]: line POS tagger, candidates in the beam are tagged sequences ending with the current word, which can be compared directly with each other. However, for the joint problem, candidates in the beam are segmented and tagged sequences up to the current character, where the last word can be a complete word or a partial word. A problem arises in whether to give POS tags to incomplete words. If partial words are given POS tags, it is likely that some partial words are “justified” as complete words by the current POS information. On the other hand, if partial words are not given POS tag features, the correct segmentation for long words can be lost during partial candidate comparison (since many short completed words with POS tags are likely to be preferred to a long incomplete word with no POS tag features).2 Another possible cause is the exponential growth in the number of possible candidates with increasing sentence size. The number increases from O(Tn) for the baseline POS tagger to O(2n−'Tn) for the joint system. As a result, for an incremental decoding algorithm, the number of possible candidates increases exponentially with the current word or character index. In the POS tagging problem, a new incoming word enlarges the number of possible candidates by a factor of T (the size of the tag set). For the joint problem, however, the enlarging factor becomes 2T with each incoming character. The speed of search space expansion is much faster, but the number of candidates is still controlled by a single, fixed-size beam at any stage. If we assume that the beam is not large enough for all the candidates at at each stage, then, from the newly generated candidates, the baseline POS tagger can keep 1/T for the next processing stage, while the joint model can keep only 1/2T, and has to discard the rest. Therefore, even when the candidate comparison standard is ignored, we can still see that the chance for the overall best candidate to fall out of the beam is largely increased. Since the search space growth is exponential, increasing the fixed beam size is not effective in solving the problem. To solve the above problems, we developed a multiple beam search algorithm, which compares candidates only with complete tagged words, and enables the size of the search space to scale with the input size. The algorithm is shown in Figure 2. In this decoder, an agenda is assigned to each character in the input sentence, recording the B best segmented and tagged partial candidates ending with the character. The input sentence is still processed incrementally. However, now when a character is processed, existing partial candidates ending with any previous characters are available. Therefore, the decoder enumerates all possible tagged words ending with the current character, and combines each word with the partial candidates ending with its previous character. All input characters are processed in the same way, and the final output is the best candidate in the final agenda. The time complexity of the algorithm is O(WTBn), with W being the maximum word size, T being the total number of POS tags and n the number of characters in the input. It is also linear in the input size. Moreover, the decoding algorithm gives competent accuracy with a small agenda size of B = 16. To further limit the search space, two optimizations are used. First, the maximum word length for each tag is recorded and used by the decoder to prune unlikely candidates. Because the majority of tags only apply to words with length 1 or 2, this method has a strong effect. Development tests showed that it improves the speed significantly, while having a very small negative influence on the accuracy. Second, like the baseline POS tagger, the tag dictionary is used for Chinese closed set tags and the tags for frequent words. To words outside the tag dictionary, the decoder still tries to assign every possible tag. Apart from features, the decoder maintains other types of information, including the tag dictionary, the word frequency counts used when building the tag dictionary, the maximum word lengths by tag, and the character categories. The above data can be collected by scanning the corpus before training starts. However, in both the baseline tagger and the joint POS tagger, they are updated incrementally during the perceptron training process, consistent with online learning.3 The online updating of word frequencies, maximum word lengths and character categories is straightforward. For the online updating of the tag dictionary, however, the decision for frequent words must be made dynamically because the word frequencies keep changing. This is done by caching the number of occurrences of the current most frequent word M, and taking all words currently above the threshold M/5000 + 5 as frequent words. 5000 is a rough figure to control the number of frequent words, set according to Zipf’s law. The parameter 5 is used to force all tags to be enumerated before a word is seen more than 5 times. Ng and Low (2004) and Shi and Wang (2007) were described in the Introduction. Both models reduced the large search space by imposing strong restrictions on the form of search candidates. In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N-best reranking approach, which limits the influence of POS tagging on segmentation to the N-best list. In comparison, our joint model does not impose any hard limitations on the interaction between segmentation and POS information.4 Fast decoding speed is achieved by using a novel multiple-beam search algorithm. Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach. Word information is used to process known-words, and character information is used for unknown words in a similar way to Ng and Low (2004). In comparison, our model handles character and word information simultaneously in a single perceptron model. The Chinese Treebank (CTB) 4 is used for the experiments. It is separated into two parts: CTB 3 (420K characters in 150K words / 10364 sentences) is used for the final 10-fold cross validation, and the rest (240K characters in 150K words / 4798 sentences) is used as training and test data for development. The standard F-scores are used to measure both the word segmentation accuracy and the overall segmentation and tagging accuracy, where the overall accuracy is TF = 2pr/(p + r), with the precision p being the percentage of correctly segmented and tagged words in the decoder output, and the recall r being the percentage of gold-standard tagged words that are correctly identified by the decoder. For direct comparison with Ng and Low (2004), the POS tagging accuracy is also calculated by the percentage of correct tags on each character. The learning curves of the baseline and joint models are shown in Figure 3, Figure 4 and Figure 5, respectively. These curves are used to show the convergence of perceptron and decide the number of training iterations for the test. It should be noticed that the accuracies from Figure 4 and Figure 5 are not comparable because gold-standard segmentation is used as the input for the baseline tagger. According to the figures, the number of training iterations for the baseline segmentor, POS tagger, and the joint system are set to 8, 6, and 7, respectively for the remaining experiments. There are many factors which can influence the accuracy of the joint model. Here we consider the special character category features and the effect of the tag dictionary. The character category features (templates 15 and 16 in Table 2) represent a Chinese character by all the tags associated with the character in the training data. They have been shown to improve the accuracy of a Chinese POS tagger (Tseng et al., 2005). In the joint model, these features also represent segmentation information, since they concern the starting and ending characters of a word. Development tests showed that the overall tagging F-score of the joint model increased from 84.54% to 84.93% using the character category features. In the development test, the use of the tag dictionary improves the decoding speed of the joint model, reducing the decoding time from 416 seconds to 256 seconds. The overall tagging accuracy also increased slightly, consistent with observations from the pure POS tagger. The error analysis for the development test is shown in Table 3. Here an error is counted when a word in the standard output is not produced by the decoder, due to incorrect segmentation or tag assignment. Statistics about the six most frequently mistaken tags are shown in the table, where each row presents the analysis of one tag from the standard output, and each column gives a wrongly assigned value. The column “Seg” represents segmentation errors. Each figure in the table shows the percentage of the corresponding error from all the errors. It can be seen from the table that the NN-VV and VV-NN mistakes were the most commonly made by the decoder, while the NR-NN mistakes are also frequent. These three types of errors significantly outnumber the rest, together contributing 14.92% of all the errors. Moreover, the most commonly mistaken tags are NN and VV, while among the most frequent tags in the corpus, PU, DEG and M had comparatively less errors. Lastly, segmentation errors contribute around half (51.47%) of all the errors. 10-fold cross validation is performed to test the accuracy of the joint word segmentor and POS tagger, and to make comparisons with existing models in the literature. Following Ng and Low (2004), we partition the sentences in CTB 3, ordered by sentence ID, into 10 groups evenly. In the nth test, the nth group is used as the testing data. Table 4 shows the detailed results for the cross validation tests, each row representing one test. As can be seen from the table, the joint model outperforms the baseline system in each test. Table 5 shows the overall accuracies of the baseline and joint systems, and compares them to the relevant models in the literature. The accuracy of each model is shown in a row, where “Ng” represents the models from Ng and Low (2004) and “Shi” represents the models from Shi and Wang (2007). Each accuracy measure is shown in a column, including the segmentation F-score (SF), the overall tagging F-score (TF) and the tagging accuracy by characters (TA). As can be seen from the table, our joint model achieved the largest improvement over the baseline, reducing the segmentation error by 14.58% and the overall tagging error by 12.18%. The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007). Despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy. One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven. However, the comparison is indirect because our partitions of the CTB corpus are different. Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split. We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further comparison. Compared with Ng and Low (2004), our baseline model gave slightly better accuracy, consistent with our previous observations about the word segmentors (Zhang and Clark, 2007). Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POS tagging systems in the literature, our proposed model achieved the best accuracy boost from the cascaded baseline, and competent overall accuracy. We proposed a joint Chinese word segmentation and POS tagging model, which achieved a considerable reduction in error rate compared to a baseline twostage system. We used a single linear model for combined word segmentation and POS tagging, and chose the generalized perceptron algorithm for joint training. and beam search for efficient decoding. However, the application of beam search was far from trivial because of the size of the combined search space. Motivated by the question of what are the comparable partial hypotheses in the space, we developed a novel multiple beam search decoder which effectively explores the large search space. Similar techniques can potentially be applied to other problems involving joint inference in NLP. Other choices are available for the decoding of a joint linear model, such as exact inference with dynamic programming, provided that the range of features allows efficient processing. The baseline feature templates for Chinese segmentation and POS tagging, when added together, makes exact inference for the proposed joint model very hard. However, the accuracy loss from the beam decoder, as well as alternative decoding algorithms, are worth further exploration. The joint system takes features only from the baseline segmentor and the baseline POS tagger to allow a fair comparison. There may be additional features that are particularly useful to the joint system. Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve the accuracy of segmentation and POS tagging. Therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and POS tagger.","We proposed a joint Chinese word segmentation and POS tagging model, which achieved a considerable reduction in error rate compared to a baseline twostage system. We used a single linear model for combined word segmentation and POS tagging, and chose the generalized perceptron algorithm for joint training. and beam search for efficient decoding. However, the application of beam search was far from trivial because of the size of the combined search space. Motivated by the question of what are the comparable partial hypotheses in the space, we developed a novel multiple beam search decoder which effectively explores the large search space. Similar techniques can potentially be applied to other problems involving joint inference in NLP. Other choices are available for the decoding of a joint linear model, such as exact inference with dynamic programming, provided that the range of features allows efficient processing. The baseline feature templates for Chinese segmentation and POS tagging, when added together, makes exact inference for the proposed joint model very hard. However, the accuracy loss from the beam decoder, as well as alternative decoding algorithms, are worth further exploration. The joint system takes features only from the baseline segmentor and the baseline POS tagger to allow a fair comparison. There may be additional features that are particularly useful to the joint system. Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve the accuracy of segmentation and POS tagging. Therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and POS tagger."
31,"We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.","We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency. Implicit discourse relations abound in text and readers easily recover the sense of such relations during semantic interpretation. But automatic sense prediction for implicit relations is an outstanding challenge in discourse processing. Discourse relations, such as causal and contrast relations, are often marked by explicit discourse connectives (also called cue words) such as “because” or “but”. It is not uncommon, though, for a discourse relation to hold between two text spans without an explicit discourse connective, as the example below demonstrates: (1) The 101-year-old magazine has never had to woo advertisers with quite so much fervor before. [because] It largely rested on its hard-to-fault demographics. In this paper we address the problem of automatic sense prediction for discourse relations in newspaper text. For our experiments, we use the Penn Discourse Treebank, the largest existing corpus of discourse annotations for both implicit and explicit relations. Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007). In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits. Explicit relations are easy to identify. The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008). So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress. In this paper we investigate the effectiveness of various features designed to capture lexical and semantic regularities for identifying the sense of implicit relations. Given two text spans, previous work has used the cross-product of the words in the spans as features. We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped. We then introduce several other methods capturing the semantics of the spans (polarity features, semantic classes, tense, etc.) and evaluate their effectiveness. This is the first study which reports results on classifying naturally occurring implicit relations in text and uses the natural distribution of the various senses. Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level. Soricut and Marcu (2003) address the task of parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysis of implicit relations that occur intersententially. Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations. They achieve 81% accuracy in sense disambiguation on this corpus. However, GraphBank annotations do not differentiate between implicits and explicits, so it is difficult to verify success for implicit relations. Experiments on artificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction. Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations. They delete the connective and use [Arg1, Arg2] as an example of an implicit relation. The approach is tested using binary classification between relations on balanced data, a setting very different from that of any realistic application. For example, a question-answering application that needs to identify causal relations (i.e. as in Girju (2003)), must not only differentiate causal relations from comparison relations, but also from expansions, temporal relations, and possibly no relation at all. In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. For our experiments, we use the Penn Discourse Treebank (PDTB; Prasad et al., 2008), the largest available annotated corpora of discourse relations. The PDTB contains discourse annotations over the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. For each explicit discourse connective (such as “but” or “so”), annotators identified the two text spans between which the relation holds and the sense of the relation. The PDTB also provides information about local implicit relations. For each pair of adjacent sentences within the same paragraph, annotators selected the explicit discourse connective which best expressed the relation between the sentences and then assigned a sense to the relation. In Example (1) above, the annotators identified “because” as the most appropriate connective between the sentences, and then labeled the implicit discourse relation Contingency. In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations. As mentioned above, each implicit and explicit relation is annotated with a sense. The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason. In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal. Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the words in the two text spans of the relation. Intuitively, one would expect that there is some relationship that holds between the words in the two arguments. Consider for example the following sentences: The recent explosion of country funds mirrors the ”closedend fund mania” of the 1920s, Mr. Foot says, when narrowly focused funds grew wildly popular. They fell into oblivion after the 1929 crash. The words “popular” and “oblivion” are almost antonyms, and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences. Similarly, a pair of words such as (rain, rot) might be indicative of a causal relation. If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations. Indeed, word pairs form the basic feature of most previous work on classifying implicit relations (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007; Sporleder and Lascarides, 2008) or the simpler task of predicting which connective should be used to express a relation (Lapata and Lascarides, 2004). Semantic relations vs. function word pairs If the hypothesis for word pair triggers of discourse relations were true, the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them. Yet, feature analysis has not been performed in prior studies to establish or refute this possibility. At the same time, feature selection is always necessary for word pairs, which are numerous and lead to data sparsity problems. Here, we present a meta analysis of the feature selection work in three prior studies. One approach for reducing the number of features follows the hypothesis of semantic relations between words. Marcu and Echihabi (2001) considered only nouns, verbs and and other cue phrases in word pairs. They found that even with millions of training examples, prediction results using all words were superior to those based on only pairs of non-function words. However, since the learning curve is steeper when function words were removed, they hypothesize that using only non-function words will outperform using all words once enough training data is available. In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives. Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model. They show that (i) stemming, (ii) using a small fixed vocabulary size consisting of only the most frequent stems (which would tend to be dominated by function words) and (iii) a cutoff on the minimum frequency of a feature, all result in improved performance. They also report that filtering stopwords has a negative impact on the results. Given these findings, we expect that pairs of function words are informative features helpful in predicting discourse relation sense. In our work that we describe next, we use feature selection to investigate the word pairs in detail. For the analysis of word pair features, we use a large collection of automatically extracted explicit examples from the experiments in BlairGoldensohn et al. (2007). The data, from now on referred to as TextRels, has explicit contrast and causal relations which were extracted from the English Gigaword Corpus (Graff, 2003) which contains over four million newswire articles. The explicit cue phrase is removed from each example and the spans are treated as belonging to an implicit relation. Besides cause and contrast, the TextRels data include a no-relation category which consists of sentences from the same text that are separated by at least three other sentences. To identify features useful for classifying comparison vs other relations, we chose a random sample of 5000 examples for Contrast and 5000 Other relations (2500 each of Cause and No-relation). For the complete set of 10,000 examples, word pair features were computed. After removing word pairs that appear less than 5 times, the remaining features were ranked by information gain using the MALLET toolkit1. Table 1 lists the word pairs with highest information gain for the Contrast vs. Other and Cause vs. Other classification tasks. All contain very frequent stop words, and interestingly for the Con1mallet.cs.umass.edu trast vs. Other task, most of the word pairs contain discourse connectives. This is certainly unexpected, given that word pairs were formed by deleting the discourse connectives from the sentences expressing Contrast. Word pairs containing “but” as one of their elements in fact signal the presence of a relation that is not Contrast. Consider the example shown below: The government says it has reached most isolated townships by now, but because roads are blocked, getting anything but basic food supplies to people remains difficult. Following Marcu and Echihabi (2001), the pair [The government says it has reached most isolated townships by now, but] and [roads are blocked, getting anything but basic food supplies to people remains difficult.] is created as an example of the Cause relation. Because of examples like this, “but-but” is a very useful word pair feature indicating Cause, as the but would have been removed for the artifical Contrast examples. In fact, the top 17 features for classifying Contrast versus Other all contain the word “but”, and are indications that the relation is Other. These findings indicate an unexpected anomalous effect in the use of synthetic data. Since relations are created by removing connectives, if an unambiguous connective remains, its presence is a reliable indicator that the example should be classified as Other. Such features might work well and lead to high accuracy results in identifying synthetic implicit relations, but are unlikely to be useful in a realistic setting of actual implicits. the-but s-but the-in of-but for-but but-but in-but was-but it-but to-but that-but the-it* and-and the-the in-in to-the of-and a-of said-but they-but of-in in-and in-of s-and Also note that the only two features predictive of the comparison class (indicated by * in Table 1): the-it and to-it, contain only function words rather than semantically related nonfunction words. This ranking explains the observations reported in Blair-Goldensohn et al. (2007) where removing stopwords degraded classifier performance and why using only nouns, verbs or adjectives (Marcu and Echihabi, 2001; Lapata and Lascarides, 2004) is not the best option2. The contrast between the “popular”/“oblivion” example we started with above can be analyzed in terms of lexical relations (near antonyms), but also could be explained by different polarities of the two words: “popular” is generally a positive word, while “oblivion” has negative connotations. While we agree that the actual words in the arguments are quite useful, we also define several higher-level features corresponding to various semantic properties of the words. The words in the two text spans of a relation are taken from the gold-standard annotations in the PDTB. Polarity Tags: We define features that represent the sentiment of the words in the two spans. Each word’s polarity was assigned according to its entry in the Multi-perspective Question Answering Opinion Corpus (Wilson et al., 2005). In this resource, each sentiment word is annotated as positive, negative, both, or neutral. We use the number of negated and non-negated positive, negative, and neutral sentiment words in the two text spans as features. If a writer refers to something as “nice” in Arg1, that counts towards the positive sentiment count (Arg1Positive); “not nice” would count towards Arg1NegatePositive. A sentiment word is negated if a word with a General Inquirer (Stone et al., 1966) Negate tag precedes it. We also have features for the cross products of these polarities between Arg1 and Arg2. We expected that these features could help Comparison examples especially. Consider the following example: wasn’t a good one. The venture, formed in 1986, was supposed to be Time’s low-cost, safe entry into women’s magazines. The word good is annotated with positive polarity, however it is negated. Safe is tagged as having positive polarity, so this opposition could indicate the Comparison relation between the two sentences. Inquirer Tags: To get at the meanings of the spans, we look up what semantic categories each word falls into according to the General Inquirer lexicon (Stone et al., 1966). The General Inquirer has classes for positive and negative polarity, as well as more fine-grained categories such as words related to virtue or vice. The Inquirer even contains a category called “Comp” that includes words that tend to indicate Comparison, such as “optimal”, “other”, “supreme”, or “ultimate”. Several of the categories are complementary: Understatement versus Overstatement, Rise versus Fall, or Pleasure versus Pain. Pairs where one argument contains words that indicate Rise and the other argument indicates Fall might be good evidence for a Comparison relation. The benefit of using these tags instead of just the word pairs is that we see more observations for each semantic class than for any particular word, reducing the data sparsity problem. For example, the pair rose:fell often indicates a Comparison relation when speaking about stocks. However, occasionally authors refer to stock prices as “jumping” rather than “rising”. Since both jump and rise are members of the Rise class, new jump examples can be classified using past rise examples. Development testing showed that including features for all words’ tags was not useful, so we include the Inquirer tags of only the verbs in the two arguments and their cross-product. Just as for the polarity features, we include features for both each tag and its negation. Money/Percent/Num: If two adjacent sentences both contain numbers, dollar amounts, or percentages, it is likely that a comparison relation might hold between the sentences. We included a feature for the count of numbers, percentages, and dollar amounts in Arg1 and Arg2. We also included the number of times each combination of number/percent/dollar occurs in Arg1 and Arg2. For example, if Arg1 mentions a percentage and Arg2 has two dollar amounts, the feature Arg1Percent-Arg2Money would have a count of 2. This feature is probably genre-dependent. Numbers and percentages often appear in financial texts but would be less frequent in other genres. WSJ-LM: This feature represents the extent to which the words in the text spans are typical of each relation. For each sense, we created unigram and bigram language models over the implicit examples in the training set. We compute each example’s probability according to each of these language models. The features are the ranks of the spans’ likelihoods according to the various language models. For example, if of the unigram models, the most likely relation to generate this example was Contingency, then the example would include the feature ContingencyUnigram1. If the third most likely relation according to the bigram models was Expansion, then it would include the feature ExpansionBigram3. Expl-LM: This feature ranks the text spans according to language models derived from the explicit examples in the TextRels corpus. However, the corpus contains only Cause, Contrast and Norelation, hence we expect the WSJ language models to be more helpful. Verbs: These features include the number of pairs of verbs in Arg1 and Arg2 from the same verb class. Two verbs are from the same verb class if each of their highest Levin verb class (Levin, 1993) levels (in the LCS Database (Dorr, 2001)) are the same. The intuition behind this feature is that the more related the verbs, the more likely the relation is an Expansion. The verb features also include the average length of verb phrases in each argument, as well as the cross product of this feature for the two arguments. We hypothesized that verb chunks that contain more words, such as “They [are allowed to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations. Our final verb features were the part of speech tags (gold-standard from the Penn Treebank) of the main verb. One would expect that Expansion would link sentences with the same tense, whereas Contingency and Temporal relations would contain verbs with different tenses. First-Last, First3: The first and last words of a relation’s arguments have been found to be particularly useful for predicting its sense (Wellner et al., 2006). Wellner et al. (2006) suggest that these words are such predictive features because they are often explicit discourse connectives. In our experiments on implicits, the first and last words are not connectives. However, some implicits have been found to be related by connective-like expressions which often appear in the beginning of the second argument. In the PDTB, these are annotated as alternatively lexicalized relations (AltLexes). To capture such effects, we included the first and last words of Arg1 as features, the first and last words of Arg2, the pair of the first words of Arg1 and Arg2, and the pair of the last words. We also add two additional features which indicate the first three words of each argument. Modality: Modal words, such as “can”, “should”, and “may”, are often used to express conditional statements (i.e. “If I were a wealthy man, I wouldn’t have to work hard.”) thus signaling a Contingency relation. We include a feature for the presence or absence of modals in Arg1 and Arg2, features for specific modal words, and their cross-products. Context: Some implicit relations appear immediately before or immediately after certain explicit relations far more often than one would expect due to chance (Pitler et al., 2008). We define a feature indicating if the immediately preceding (or following) relation was an explicit. If it was, we include the connective trigger of the relation and its sense as features. We use oracle annotations of the connective sense, however, most of the connectives are unambiguous. One might expect a different distribution of relation types in the beginning versus further in the middle of a paragraph. We capture paragraphposition information using a feature which indicates if Arg1 begins a paragraph. Word pairs Four variants of word pair models were used in our experiments. All the models were eventually tested on implicit examples from the PDTB, but the training set-up was varied. Wordpairs-TextRels In this setting, we trained a model on word pairs derived from unannotated text (TextRels corpus). Wordpairs-PDTBImpl Word pairs for training were formed from the cross product of words in the textual spans (Arg1 x Arg2) of the PDTB implicit relations. Wordpairs-selected Here, only word pairs from Wordpairs-PDTBImpl with non-zero information gain on the TextRels corpus were retained. Wordpairs-PDTBExpl In this case, the model was formed by using the word pairs from the explicit relations in the sections of the PDTB used for training. For all experiments, we used sections 2-20 of the PDTB for training and sections 21-22 for testing. Sections 0-1 were used as a development set for feature design. We ran four binary classification tasks to identify each of the main relations from the rest. As each of the relations besides Expansion are infrequent, we train using equal numbers of positive and negative examples of the target relation. The negative examples were chosen at random. We used all of sections 21 and 22 for testing, so the test set is representative of the natural distribution. The training sets contained: Comparison (1927 positive, 1927 negative), Contingency (3500 each), Expansion3 (6356 each), and Temporal (730 each). The test set contained: 151 examples of Comparison, 291 examples of Contingency, 986 examples of Expansion, 82 examples of Temporal, and 13 examples of No-relation. We used Naive Bayes, Maximum Entropy (MaxEnt), and AdaBoost (Freund and Schapire, 1996) classifiers implemented in MALLET. The performance using only our semantically informed features is shown in Table 7. Only the Naive Bayes classification results are given, as space is limited and MaxEnt and AdaBoost gave slightly lower accuracies overall. The table lists the f-score for each of the target relations, with overall accuracy shown in brackets. Given that the experiments are run on natural distribution of the data, which are skewed towards Expansion relations, the f-score is the more important measure to track. Our random baseline is the f-score one would achieve by randomly assigning classes in proportion to its true distribution in the test set. The best results for all four tasks are considerably higher than random prediction, but still low overall. Our features provide 6% to 18% absolute improvements in f-score over the baseline for each of the four tasks. The largest gain was in the Contingency versus Other prediction task. The least improvement was for distinguishing Expansion versus Other. However, since Expansion forms the largest class of relations, its f-score is still the highest overall. We discuss the results per relation class next. Comparison We expected that polarity features would be especially helpful for identifying Comparison relations. Surprisingly, polarity was actually one of the worst classes of features for Comparison, achieving an f-score of 16.33 (in contrast to using the first, last and first three words of the sentences as features, which leads to an f-score of 21.01). We examined the prevalence of positivenegative or negative-positive polarity pairs in our training set. 30% of the Comparison examples contain one of these opposite polarity pairs, while 31% of the Other examples contain an opposite polarity pair. To our knowledge, this is the first study to examine the prevalence of polarity words in the arguments of discourse relations in their natural distributions. Contrary to popular belief, Comparisons do not tend to have more opposite polarity pairs. The two most useful classes of features for recognizing Comparison relations were the first, last and first three words in the sentence and the context features that indicate the presence of a paragraph boundary or of an explicit relation just before or just after the location of the hypothesized implicit relation (19.32 f-score). Contingency The two best features for the Contingency vs. Other distinction were verb information (36.59 f-score) and first, last and first three words in the sentence (36.75 f-score). Context again was one of the features that led to improvement. This makes sense, as Pitler et al. (2008) found that implicit contingencies are often found immediately following explicit comparisons. We were surprised that the polarity features were helpful for Contingency but not Comparison. Again we looked at the prevalence of opposite polarity pairs. While for Comparison versus Other there was not a significant difference, for Contingency there are quite a few more opposite polarity pairs (52%) than for not Contingency (41%). The language model features were completely useless for distinguishing contingencies from other relations. Expansion As Expansion is the majority class in the natural distribution, recall is less of a problem than precision. The features that help achieve the best f-score are all features that were found to be useful in identifying other relations. Polarity tags, Inquirer tags and context were the best features for identifying expansions with f-scores around 70%. Temporal Implicit temporal relations are relatively rare, making up only about 5% of our test set. Most temporal relations are explicitly marked with a connective like “when” or “after”. Yet again, the first and last words of the sentence turned out to be useful indicators for temporal relations (15.93 f-score). The importance of the first and last words for this distinction is clear. It derives from the fact that temporal implicits often contain words like “yesterday” or “Monday” at the end of the sentence. Context is the next most helpful feature for temporal relations. For Comparison and Contingency, we analyze the behavior of word pair features under several different settings. Specifically we want to address two important related questions raised in recent work by others: (i) is unannotated data from explicits useful for training models that disambiguate implicit discourse relations and (ii) are explicit and implicit relations intrinsically different from each other. Wordpairs-TextRels is the worst approach. The best use of word pair features is Wordpairsselected. This model gives 4% better absolute fscore for Comparison and 14% for Contingency over Wordpairs-TextRels. In this setting the TextRels data was used to choose the word pair features, but the probabilities for each feature were estimated using the training portion of the PDTB implicit examples. We also confirm that even within the PDTB, information from annotated explicit relations (Wordpairs-PDTBExpl) is not as helpful as information from annotated implicit relations (Wordpairs-PDTBImpl). The absolute difference in f-score between the two models is close to 2% for Comparison, and 6% for Contingency. Adding other features to word pairs leads to improved performance for Contingency, Expansion and Temporal relations, but not for Comparison. For contingency detection, the best combination of our features included polarity, verb information, first and last words, modality, context with Wordpairs-selected. This combination led to a definite improvement, reaching an f-score of 47.13 (16% absolute improvement in f-score over Wordpairs-TextRels). For detecting expansions, the best combination of our features (polarity+Inquirer tags+context) outperformed Wordpairs-PDTBImpl by a wide margin, close to 13% absolute improvement (fscores of 76.42 and 63.84 respectively). Our results from the previous section show that classification of implicits benefits from information about nearby relations, and so we expected improvements using a sequence model, rather than classifying each relation independently. We trained a CRF classifier (Lafferty et al., 2001) over the sequence of implicit examples from all documents in sections 02 to 20. The test set is the same as used for the 2-way classifiers. We compare against a 6-way4 Naive Bayes classifier. Only word pairs were used as features for both. Overall 6-way prediction accuracy is 43.27% for the Naive Bayes model and 44.58% for the CRF model. We have presented the first study that predicts implicit discourse relations in a realistic setting (distinguishing a relation of interest from all others, where the relations occur in their natural distributions). Also unlike prior work, we separate the task from the easier task of explicit discourse prediction. Our experiments demonstrate that features developed to capture word polarity, verb classes and orientation, as well as some lexical features are strong indicators of the type of discourse relation. We analyze word pair features used in prior work that were intended to capture such semantic oppositions. We show that the features in fact do not capture semantic relation but rather give information about function word co-occurrences. However, they are still a useful source of information for discourse relation prediction. The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations, but then trained on manually annotated implicit relations. Context, in terms of paragraph boundaries and nearby explicit relations, also proves to be useful for the prediction of implicit discourse relations. It is helpful when added as a feature in a standard, instance-by-instance learning model. A sequence model also leads to over 1% absolute improvement for the task.","We have presented the first study that predicts implicit discourse relations in a realistic setting (distinguishing a relation of interest from all others, where the relations occur in their natural distributions). Also unlike prior work, we separate the task from the easier task of explicit discourse prediction. Our experiments demonstrate that features developed to capture word polarity, verb classes and orientation, as well as some lexical features are strong indicators of the type of discourse relation. We analyze word pair features used in prior work that were intended to capture such semantic oppositions. We show that the features in fact do not capture semantic relation but rather give information about function word co-occurrences. However, they are still a useful source of information for discourse relation prediction. The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations, but then trained on manually annotated implicit relations. Context, in terms of paragraph boundaries and nearby explicit relations, also proves to be useful for the prediction of implicit discourse relations. It is helpful when added as a feature in a standard, instance-by-instance learning model. A sequence model also leads to over 1% absolute improvement for the task."
32,"Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This an of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).","Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This an of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set). Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. While for many years it was thought not to help state-of-the art parsers, more recent work has shown otherwise. In this paper we apply this technique to parser adaptation. In particular we self-train the standard Charniak/Johnson Penn-Treebank (C/J) parser using unannotated biomedical data. As is well known, biomedical data is hard on parsers because it is so far from more “standard” English. To our knowledge this is the first application of self-training where the gap between the training and self-training data is so large. In section two, we look at previous work. In particular we note that there is, in fact, very little data on self-training when the corpora for self-training is so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation — initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to address this issue: the work by Bacchiani et al. (2006) and McClosky et al. (2006b). However, in both cases the evidence is equivocal. Bacchiani and Roark train the Roark parser (Roark, 2001) on trees from the Brown treebank and then self-train and test on data from Wall Street Journal. While they show some improvement (from 75.7% to 80.5% f-score) there are several aspects of this work which leave its results less than convincing as to the utility of selftraining for adaptation. The first is the parsing results are quite poor by modern standards.1 Steedman et al. (2003) generally found that selftraining does not work, but found that it does help if the baseline results were sufficiently bad. Secondly, the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great. One way to see this is to look at out-of-vocabulary statistics. The Brown corpus has an out-of-vocabulary rate of approximately 6% when given WSJ training as the lexicon. In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% (Lease and Charniak, 2005). Thus the bridge the selftrained parser is asked to build is quite short. This second point is emphasized by the second paper on self-training for adaptation (McClosky et al., 2006b). This paper is based on the C/J parser and thus its results are much more in line with modern expectations. In particular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data. Note this last point. It was not the case that it used the self-training to bridge the corpora difference. It self-trained on NANC, not Brown. NANC is a news corpus, quite like WSJ data. Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular. It said nothing about the task we address here. Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation. Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers. Their best result was an f-score of 80.2%. This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 'This is not a criticism of the work. The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank. the parser of Bikel (2004). The other parsers were not close. However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al., 2006)). However, since the newer parsers do not perform quite as well as the C/J parser on WSJ data, it is probably the case that they would not significantly alter the landscape. We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences — a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors. Thus the Genia treebank data are all from a small domain within Biology. As already noted, the Medline abstracts used for self-training were chosen randomly and thus span a large number of biomedical sub-domains. The results, the central results of this paper, are shown in Figure 1. Clegg and Shepherd (2005) do not provide separate precision and recall numbers. However we can see that the Medline self-trained parser achieves an f-score of 84.3%, which is an absolute reduction in error of 4.1%. This corresponds to an error rate reduction of 20% over the L/C baseline. Prior to the above experiment on the test data, we did several preliminary experiments on development data from the Genia Treebank. These results are summarized in Figure 2. Here we show the f-score for four versions of the parser as a function of number of self-training sentences. The dashed line on the bottom is the raw C/J parser with no self-training. At 80.4, it is clearly the worst of the lot. On the other hand, it is already better than the 80.2% best previous result for biomedical data. This is solely due to the introduction of the 50-best reranker which distinguishes the C/J parser from the preceding Charniak parser. The almost flat line above it is the C/J parser with NANC self-training data. As mentioned previously, NANC is a news corpus, quite like the original WSJ data. At 81.4% it gives us a one percent improvement over the original WSJ parser. The topmost line, is the C/J parser trained on Medline data. As can be seen, even just a thousand lines of Medline is already enough to drive our results to a new level and it continues to improve until about 150,000 sentences at which point performance is nearly flat. However, as 270,000 sentences is fractionally better than 150,000 sentences that is the number of self-training sentences we used for our results on the test set. Lastly, the middle jagged line is for an interesting idea that failed to work. We mention it in the hope that others might be able to succeed where we have failed. We reasoned that textbooks would be a particularly good bridging corpus. After all, they are written to introduce someone ignorant of a field to the ideas and terminology within it. Thus one might expect that the English of a Biology textbook would be intermediate between the more typical English of a news article and the specialized English native to the domain. To test this we created a corpus of seven texts (`BioBooks&quot;) on various areas of biology that were available on the web. We observe in Figure 2 that for all quantities of self-training data one does better with Medline than BioBooks. For example, at 37,000 sentences the BioBook corpus is only able to achieve and an f-measure of 82.8% while the Medline corpus is at 83.4%. Furthermore, BioBooks levels off in performance while Medline has significant improvement left in it. Thus, while the hypothesis seems reasonable, we were unable to make it work. We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts. By doing so we achieved a 20% error reduction over the best previous result for biomedical parsing. In terms of the gap between the supervised data and the self-trained data, this is the largest that has been attempted. Furthermore, the resulting parser is of interest in its own right, being as it is the most accurate biomedical parser yet developed. This parser is available on the web.4 Finally, there is no reason to believe that 84.3% is an upper bound on what can be achieved with current techniques. Lease and Charniak (2005) achieve their results using small amounts of hand-annotated biomedical part-ofspeech-tagged data and also explore other possible sources or information. It is reasonable to assume that its use would result in further improvement.","We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts. By doing so we achieved a 20% error reduction over the best previous result for biomedical parsing. In terms of the gap between the supervised data and the self-trained data, this is the largest that has been attempted. Furthermore, the resulting parser is of interest in its own right, being as it is the most accurate biomedical parser yet developed. This parser is available on the web.4 Finally, there is no reason to believe that 84.3% is an upper bound on what can be achieved with current techniques. Lease and Charniak (2005) achieve their results using small amounts of hand-annotated biomedical part-ofspeech-tagged data and also explore other possible sources or information. It is reasonable to assume that its use would result in further improvement."
33,"We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern. A candidate is if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies.","We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern. A candidate is if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. Knowing the semantic classes of words (e.g., “trout” is a kind of FISH) can be extremely valuable for many natural language processing tasks. Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), they are rarely complete, especially for large open classes (e.g., classes of people and objects) and rapidly changing categories (e.g., computer technology). (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet. Automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized categories or domains. A variety of methods have been developed for automatic semantic class identification, under the rubrics of lexical acquisition, hyponym acquisition, semantic lexicon induction, semantic class learning, and web-based information extraction. Many of these approaches employ surface-level patterns to identify words and their associated semantic classes. However, such patterns tend to overgenerate (i.e., deliver incorrect results) and hence require additional filtering mechanisms. To overcome this problem, we employed one single powerful doubly-anchored hyponym pattern to query the web and extract semantic class instances: CLASS NAME such as CLASS MEMBER and *. We hypothesized that a doubly-anchored pattern, which includes both the class name and a class member, would achieve high accuracy because of its specificity. To address concerns about coverage, we embedded the search in a bootstrapping process. This method produced many correct instances, but despite the highly restrictive nature of the pattern, still produced many incorrect instances. This result led us to explore new ways to improve the accuracy of hyponym patterns without requiring additional training resources. The main contribution of this work is a novel method for combining hyponym patterns with graph structures that capture two properties associated with pattern extraction: popularity and productivity. Intuitively, a candidate word (or phrase) is popular if it was discovered many times by other words (or phrases) in a hyponym pattern. A candidate word is productive if it frequently leads to the discovery of other words. Together, these two measures capture not only frequency of occurrence, but also crosschecking that the word occurs both near the class name and near other class members. We present two algorithms that use hyponym pattern linkage graphs (HPLGs) to represent popularity and productivity information. The first method uses a dynamically constructed HPLG to assess the popularity of each candidate and steer the bootstrapping process. This approach produces an efficient bootstrapping process that performs reasonably well, but it cannot take advantage of productivity information because of the dynamic nature of the process. The second method is a two-step procedure that begins with an exhaustive pattern search that acquires popularity and productivity information about candidate instances. The candidates are then ranked based on properties of the HPLG. We conducted experiments with four semantic classes, achieving high accuracies and outperforming the results reported by others who have worked on the same classes. A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1, although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The work most closely related to ours is Hearst’s early work on hyponym learning (Hearst, 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer to these as hyponym patterns. Pasca’s previously mentioned system (Pas¸ca, 2004) applies hyponym patterns to the web and acquires contexts around them. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the web and then evaluates them further by computing mutual information scores based on web queries. The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. However, their graph is based entirely on syntactic relations between words, while our graph captures the ability of instances to find each other in a hyponym pattern based on web querying, without any part-ofspeech tagging or parsing. Our work was motivated by early research on hyponym learning (Hearst, 1992), which applied patterns to a corpus to associate words with semantic classes. Hearst’s system exploited patterns that explicitly link a class name with a class member, such as “X and other Ys” and “Ys such as X”. Relying on surface-level patterns, however, is risky because incorrect items are frequently extracted due to polysemy, idiomatic expressions, parsing errors, etc. Our work began with the simple idea of using an extremely specific pattern to extract semantic class members with high accuracy. Our expectation was that a very specific pattern would virtually eliminate the most common types of false hits that are caused by phenomena such as polysemy and idiomatic expressions. A concern, however, was that an extremely specific pattern would suffer from sparse data and not extract many new instances. By using the web as a corpus, we hoped that the pattern could extract at least a few instances for virtually any class, and then we could gain additional traction by bootstrapping these instances. All of the work presented in this paper uses just one doubly-anchored pattern to identify candidate instances for a semantic class: <class name> such as <class member> and * This pattern has two variables: the name of the semantic class to be learned (class name) and a member of the semantic class (class member). The asterisk (*) indicates the location of the extracted words. We describe this pattern as being doubly-anchored because it is instantiated with both the name of the semantic class as well as a class member. For example, the pattern “CARS such as FORD and *” will extract automobiles, and the pattern “PRESIDENTS such as FORD and *” will extract presidents. The doubly-anchored nature of the pattern serves two purposes. First, it increases the likelihood of finding a true list construction for the class. Our system does not use part-of-speech tagging or parsing, so the pattern itself is the only guide for finding an appropriate linguistic context. Second, the doubly-anchored pattern virtually eliminates ambiguity because the class name and class member mutually disambiguate each other. For example, the word FORD could refer to an automobile or a person, but in the pattern “CARS such as FORD and *” it will almost certainly refer to an automobile. Similarly, the class “PRESIDENT” could refer to country presidents or corporate presidents, and “BUSH” could refer to a plant or a person. But in the pattern “PRESIDENTS such as BUSH”, both words will surely refer to country presidents. Another advantage of the doubly-anchored pattern is that an ambiguous or underspecified class name will be constrained by the presence of the class member. For example, to generate a list of company presidents, someone might naively define the class name as PRESIDENTS. A singly-anchored pattern (e.g., “PRESIDENTS such as *”) might generate lists of other types of presidents (e.g., country presidents, university presidents, etc.). Because the doubly-anchored pattern also requires a class member (e.g., “PRESIDENTS such as BILL GATES and *”), it is likely to generate only the desired types of instances. To evaluate the performance of the doubly-anchored pattern, we began by using the pattern to search the web and embedded this process in a simple bootstrapping loop, which is presented in Figure 1. As input, the user must provide the name of the desired semantic class (Class) and a seed example (Seed), which are used to instantiate the pattern. On the first iteration, the pattern is given to Google as a web query, and new class members are extracted from the retrieved text snippets. We wanted the system to be as language-independent as possible, so we refrained from using any taggers or parsing tools. As a result, instances are extracted using only word boundaries and orthographic information. For proper name classes, we extract all capitalized words that immediately follow the pattern. For common noun classes, we extract just one word, if it is not capitalized. Examples are shown below, with the extracted items underlined: countries such as China and Sri Lanka are ... fishes such as trout and bass can ... One limitation is that our system cannot learn multi-word instances of common noun categories, or proper names that include uncapitalized words (e.g., “United States of America”). These limitations could be easily overcome by incorporating a noun phrase (NP) chunker and extracting NPs. Each new class member is then used as a seed instance in the bootstrapping loop. We implemented this process as breadth-first search, where each “ply” of the search process is the result of bootstrapping the class members learned during the previous iteration as seed instances for the next one. During each iteration, we issue a new web query and add the newly extracted class members to the queue for the next cycle. We run this bootstrapping process for a fixed number of iterations (search ply), or until no new class members are produced. We will refer to this process as reckless bootstrapping because there are no checks of any kind. Every term extracted by the pattern is assumed to be a class member. Table 1 shows the results for 4 iterations of reckless bootstrapping for four semantic categories: U.S. states, countries, singers, and fish. The first two categories are relatively small, closed sets (our gold standard contains 50 U.S. states and 194 countries). The singers andfish categories are much larger, open sets (see Section 4 for details). quality deteriorates rapidly as bootstrapping progresses. Figure 2 shows the recall and precision curves for countries and states. High precision is achieved only with low levels of recall for countries. Our initial hypothesis was that such a specific pattern would be able to maintain high precision because non-class members would be unlikely to cooccur with the pattern. But we were surprised to find that many incorrect entries were generated for reasons such as broken expressions like “Merce -dez”, misidentified list constructions (e.g., “In countries such as China U.S. Policy is failing...”), and incomplete proper names due to insufficient length of the retrieved text snippet. Incorporating a noun phrase chunker would eliminate some of these cases, but far from all of them. We concluded that even such a restrictive pattern is not sufficient for semantic class learning on its own. In the next section, we present a new approach that creates a Hyponym Pattern Linkage Graph to steer bootstrapping and improve accuracy. Intuitively, we expect true class members to occur frequently in pattern contexts with other class members. To operationalize this intuition, we create a hyponym pattern linkage graph, which represents the frequencies with which candidate instances generate each other in the pattern contexts. We define a hyponym pattern linkage graph (HPLG) as a G = (V, E), where each vertex v ∈ V is a candidate instance and each edge (u, v) ∈ E means that instance v was generated by instance u. The weight w of an edge is the frequency with which u generated v. For example, consider the following sentence, where the pattern is italicized and the extracted instance is underlined: Countries such as China and Laos have been... In the HPLG, an edge e = (China, Laos) would be created because the pattern anchored by China extracted Laos as a new candidate instance. If this pattern extracted Laos from 15 different snippets, then the edge’s weight would be 15. The in-degree of a node represents its popularity, i.e., the number of instance occurrences that generated it. The graph is constructed dynamically as bootstrapping progresses. Initially, the seed is the only trusted class member and the only vertex in the graph. The bootstrapping process begins by instantiating the doubly-anchored pattern with the seed class member, issuing a web query to generate new candidate instances, and adding these new instances to the graph. A score is then assigned to every node in the graph, using one of several different metrics defined below. The highest-scoring unexplored node is then added to the set of trusted class members, and used as the seed for the next bootstrapping iteration. We experimented with three scoring functions for selecting nodes. The In-Degree (inD) score for vertex v is the sum of the weights of all incoming edges (u, v), where u is a trusted class member. Intuitively, this captures the popularity of v among instances that have already been identified as good instances. The Best Edge (BE) score for vertex v is the maximum edge weight among the incoming edges (u, v), where u is a trusted class member. The Key Player Problem (KPP) measure is used in social network analysis (Borgatti and Everett, 2006) to identify nodes whose removal would result in a residual network of minimum cohesion. A node receives a high value if it is highly connected and relatively close to most other nodes in the graph. The KPP score for vertex v is computed as: where d(u, v) is the shortest path between two vertices, where u is a trusted node. For tie-breaking, the distances are multiplied by the weight of the edge. Note that all of these measures rely only on incoming edges because a node does not acquire outgoing edges until it has already been selected as a trusted class member and used to acquire new instances. In the next section, we describe a two-step process for creating graphs that can take advantage of both incoming and outgoing edges. One way to try to confirm (or disconfirm) whether a candidate instance is a true class member is to see whether it can produce new candidate instances. If we instantiate our pattern with the candidate (i.e., “CLASS NAME such as CANDIDATE and *”) and successfully extract many new instances, then this is evidence that the candidate frequently occurs with the CLASS NAME in list constructions. We will refer to the ability of a candidate to generate new instances as its productivity. The previous bootstrapping algorithm uses a dynamically constructed graph that is constantly evolving as new nodes are selected and explored. Each node is scored based only on the set of instances that have been generated and identified as “trusted” at that point in the bootstrapping process. To use productivity information, we must adopt a different procedure because we need to know not only who generated each candidate, but also the complete set of instances that the candidate itself can generate. We adopted a two-step process that can use both popularity and productivity information in a hyponym pattern linkage graph to assess the quality of candidate instances. First, we perform reckless bootstrapping for a class name and seed until no new instances are generated. Second, we assign a score to each node in the graph using a scoring function that takes into account both the in-degree (popularity) and out-degree (productivity) of each node. We experimented with four different scoring functions, some of which were motivated by work on word sense disambiguation to identify the most “important” node in a graph containing its possible senses (Navigli and Lapata, 2007). The Out-degree (outD) score for vertex v is the weighted sum of v’s outgoing edges, normalized by the number of other nodes in the graph. This measure captures only productivity, while the next three measures consider both productivity and popularity. The Total-degree (totD) score for vertex v is the weighted sum of both incoming and outgoing edges, normalized by the number of other nodes in the graph. The Betweenness (BT) score (Freeman, 1979) considers a vertex to be important if it occurs on many shortest paths between other vertices. where Qst is the number of shortest paths from s to t, and Qst(v) is the number of shortest paths from s to t that pass through vertex v. PageRank (Page et al., 1998) establishes the relative importance of a vertex v through an iterative Markov chain model. The PageRank (PR) score of a vertex v is determined on the basis of the nodes it is connected to. a is a damping factor that we set to 0.85. We discarded all instances that produced zero productivity links, meaning that they did not generate any other candidates when used in web queries. We evaluated our algorithms on four semantic categories: U.S. states, countries, singers, and fish. The states and countries categories are relatively small, closed sets: our gold standards consist of 50 U.S. states and 194 countries (based on a list found on Wikipedia). The singers and fish categories are much larger, open classes. As our gold standard for fish, we used a list of common fish names found on Wikipedia.2 All the singer names generated by our algorithms were manually reviewed for correctness. We evaluated performance in terms of accuracy (the percentage of instances that were correct).3 Table 2 shows the accuracy results of the two algorithms that use hyponym pattern linkage graphs. We display results for the top-ranked N candidates, for all instances that have a productivity value > zero.4 The Popularity columns show results for the bootstrapping algorithm described in Section 3.3, using three different scoring functions. The results for the ranking algorithm described in Section 3.4 are shown in the Productivity (Prd) and Popularity&Productivity (Pop&Prd) columns. For the states, countries, and singers categories, we randomly selected 5 different initial seeds and then averaged the results. For the fish category we ran each algorithm using just the seed “salmon”. The popularity-based metrics produced good accuracies on the states, countries, and singers categories under all 3 scoring functions. For fish, KPP performed better than the others. The Out-degree (outD) scoring function, which uses only Productivity information, obtained the best results across all 4 categories. OutD achieved 100% accuracy for the first 50 states and fish, 100% accuracy for the top 150 countries, and 97% accuracy for the top 50 singers. The three scoring metrics that use both popularity and productivity also performed well, but productivity information by itself seems to perform better in some cases. It can be difficult to compare the results of different semantic class learners because there is no standard set of benchmark categories, so researchers report results for different classes. For the state and country categories, however, we can compare our results with that of other web-based semantic class learners such as Pasca (Pas¸ca, 2007a) and the KnowItAll system (Etzioni et al., 2005). For the U.S. states category, our system achieved 100% recall and 100% precision for the first 50 items generated, and KnowItAll performed similarly achieving 98% recall with 100% precision. Pasca did not evaluate his system on states. For the countries category, our system achieved 100% precision for the first 150 generated instances (77% recall). (Pas¸ca, 2007a) reports results of 100% precision for the first 25 instances generated, and 82% precision for the first 150 instances generated. The KnowItAll system (Etzioni et al., 2005) achieved 97% precision with 58% recall, and 79% precision with 87% recall.5 To the best of our knowledge, other researchers have not reported results for the singer and fish categories. Figure 3 shows the learning curve for both algorithms using their best scoring functions on the singer category with Placido Domingo as the initial seed. In total, 400 candidate words were generated. The Out-degree scoring function ranked the candidates well. Figure 3 also includes a vertical line indicating where the candidate list was cut (at 180 instances) based on the zero productivity cutoff. One observation is that the rankings do a good job of identifying borderline cases, which typically are ranked just below most correct instances but just above the obviously bad entries. For example, for states, the 50 U.S. states are ranked first, followed by 14 more entries (in order): The first 7 entries are all former states of the Soviet Union. In retrospect, we realized that we should have searched for “U.S. states” instead ofjust “states”. This example illustrates the power of the doubly-anchored hyponym pattern to correctly identify our intended semantic class by disambiguating our class name based on the seed class member. The algorithms also seem to be robust with respect to initial seed choice. For the states, countries, and singers categories, we ran experiments with 5 different initial seeds, which were randomly selected. The 5 country seeds represented a diverse set of nations, some of which are rarely mentioned in the news: Brazil, France, Guinea-Bissau, Uganda, and Zimbabwe. All of these seeds obtained > 92% recall with > 90% precision. We examined the incorrect instances produced by our algorithms and found that most of them fell into five categories. Type 1 errors were caused by incorrect proper name extraction. For example, in the sentence “states such as Georgia and English speaking countries like Canada...”, “English” was extracted as a state. These errors resulted from complex noun phrases and conjunctions, as well as unusual syntactic constructions. An NP chunker might prevent some of these cases, but we suspect that many of them would have been misparsed regardless. Type 2 errors were caused by instances that formerly belonged to the semantic class (e.g., SerbiaMontenegro and Czechoslovakia are no longer countries). In this error type, we also include borderline cases that could arguably belong to the semantic class (e.g., Wales as a country). Type 3 errors were spelling variants (e.g., Kyrgystan vs. Kyrgyzhstan) and name variants (e.g., Beyonce vs. Beyonce Knowles). Officially, every entity has one official spelling and one complete name, but in practice there are often variations that may occur nearly as frequently as the official name. For example, it is most common to refer to the singer Beyonce by just her first name. Type 4 errors were caused by sentences that were just flat out wrong in their factual assertions. For example, some sentences referred to “North America” as a country. Type 5 errors were caused by broken expressions found in the retrieved snippets (e.g. Michi -gan). These errors may be fixable by cleaning up the web pages or applying heuristics to prevent or recognize partial words. It is worth noting that incorrect instances of Types 2 and 3 may not be problematic to encounter in a dictionary or ontology. Name variants and former class members may in fact be useful to have. Combining hyponym patterns with pattern linkage graphs is an effective way to produce a highly accurate semantic class learner that requires truly minimal supervision: just the class name and one class member as a seed. Our results consistently produced high accuracy and for the states and countries categories produced very high recall. The singers and fish categories, which are much larger open classes, also achieved high accuracy and generated many instances, but the resulting lists are far from complete. Even on the web, the doublyanchored hyponym pattern eventually ran out of steam and could not produce more instances. However, all of our experiments were conducted using just a single hyponym pattern. Other researchers have successfully used sets of hyponym patterns (e.g., (Hearst, 1992; Etzioni et al., 2005; Pas¸ca, 2004)), and multiple patterns could be used with our algorithms as well. Incorporating additional hyponym patterns will almost certainly improve coverage, and could potentially improve the quality of the graphs as well. Our popularity-based algorithm was very effective and is practical to use. Our best-performing algorithm, however, was the 2-step process that begins with an exhaustive search (reckless bootstrapping) and then ranks the candidates using the Outdegree scoring function, which represents productivity. The first step is expensive, however, because it exhaustively applies the pattern to the web until no more extractions are found. In our evaluation, we ran this process on a single PC and it usually finished overnight, and we were able to learn a substantial number of new class instances. If more hyponym patterns are used, then this could get considerably more expensive, but the process could be easily parallelized to perform queries across a cluster of machines. With access to a cluster of ordinary PCs, this technique could be used to automatically create extremely large, high-quality semantic lexicons, for virtually any categories, without external training resources.","Combining hyponym patterns with pattern linkage graphs is an effective way to produce a highly accurate semantic class learner that requires truly minimal supervision: just the class name and one class member as a seed. Our results consistently produced high accuracy and for the states and countries categories produced very high recall. The singers and fish categories, which are much larger open classes, also achieved high accuracy and generated many instances, but the resulting lists are far from complete. Even on the web, the doublyanchored hyponym pattern eventually ran out of steam and could not produce more instances. However, all of our experiments were conducted using just a single hyponym pattern. Other researchers have successfully used sets of hyponym patterns (e.g., (Hearst, 1992; Etzioni et al., 2005; Pas¸ca, 2004)), and multiple patterns could be used with our algorithms as well. Incorporating additional hyponym patterns will almost certainly improve coverage, and could potentially improve the quality of the graphs as well. Our popularity-based algorithm was very effective and is practical to use. Our best-performing algorithm, however, was the 2-step process that begins with an exhaustive search (reckless bootstrapping) and then ranks the candidates using the Outdegree scoring function, which represents productivity. The first step is expensive, however, because it exhaustively applies the pattern to the web until no more extractions are found. In our evaluation, we ran this process on a single PC and it usually finished overnight, and we were able to learn a substantial number of new class instances. If more hyponym patterns are used, then this could get considerably more expensive, but the process could be easily parallelized to perform queries across a cluster of machines. With access to a cluster of ordinary PCs, this technique could be used to automatically create extremely large, high-quality semantic lexicons, for virtually any categories, without external training resources."
34,"reranking techniques ofsuffer from the limited scope of the best list, which rules out many potentially alternatives. We instead propose a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.","reranking techniques ofsuffer from the limited scope of the best list, which rules out many potentially alternatives. We instead propose a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank. Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing (Collins, 2000) and machine translation (Shen et al., 2005). Typically, this method first generates a list of top-n candidates from a baseline system, and then reranks this n-best list with arbitrary features that are not computable or intractable to compute within the baseline system. But despite its apparent success, there remains a major drawback: this method suffers from the limited scope of the nbest list, which rules out many potentially good alternatives. For example 41% of the correct parses were not in the candidates of ∼30-best parses in (Collins, 2000). This situation becomes worse with longer sentences because the number of possible interpretations usually grows exponentially with the sentence length. As a result, we often see very few variations among the n-best trees, for example, 50best trees typically just represent a combination of 5 to 6 binary ambiguities (since 25 < 50 < 26). Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005). However, we miss the benefits of non-local features that are not representable here. Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features. Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope. So we propose forest reranking, a technique inspired by forest rescoring (Huang and Chiang, 2007) that approximately reranks the packed forest of exponentially many parses. The key idea is to compute non-local features incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see Table 1). This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing. Although previous work on discriminative parsing has mainly focused on short sentences (≤ 15 words) (Taskar et al., 2004; Turian and Melamed, 2007), our work scales to the whole Treebank, where only at the root exact N/A we achieved an F-score of 91.7, which is a 19% error reduction from the 1-best baseline, and outperforms both 50-best and 100-best reranking. This result is also better than any previously reported systems trained on the Treebank. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989). For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions. Shown in Figure 1, this sentence has (at least) two derivations depending on the attachment of the prep. phrase PP3,6 “with a mirror”: it can either be attached to the verb “saw”, or be attached to “him”, which will be further combined with the verb to form the same VP as above. These two derivations can be represented as a single forest by sharing common sub-derivations. Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xz,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wz+1 ... wj). Each hyperedge e E E is a pair (tails(e), head(e)), where head(e) E V is the consequent node in the deductive step, and tails(e) E V ∗ is the list of antecedent nodes. For example, the hyperedge for deduction (*) is notated: We also denote IN(v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, in the forest in Figure 1, IN(VP1,6) is {e1, e2}, with e2 = ((VBD1,2, NP2,6), VP1,6). We call jej the arity of hyperedge e, which counts the number of tail nodes in e. The arity of a hypergraph is the maximum arity over all hyperedges. A CKY forest has an arity of 2, since the input grammar is required to be binary branching (cf. Chomsky Normal Form) to ensure cubic time parsing complexity. However, in this work, we use forests from a Treebank parser (Charniak, 2000) whose grammar is often flat in many productions. For example, the arity of the forest in Figure 1 is 3. Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it. There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length. We first establish a unified framework for parse reranking with both n-best lists and packed forests. For a given sentence s, a generic reranker selects the best parse y� among the set of candidates cand(s) according to some scoring function: In n-best reranking, cand(s) is simply a set of n-best parses from the baseline parser, that is, cand(s) = {y1, y2, ... , yn}. Whereas in forest reranking, cand(s) is a forest implicitly representing the set of exponentially many parses. As usual, we define the score of a parse y to be the dot product between a high dimensional feature representation and a weight vector w: where the feature extractor f is a vector of d functions f = (f1, ... , fd), and each feature fj maps a parse y to a real number fj(y). Following (Charniak and Johnson, 2005), the first feature f1(y) = log Pr(y) is the log probability of a parse from the baseline generative parser, while the remaining features are all integer valued, and each of them counts the number of times that a particular configuration occurs in parse y. For example, one such feature f2000 might be a question “how many times is a VP of length 5 surrounded by the word ‘has’ and the period? ” which is an instance of the WordEdges feature (see Figure 2(c) and Section 3.2 for details). Using a machine learning algorithm, the weight vector w can be estimated from the training data where each sentence si is labelled with its correct (“gold-standard”) parse y∗i . As for the learner, Collins (2000) uses the boosting algorithm and Charniak and Johnson (2005) use the maximum entropy estimator. In this work we use the averaged perceptron algorithm (Collins, 2002) since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods. Shown in Pseudocode 1, the perceptron algorithm makes several passes over the whole training data, and in each iteration, for each sentence si, it tries to predict a best parse yi among the candidates cand(si) using the current weight setting. Intuitively, we want the gold parse y∗i to be picked, but in general it is not guaranteed to be within cand(si), because the grammar may fail to cover the gold parse, and because the gold parse may be pruned away due to the limited scope of cand(si). So we define an oracle parse yz to be the candidate that has the highest Parseval F-score with respect to the gold tree y∗i :1 where function F returns the F-score. Now we train the reranker to pick the oracle parses as often as possible, and in case an error is made (line 6), perform an update on the weight vector (line 7), by adding the difference between two feature representations. 1If one uses the gold y∗i for oracle yz , the perceptron will continue to make updates towards something unreachable even when the decoder has picked the best possible candidate. Pseudocode 1 Perceptron for Generic Reranking In n-best reranking, since all parses are explicitly enumerated, it is trivial to compute the oracle tree.2 However, it remains widely open how to identify the forest oracle. We will present a dynamic programming algorithm for this problem in Sec. 4.1. We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data. This averaging effect has been shown to reduce overfitting and produce much more stable results (Collins, 2002). A key difference between n-best and forest reranking is the handling of features. In n-best reranking, all features are treated equivalently by the decoder, which simply computes the value of each one on each candidate parse. However, for forest reranking, since the trees are not explicitly enumerated, many features can not be directly computed. So we first classify features into local and non-local, which the decoder will process in very different fashions. We define a feature f to be local if and only if it can be factored among the local productions in a tree, and non-local if otherwise. For example, the Rule feature in Fig. 2(a) is local, while the ParentRule feature in Fig. 2(b) is non-local. It is worth noting that some features which seem complicated at the first sight are indeed local. For example, the WordEdges feature in Fig. 2(c), which classifies a node by its label, span length, and surrounding words, is still local since all these information are encoded either in the node itself or in the input sentence. In contrast, it would become non-local if we replace the surrounding words by surrounding POS 2In case multiple candidates get the same highest F-score, we choose the parse with the highest log probability from the baseline parser to be the oracle parse (Collins, 2000). tags, which are generated dynamically. More formally, we split the feature extractor f = (f1, ... , fd) into f = (fL; fN) where fL and fN are the local and non-local features, respectively. For the former, we extend their domains from parses to hyperedges, where f(e) returns the value of a local feature f E fL on hyperedge e, and its value on a parsey factors across the hyperedges (local productions), and we can pre-compute fL(e) for each e in a forest. Non-local features, however, can not be precomputed, but we still prefer to compute them as early as possible, which we call “on-the-fly” computation, so that our decoder can be sensitive to them at internal nodes. For instance, the NGramTree feature in Fig. 2 (d) returns the minimum tree fragement spanning a bigram, in this case “saw” and “the”, and should thus be computed at the smallest common ancestor of the two, which is the VP node in this example. Similarly, the ParentRule feature in Fig. 2 (b) can be computed when the S subtree is formed. In doing so, we essentially factor non-local features across subtrees, where for each subtree y′ in a parse y, we define a unit feature �f(y′) to be the part of f(y) that are computable within y′, but not computable in any (proper) subtree of y′. Then we have: Intuitively, we compute the unit non-local features at each subtree from bottom-up. For example, for the binary-branching node Ai,k in Fig. 3, the unit NGramTree instance is for the pair (wj−1, wj) on the boundary between the two subtrees, whose smallest common ancestor is the current node. Other unit NGramTree instances within this span have already been computed in the subtrees, except those for the boundary words of the whole node, wi and wk−1, which will be computed when this node is further combined with other nodes in the future. Before moving on to approximate decoding with non-local features, we first describe the algorithm for exact decoding when only local features are present, where many concepts and notations will be re-used later. We will use D(v) to denote the top derivations of node v, where D1(v) is its 1-best derivation. We also use the notation (e, j) to denote the derivation along hyperedge e, using the jith subderivation for tail ui, so (e, 1) is the best derivation along e. The exact decoding algorithm, shown in Pseudocode 2, is an instance of the bottom-up Viterbi algorithm, which traverses the hypergraph in a topological order, and at each node v, calculates its 1-best derivation using each incoming hyperedge e E IN(v). The cost of e, c(e), is the score of its Pseudocode 2 Exact Decoding with Local Features Pseudocode 3 Cube Pruning for Non-local Features (pre-computed) local features w · fL(e). This algorithm has a time complexity of O(E), and is almost identical to traditional chart parsing, except that the forest might be more than binary-branching. For non-local features, we adapt cube pruning from forest rescoring (Chiang, 2007; Huang and Chiang, 2007), since the situation here is analogous to machine translation decoding with integrated language models: we can view the scores of unit nonlocal features as the language model cost, computed on-the-fly when combining sub-constituents. Shown in Pseudocode 3, cube pruning works bottom-up on the forest, keeping a beam of at most k derivations at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation. When combining the subderivations along a hyperedge e to form a new subtree y′ = (e, j), we also compute its unit non-local feature values fN((e, j)) (line 25). A priority queue (heap in Pseudocode 3) is used to hold the candidates for the next-best derivation, which is initialized to the set of best derivations along each hyperedge (lines 7 to 9). Then at each iteration, we pop the best derivation (lines 12), and push its successors back into the priority queue (line 14). Analogous to the language model cost in forest rescoring, the unit feature cost here is a non-monotonic score in the dynamic programming backbone, and the derivations may thus be extracted out-of-order. So a buffer buf is used to hold extracted derivations, which is sorted at the end (line 15) to form the list of top-k derivations D(v) of node v. The complexity of this algorithm is O(E + V k log kJV) (Huang and Chiang, 2005), where O(JV) is the time for on-the-fly feature extraction for each subtree, which becomes the bottleneck in practice. Recall that the Parseval F-score is the harmonic mean of labelled precision P and labelled recall R: where |y |and |y* |are the numbers of brackets in the test parse and gold parse, respectively, and |y n y*| is the number of matched brackets. Since the harmonic mean is a non-linear combination, we can not optimize the F-scores on sub-forests independently with a greedy algorithm. In other words, the optimal F-score tree in a forest is not guaranteed to be composed of two optimal F-score subtrees. We instead propose a dynamic programming algorithm which optimizes the number of matched brackets for a given number of test brackets. For example, our algorithm will ask questions like, “when a test parse has 5 brackets, what is the maximum number of matched brackets?” More formally, at each node v, we compute an oracle function ora[v] : N H N, which maps an integer t to ora[v](t), the max. number of matched brackets Pseudocode 4 Forest Oracle Algorithm When node v is combined with another node u along a hyperedge e = ((v, u), w), we need to combine the two oracle functions ora[v] and ora[u] by distributing the test brackets of w between v and u, and optimize the number of matched bracktes. To do this we define a convolution operator ® between two functions f and g: where 1 is the indicator function, returning 1 if node w is found in the gold tree y*, in which case we increment the number of matched brackets. We can also express Eq. 9 in a purely functional form where 1� is a translation operator which shifts a function along the axes: Above we discussed the case of one hyperedge. If there is another hyperedge e′ deriving node w, we also need to combine the resulting oracle functions from both hyperedges, for which we define a pointwise addition operator ®: Shown in Pseudocode 4, we perform these computations in a bottom-up topological order, and finally at the root node TOP, we can compute the best global F-score by maximizing over different numbers of test brackets (line 7). The oracle tree y+ can be recursively restored by keeping backpointers for each ora[v](t), which we omit in the pseudocode. The time complexity of this algorithm for a sentence of l words is O(JEJ · l2(a−1)) where a is the arity of the forest. For a CKY forest, this amounts to O(l3 · l2) = O(l5), but for general forests like those in our experiments the complexities are much higher. In practice it takes on average 0.05 seconds for forests pruned by p = 10 (see Section 4.2), but we can pre-compute and store the oracle for each forest before training starts. Our forest pruning algorithm (Jonathan Graehl, p.c.) is very similar to the method based on marginal probability (Charniak and Johnson, 2005), except that ours prunes hyperedges as well as nodes. Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost a(v) for each node v, and then compute the merit aQ(e) for each hyperedge: Intuitively, this merit is the cost of the best derivation that traverses e, and the difference S(e) = aQ(e) − Q(TOP) can be seen as the distance away from the globally best derivation. We prune away all hyperedges that have S(e) > p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The key difference from (Charniak and Johnson, 2005) is that in this algorithm, a node can “partially” survive the beam, with a subset of its hyperedges pruned. In practice, this method prunes on average 15% more hyperedges than their method. We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (Marcus et al., 1993). The baseline parser is the Charniak parser, which we modified to output a packed forest for each sentence.3 We use the standard split of the Treebank: sections 02-21 as the training data (39832 sentences), section 22 as the development set (1700 sentences), and section 23 as the test set (2416 sentences). Following (Charniak and Johnson, 2005), the training set is split into 20 folds, each containing about 1992 sentences, and is parsed by the Charniak parser with a model trained on sentences from the remaining 19 folds. The development set and the test set are parsed with a model trained on all 39832 training sentences. We implemented both n-best and forest reranking systems in Python and ran our experiments on a 64bit Dual-Core Intel Xeon with 3.0GHz CPUs. Our feature set is summarized in Table 2, which closely follows Charniak and Johnson (2005), except that we excluded the non-local features Edges, NGram, and CoPar, and simplified Rule and NGramTree features, since they were too complicated to compute.4 We also added four unlexicalized local features from Collins (2000) to cope with data-sparsity. Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec. 02-21), and used a cut-off of 5 to prune away low-count features. There are 0.8M features in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M features in the updated version.5 However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Tables 3 and 4). This result confirms that our feature set design is appropriate, and the averaged perceptron learner is a reasonable candidate for reranking. The forests dumped from the Charniak parser are huge in size, so we use the forest pruning algorithm in Section 4.2 to prune them down to a reasonable size. In the following experiments we use a threshold of p = 10, which results in forests with an average number of 123.1 hyperedges per forest. Then for each forest, we annotate its forest oracle, and on each hyperedge, pre-compute its local features.6 Shown in Figure 4, these forests have an forest oracle of 97.8, which is 1.1% higher than the 50-best oracle (96.7), and are 8 times smaller in size. Table 3 compares the performance of forest reranking against standard n-best reranking. For both systems, we first use only the local features, and then all the features. We use the development set to determine the optimal number of iterations for averaged perceptron, and report the F1 score on the test set. With only local features, our forest reranker achieves an F-score of 91.25, and with the addition of noning on sec. 23. The pre-comp. column is for feature extraction, and training column shows the number of perceptron iterations that achieved best results on the dev set, and average time per iteration. local features, the accuracy rises to 91.69 (with beam size k = 15), which is a 0.26% absolute improvement over 50-best reranking.7 This improvement might look relatively small, but it is much harder to make a similar progress with n-best reranking. For example, even if we double the size of the n-best list to 100, the performance only goes up by 0.06% (Table 3). In fact, the 100best oracle is only 0.5% higher than the 50-best one (see Fig. 4). In addition, the feature extraction step in 100-best reranking produces huge data files and takes 44 hours in total, though this part can be parallelized.8 On two CPUs, 100-best reranking takes 25 hours, while our forest-reranker can also finish in 26 hours, with a much smaller disk space. Indeed, this demonstrates the severe redundancies as another disadvantage of n-best lists, where many subtrees are repeated across different parses, while the packed forest reduces space dramatically by sharing common sub-derivations (see Fig. 4). To put our results in perspective, we also compare them with other best-performing systems in Table 4. Our final result (91.7) is better than any previously reported system trained on the Treebank, although 7It is surprising that 50-best reranking with local features achieves an even higher F-score of 91.28, and we suspect this is due to the aggressive updates and instability of the perceptron, as we do observe the learning curves to be non-monotonic. We leave the use of more stable learning algorithms to future work. best-performing systems on the whole Section 23. Types D, G, and S denote discriminative, generative, and semi-supervised approaches, respectively. McClosky et al. (2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data. Moreover, their technique is orthogonal to ours, and we suspect that replacing their n-best reranker by our forest reranker might get an even better performance. Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008). Therefore, previous work often resorts to extremely short sentences (< 15 words) or only looked at local features (Taskar et al., 2004; Henderson, 2004; Turian and Melamed, 2007). In comparison, thanks to the efficient decoding, our work not only scaled to the whole Treebank, but also successfully incorporated non-local features, which showed an absolute improvement of 0.44% over that of local features alone.","We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (Marcus et al., 1993). The baseline parser is the Charniak parser, which we modified to output a packed forest for each sentence.3 We use the standard split of the Treebank: sections 02-21 as the training data (39832 sentences), section 22 as the development set (1700 sentences), and section 23 as the test set (2416 sentences). Following (Charniak and Johnson, 2005), the training set is split into 20 folds, each containing about 1992 sentences, and is parsed by the Charniak parser with a model trained on sentences from the remaining 19 folds. The development set and the test set are parsed with a model trained on all 39832 training sentences. We implemented both n-best and forest reranking systems in Python and ran our experiments on a 64bit Dual-Core Intel Xeon with 3.0GHz CPUs. Our feature set is summarized in Table 2, which closely follows Charniak and Johnson (2005), except that we excluded the non-local features Edges, NGram, and CoPar, and simplified Rule and NGramTree features, since they were too complicated to compute.4 We also added four unlexicalized local features from Collins (2000) to cope with data-sparsity. Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec. 02-21), and used a cut-off of 5 to prune away low-count features. There are 0.8M features in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M features in the updated version.5 However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Tables 3 and 4). This result confirms that our feature set design is appropriate, and the averaged perceptron learner is a reasonable candidate for reranking. The forests dumped from the Charniak parser are huge in size, so we use the forest pruning algorithm in Section 4.2 to prune them down to a reasonable size. In the following experiments we use a threshold of p = 10, which results in forests with an average number of 123.1 hyperedges per forest. Then for each forest, we annotate its forest oracle, and on each hyperedge, pre-compute its local features.6 Shown in Figure 4, these forests have an forest oracle of 97.8, which is 1.1% higher than the 50-best oracle (96.7), and are 8 times smaller in size. Table 3 compares the performance of forest reranking against standard n-best reranking. For both systems, we first use only the local features, and then all the features. We use the development set to determine the optimal number of iterations for averaged perceptron, and report the F1 score on the test set. With only local features, our forest reranker achieves an F-score of 91.25, and with the addition of noning on sec. 23. The pre-comp. column is for feature extraction, and training column shows the number of perceptron iterations that achieved best results on the dev set, and average time per iteration. local features, the accuracy rises to 91.69 (with beam size k = 15), which is a 0.26% absolute improvement over 50-best reranking.7 This improvement might look relatively small, but it is much harder to make a similar progress with n-best reranking. For example, even if we double the size of the n-best list to 100, the performance only goes up by 0.06% (Table 3). In fact, the 100best oracle is only 0.5% higher than the 50-best one (see Fig. 4). In addition, the feature extraction step in 100-best reranking produces huge data files and takes 44 hours in total, though this part can be parallelized.8 On two CPUs, 100-best reranking takes 25 hours, while our forest-reranker can also finish in 26 hours, with a much smaller disk space. Indeed, this demonstrates the severe redundancies as another disadvantage of n-best lists, where many subtrees are repeated across different parses, while the packed forest reduces space dramatically by sharing common sub-derivations (see Fig. 4). To put our results in perspective, we also compare them with other best-performing systems in Table 4. Our final result (91.7) is better than any previously reported system trained on the Treebank, although 7It is surprising that 50-best reranking with local features achieves an even higher F-score of 91.28, and we suspect this is due to the aggressive updates and instability of the perceptron, as we do observe the learning curves to be non-monotonic. We leave the use of more stable learning algorithms to future work. best-performing systems on the whole Section 23. Types D, G, and S denote discriminative, generative, and semi-supervised approaches, respectively. McClosky et al. (2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data. Moreover, their technique is orthogonal to ours, and we suspect that replacing their n-best reranker by our forest reranker might get an even better performance. Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008). Therefore, previous work often resorts to extremely short sentences (< 15 words) or only looked at local features (Taskar et al., 2004; Henderson, 2004; Turian and Melamed, 2007). In comparison, thanks to the efficient decoding, our work not only scaled to the whole Treebank, but also successfully incorporated non-local features, which showed an absolute improvement of 0.44% over that of local features alone."
35,"In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. recently, Cutting al. suggest that training can be achieved with a minimal lexicon and a limited amount priori about probabilities, by using an Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model. 1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of the probabilities, express the probability that a tag follows the preceding (or two for a second order model); and the the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational ef","In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. recently, Cutting al. suggest that training can be achieved with a minimal lexicon and a limited amount priori about probabilities, by using an Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model. 1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of the probabilities, express the probability that a tag follows the preceding (or two for a second order model); and the the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational ef Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model); and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency. For an introduction to the algorithms, see Cutting et at. (1992), or the lucid description by Sharman (1990). There are two principal sources for the parameters of the model. If a tagged corpus prepared by a human annotator is available, the transition and lexical probabilities can be estimated from the frequencies of pairs of tags and of tags associated with words. Alternatively, a procedure called BaumWelch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial model, and the resulting probabilities used to determine new values for the lexical and transition probabilities. By iterating the algorithm with the same corpus, the parameters of the model can be made to converge on values which are locally optimal for the given text. The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model. The algorithm is again described by Cutting et ad. and by Sharman, and a mathematical justification for it can be found in Huang et at. (1990). The first major use of HMMs for part of speech tagging was in CLAWS (Garside et a/., 1987) in the 1970s. With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alternatives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992; Brill, 1992), DeRose (DeRose, 1988) and Kupiec (Kupiec, 1992). One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992). An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus. The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible. Instead, an approximate model is constructed by hand, which is then improved by BW re-estimation on an untagged training corpus. In the above example, 8 iterations were sufficient. The initial model set up so that some transitions and some tags in the lexicon are favoured, and hence having a higher initial probability. Convergence of the model is improved by keeping the number of parameters in the model down. To assist in this, low frequency items in the lexicon are grouped together into equivalence classes, such that all words in a given equivalence class have the same tags and lexical probabilities, and whenever one of the words is looked up, then the data common to all of them is used. Re-estimation on any of the words in a class therefore counts towards re-estimation for all of them'. The results of the Xerox experiment appear very encouraging. Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible. However, some careful examination of the experiment is needed. In the first place, Cutting et a/. do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation. Secondly, it is unclear how much the initial biasing contributes the success rate. If significant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such intervention is needed on each new text domain. The kind of biasing Cutting et a/. describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not. The aim of this paper is to examine the role that training plays in the tagging process, by an experimental evaluation of how the accuracy of the tagger varies with the initial conditions. The results suggest that a completely unconstrained initial model does not produce good quality results, and that one 'The technique was originally developed by Kupiec (Kupiec, 1989). accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source. A second experiment shows that there are different patterns of re-estimation, and that these patterns vary more or less regularly with a broad characterisation of the initial conditions. The outcome of the two experiments together points to heuristics for making effective use of training and reestimation, together with some directions for further research. Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions. We will discuss this work below. The principal contribution of this work is to separate the effect of the lexical and transition parameters of the model, and to show how the results vary with different degree of similarity between the training and test data. The experiments were conducted using two taggers, one written in C at Cambridge University Computer Laboratory, and the other in C++ at Sharp Laboratories. Both taggers implement the FB, Viterbi and BW algorithms. For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i. Writing these as f(i, j), f(i) and f(i, w) respectively, the transition probability from tag i to tag j is estimated as f (i, j)/ f (i) and the lexical probability as f(i, w)/ f (i). Other estimation formulae have been used in the past. For example, CLAWS (Garside et al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag. Consulting the BaumWelch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy. Any transitions not seen in the training corpus are given a small, non-zero probability. The lexicon lists, for each word, all of tags seen in the training corpus with their probabilities. For words not found in the lexicon, all open-class tags are hypothesised. with equal probabilities. These words are added to the lexicon at the end of first iteration when re-estimation is being used, so that the probabilities of their hypotheses subsequently diverge from being uniform. To measure the accuracy of the tagger, we compare the chosen tag with one provided by a human annotator. Various methods of quoting accuracy have been used in the literature, the most common being the proport ion of words (tokens) receiving the correct tag. A better measure is the proportion of ambiguous words which are given the correct tag, where by ambiguous we mean that more than one tag was hypothesised. The former figure looks more impressive, but the latter gives a better measure of how well the tagger is doing, since it factors out the trivial assignment of tags to non-ambiguous words. For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1 — a + pa. All of the accuracy figures quoted below are for ambiguous words only. The training and test corpora were drawn from the LOB corpus and the Penn treebank. The hand tagging of these corpora is quite different. For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48. The general pattern of the results presented does not vary greatly with the corpus and tagset used. The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation. A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows: DO Un-degraded lexical probabilities, calculated from f (i, w) / f (i). D1 Lexical probabilities are correctly ordered, so that the most frequent tag has the highest lexical probability and so on, but the absolute values are otherwise unreliable. D2 Lexical probabilities are proportional to the overall tag frequencies, and are hence independent of the actual occurrence of the word in the training corpus. D3 All lexical probabilities have the same value, so that the lexicon contains no information other than the possible tags for each word. TO Un-degraded transition probabilities, calculated from f (i, j)/ f (i). Ti All transition probabilities have the same value. We could expect to achieve D1 from, say, a printed dictionary listing parts of speech in order of frequency. Perfect training is represented by case DO+TO. The Xerox experiments (Cutting et a/., 1992) correspond to something between D1 and D2, and between TO and Ti, in that there is some initial biasing of the probabilities. For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-l3-3 from parts B to J inclusive. Corpus LOBB-J was used to train the model, and LOB-B, LOBL and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data. In each case, the best accuracy (on ambiguous words, as usual) from the FB algorithm was noted. As an additional test, we tried assigning the most probable tag from the DO lexicon, completely ignoring tag-tag transitions. The results are summarised in table 1, for various corpora, where F denotes the &quot;most frequent tag&quot; test. As an example of how these figures relate to overall accuracies, LOB-B contains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J, and the overall accuracy in the DO+TO case is hence 98.69%. The general pattern of the results is similar across the three test corpora, with the only difference of interest being that case D3+TO does better for LOB-L than for the other two cases, and in particular does better than cases DO+T1 and Dl+Tl. A possible explanation is that in this case the test data does not overlap with the training data, and hence the good quality lexicons (DO and D1) have less of an influence. It is also interesting that D3+T1 does better than D2-FT1. The reasons for this are unclear, and the results are not always the same with other corpora, which suggests that they are not statistically significant. Several follow-up experiments were used to confirm the results: using corpora from the Penn treebank, using equivalence classes to ensure that all lexical entries have a total relative frequency of at least 0.01, and using larger corpora. The specific accuracies were different in the various tests, but the overall patterns remained much the same, suggesting that they are not an artifact of the tagset or of details of the text. The observations we can make about these results are as follows. Firstly, two of the tests, D2+T1 and D3-1-T1, give very poor performance. Their accuracy is not even as good as that achieved by picking the most frequent tag (although this of course implies a lexicon of DO or D1 quality). It follows that if BaumWelch re-estimation is to be an effective technique, the initial data must have either biasing in the transitions (the TO cases) or in the lexical probabilities (cases DO+T1 and D1-FT1), but it is not necessary to have both (D2/D3+TO and DO/Did-T1). Secondly, training from a hand-tagged corpus (case DO+TO) always does best, even when the test data is from a different source to the training data, as it is for LOB-L. So perhaps it is worth investing effort in hand-tagging training corpora after all, rather than just building a lexicon and letting reestimation sort out the probabilities. But how can we ensure that re-estimation will produce a good quality model? We look further at this issue in the next section. During the first experiment, it became apparent that Baum-Welch re-estimation sometimes decreases the accuracy as the iteration progresses. A second experiment was conducted to decide when it is appropriate to use Baum-Welch re-estimation at all. There seem to be three patterns of behaviour: Classical A general trend of rising accuracy on each iteration, with any falls in accuracy being local. It indicates that the model is converging towards an optimum which is better than its starting point. Initial maximum Highest accuracy on the first iteration, and falling thereafter. In this case the initial model is of better quality than BW can achieve. That is, while BW will converge on an optimum, the notion of optimality is with respect to the HMM rather than to the linguistic judgements about correct tagging. Early maximum Rising accuracy for a small number of iterations (2-4), and then falling as in initial maximum. An example of each of the three behaviours is shown in figure 1. The values of the accuracies and the test conditions are unimportant here; all we want to show is the general patterns. The second experiment had the aim of trying to discover which pattern applies under which circumstances, in order to help decide how to train the model. Clearly, if the expected pattern is initial maximum, we should not use BW at all, if early maximum, we should halt the process after a few iterations, and if classical, we should halt the process in a &quot;standard&quot; way, such as comparing the perplexity of successive models. The tests were conducted in a similar manner to those of the first experiment, by building a lexicon and transitions from a hand tagged training corpus, and then applying them to a test corpus with varying degrees of degradation. Firstly, four different degrees of degradation were used: no degradation at all, D2 degradation of the lexicon, Ti degradation of the transitions, and the two together. Secondly, we selected test corpora with varying degrees of similarity to the training corpus: the same text, text from a similar domain, and text which is significantly different. Two tests were conducted with each combination of the degradation and similarity, using different corpora (from the Penn treebank) ranging in size from approximately 50000 words to 500000 words. The re-estimation was allowed to run for ten iterations. The results appear in table 2, showing the best accuracy achieved (on ambiguous words). the iteration at which it occurred, and the pattern of re-estimation (I = initial maximum, E = early maximum, C = classical). The patterns are summarised in table 3, each entry in the table showing the patterns for the two tests under the given conditions. Although there is some variations in the readings, for example in the &quot;similar/DO+TO&quot; case, we can draw some general conclusions about the patterns obtained from different sorts of data. When the lexicon is degraded (D2), the pattern is always classical. With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum. When the test corpus is very similar to the model, then the pattern is initial maximum. Furthermore, examining the accuracies in table 2, in the cases of initial maximum and early maximum, the accuracy tends to be significantly higher than with classical behaviour. It seems likely that what is going on is that the model is converging to towards something of similar &quot;quality&quot; in each case, but when the pattern is classical, the convergence starts from a lower quality model and improves, and in the other cases, it starts from a higher quality one and deteriorates. In the case of early maximum, the few iterations where the accuracy is improving correspond to the creation of entries for unknown words and the fine tuning of ones for known ones, and these changes outweigh those produced by the re-estimation. From the observations in the previous section, we propose the following guidelines for how to train a HMM for use in tagging: able, use BW re-estimation with standard convergence tests such as perplexity. Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained. Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text. In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour. Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text. The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased. While these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model. Some preliminary experiments with using measures such as perplexity and the average probability of hypotheses show that, while they do give an indication of convergence during re-estimation, neither shows a strong correlation with the accuracy. Perhaps what is needed is a &quot;similarity measure&quot; between two models M and M', such that if a corpus were tagged with model M, M' is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus. However, preliminary experiments using such measures as the Kullback-Liebler distance between the initial and new models have again showed that it does not give good predictions of accuracy. In the end it may turn out there is simply no way of making the prediction without a source of information extrinsic to both model and corpus.","From the observations in the previous section, we propose the following guidelines for how to train a HMM for use in tagging: able, use BW re-estimation with standard convergence tests such as perplexity. Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained. Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text. In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour. Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text. The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased. While these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model. Some preliminary experiments with using measures such as perplexity and the average probability of hypotheses show that, while they do give an indication of convergence during re-estimation, neither shows a strong correlation with the accuracy. Perhaps what is needed is a &quot;similarity measure&quot; between two models M and M', such that if a corpus were tagged with model M, M' is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus. However, preliminary experiments using such measures as the Kullback-Liebler distance between the initial and new models have again showed that it does not give good predictions of accuracy. In the end it may turn out there is simply no way of making the prediction without a source of information extrinsic to both model and corpus."
36,"Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.","Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora. A large number of current language processing systems use a part-of-speech tagger for pre-processing. The tagger assigns a (unique or ambiguous) part-ofspeech tag to each token in the input and passes its output to the next processing level, usually a parser. Furthermore, there is a large interest in part-ofspeech tagging for corpus annotation projects, who create valuable linguistic resources by a combination of automatic processing and human correction. For both applications, a tagger with the highest possible accuracy is required. The debate about which paradigm solves the part-of-speech tagging problem best is not finished. Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996). They are only surpassed by combinations of different systems, forming a &quot;voting tagger&quot;. Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words. This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging. The tagger comparison was organized as a &quot;blackbox test&quot;: set the same task to every tagger and compare the outcomes. This paper describes the models and techniques used by TnT together with the implementation. The reader will be surprised how simple the underlying model is. The result of the tagger comparison seems to support the maxime &quot;the simplest is the best&quot;. However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application. We argue that it is not only the choice of the general model that determines the result of the tagger but also the various &quot;small&quot; decisions on alternatives. The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999). TnT uses second order Markov models for part-ofspeech tagging. The states of the model represent tags, outputs represent the words. Transition probabilities depend on the states, thus pairs of tags. Output probabilities only depend on the most recent category. To be explicit, we calculate for a given sequence of words w1 of length T. t1 tr are elements of the tagset, the additional tags t_1, to, and t7-,±1 are beginning-of-sequence and end-of-sequence markers. Using these additional tags, even if they stem from rudimentary processing of punctuation marks, slightly improves tagging results. This is different from formulas presented in other publications, which just stop with a &quot;loose end&quot; at the last word. If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of [.!? ;] as a token. Transition and output probabilities are estimated from a tagged corpus. As a first step, we use the maximum likelihood probabilities P which are derived from the relative frequencies: for all t1, t2, t3 in the tagset and w3 in the lexicon. N is the total number of tokens in the training corpus. We define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero. As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below). Trigram probabilities generated from a corpus usually cannot directly be used because of the sparsedata problem. This means that there are not enough instances for each trigram to reliably estimate the probability. Furthermore, setting a probability to zero because the corresponding trigram never occured in the corpus has an undesired effect. It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability. The smoothing paradigm that delivers the best results in TnT is linear interpolation of unigrams, bigrams, and trigrams. Therefore, we estimate a trigram probability as follows: P are maximum likelihood estimates of the probabilities, and A1 + A2 ± A3 = 1, SO P again represent probability distributions. We use the context-independent variant of linear interpolation, i.e., the values of the As do not depend on the particular trigram. Contrary to intuition, this yields better results than the context-dependent variant. Due to sparse-data problems, one cannot estimate a different set of As for each trigram. Therefore, it is common practice to group trigrams by frequency and estimate tied sets of As. However, we are not aware of any publication that has investigated frequency groupings for linear interpolation in part-of-speech tagging. All groupings that we have tested yielded at most equivalent results to contextindependent linear interpolation. Some groupings even yielded worse results. The tested groupings included a) one set of As for each frequency value and b) two classes (low and high frequency) on the two ends of the scale, as well as several groupings in between and several settings for partitioning the classes. The values of A1, A2, and A3 are estimated by deleted interpolation. This technique successively removes each trigram from the training corpus and estimates best values for the As from all other ngrams in the corpus. Given the frequency counts for uni-, bi-, and trigrams, the weights can be very efficiently determined with a processing time linear in the number of different trigrams. The algorithm is given in figure 1. Note that subtracting 1 means taking unseen data into account. Without this subtraction the model would overfit the training data and would generally yield worse results. Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993). Tag probabilities are set according to the word's ending. The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (.11) in 98% of the cases (e.g. fashionable, variable) , the rest of 2% are nouns (e.g. cable, variable). The probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum length. The term suffix as used here means &quot;final sequence of characters of a word&quot; which is not necessarily a linguistically meaningful suffix. Probabilities are smoothed by successive abstraction. This calculates the probability of a tag t given the last m letters i of an n letter word: P(t1/7„,+1,,..ln). The sequence of increasingly more general contexts omits more and more characters of the suffix, such that P(tlin-m+2, • • • P(tlin_m+3, ,i), , P(t) are used for smoothing. The recursion formula is set A = A2 = A3 = 0 foreach trigram t1,t2,t3 with f (ti,t2,t3) >0 depending on the maximum of the following three values: for i = m ... 0, using the maximum likelihood estimates P from frequencies in the lexicon, weights Oi and the initialization For the Markov model, we need the inverse conditional probabilities P(1,2_1+1, ... /Tilt) which are obtained by Bayesian inversion. A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0, (Samuelsson, 1993). This leaves room for interpretation. We use the longest suffix that we can find in the training set (i.e., for which the frequency is greater than or equal to 1), but at most 10 characters. This is an empirically determined choice. 2) We use a context-independent approach for 0„ as we did for the contextual weights A. It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10. 3) We use different estimates for uppercase and lowercase words, i.e., we maintain two different suffix tries depending on the capitalization of the word. This information improves the tagging results. 4) Another freedom concerns the choice of the words in the lexicon that should be used for suffix handling. Should we use all words, or are some of them better suited than others? Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words. Therefore, we restrict the procedure of suffix handling to words with a frequency smaller than or equal to some threshold value. Empirically, 10 turned out to be a good choice for this threshold. Additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information. Tags are usually not informative about capitalization, but probability distributions of tags around capitalized words are different from those not capitalized. The effect is larger for English, which only capitalizes proper names, and smaller for German, which capitalizes all nouns. We use flags ci that are true if wi is a capitalized word and false otherwise. These flags are added to the contextual probability distributions. Instead of and equations (3) to (5) are updated accordingly. This is equivalent to doubling the size of the tagset and using different tags depending on capitalization. The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam search. Each state that receives a 6 value smaller than the largest 6 divided by some threshold value 0 is excluded from further processing. While the Viterbi algorithm is guaranteed to find the sequence of states with the highest probability, this is no longer true when beam search is added. Nevertheless, for practical purposes and the right choice of 0, there is virtually no difference between the algorithm with and without a beam. Empirically, a value of 0 = 1000 turned out to approximately double the speed of the tagger without affecting the accuracy. The tagger currently tags between 30,000 and 60,000 tokens per second (including file I/O) on a Pentium 500 running Linux. The speed mainly depends on the percentage of unknown words and on the average ambiguity rate. We evaluate the tagger's performance under several aspects. First of all, we determine the tagging accuracy averaged over ten iterations. The overall accuracy, as well as separate accuracies for known and unknown words are measured. Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set). An important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments. We distinguish reliable from unreliable assignments by the quotient of the best and second best assignmentsl . All assignments for which this quotient is larger than some threshold are regarded as reliable, the others as unreliable. As we will see below, accuracies for reliable assignments are much higher. The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training. Each result is obtained by repeating the experiment 10 times with different partitions and averaging the single outcomes. In all experiments, contiguous test sets are used. The alternative is a round-robin procedure that puts every 10th sentence into the test set. We argue that contiguous test sets yield more realistic results because completely unseen articles are tagged. Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words. Therefore, we expect even 'By definition, this quotient is oo if there is only one possible tag for a given word. higher results when testing on every 10th sentence instead of a contiguous set of 10%. In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed. The tagger is allowed to assign exactly one tag to each token. We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens. The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon. The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997). It was developed at the Saarland University in Saarbriicken2. Part of it was tagged at the IMS Stuttgart. This evaluation only uses the partof-speech annotation and ignores structural annotations. Tagging accuracies for the NEGRA corpus are shown in table 2. Figure 3 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times, training and test sets were randomly chosen and disjoint, results were averaged. The training length is given on a logarithmic scale. It is remarkable that tagging accuracy for known words is very high even for very small training corpora. This means that we have a good chance of getting the right tag if a word is seen at least once during training. Average percentages of unknown tokens are shown in the bottom line of each diagram. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. If there is an alternative that has a probability &quot;close to&quot; that of the best assignment, this alternative can be viewed as almost equally well suited. The notion of &quot;close to&quot; is expressed by the distance of probabilities, and this in turn is expressed by the quotient of probabilities. So, the distance of the probabilities of a best tag tbest and an alternative tag tau is expressed by p(tbest)/P(talt)7 which is some value greater or equal to 1 since the best tag assignment has the highest probability. Figure 4 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments). As expected, we find that accuracies for percentage known unknown • overall unknowns acc. acc. acc. a Table 5: Part-of-speech tagging accuracy for the Penn Treebank. The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy. percentage known unknown overall unknowns acc. acc. acc. reliable assignments are much higher than for unreliable assignments. This distinction is, e.g., useful for annotation projects during the cleaning process, or during pre-processing, so the tagger can emit multiple tags if the best tag is classified as unreliable. We use the Wall Street Journal as contained in the Penn Treebank for our experiments. The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993). This evaluation only uses the part-ofspeech annotation. The Wall Street Journal part of the Penn Treebank consists of approx. 50,000 sentences (1.2 million tokens). Tagging accuracies for the Penn Treebank are shown in table 5. Figure 6 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times. Training and test sets were disjoint, results are averaged. The training length is given on a logarithmic scale. As for the NEGRA corpus, tagging accuracy is very high for known tokens even with small amounts of training data. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. Figure 7 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments). Again, we find that accuracies for reliable assignments are much higher than for unreliable assignments. Average part-of-speech tagging accuracy is between 96% and 97%, depending on language and tagset, which is at least on a par with state-of-the-art results found in the literature, possibly better. For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of results while Ratnaparkhi only makes one test run. The accuracy for known tokens is significantly higher than for unknown tokens. For the German newspaper data, results are 8.7% better when the word was seen before and therefore is in the lexicon, than when it was not seen before (97.7% vs. 89.0%). Accuracy for known tokens is high even with very small amounts of training data. As few as 1000 tokens are sufficient to achieve 95%-96% accuracy for them. It is important for the tagger to have seen a word at least once during training. Stochastic taggers assign probabilities to tags. We exploit the probabilities to determine reliability of assignments. For a subset that is determined during processing by the tagger we achieve accuracy rates of over 99%. The accuracy of the complement set is much lower. This information can, e.g., be exploited in an annotation project to give an additional treatment to the unreliable assignments, or to pass selected ambiguities to a subsequent processing step. We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature. For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor. The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words. Note that the decisions we made yield good results for both the German and the English Corpus. They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both. TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt).","We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature. For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor. The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words. Note that the decisions we made yield good results for both the German and the English Corpus. They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both. TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt)."
37,"Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.","Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard. Yet, relatively few have embedded one of these algorithms in a task. Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition. In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction. The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998. Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers? Manually creating sourcespecific training data for syntax was not required. Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation. We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998). The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts). For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it. For each person, one must find all of the person's names within the document, his/her type (civilian or military), and any significant descriptions (e.g., titles). For each location, one must also give its type (city, province, county, body of water, etc.). For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE. For the following example, the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein, a historian at the University of Pittsburgh who helped write...&quot; Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones. Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility. However, pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline. For example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure. There is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging. An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. A second consideration influenced our decision toward an integrated model. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al. 1997). Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model. Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model. Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model. If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model. Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties — especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs — would also benefit semantic analysis. Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations. An example of an augmented parse tree is shown in Figure 3. The five key facts in this example are: Here, each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label. For example, &quot;per-r&quot; identifies &quot;Nance&quot; as a named person, and &quot;per-desc-r&quot; identifies &quot;a paid consultant to ABC News&quot; as a person description. Other labels indicate relations among entities. For example, the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection. Further details are discussed in the section Tree Augmentation. To train our integrated model, we required a large corpus of augmented parse trees. Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events. Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches. Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events. The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus. Initially, we tried to annotate the training corpus by hand marking, for each sentence, the entire augmented tree. It soon became painfully obvious that this task could not be performed in the available time. Our annotation staff found syntactic analysis particularly complex and slow going. By necessity, we adopted the strategy of hand marking only the semantics. Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed. To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3. In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees. For each sentence, combining these two sources involved five steps. These steps are given below: syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument. For example, in the phrase &quot;Lt. Cmdr. David Edwin Lewis,&quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5. Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997). The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process. For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993). We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3. At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements. We pick up the derivation just after the topmost S and its head word, said, have been produced. The next steps are to generate in order: In this case, there are none. 8. Post-modifier constituents for the PER/NP. First a comma, then an SBAR structure, and then a second comma are each generated in turn. This generation process is continued until the entire tree has been produced. We now briefly summarize the probability structure of the model. The categories for head constituents, cl„ are predicted based solely on the category of the parent node, cp: Modifier constituent categories, cm, are predicted based on their parent node, cp, the head constituent of their parent node, chp, the previously generated modifier, c„,_1, and the head word of their parent, wp. Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags, t,,, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the head word, th, and the head word itself, wh: Head words, w„„ for modifiers are predicted based on the modifier, cm, the part-of-speech tag of the modifier word , t„„ the part-ofspeech tag of the head word , th, and the head word itself, wh: lAwmicm,tm,th,wh), e.g. Finally, word features, fm, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the modifier word , t„„ the part-of-speech tag of the head word th, the head word itself, wh, and whether or not the modifier head word, w„„ is known or unknown. The probability of a complete tree is the product of the probabilities of generating each element in the tree. If we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write: Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus. However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993). For modifier constituents, the mixture components are: For part-of-speech tags, the mixture components are: Finally, for word features, the mixture components are: Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation. More precisely, it must find the most likely augmented parse tree. Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search. The search is kept tractable through a combination of CKY-style dynamic programming and pruning of low probability elements. Whenever two or more constituents are equivalent relative to all possible later parsing decisions, we apply dynamic programming, keeping only the most likely constituent in the chart. Two constituents are considered equivalent if: threshold of the highest scoring constituent are maintained; all others are pruned. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997). We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node. Thus, the scores used in pruning can be considered as the product of: 1. The probability of generating a constituent of the specified category, starting at the topmost node. 2. The probability of generating the structure beneath that constituent, having already generated a constituent of that category. Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence. The semantics — that is, the entities and relations — can then be directly extracted from these sentential trees. Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging. The evaluation results are summarized in Table 1. In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants. Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points. Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding. We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test. The results are summarized in Table 2. While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-of-the-art levels for all cases. We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction. A single model proved capable of performing all necessary sentential processing, both syntactic and semantic. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model.","We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction. A single model proved capable of performing all necessary sentential processing, both syntactic and semantic. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model."
38,"There are five missing brackets which are indicated as &quot;*[&quot; or &quot;1&quot;. Words with a second NP tag were identified as proper nouns in a prepass. [A/AT former/AP top/NN aide/NN] to/IN [At-torney/NP/NP General/NP/NP Edwin/NP/NP Meese/NP/NP] interceded/VBD to/TO extend/VB [an/AT aircraftNN company/NN 's/$ govern-ment/NN contract/NN] ,/, then/RB went/VBD into/IN [business/NM] with/IN [a/AT lobby-ist/NN1 [who/WPS] worked/VBD for/IN [the/AT defense/NN contractor/NN] ,/, according/IN to/IN [a/AT published/VBN report/NN] ./. [James/NP/NP E/NP./NP Jenkins/NP/NP] ,/, [a/AT one-time/JJ senior/JJ deputy/NN] to/IN [Meese/NP/NP] joined/VBD [the/AT board/NN] of/IN [directors/NNS] of/IN [Transworld/NP/NP Group/NP/NP Ltd/NP./NP] on/IN [April/NP/NP 28/CD] ,/, [1984/CD] ,/, [the/AT Chicago/NP/NP Tribune/NP/NP] reporteci/VBD in/IN [its/PP$ Tuesday/NR editions/NNS] ./. [The/AT principal/JJ figure/NN] in/IN [Trans-world/NP/NP] was/BEDZ [Richard/NP/NP Mill-man/NP/NP] ,/, [a/AT lobbyist/NN] for/IN [Fair-child/NP/NP Industries/NP/NP Inc/NP./NP] ,/, Virginia/NP/NP de fense/NN con- 142 tractor/NN] I, [the/AT Tribune/NP/NP] said/VBD .1. [A/AT federal/JJ grand/JJ jury/NN] is/BEZ in- [the/AT Fairchild/NP/NP transaction/NN] and/CC [other/AP actions/NNS] of/IN [Meese/NP/NP] and/CC [former/AP White/NP/NP House/NP/NP aide/NN Nofziger/NP/NP] in/IN [connection/NN] with/IN [Wedtech/NP/NP New/NP/NP York/NP/NP defense/NN company/NN] [that/WPS] received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./. [Jenkins/NP/NP] left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP] as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1. [Deborah/NP/NP Tucker/NP/NP] ,/, [a/AT spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR] that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./. [Tucker/NP/NP] said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN 's/$ exof/1N [McKay/NP/NP 's/$ investigation/NN] to/TO include/VB [Meese/NP/NP] ./. [The/AT company/NN] is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/, [Tucker/NP/NP] said/VBD ./. [A/AT source/NN *] close/NN***] to/IN [McKay/NP/NP] said/VBD [last/AP week/NN&quot; that/CS [Meese/NP/NP] isn't/BEZ* under/IN [cruninalaJ investigation/NN] in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ [a/AT witness/NN] ./. [The/NP Tribune/NP/NP] said/VBD [Mill- ,/, acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/, [Va/NP.-based/NP company/NN] ,/, went/VBD to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO] and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB [the/AT production/NN] of/1/•1 [Fairchild/NP/NP 's/$ A-10/NP bomber/NN] for/IN [a/AT year/NN] ./. [Millman/NP/NP] said/VBD there/RB was/BEDZ [a/AT lucrative/JJ market/NN] in/IN [Third/NP/NP World/NP/NP countries/NNS] ,/, but/CC that/CS [Fairchild/NP/NP 's/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./. [The/AT Air/NP/NP Force/NP/NP] had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./. [The/AT newspaper/NN] said/VBD [one/CD source/NN] reported/VBD that/CS after/CS [Millman/NP/NP] made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./. [Memos/NP***] signed/VBD by/IN [Meese/NP/NP] ,/, stressing/VBG [the/AT importance/NN] of/IN [Fairchild/NP/NP 's/$ arranging/VBG sales/NNS] in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./. [Millman/NP/NP] did/DOD not/* return/VB [telephone/NN calls/NNS] to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS] [Monday/NR] ,I, [the/AT Tribune/NP/NP] said/VBD ./.","There are five missing brackets which are indicated as &quot;*[&quot; or &quot;1&quot;. Words with a second NP tag were identified as proper nouns in a prepass. [A/AT former/AP top/NN aide/NN] to/IN [At-torney/NP/NP General/NP/NP Edwin/NP/NP Meese/NP/NP] interceded/VBD to/TO extend/VB [an/AT aircraftNN company/NN 's/$ govern-ment/NN contract/NN] ,/, then/RB went/VBD into/IN [business/NM] with/IN [a/AT lobby-ist/NN1 [who/WPS] worked/VBD for/IN [the/AT defense/NN contractor/NN] ,/, according/IN to/IN [a/AT published/VBN report/NN] ./. [James/NP/NP E/NP./NP Jenkins/NP/NP] ,/, [a/AT one-time/JJ senior/JJ deputy/NN] to/IN [Meese/NP/NP] joined/VBD [the/AT board/NN] of/IN [directors/NNS] of/IN [Transworld/NP/NP Group/NP/NP Ltd/NP./NP] on/IN [April/NP/NP 28/CD] ,/, [1984/CD] ,/, [the/AT Chicago/NP/NP Tribune/NP/NP] reporteci/VBD in/IN [its/PP$ Tuesday/NR editions/NNS] ./. [The/AT principal/JJ figure/NN] in/IN [Trans-world/NP/NP] was/BEDZ [Richard/NP/NP Mill-man/NP/NP] ,/, [a/AT lobbyist/NN] for/IN [Fair-child/NP/NP Industries/NP/NP Inc/NP./NP] ,/, Virginia/NP/NP de fense/NN con- 142 tractor/NN] I, [the/AT Tribune/NP/NP] said/VBD .1. [A/AT federal/JJ grand/JJ jury/NN] is/BEZ in- [the/AT Fairchild/NP/NP transaction/NN] and/CC [other/AP actions/NNS] of/IN [Meese/NP/NP] and/CC [former/AP White/NP/NP House/NP/NP aide/NN Nofziger/NP/NP] in/IN [connection/NN] with/IN [Wedtech/NP/NP New/NP/NP York/NP/NP defense/NN company/NN] [that/WPS] received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./. [Jenkins/NP/NP] left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP] as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1. [Deborah/NP/NP Tucker/NP/NP] ,/, [a/AT spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR] that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./. [Tucker/NP/NP] said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN 's/$ exof/1N [McKay/NP/NP 's/$ investigation/NN] to/TO include/VB [Meese/NP/NP] ./. [The/AT company/NN] is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/, [Tucker/NP/NP] said/VBD ./. [A/AT source/NN *] close/NN***] to/IN [McKay/NP/NP] said/VBD [last/AP week/NN&quot; that/CS [Meese/NP/NP] isn't/BEZ* under/IN [cruninalaJ investigation/NN] in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ [a/AT witness/NN] ./. [The/NP Tribune/NP/NP] said/VBD [Mill- ,/, acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/, [Va/NP.-based/NP company/NN] ,/, went/VBD to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO] and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB [the/AT production/NN] of/1/•1 [Fairchild/NP/NP 's/$ A-10/NP bomber/NN] for/IN [a/AT year/NN] ./. [Millman/NP/NP] said/VBD there/RB was/BEDZ [a/AT lucrative/JJ market/NN] in/IN [Third/NP/NP World/NP/NP countries/NNS] ,/, but/CC that/CS [Fairchild/NP/NP 's/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./. [The/AT Air/NP/NP Force/NP/NP] had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./. [The/AT newspaper/NN] said/VBD [one/CD source/NN] reported/VBD that/CS after/CS [Millman/NP/NP] made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./. [Memos/NP***] signed/VBD by/IN [Meese/NP/NP] ,/, stressing/VBG [the/AT importance/NN] of/IN [Fairchild/NP/NP 's/$ arranging/VBG sales/NNS] in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./. [Millman/NP/NP] did/DOD not/* return/VB [telephone/NN calls/NNS] to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS] [Monday/NR] ,I, [the/AT Tribune/NP/NP] said/VBD ./. It is well-known that part of speech depends on context. The word &quot;table,&quot; for example, can be a verb in some contexts (e.g., &quot;He will table the motion&quot;) and a noun in others (e.g., &quot;The table is ready&quot;). A program has been written which tags each word in an input sentence with the most likely part of speech. The program produces the following output for the two &quot;table&quot; sentences just mentioned: (PPS = subject pronoun; MD = modal; VB = verb (no inflection); AT = article; NN = noun; BEZ = present 3rd sg form of &quot;to be&quot;; JJ = adjective; notation is borrowed from [Francis and Kucera, pp. 6-8]) Part of speech tagging is an important practical problem with potential applications in many areas including speech synthesis, speech recognition, spelling correction, proof-reading, query answering, machine translation and searching large text data bases (e.g., patents, newspapers). The author is particularly interested in speech synthesis applications, where it is clear that pronunciation sometimes depends on part of speech. Consider the following three examples where pronunciation depends on part of speech. First, there are words like &quot;wind&quot; where the noun has a different vowel than the verb. That is, the noun &quot;wind&quot; has a short vowel as in &quot;the wind is strong,&quot; whereas the verb &quot;wind&quot; has a long vowel as in &quot;Don't forget to wind your watch.&quot; Secondly, the pronoun &quot;that&quot; is stressed as in &quot;Did you see THAT?&quot; unlike the complementizer &quot;that,&quot; as in &quot;It is a shame that he's leaving.&quot; Thirdly, note the difference between &quot;oily FLUID&quot; and &quot;TRANSMISSION fluid&quot;; as a general rule, an adjective-noun sequence such as &quot;oily FLUID&quot; is typically stressed on the right whereas a noun-noun sequence such as &quot;TRANSMISSION fluid&quot; is typically stressed on the left. These are but three of the many constructions which would sound more natural if the synthesizer had access to accurate part of speech information. Perhaps the most important application of tagging programs is as a tool for future research. A number of large projects such as [Cobuild] have recently been collecting large corpora (101000 million words) in order to better describe how language is actually used in practice: &quot;For the first time, a dictionary has been compiled by the thorough examination of representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers... the dictionary is based on hard, measureable evidence.&quot; [Cobuild, p. xv] It is likely that there will be more and more research projects collecting larger and larger corpora. A reliable parts program might greatly enhance the value of these corpora to many of these researchers. The program uses a linear time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word j), and (b) contextual probabilities (probability of observing part of speech i given k previous parts of speech). Probability estimates were obtained by training on the Tagged Brown Corpus [Francis and Kucera], a corpus of approximately 1,000,000 words with part of speech tags assigned laboriously by hand over many years. Program performance is encouraging (95-99% &quot;correct&quot;, depending on the definition of &quot;correct&quot;). A small 400 word sample is presented in the Appendix, and is judged to be 99.5% correct. It is surprising that a local &quot;bottom-up&quot; approach can perform so well. Most errors are attributable to defects in the lexicon; remarkably few errors are related to the inadequacies of the extremely over-simplified grammar (a trigram model). Apparently, &quot;long distance&quot; dependences are not very important, at least most of the time. One might have thought that ngram models weren't adequate for the task since it is wellknown that they are inadequate for determining grammaticality: &quot;We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar. Furthermore, the particular subclass of such processes that produce norder statistical approximations to English do not come closer, with increasing n, to matching the output of an English grammar.&quot; [Chomsky, p. 113] Chomslcy's conclusion was based on the observation that constructions such as: have long distance dependencies that span across any fixed length window n. Thus, ngram models are clearly inadequate for many natural language applications. However, for the tagging application, the ngram approximation may be acceptable since long distance dependencies do not seem to be very important. Statistical ngram models were quite popular in the 1950s, and have been regaining popularity over the past few years. The IBM speech group is perhaps the strongest advocate of ngram methods, especially in other applications such as speech recognition. Robert Mercer (private communication, 1982) has experimented with the tagging application, using a restricted corpus (laser patents) and small vocabulary (1000 words). Another group of researchers working in Lancaster around the same time, Leech, Garside and Atwell, also found ngram models highly effective; they report 96.7% success in automatically tagging the LOB Corpus, using a bigram model modified with heuristics to cope with more important trigrams. The present work developed independently from the LOB project. Many people who have not worked in computational linguistics have a strong intuition that lexical ambiguity is usually not much of a problem. It is commonly believed that most words have just one part of speech, and that the few exceptions such as &quot;table&quot; are easily disambiguated by context in most cases. In contrast, most experts in computational linguists have found lexical ambiguity to be a major issue; it is said that practically any content word can be used as a noun, verb or adjective,I and that local context is not always adequate to disambiguate. Introductory texts are full of ambiguous sentences such as where no amount of syntactic parsing will help. These examples are generally taken to indicate that the parser must allow for multiple possibilities and that grammar formalisms such as LR(k) are inadequate for natural language since these formalisms cannot cope with ambiguity. This argument was behind a large set of objections to Marcus' &quot;LR(k)-like&quot; Deterministic Parser. Although it is clear that an expert in computational linguistics can dream up arbitrarily hard sentences, it may be, as Marcus suggested, that most texts are not very hard in practice. Recall that Marcus hypothesized most decisions can be resolved by the parser within a small window (i.e., three buffer cells), and there are only a few problematic cases where the parser becomes confused. He called these confusing cases &quot;garden paths,&quot; by analogy with the famous example: • The horse raced past the barn fell. With just a few exceptions such as these &quot;garden paths,&quot; Marcus assumes, there is almost always a unique &quot;best&quot; interpretation which Can be found with very limited resources. The proposed stochastic approach is largely compatible with this; the proposed approach 1. From an information theory point of view, one can quantity ambiguity in bits. In the case of the Brown Tagged Corpus, the lexical entropy, the conditional entropy of the part of speech given the word is about 0.25 bits per part of speech. This is considerably smaller than the contextual entropy, the conditional entropy of the part of speech given the next two parts of speech. This entropy is estimated to be about 2 bits per part of speech. assumes that it is almost always sufficient to assign each word a unique &quot;best&quot; part of speech (and this can be accomplished with a very efficient linear time dynamic programming algorithm). After reading introductory discussions of &quot;Flying planes can be dangerous,&quot; one might have expected that lexical ambiguity was so pervasive that it would be hopeless to try to assign just one part of speech to each word and in just one linear time pass over the input words. However, the proposed stochastic method is considerably simpler than what Marcus had in mind. His thesis parser used considerably more syntax than the proposed stochastic method. Consider the following pair described in [Marcus]: where it appears that the parser needs to look past an arbitrarily long noun phrase in order to correctly analyze &quot;have,&quot; which could be either a tenseless main verb (imperative) or a tensed auxiliary verb (question). Marcus' rather unusual example can no longer be handled by Fidditch, a more recent Marcus-style parser with very large coverage. In order to obtain such large coverage, Fidditch has had to take a more robust/modest view of lexical disambiguation. Whereas Marcus' Parsifal program distinguished patterns such as &quot;have NP tenseless&quot; and &quot;have NP past-participle,&quot; most of Fidditch's diagnostic rules are less ambitious and look only for the start of a noun phrase and do not attempt to look past an arbitrarily long noun phrase. For example, Fidditch has the following lexical disambiguation rule: which says that a preposition is more likely than a noun before a noun phrase. More precisely, the rule says that if a noun/preposition ambiguous word (e.g., &quot;out&quot;) is followed by something that starts a noun phrase (e.g., a determiner), then rule out the noun possibility. This type of lexical diagnostic rule can be captured with bigram and trigram statistics; it turns out that the sequence ...preposition determiner.., is much more common in the Brown Corpus (43924 observations) than the sequence ...noun determiner... (1135 observations). Most lexical disambiguation rules in Fidditch can be reformulated in terms of bigram and trigram statistics in this way. Moreover, it is worth doing so, because bigram and trigram statistics are much easier to obtain than Fidditch-type disambiguation rules, which are extremely tedious to program, test and debug. In addition, the proposed stochastic approach can naturally take advantage of lexical probabilities in a way that is not easy to capture with parsers that do not make use of frequency information. Consider, for example, the word &quot;see,&quot; which is almost always a verb, but does have an archaic nominal usage as in &quot;the Holy See.&quot; For practical purposes, &quot;see&quot; should not be considered noun/verb ambiguous in the same sense as truly ambiguous words like &quot;program,&quot; &quot;house&quot; and &quot;wind&quot;; the nominal usage of &quot;see&quot; is possible, but not likely. If every possibility in the dictionary must be given equal weight, parsing is very difficult. Dictionaries tend to focus on what is possible, not on what is likely. Consider the trivial sentence, &quot;I see a bird.&quot; For all practical purposes, every word in the sentence is unambiguous. According to [Francis and Kucera], the word &quot;I&quot; appears as a pronoun (PPLS) in 5837 out of 5838 observations (-100%), &quot;see&quot; appears as a verb in 771 out of 772 observations (-100%), &quot;a&quot; appears as an article in 23013 out of 23019 observations C100%) and &quot;bird&quot; appears as a noun in 26 out of 26 observations (-100%). However, according to Webster's Seventh New Collegiate Dictionary, every word is ambiguous. In addition to the desired assignments of tags, the first three words are listed as nouns and the last as an intransitive verb. One might hope that these spurious assignments could be ruled out by the parser as syntactically ill-formed. Unfortunately, this is unlikely to work. If the parser is going to accept noun phrases of the form: Similarly, the parser probably also has to accept &quot;bird&quot; as an intransitive verb, since there is nothing syntactically wrong with: These part of speech assignments aren't wrong; they are just extremely improbable. Consider once again the sentence, &quot;I see a bird.&quot; The problem is to find an assignment of parts of speech to words that optimizes both lexical and contextual probabilities, both of which are estimated from the Tagged Brown Corpus. The lexical probabilities are estimated from the following frequencies: (PPSS = pronoun; NP = proper noun; VB = verb; U11 = intellection; IN = preposition; AT = article; NN = noun) The lexical probabilities are estimated in the obvious way. For example, the probability that &quot;I&quot; is a pronoun, Prob(PPSS I &quot;I&quot;), is estimated as the freq(PPSS I &quot;I&quot;)/freq(&quot;I&quot;) or 5837/5838. The probability that &quot;see&quot; is a verb is estimated to be 771/772. The other lexical probability estimates follow the same pattern. The contextual probability, the probability of observing part of speech X given the following two parts of speech Y and Z, is estimated by dividing the trigram frequency XYZ by the bigram frequency YZ. Thus, for example, the probability of observing a verb before an article and a noun is estimated to be the ratio of the freq(VB, AT, NN) over the freq(AT, NN) or 3412/53091 = 0.064. The probability of observing a noun in the same context is estimated as the ratio of freq(NN, AT, NN) over 53091 or 629/53091 = 0.01. The other contextual probability estimates follow the same pattern. A search is performed in order to find the assignment of part of speech tags to words that optimizes the product of the lexical and contextual probabilities. Conceptually, the search enumerates all possible assignments of parts of speech to input words. In this case, there are four input words, three of which are two ways ambiguous, producing a set of 2*2*2*1=8 possible assignments of parts of Each of the eight sequences are then scored by the product of the lexical probabilities and the contextual probabilities, and the best sequence is selected. In this case, the first sequence is by far the best. In fact, it is not necessary to enumerate all possible assignments because the scoring function cannot see more than two words away. In other words, in the process of enumerating part of speech sequences, it is possible in some cases to know that some sequence cannot possibly compete with another and can therefore be abandoned. Because of this fact, only 0(n) paths will be enumerated. Let us illustrate this optimization with an example: Find all assignments of parts of speech to &quot;bird&quot; and score the partial sequence. Henceforth, all scores are to be interpreted as log probabilities.","Consider once again the sentence, &quot;I see a bird.&quot; The problem is to find an assignment of parts of speech to words that optimizes both lexical and contextual probabilities, both of which are estimated from the Tagged Brown Corpus. The lexical probabilities are estimated from the following frequencies: (PPSS = pronoun; NP = proper noun; VB = verb; U11 = intellection; IN = preposition; AT = article; NN = noun) The lexical probabilities are estimated in the obvious way. For example, the probability that &quot;I&quot; is a pronoun, Prob(PPSS I &quot;I&quot;), is estimated as the freq(PPSS I &quot;I&quot;)/freq(&quot;I&quot;) or 5837/5838. The probability that &quot;see&quot; is a verb is estimated to be 771/772. The other lexical probability estimates follow the same pattern. The contextual probability, the probability of observing part of speech X given the following two parts of speech Y and Z, is estimated by dividing the trigram frequency XYZ by the bigram frequency YZ. Thus, for example, the probability of observing a verb before an article and a noun is estimated to be the ratio of the freq(VB, AT, NN) over the freq(AT, NN) or 3412/53091 = 0.064. The probability of observing a noun in the same context is estimated as the ratio of freq(NN, AT, NN) over 53091 or 629/53091 = 0.01. The other contextual probability estimates follow the same pattern. A search is performed in order to find the assignment of part of speech tags to words that optimizes the product of the lexical and contextual probabilities. Conceptually, the search enumerates all possible assignments of parts of speech to input words. In this case, there are four input words, three of which are two ways ambiguous, producing a set of 2*2*2*1=8 possible assignments of parts of Each of the eight sequences are then scored by the product of the lexical probabilities and the contextual probabilities, and the best sequence is selected. In this case, the first sequence is by far the best. In fact, it is not necessary to enumerate all possible assignments because the scoring function cannot see more than two words away. In other words, in the process of enumerating part of speech sequences, it is possible in some cases to know that some sequence cannot possibly compete with another and can therefore be abandoned. Because of this fact, only 0(n) paths will be enumerated. Let us illustrate this optimization with an example: Find all assignments of parts of speech to &quot;bird&quot; and score the partial sequence. Henceforth, all scores are to be interpreted as log probabilities."
39,"Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rulebased methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.","Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rulebased methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. There has been a dramatic increase in the application of probabilistic models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact that very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [Church 88; Cutting et al. 92; DeRose 88; Deroualt and Merialdo 86; Garside et al. 87; Jelinek 85; Kupiec 89; Meteer et al. 911. Stochastic taggers have obtained a high degree of accuracy without performing any syntactic analysis on the input. These stochastic part of speech taggers make use of a Markov model which captures lexical and contextual information. The parameters of the model can be estimated from tagged ([Church 88; DeRose 88; Deroualt and Merialdo 86; Garside et al. 87; Meteer et al. 91]) or untag,ged ([Cutting et al. 92; Jelinek 85; Kupiec 89]) text. Once the parameters of the model are estimated, a sentence can then be automatically tagged by assigning it the tag sequence which is assigned the highest probability by the model. Performance is often enhanced with the aid of various higher level pre- and postprocessing procedures or by manually tuning the model. A number of rule-based taggers have been built [Klein and Simmons 63; Green and Rubin 71; Hindle 89]. [Klein and Simmons 63] and [Green and Rubin 71] both have error rates substantially higher than state of the art stochastic taggers. [Hindle 89] disambiguates words within a deterministic parser. We wanted to determine whether a simple rule-based tagger without any knowledge of syntax can perform as well as a stochastic tagger, or if part of speech tagging really is a domain to which stochastic techniques are better suited. In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models. The rule-based tagger overcomes the limitations common in rule-based approaches to language processing: it is robust, and the rules are automatically acquired. In addition, the tagger has many advantages over stochastic taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of finding and implementing improvements to the tagger, and better portability from one tag set or corpus genre to another. The tagger works by automatically recognizing and remedying its weaknesses, thereby incrementally improving its performance. The tagger initially tags by assigning each word its most likely tag, estimated by examining a large tagged corpus, without regard to context. In both sentences below, run would be tagged as a verb: The run lasted thirty minutes. 3. One of the two preceding (following) words is tagged We run three miles every day. The initial tagger has two procedures built in to improve performance; both make use of no contextual information. One procedure is provided with information that words that were not in the training corpus and are capitalized tend to be proper nouns, and attempts to fix tagging mistakes accordingly. This information could be acquired automatically (see below), but is prespecified in the current implementation. In addition, there is a procedure which attempts to tag words not seen in the training corpus by assigning such words the tag most common for words ending in the same three letters. For example, blahblahous would be tagged as an adjective, because this is the most common tag for words ending in ous. This information is derived automatically from the training corpus. This very simple algorithm has an error rate of about 7.9% when trained on 90% of the tagged Brown Corpus' [Francis and Kueera 82], and tested on a separate 5% of the corpus.2 Training consists of compiling a list of the most common tag for each word in the training corpus. The tagger then acquires patches to improve its performance. Patch templates are of the form: The initial tagger was trained on 90% of the corpus (the training corpus). 5% was held back to be used for the patch acquisition procedure (the patch corpus) and 5% for testing. Once the initial tagger is trained, it is used to tag the patch corpus. A list of tagging errors is compiled by comparing the output of the tagger to the correct tagging of the patch corpus. This list consists of triples < taga,tagb,number >, indicating the number of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch corpus. Next, for each error triple, it is determined which instantiation of a template from the prespecified set of pdtch templates results in the greatest error reduction. Currently, the patch templates are: Change tag a to tag b when: 8. The previous word is (is not) capitalized. For each error triple < taga,tagb, number > and patch, we compute the reduction in error which results from applying the patch to remedy the mistagging of a word as taga when it should have been tagged tagb. We then compute the number of new errors caused by applying the patch; that is, the number of times the patch results in a word being tagged as tagb when it should be tagged taga. The net improvement is calculated by subtracting the latter value from the former. For example, when the initial tagger tags the patch corpus, it mistags 159 words as verbs when they should be nouns. If the patch change the tag from verb to noun if one of the two preceding words is tagged as a determiner is applied, it corrects 98 of the 159 errors. However, it results in an additional 18 errors from changing tags which really should have been verb to noun. This patch results in a net decrease of 80 errors on the patch corpus. The patch which results in the greatest improvement to the patch corpus is added to the list of patches. The patch is then applied in order to improve the tagging of the patch corpus, and the patch acquisition procedure continues. The first ten patches found by the system are listed below3. The first patch states that if a word is tagged TO and the following word is tagged AT, then switch the tag from TO to IN. This is because a noun phrase is much more likely to immediately follow a preposition than to immediately follow infinitive TO. The second patch states that a tag should be switched from VBN to VBD if the preceding word is capitalized. This patch arises from two facts: the past verb tag is more likely than the past participle verb tag after a proper noun, and is also the more likely tag for the second word of the sentence.4 The third patch states that VBD should be changed to VBN if any of the preceding three words are tagged HVD. Once the list of patches has been acquired, new text can be tagged as follows. First, tag the text using the basic lexical tagger. Next, apply each patch in turn to the corpus to decrease the error rate. A patch which changes the tagging of a word from a to b only applies if the word has been tagged b somewhere in the training corpus. Note that one need not be too careful when constructing the list of patch templates. Adding a bad template to the list will not worsen performance. If a template is bad, then no rules which are instantiations of that template will appear in the final list of patches learned by the tagger. This makes it easy to experiment with extensions to the tagger. The tagger was tested on 5% of the Brown Corpus including sections from every genre. First, the test corpus was tagged by the simple lexical tagger. Next, each of the patches was in turn applied to the corpus. Below is a graph showing the improvement in accuracy from applying patches. It is significant that with only 71 patches, an error rate of 5.1% was obtained'. Of the 71 patches, 66 resulted in a reduction in the number of errors in the test corpus, 3 resulted in no net change, and 2 resulted in a higher number of errors. Almost all patches which were effective on the training corpus were also effective on the test corpus. Unfortunately, it is difficult to compare our results with other published results. In [Meteer et at. 91], an error rate of 3-4% on one domain, Wall Street Journal articles and 5.6% on another domain, texts on terrorism in Latin American countries, is quoted. However, both the domains and the tag set are different from what we use. [Church 88] reports an accuracy of &quot;95-99% correct, depending on the definition of correct&quot;. We implemented a version of the algorithm described by Church. When trained and tested on the same samples used in our experiment, we found the error rate to be about 4.5%. [DeRose 88] quotes a 4% error rate; however, the sample used for testing was part of the training corpus. [Garside et al. 87] reports an accuracy of 96-97%. Their probabilistic tagger has been augmented with a handcrafted procedure to pretag problematic &quot;idioms&quot;. This procedure, which requires that a list of idioms be laboriously created by hand, contributes 3% toward the accuracy of their tagger, according to [DeRose 88]. The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus. It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule-based tagger. For example, their tagger had difficulty tagging as old as. An explicit rule was written to pretag as old as with the proper tags. According to the tagging scheme of the Brown Corpus, the first as should be tagged as a qualifier, and the second as a subordinating conjunction. In the rule-based tagger, the most common tag for as is subordinating conjunction. So initially, the second as is tagged correctly and the first as is tagged incorrectly. To remedy this, the system acquires the patch: if the current word is tagged as a subordinating conjunction, and so is the word two positions ahead, then change the tag of the current word to gualifier.6 The rule-based tagger has automatically learned how to properly tag this &quot;idiom.&quot; Regardless of the precise rankings of the various taggers, we have demonstrated that a simple rule-based tagger with very few rules performs on par with stochastic taggers. 6This was one of the 71 patches acquired by the rule-based tagger.","The tagger was tested on 5% of the Brown Corpus including sections from every genre. First, the test corpus was tagged by the simple lexical tagger. Next, each of the patches was in turn applied to the corpus. Below is a graph showing the improvement in accuracy from applying patches. It is significant that with only 71 patches, an error rate of 5.1% was obtained'. Of the 71 patches, 66 resulted in a reduction in the number of errors in the test corpus, 3 resulted in no net change, and 2 resulted in a higher number of errors. Almost all patches which were effective on the training corpus were also effective on the test corpus. Unfortunately, it is difficult to compare our results with other published results. In [Meteer et at. 91], an error rate of 3-4% on one domain, Wall Street Journal articles and 5.6% on another domain, texts on terrorism in Latin American countries, is quoted. However, both the domains and the tag set are different from what we use. [Church 88] reports an accuracy of &quot;95-99% correct, depending on the definition of correct&quot;. We implemented a version of the algorithm described by Church. When trained and tested on the same samples used in our experiment, we found the error rate to be about 4.5%. [DeRose 88] quotes a 4% error rate; however, the sample used for testing was part of the training corpus. [Garside et al. 87] reports an accuracy of 96-97%. Their probabilistic tagger has been augmented with a handcrafted procedure to pretag problematic &quot;idioms&quot;. This procedure, which requires that a list of idioms be laboriously created by hand, contributes 3% toward the accuracy of their tagger, according to [DeRose 88]. The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus. It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule-based tagger. For example, their tagger had difficulty tagging as old as. An explicit rule was written to pretag as old as with the proper tags. According to the tagging scheme of the Brown Corpus, the first as should be tagged as a qualifier, and the second as a subordinating conjunction. In the rule-based tagger, the most common tag for as is subordinating conjunction. So initially, the second as is tagged correctly and the first as is tagged incorrectly. To remedy this, the system acquires the patch: if the current word is tagged as a subordinating conjunction, and so is the word two positions ahead, then change the tag of the current word to gualifier.6 The rule-based tagger has automatically learned how to properly tag this &quot;idiom.&quot; Regardless of the precise rankings of the various taggers, we have demonstrated that a simple rule-based tagger with very few rules performs on par with stochastic taggers. 6This was one of the 71 patches acquired by the rule-based tagger."
40,"We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.","We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. Following [5,10], our parser is based upon a probabilistic generative model. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368. The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability. That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w). What fundamentally distinguishes probabilistic generative parsers is how they compute p(r), and it is to that topic we turn next. The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c). Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question. Much of the interesting work is determining what goes into H (c). Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c). In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus. The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. The method we use follows that of [10]. In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols. (We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.) For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;. For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c). M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children. To the left of M is a sequence of one or more left labels Li (c) including the special termination symbol A, which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c). Thus an expansion e(c) looks like: The expansion is generated by guessing first M, then in order L1 through L,„.+1 (= A), and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself. In the simplest of such models, a zeroorder Markov grammar, each label on the righthand side is generated conditioned only on / — that is, according to the distributions p(Li j1), p(M I 1), and p(Ri I 1). More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar. So, for example, in a second-order Markov PCFG, L2 would be conditioned on L1 and M. In our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in Equation 1, e.g., p(e t, h, H). Thus we would use p(L2 I L1, M, 1, t, h, H). Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion. The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning. For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H). Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability. In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17]. A complete review of log-linear models is beyond the scope of this paper. Rather, we concentrate on the aspects of these models that most directly influenced the model presented here. To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features. For example, in computing the probability of the head's pre-terminal t we might want a feature schema f (t, 1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1, and zero otherwise. This feature is obviously composed of two sub-features, one recognizing t, the other 1. If both return 1, then the feature returns 1. Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A. The function Z(H), called the partition function, is a normalizing constant (for fixed H), so the probabilities over all a sum to one. Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(°,11). The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely. Maximum-entropy models have two benefits for a parser builder. First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used. This point is emphasized by Ratnaparkhi in discussing his parser [17). Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another. This is useful if one is using a loglinear model for smoothing. That is, suppose we want to compute a conditional probability p(a b, c), but we are not sure that we have enough examples of the conditioning event b, c in the training corpus to ensure that the empirically obtained probability P (a I b, c) is accurate. The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c). This method is known as &quot;deleted interpolation&quot; smoothing. In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4. The fact that the features are very far from independent is not a concern. Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate. As it stands, this last equation is pretty much content-free. But let us look at how it works for a particular case in our parsing scheme. Consider the probability distribution for choosing the pre-terminal for the head of a constituent. In Equation 1 we wrote this as p(t I 1, H). As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la). That is, we wish to compute p(t 1, lp, tp, lb, lg, hp). We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history. In many cases this is clearly warranted. For example, it does not seem to make much sense to condition on, say, hp without first conditioning on ti,. In other cases, however, we seem to be conditioning on apples and oranges, so to speak. For example, one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling, or the grandparent label. One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7. Rather than conditioning each term on the previous ones, they are now conditioned only on those aspects of the history that seem most relevant. The hope is that by doing this we will have less difficulty with the splitting of conditioning events, and thus somewhat less difficulty with sparse data. We make one more point on the connection of Equation 7 to a maximum entropy formulation. Suppose we were, in fact, going to compute a true maximum entropy model based upon the features used in Equation 7, Ii (t,1), f2(t,1,1p), f3(t,1,lp) . ... This requires finding the appropriate Ais for Equation 3, which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values. With no prior knowledge of values for the Ai one traditionally starts with Ai = 0, this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question. With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence. We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7. (Our experience is that rather than requiring 50 or so iterations, three suffice.) Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7. The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H). In the simple (content-free) form (Equation 6), it is clear that Z(H) = 1. In the more interesting version, Equation 7, this is not true in general, but one would not expect it to differ much from one, and we assume that as long as we are not publishing the raw probabilities (as we would be doing, for example, in publishing perplexity results) the difference from one should be unimportant. As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly. Naturally, the distributions required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words. We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word. As noted above, the probability model uses five smoothed probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with < 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length < 40 and all sentences of length < 100. Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work. As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones. The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1. As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers. Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9]. In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel. However, we do not think this aspect is the sole or even the most important reason for its comparative success. Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them. We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives. That parser, as stated in Figure 1, achieves an average precision/recall of 87.5. As noted in [5], that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus. This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. First, it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads. (It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.) Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses. Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40. In this section we evaluate the effects of the various changes we have made by running various versions of our current program. To avoid repeated evaluations based upon the testing corpus, here our evaluation is based upon sentences of length < 40 from the development corpus. We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus. For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus. This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus. This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus. The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program. It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar. This parser achieves an average precision/recall of 86.2%. This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus. Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser. One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three. As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question. In contrast, the current parser first guesses the head's pre-terminal, then the head, and then the expansion. It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12]. However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader. Indeed, it was lost on the present author until he went back after the fact and found it there. In Figure 2 we show that this one factor improves performance by nearly 2%. It may not be obvious why this should make so great a difference, since most words are effectively unambiguous. (For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].) We believe that two factors contribute to this performance gain. The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t). This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based. This one &quot;fix&quot; makes slightly over a percent difference in the results. The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion. For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h). So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not), the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened. For example, the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214. The second modification is the explicit marking of noun and verb-phrase coordination. We have already noted the importance of conditioning on the parent label /p. So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc. Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp. But nps and vps can occur with np and vp parents in non-coordinate structures as well. For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3. Note that the subordinate vp has a vp parent. Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure. A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly. Something very much like this is done in [15]. As shown in Figure 2, conditioning on this information gives a 0.6% improvement. We believe that this is mostly due to improvements in guessing the sub-constituent's pre-terminal and head. Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while. Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b. When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'. Note that we also tried including this information using a standard deleted-interpolation model. The results here are shown in the line &quot;Standard Interpolation&quot;. Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model. Indeed, the resulting performance is worse than not using this information at all. Up to this point all the models considered in this section are tree-bank grammar models. That is, the PCFG grammar rules are read directly off the training corpus. As already noted, our best model uses a Markov-grammar approach. As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser. However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser.","In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel. However, we do not think this aspect is the sole or even the most important reason for its comparative success. Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them. We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives. That parser, as stated in Figure 1, achieves an average precision/recall of 87.5. As noted in [5], that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus. This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. First, it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads. (It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.) Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses. Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40. In this section we evaluate the effects of the various changes we have made by running various versions of our current program. To avoid repeated evaluations based upon the testing corpus, here our evaluation is based upon sentences of length < 40 from the development corpus. We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus. For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus. This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus. This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus. The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program. It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar. This parser achieves an average precision/recall of 86.2%. This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus. Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser. One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three. As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question. In contrast, the current parser first guesses the head's pre-terminal, then the head, and then the expansion. It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12]. However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader. Indeed, it was lost on the present author until he went back after the fact and found it there. In Figure 2 we show that this one factor improves performance by nearly 2%. It may not be obvious why this should make so great a difference, since most words are effectively unambiguous. (For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].) We believe that two factors contribute to this performance gain. The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t). This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based. This one &quot;fix&quot; makes slightly over a percent difference in the results. The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion. For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h). So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not), the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened. For example, the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214. The second modification is the explicit marking of noun and verb-phrase coordination. We have already noted the importance of conditioning on the parent label /p. So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc. Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp. But nps and vps can occur with np and vp parents in non-coordinate structures as well. For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3. Note that the subordinate vp has a vp parent. Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure. A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly. Something very much like this is done in [15]. As shown in Figure 2, conditioning on this information gives a 0.6% improvement. We believe that this is mostly due to improvements in guessing the sub-constituent's pre-terminal and head. Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while. Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b. When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'. Note that we also tried including this information using a standard deleted-interpolation model. The results here are shown in the line &quot;Standard Interpolation&quot;. Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model. Indeed, the resulting performance is worse than not using this information at all. Up to this point all the models considered in this section are tree-bank grammar models. That is, the PCFG grammar rules are read directly off the training corpus. As already noted, our best model uses a Markov-grammar approach. As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser. However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser."
41,"This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results.","This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results. Word sense disambiguation is often cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense—tagged text using methods from statistics or machine learning. These approaches typically represent the context in which each sense—tagged instance of a word occurs with a set of linguistically motivated features. A learning algorithm induces a representative model from these features which is employed as a classifier to perform disambiguation. This paper presents a corpus—based approach that results in high accuracy by combining a number of very simple classifiers into an ensemble that performs disambiguation via a majority vote. This is motivated by the observation that enhancing the feature set or learning algorithm used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus—based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)). In natural language processing, ensemble techniques have been successfully applied to part— of—speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble. This paper begins with an introduction to the Naive Bayesian classifier. The features used to represent the context in which ambiguous words occur are presented, followed by the method for selecting the classifiers to include in the ensemble. Then, the line and interesi data is described. Experimental results disambiguating these words with an ensemble of Naive Bayesian classifiers are shown to rival previously published results. This paper closes with a discussion of the choices made in formulating this methodology and plans for future work. A Naive Bayesian classifier assumes that all the feature variables representing a problem are conditionally independent given the value of a classification variable. In word sense disambiguation, the context in which an ambiguous word occurs is represented by the feature variables (F1, F2, , Fn) and the sense of the ambiguous word is represented by the classification variable (S). In this paper, all feature variables Fi are binary and represent whether or not a particular word occurs within some number of words to the left or right of an ambiguous word, i.e., a window of context. For a Naive Bayesian classifier, the joint probability of observing a certain combination of contextual features with a particular sense is expressed as: The parameters of this model are p(S) and FilS)• The sufficient statistics, i.e., the summaries of the data needed for parameter estimation, are the frequency counts of the events described by the interdependent variables (Fi, S). In this paper, these counts are the number of sentences in the sensetagged text where the word represented by Fi occurs within some specified window of context of the ambiguous word when it is used in sense S. Any parameter that has a value of zero indicates that the associated word never occurs with the specified sense value. These zero values are smoothed by assigning them a very small default probability. Once all the parameters have been estimated, the model has been trained and can be used as a classifier to perform disambiguation by determining the most probable sense for an ambiguous word, given the context in which it occurs. The contextual features used in this paper are binary and indicate if a given word occurs within some number of words to the left or right of the ambiguous word. No additional positional information is contained in these features; they simply indicate if the word occurs within some number of surrounding words. Punctuation and capitalization are removed from the windows of context. All other lexical items are included in their original form; no stemming is performed and non-content words remain. This representation of context is a variation on the bag-of-words feature set, where a single window of context includes words that occur to both the left and right of the ambiguous word. An early use of this representation is described in (Gale et al., 1992), where word sense disambiguation is performed with a Naive Bayesian classifier. The work in this paper differs in that there are two windows of context, one representing words that occur to the left of the ambiguous word and another for those to the right. The left and right windows of context have nine different sizes; 0, 1, 2, 3, 4, 5, 10, 25, and 50 words. The first step in the ensemble approach is to train a separate Naive Bayesian classifier for each of the 81 possible combination of left and right window sizes. Naive_Bayes (1,r) represents a classifier where the model parameters have been estimated based on frequency counts of shallow lexical features from two windows of context; one including 1 words to the left of the ambiguous word and the other including r words to the right. Note that Naive_Bayes (0,0) includes no words to the left or right; this classifier acts as a majority classifier that assigns every instance of an ambiguous word to the most frequent sense in the training data. Once the individual classifiers are trained they are evaluated using previously held-out test data. The crucial step in building an ensemble is selecting the classifiers to include as members. The approach here is to group the 81 Naive Bayesian classifiers into general categories representing the sizes of the windows of context. There are three such ranges; narrow corresponds to windows 0, 1 and 2 words wide, medium to windows 3, 4, and 5 words wide, and wide to windows 10, 25, and 50 words wide. There are nine possible range categories since there are separate left and right windows. For example, Naive_Bayes(1,3) belongs to the range category (narrow, medium) since it is based on a one word window to the left and a three word window to the right. The most accurate classifier in each of the nine range categories is selected for inclusion in the ensemble. Each of the nine member classifiers votes for the most probable sense given the particular context represented by that classifier; the ensemble disambiguates by assigning the sense that receives a majority of the votes. The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997). The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments. The senses and their fresense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line - the experiments in this paper and previous work use a uniformly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for the use of money 1252 a share in a company or business 500 readiness to give attention 361 advantage, advancement or favor 178 activity that one gives attention to 66 causing attention to be given to 11 total 2368 Table 2: Distribution of senses for interest - the experiments in this paper and previous work use the entire corpus, where each sense occurs the number of times shown above. quency distribution are shown in Table 2. Unlike line, the sense distribution is skewed; the majority sense occurs in 53% of the sentences, while the smallest minority sense occurs in less than 1%. Eighty-one Naive Bayesian classifiers were trained and tested with the line and interest data. Fivefold cross validation was employed; all of the sensetagged examples for a word were randomly shuffled and divided into five equal folds. Four folds were used to train the Naive Bayesian classifier while the remaining fold was randomly divided into two equal sized test sets. The first, devtest, was used to evaluate the individual classifiers for inclusion in the ensemble. The second, test, was used to evaluate the accuracy of the ensemble. Thus the training data for each word consists of 80% of the available sensetagged text, while each of the test sets contains 10%. This process is repeated five times so that each fold serves as the source of the test data once. The average accuracy of the individual Naive Bayesian classifiers across the five folds is reported in Tables 3 and 4. The standard deviations were between .01 and .025 and are not shown given their relative consistency. Each classifier is based upon a distinct representation of context since each employs a different combination of right and left window sizes. The size and range of the left window of context is indicated along the horizontal margin in Tables 3 and 4 while the right window size and range is shown along the vertical margin. Thus, the boxes that subdivide each table correspond to a particular range category. The classifier that achieves the highest accuracy in each range category is included as a member of the ensemble. In case of a tie, the classifier with the smallest total window of context is included in the ensemble. The most accurate single classifier for line is Naive_Bayes (4,25), which attains accuracy of 84% The accuracy of the ensemble created from the most accurate classifier in each of the range categories is 88%. The single most accurate classifier for interest is Naive_Bayes(4,1), which attains accuracy of 86% while the ensemble approach reaches 89%. The increase in accuracy achieved by both ensembles over the best individual classifier is statistically significant, as judged by McNemar's test with p = .01. These experiments use the same sense-tagged corpora for interest and line as previous studies. Summaries of previous results in Tables 5 and 6 show that the accuracy of the Naive Bayesian ensemble is comparable to that of any other approach. However, due to variations in experimental methodologies, it can not be concluded that the differences among the most accurate methods are statistically significant. For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data. Similar differences in training and testing methodology exist among the other studies. Still, the results in this paper are encouraging due to the simplicity of the approach. The interest data was first studied by (Bruce and Wiebe, 1994). They employ a representation of context that includes the part-of-speech of the two words surrounding interest, a morphological feature indicating whether or not interest is singular or plural, and the three most statistically significant cooccurring words in the sentence with interest, as determined by a test of independence. These features are abbreviated as p-o-s, morph, and co-occur in Table 5. A decomposable probabilistic model is induced from the sense-tagged corpora using a backward sequential search where candidate models are evaluated with the log-likelihood ratio test. The selected model was used as a probabilistic classifier on a held-out set of test data and achieved accuracy of 78%. The interest data was included in a study by (Ng accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 89% when evaluated with the test data. and Lee, 1996), who represent the context of an ambiguous word with the part-of-speech of three words to the left and right of interest, a morphological feature indicating if interest is singular or plural, an unordered set of frequently occurring keywords that surround interest, local collocations that include interest, and verb-object syntactic relationships. These features are abbreviated p-o-s, morph, co-occur, collocates, and verb-obj in Table 5. A nearest-neighbor classifier was employed and achieved an average accuracy of 87% over repeated trials using randomly drawn training and test sets. (Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data. The first compares a range of probabilistic model selection methodologies and finds that none outperform the Naive Bayesian classifier, which attains accuracy of 74%. The second compares a range of machine learning algorithms and finds that a decision tree learner (78%) and a Naive Bayesian classifier (74%) are most accurate. The line data was first studied by (Leacock et al., 1993). They evaluate the disambiguation accuracy of a Naive Bayesian classifier, a content vector, and a neural network. The context of an ambiguous word is represented by a bag-of-words where the window of context is two sentences wide. This feature set is abbreviated as 2 sentence b-o-w in Table 6. When the Naive Bayesian classifier is evaluated words are not stemmed and capitalization remains. However, with the content vector and the neural network words are stemmed and words from a stop-list are removed. They report no significant differences in accuracy among the three approaches; the Naive Bayesian classifier achieved 71% accuracy, the content vector 72%, and the neural network 76%. The line data was studied again by (Mooney, 1996), where seven different machine learning methodologies are compared. All learning algorithms represent the context of an ambiguous word using the bag-of-words with a two sentence window of context. In these experiments words from a stoplist are removed, capitalization is ignored, and words are stemmed. The two most accurate methods in this study proved to be a Naive Bayesian classifier (72%) and a perceptron (71%). The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998). The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local context while the other represents topical context. The latter utilize a Naive Bayesian classifier. In both cases context is represented by a set of topical and local features. The topical features correspond to the open—class words that occur in a two sentence window of context. The local features occur within a window of context three words to the left and right of the ambiguous word and include co—occurrence features as well as the part—of—speech of words in this window. These features are represented as local & topical b-o-w and p-o-s in Table 6. (Towel! and Voorhees, 1998) report accuracy of 87% while (Leacock et al., 1998) report accuracy of 84%. The word sense disambiguation ensembles in this paper have the following characteristics: Each point is discussed below. The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data. This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features that have been shown to be optimal for word sense disambiguation. Shallow lexical features such as co—occurrences and collocations are recognized as potent sources of disambiguation information. While many other contextual features are often employed, it isn't clear that they offer substantial advantages. For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%. Preliminary experiments for this paper used feature sets that included collocates, co—occurrences, part—of— speech and grammatical information for surrounding words. However, it was clear that no combination of features resulted in disambiguation accuracy significantly higher than that achieved with co—occurrence features. The most accurate classifier from each of nine possible category ranges is selected as a member of the ensemble. This is based on preliminary experiments that showed that member classifiers with similar sized windows of context often result in little or no overall improvement in disambiguation accuracy. This was expected since slight differences in window sizes lead to roughly equivalent representations of context and classifiers that have little opportunity for collective improvement. For example, an ensemble was created for interest using the nine classifiers in the range category (medium, medium). The accuracy of this ensemble was 84%, slightly less than the most accurate individual classifiers in that range which achieved accuracy of 86%. Early experiments also revealed that an ensemble based on a majority vote of all 81 classifiers performed rather poorly. The accuracy for interest was approximately 81% and line was disambiguated with slightly less than 80% accuracy. The lesson taken from these results was that an ensemble should consist of classifiers that represent as differently sized windows of context as possible; this reduces the impact of redundant errors made by classifiers that represent very similarly sized windows of context. The ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors. This is discussed in the context of combining part—of—speech taggers in (Brill and Wu, 1998). They provide a measure for assessing the complementarity of errors between two taggers that could be adapted for use with larger ensembles such as the one discussed here, which has nine members. In this paper ensemble disambiguation is based on a simple majority vote of the nine member classifiers. An alternative strategy is to weight each vote by the estimated joint probability found by the Naive Bayesian classifier. However, a preliminary study found that the accuracy of a Naive Bayesian ensemble using a weighted vote was poor. For interest, it resulted in accuracy of 83% while for line it was 82%. The simple majority vote resulted in accuracy of 89% for interest and 88% for line. A number of issues have arisen in the course of this work that merit further investigation. The simplicity of the contextual representation can lead to large numbers of parameters in the Naive Bayesian model when using wide windows of context. Some combination of stop-lists and stemming could reduce the numbers of parameters and thus improve the overall quality of the parameter estimates made from the training data. In addition to simple co—occurrence features, the use of collocation features seems promising. These are distinct from co—occurrences in that they are words that occur in close proximity to the ambiguous word and do so to a degree that is judged statistically significant. One limitation of the majority vote in this paper is that there is no mechanism for dealing with outcomes where no sense gets a majority of the votes. This did not arise in this study but will certainly occur as Naive Bayesian ensembles are applied to larger sets of data. Finally, further experimentation with the size of the windows of context seems warranted. The current formulation is based on a combination of intuition and empirical study. An algorithm to determine optimal windows sizes is currently under development. This paper shows that word sense disambiguation accuracy can be improved by combining a number of simple classifiers into an ensemble. A methodology for formulating an ensemble of Naive Bayesian classifiers is presented, where each member classifier is based on co—occurrence features extracted from a different sized window of context. This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously published results.","This paper shows that word sense disambiguation accuracy can be improved by combining a number of simple classifiers into an ensemble. A methodology for formulating an ensemble of Naive Bayesian classifiers is presented, where each member classifier is based on co—occurrence features extracted from a different sized window of context. This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously published results."
42,"We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The errorrecognition system, ALEK, performs with about 80% precision and 20% recall.","We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The errorrecognition system, ALEK, performs with about 80% precision and 20% recall. A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence (Miller and Gildea, 1987). Much information about usage can be obtained from quite a limited context: Choueka and Lusignan (1985) found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it. Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000). The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word. We have developed a statistical system, ALEK (Assessing Lexical Knowledge), that uses statistical analysis for this purpose. A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate. Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word. The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences. A requirement for ALEK has been that all steps in the process be automated, beyond choosing the words to be tested and assessing the results. Once a target word is chosen, preprocessing, building a model of the word's appropriate usage, and identifying usage errors in essays is performed without manual intervention. ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service. TOEFL is taken by foreign students who are applying to US undergraduate and graduate-level programs. Approaches to detecting errors by non-native writers typically produce grammars that look for specific expected error types (Schneider and McCoy, 1998; Park, Palmer and Washburn, 1997). Under this approach, essays written by ESL students are collected and examined for errors. Parsers are then adapted to identify those error types that were found in the essay collection. We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem. Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for each sense. They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)). Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're. He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context. However, most grammatical errors are not the result of simple word confusions. This complicates the task of building a model of incorrect usage. One approach we considered was to proceed without such a model: represent appropriate word usage (across senses) in a single model and compare a novel example to that model. The most appealing part of this formulation was that we could bypass the knowledge acquisition bottleneck. All occurrences of the word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage. Inappropriate usage would be signaled by contextual cues that do not occur in training. Unfortunately, this approach was not effective for error detection. An example of a word usage error is often very similar to the model of appropriate usage. An incorrect usage can contain two or three salient contextual elements as well as a single anomalous element. The problem of error detection does not entail finding similarities to appropriate usage, rather it requires identifying one element among the contextual cues that simply does not fit. What kinds of anomalous elements does ALEK identify? Writers sometimes produce errors that violate basic principles of English syntax (e.g., a desks), while other mistakes show a lack of information about a specific vocabulary item (e.g., a knowledge). In order to detect these two types of problems, ALEK uses a 30-million word general corpus of English from the San Jose Mercury News (hereafter referred to as the general corpus) and, for each target word, a set of 10,000 example sentences from North American newspaper text' (hereafter referred to as the word-specific corpus). The corpora are extracted from the ACL-DCI corpora. In selecting the sentences for the word ALEK infers negative evidence from the contextual cues that do not co-occur with the target word — either in the word specific corpus or in the general English one. It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994). The Brill tagger output is post-processed to &quot;enrich&quot; some closed class categories of its tag set, such as subject versus object pronoun and definite versus indefinite determiner. The enriched tags were adapted from Francis and Kaera (1982). After the sentences have been preprocessed, ALEK counts sequences of adjacent part-ofspeech tags and function words (such as determiners, prepositions, and conjunctions). For example, the sequence a/AT full-time/I. 1 jobINN contributes one occurrence each to the bigrams AT+JJ, JJ+NN, a+JJ, and to the part-of-speech tag trigram AT+JJ+NN. Each individual tag and function word also contributes to its own unigram count. These frequencies form the basis for the error detection measures. From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks). Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent. Here we use this measure for the opposite purpose — to find combinations that occur less often than expected. ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the singular determiner a preceding a singular noun is common in English but rare when the noun is specific corpora, we tried to minimize the mismatch between the domains of newspapers and TOEFL essays. For example, in the newspaper domain, concentrate is usually used as a noun, as in orange juice concentrate but in TOEFL essays it is a verb 91% of the time. Sentence selection for the word specific corpora was constrained to reflect the distribution of part-of-speech tags for the target word in a random sample of TOEFL essays. knowledge). These divergences between the two corpora reflect syntactic properties that are peculiar to the target word. The system computes mutual information comparing the proportion of observed occurrences of bigrams in the general corpus to the proportion expected based on the assumption of independence, as shown below: Here, P(AB) is the probability of the occurrence of the AB bigram, estimated from its frequency in the general corpus, and P(A) and P(B) are the probabilities of the first and second elements of the bigram, also estimated from the general corpus. Ungrammatical sequences should produce bigram probabilities that are much smaller than the product of the unigram probabilities (the value of MI will be negative). Trigram sequences are also used, but in this case the mutual information computation compares the co-occurrence of ABC to a model in which A and C are assumed to be conditionally independent given B (see Lin, 1998). Once again, a negative value is often indicative of a sequence that violates a rule of English. ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus. The measures for bigrams and trigrams are similar to those given above except that the probability in the numerator is estimated from the wordspecific corpus and the probabilities in the denominator come from the general corpus. To return to a previous example, the phrase a knowledge contains the tag bigram for singular determiner followed by singular noun (AT NN). This sequence is much less common in the word-specific corpus for knowledge than would be expected from the general corpus unigram probabilities of AT and NN. In addition to bigram and trigram measures, ALEK compares the target word's part-ofspeech tag in the word-specific corpus and in the general corpus. Specifically, it looks at the conditional probability of the part-of-speech tag given the major syntactic category (e.g., plural noun given noun) in both distributions, by computing the following value. For example, in the general corpus, about half of all noun tokens are plural, but in the training set for the noun knowledge, the plural knowledges occurs rarely, if at all. The mutual information measures provide candidate errors, but this approach overgenerates — it finds rare, but still quite grammatical, sequences. To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times. This increases ALEK's precision at the price of reduced recall. For example, a knowledge will not be treated as an error because it appears in the training corpus as part of the longer a knowledge of sequence (as in a knowledge of mathematics). ALEK also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the x2 (chi square) statistic for the difference between the bigram proportions found in the word-specific and in the general corpus: = The x2 measure faces the same problem of overgenerating errors. Due to the large sample sizes, extreme values can be obtained even though effect size may be minuscule. To reduce false positives, ALEK requires that effect sizes be at least in the moderate-to-small range (Cohen and Cohen, 1983). Direct evidence from the word specific corpus can also be used to control the overgeneration of errors. For each candidate error, ALEK compares the larger context in which the bigram appears to the contexts that have been analyzed in the word-specific corpus. From the wordspecific corpus, ALEK forms templates, sequences of words and tags that represent the local context of the target. If a test sentence contains a low probability bigram (as measured by the x2 test), the local context of the target is compared to all the templates of which it is a part. Exceptions to the error, that is longer grammatical sequences that contain rare subsequences, are found by examining conditional probabilities. To illustrate this, consider the example of a knowledge and a knowledge of The conditional probability of of given a knowledge is high, as it accounts for almost all of the occurrences of a knowledge in the wordspecific corpus. Based on this high conditional probability, the system will use the template for a knowledge of to keep it from being marked as an error. Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error. TOEFL essays are graded on a 6 point scale, where 6 demonstrates &quot;clear competence&quot; in writing on rhetorical and syntactic levels and 1 demonstrates &quot;incompetence in writing&quot;. If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these ngrams. To test this prediction, we randomly selected from the TOEFL pool 50 essays for each of the 6 score values from 1.0 to 6.0. For each score value, all 50 essays were concatenated to form a super-essay. In every super-essay, for each adjacent pair and triple of tags containing a noun, verb, or adjective, the bigram and trigram mutual information values were computed based on the general corpus. Table 1 shows the proportions of bigrams and trigrams with mutual information less than —3.60. As predicted, there is a significant negative correlation between the score and the proportion of low probability bigrams (r,= -.94, n=6, p<.01, two-tailed) and trigrams (r,= -.84, n=6, p<.05, two-tailed). ALEK was developed using three target words that were extracted from TOEFL essays: concentrate, interest, and knowledge. These words were chosen because they represent different parts of speech and varying degrees of polysemy. Each also occurred in at least 150 sentences in what was then a small pool of TOEFL essays. Before development began, each occurrence of these words was manually labeled as an appropriate or inappropriate usage — without taking into account grammatical errors that might have been present elsewhere in the sentence but which were not within the target word's scope. Critical values for the statistical measures were set during this development phase. The settings were based empirically on ALEK's performance so as to optimize precision and recall on the three development words. Candidate errors were those local context sequences that produced a mutual information value of less than —3.60 based on the general corpus; mutual information of less than —5.00 for the specific/general comparisons; or a x2 value greater than 12.82 with an effect size greater than 0.30. Precision and recall for the three words are shown below. ALEK was tested on 20 words. These words were randomly selected from those which met two criteria: (1) They appear in a university word list (Nation, 1990) as words that a student in a US university will be expected to encounter and (2) there were at least 1,000 sentences containing the word in the TOEFL essay pool. To build the usage model for each target word, 10,000 sentences containing it were extracted from the North American News Corpus. Preprocessing included detecting sentence boundaries and part-of-speech tagging. As in the development system, the model of general English was based on bigram and trigram frequencies of function words and part-ofspeech tags from 30-million words of the San Jose Mercury News. For each test word, all of the test sentences were marked by ALEK as either containing an error or not containing an error. The size of the test set for each word ranged from 1,400 to 20,000 with a mean of 8,000 sentences. To evaluate the system, for each test word we randomly extracted 125 sentences that ALEK classified as containing no error (C-set) and 125 sentences which it labeled as containing an error (E-set). These 250 sentences were presented to a linguist in a random order for blind evaluation. The linguist, who had no part in ALEK's development, marked each usage of the target word as incorrect or correct and in the case of incorrect usage indicated how far from the target one would have to look in order to recognise that there was an error. For example, in the case of &quot;an period&quot; the error occurs at a distance of one word from period. When the error is an omission, as in &quot;lived in Victorian period&quot;, the distance is where the missing word should have appeared. In this case, the missing determiner is 2 positions away from the target. When more than one error occurred, the distance of the one closest to the target was marked. Table 3 lists the precision and recall for the 20 test words. The column labelled &quot;Recall&quot; is the proportion of human-judged errors in the 250sentence sample that were detected by ALEK. &quot;Total Recall&quot; is an estimate that extrapolates from the human judgements of the sample to the entire test set. We illustrate this with the results for pollution. The human judge marked as incorrect usage 91.2% of the sample from ALEK's E-set and 18.4% of the sample from its C-set. To estimate overall incorrect usage, we computed a weighted mean of these two rates, where the weights reflected the proportion of sentences that were in the E-set and C-set. The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%. With the human judgements as the gold standard, the estimated overall rate of incorrect usage is (.083 x .912 + .917 x .184) = .245. ALEK's estimated recall is the proportion of sentences in the E-set times its precision, divided by the overall estimated error rate (.083 x .912) / .245 = .310. The precision results vary from word to word. Conclusion and pollution have precision in the low to middle 90's while individual's precision is 57%. Overall, ALEK's predictions are about 78% accurate. The recall is limited in part by the fact that the system only looks at syntactic information, while many of the errors are semantic. Nicholls (1999) identifies four error types: an unnecessary word (*affect to their emotions), a missing word (*opportunity of job. ), a word or phrase that needs replacing (*every jobs), a word used in the wrong form (*pollutions). ALEK recognizes all of these types of errors. For closed class words, ALEK identified whether a word was missing, the wrong word was used (choice), and when an extra word was used. Open class words have a fourth error category, form, including inappropriate compounding and verb agreement. During the development stage, we found it useful to add additional error categories. Since TEOFL graders are not supposed to take punctuation into account, punctuation errors were only marked when they caused the judge to &quot;garden path&quot; or initially misinterpret the sentence. Spelling was marked either when a function word was misspelled, causing part-ofspeech tagging errors, or when the writer's intent was unclear. The distributions of categories for hits and misses, shown in Table 4, are not strikingly different. However, the hits are primarily syntactic in nature while the misses are both semantic (as in open-class:choice) and syntactic (as in closed-class:missing). ALEK is sensitive to open-class word confusions (affect vs effect) where the part of speech differs or where the target word is confused with another word (*In this aspect, ... instead of In this respect,...). In both cases, the system recognizes that the target is in the wrong syntactic environment. Misses can also be syntactic — when the target word is confused with another word but the syntactic environment fails to trigger an error. In addition, ALEK does not recognize semantic errors when the error involves the misuse of an open-class word in combination with the target (for example, make in &quot;they make benefits&quot;). Closed class words typically are either selected by or agree with a head word. So why are there so many misses, especially with prepositions? The problem is caused in part by polysemy — when one sense of the word selects a preposition that another sense does not. When concentrate is used spatially, it selects the preposition in, as &quot;the stores were concentrated in the downtown area&quot;. When it denotes mental activity, it selects the preposition on, as in &quot;Susan concentrated on her studies&quot;. Since ALEK trains on all senses of concentrate, it does not detect the error in &quot;Susan concentrated in her studies&quot;. Another cause is that adjuncts, especially temporal and locative adverbials, distribute freely in the wordspecific corpora, as in &quot;Susan concentrated in her room.&quot; This second problem is more tractable than the polysemy problem — and would involve training the system to recognize certain types of adjuncts. False positives, when ALEK &quot;identifies&quot; an error where none exists, fall into six major categories. The percentage of each false positive type in a random sample of 200 false positives is shown in Table 5. Domain mismatch: Mismatch of the newspaper-domain word-specific corpora and essay-domain test corpus. One notable difference is that some TOEFL essay prompts call for the writer's opinion. Consequently, TOEFL essays often contain first person references, whereas newspaper articles are written in the third person. We need to supplement the word-specific corpora with material that more closely resembles the test corpus. Tagger: Incorrect analysis by the part-of-speech tagger. When the part-of-speech tag is wrong, ALEK often recognizes the resulting n-gram as anomalous. Many of these errors are caused by training on the Brown corpus instead of a corpus of essays. Syntactic analysis: Errors resulting from using part-of-speech tags instead of supertags or a full parse, which would give syntactic relations between constituents. For example, ALEK false alarms on arguments of ditransitive verbs such as offer and flags as an error &quot;you benefits&quot; in &quot;offers you benefits&quot;. Free distribution: Elements that distribute freely, such as adverbs and conjunctions, as well as temporal and locative adverbial phrases, tend to be identified as errors when they occur in some positions. Punctuation: Most notably omission of periods and commas. Since these errors are not indicative of one's ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence. Infrequent tags. An undesirable result of our &quot;enriched&quot; tag set is that some tags, e.g., the post-determiner last, occur too infrequently in the corpora to provide reliable statistics. Solutions to some of these problems will clearly be more tractable than to others. Comparison of these results to those of other systems is difficult because there is no generally accepted test set or performance baseline. Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97. We created files of sentences used for the three development words concentrate, interest, and knowledge, and manually corrected any errors outside the local context around the target before checking them with Word97. The performance for concentrate showed overall precision of 0.89 and recall of 0.07. For interest, precision was 0.85 with recall of 0.11. In sentences containing knowledge, precision was 0.99 and recall was 0.30. Word97 correctly detected the ungrammaticality of knowledges as well as a knowledge, while it avoided flagging a knowledge of. In summary, Word97's precision in error detection is impressive, but the lower recall values indicate that it is responding to fewer error types than does ALEK. In particular, Word97 is not sensitive to inappropriate selection of prepositions for these three words (e.g., *have knowledge on history, *to concentrate at science). Of course, Word97 detects many kinds of errors that ALEK does not. Research has been reported on grammar checkers specifically designed for an ESL population. These have been developed by hand, based on small training and test sets. Schneider and McCoy (1998) developed a system tailored to the error productions of American Sign Language signers. This system was tested on 79 sentences containing determiner and agreement errors, and 101 grammatical sentences. We calculate that their precision was 78% with 54% recall. Park, Palmer and Washburn (1997) adapted a categorial grammar to recognize &quot;classes of errors [that] dominate&quot; in the nine essays they inspected. This system was tested on eight essays, but precision and recall figures are not reported. The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text. Preliminary results indicate that ALEK's error detection is predictive of TOEFL scores. If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores. We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK's component measures, the general corpus n-grams. However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well. Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points. As predicted, the correlation is negative (F., = -1.00, n = 6,p <.001, two-tailed). These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined. ALEK and by a human judge For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge's classification. Here, too, there is a negative correlation: rs = –.90, n = 5, p < .05, two-tailed. Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does. To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3. ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language. However, its techniques could be incorporated into a grammar checker for native speakers.","The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text. Preliminary results indicate that ALEK's error detection is predictive of TOEFL scores. If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores. We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK's component measures, the general corpus n-grams. However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well. Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points. As predicted, the correlation is negative (F., = -1.00, n = 6,p <.001, two-tailed). These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined. ALEK and by a human judge For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge's classification. Here, too, there is a negative correlation: rs = –.90, n = 5, p < .05, two-tailed. Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does. To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3. ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language. However, its techniques could be incorporated into a grammar checker for native speakers."
43,"We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation.","We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation. Diathesis alternations are alternate ways in which the arguments of a verb are expressed syntactically. The syntactic changes are sometimes accompanied by slight changes in the meaning of the verb. An example of the causative alternation is given in (1) below. In this alternation, the object of the transitive variant can also appear as the subject of the intransitive variant. In the conative alternation, the transitive form alternates with a prepositional phrase construction involving either at or on. An example of the conative alternation is given in (2). We refer to alternations where a particular semantic role appears in different grammatical roles in alternate realisations as &quot;role switching alternations&quot; (RsAs). It is these alternations that our method applies to. Recently, there has been interest in corpus-based methods to identify alternations (McCarthy and Korhonen, 1998; Lapata, 1999), and associated verb classifications (Stevenson and Merlo, 1999). These have either relied on a priori knowledge specified for the alternations in advance, or are not suitable for a wide range of alternations. The fully automatic method outlined here is applied to the causative and conative alternations, but is applicable to other RSAS. Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have suggested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (scF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not. They have also been suggested for the recovery of predicate argument structure, necessary for SCF acquisition (Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987). And Ribas (1995) showed that selectional preferences acquired using alternations performed better on a word sense disambiguation task compared to preferences acquired without alternations. He used alternations to indicate where the argument head data from different slots can be combined since it occupies the same semantic relationship with the predicate. Different diathesis alternations give different emphasis and nuances of meaning to the same basic content. These subtle changes of meaning are important in natural language generation (Stede, 1998). Alternations provide a means of reducing redundancy in the lexicon since the alternating scFs need not be enumerated for each individual verb if a marker is used to specify which verbs the alternation applies to. Alternations also provide a means of generalizing patterns of behaviour over groups of verbs, typically the group members are semantically related. Levin (1993) provides a classification of over 3000 verbs according to their participation in alternations involving NP and PP constituents. Levin's classification is not intended to be exhaustive. Automatic identification of alternations would be a useful tool for extending the classification with new participants. Levin's taxonomy might also be used alongside observed behaviour, to predict unseen behaviour. Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998). Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme. Dorr and Jones (1996) extend the classification by using grammatical information in LDOCE alongside semantic information in WordNet. What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource. Using corpora by-passes reliance on the availability and adequacy of mRDs. Additionally, the frequency information in corpora is helpful for estimating alternation productivity (Lapata, 1999). Estimations of productivity have been suggested for controlling the application of alternations (Briscoe and Copestake, 1996). We propose a method to acquire knowledge of alternation participation directly from corpora, with frequency information available as a by-product. We use both syntactic and semantic information for identifying participants in RsAs. Firstly, syntactic processing is used to find candidates taking the alternating SCFS. Secondly, selectional preference models are acquired for the argument heads associated with a specific slot in a specific SCF of a verb. We use the SCF acquisition system of Briscoe and Carroll (1997), with a probabilistic LR parser (Inui et al., 1997) for syntactic processing. The corpus data is POS tagged and lemmatised before the LR parser is applied. Subcategorization patterns are extracted from the parses, these include both the syntactic categories and the argument heads of the constituents. These subcategorization patterns are then classified according to a set of 161 SCF classes. The SCF entries for each verb are then subjected to a statistical filter which removes scFs that have occurred with a frequency less than would be expected by chance. The resulting SCF lexicon lists each verb with the scFs it takes. Each SCF entry includes a frequency count and lists the argument heads at all slots. Selectional preferences are automatically acquired for the slots involved in the role switching. We refer to these as the target slots. For the causative alternation, the slots are the direct object slot of the transitive SCF and the subject slot of the intransitive. For the conative, the slots are the direct object of the transitive and the PP of the up v pp SCF. Selectional preferences are acquired using the method devised by Li and Abe (1995). The preferences for a slot are represented as a tree cut model (Tcm). This is a set of disjoint classes that partition the leaves of the WordNet noun hypernym hierarchy. A conditional probability is attached to each of the classes in the set. To ensure the TCM covers all the word senses in WordNet, we modify Li and Abe's original scheme by creating hyponym leaf classes below all WordNet's hypernym (internal) classes. Each leaf holds the word senses previously held at the internal class. The nominal argument heads from a target slot are collected and used to populate the WordNet hierarchy with frequency information. The head lemmas are matched to the classes which contain them as synonyms. Where a lemma appears as a synonym in more than one class, its frequency count is divided between all classes for which it has direct membership. The frequency counts from hyponym classes are added to the count for each hypernym class. A root node, created above all the WordNet roots, contains the total frequency count for all the argument head lemmas found within WordNet. The minimum description length principle (MDL) (Rissanen, 1978) is used to find the best TCM by considering the cost (in bits) of describing both the model and the argument head data encoded in the model. The cost (or description length) for a TCM is calculated according to equation 1. The number of parameters of the model is given by k, this is the number of classes in the TCM minus one. S is the sample size of the argument head data. The cost of describing each argument head (n) is calculated using the log of the probability estimate for the classes on the TCM that n belongs to (ca). A small portion of the TCM for the object slot of start in the transitive frame is displayed in figure 1. WordNet classes are displayed in boxes with a label which best reflects the sense of the class. The probability estimates are shown for the classes along the TCM. Examples of the argument head data are displayed below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes. We assume that verbs which participate will show a higher degree of similarity between the preferences at the target slots compared with non-participating verbs. To compare the preferences we compare the probability distributions across WordNet using a measure of distributional similarity. Since the probability distributions may be at different levels of WordNet, we map the Tcms at the target slots to a common tree cut, a &quot;base cut&quot;. We experiment with two different types of base cut. The first is simply a base cut at the eleven root classes of WordNet. We refer to this as the &quot;root base cut&quot; (RBC). The second is termed the &quot;union base cut&quot; (PBc). This is obtained by taking all classes from the union of the two Tcms which are not subsumed by another class in this union. Duplicates are removed. Probabilities are assigned to the classes of a base cut using the estimates on the original TCM. The probability estimate for a hypernym class is obtained by combining the probability estimates for all its hyponyms on the original cut. Figure 2 exemplifies this process for two Tcms (Tcml and Tcm2) in an imaginary hierarchy. The UBC is at the classes B, C and D. To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence (asp) proposed by Lee (1999). 1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence. p1(x) and p2(x) are the two probability distributions which are being compared. The a constant is a value between 0 and We also experimented with euclidian distance, the LI norm, and cosine measures. The differences in performance of these measures were not statistically significant. 1 which smooths p1(x) with p2(x) so that asri is always defined. We use the same value (0.99) for a as Lee. If a is set to 1 then this measure is equivalent to the Kulback-Liebler divergence. We experiment with a SCF lexicon produced from 19.3 million words of parsed text from the BNC (Leech, 1992). We used the causative and conative alternations, since these have enough candidates in our lexicon for experimentation. Evaluation is performed on verbs already filtered by the syntactic processing. The SCF acquisition system has been evaluated elsewhere (Briscoe and Carroll, 1997). We selected candidate verbs which occurred with 10 or more nominal argument heads at the target slots. The argument heads were restricted to those which can be classified in the WordNet hypernym hierarchy. Candidates were selected by hand so as to obtain an even split between candidates which did participate in the alternation (positive candidates) and those which did not (negative candidates). Four human judges were used to determine the &quot;gold standard&quot;. The judges were asked to specify a yes or no decision on participation for each verb. They were also permitted a don't know verdict. The kappa statistic (Siegel and Castellan, 1988) was calculated to ensure that there was significant agreement between judges for the initial set of candidates. From these, verbs were selected which had 75% or more agreement, i.e. three or more judges giving the same yes or no decision for the verb. For the causative alternation we were left with 46 positives and 53 negatives. For the conative alternation we had 6 of each. In both cases, we used the Mann Whitney U test to see if there was a significant relationship between the similarity measure and participation. We then used a threshold on the similarity scores as the decision point for participation to determine a level of accuracy. We experimented with both the mean and median of the scores as a threshold. Seven of the negative causative candidates were randomly chosen and removed to ensure an even split between positive and negative candidates for determining accuracy using the mean and median as thresholds. The following subsection describes the results of the experiments using the method described in section 3 above. Subsection 4.2 describes an experiment on the same data to determine participation using a similarity measure based on the intersection of the lemmas at the target slots. The results for the causative alternation are displayed in table 1 for both the RBC and the UBC. The relationship between participation and asp is highly significant in both cases, with values of p well below 0.01. Accuracy for the mean and median thresholds are displayed in the fourth and fifth columns. Both thresholds outperform the random baseline of 50%. The results for the UBC are slightly improved, compared to those for the RBC, however the improvement is not significant. The numbers of false negative (FN) and false positive (FP) errors for the mean and median thresholds are displayed in table 2, along with the threshold and accuracy. The outcomes for each individual verb for the experiment using the RBC and the mean threshold are as follows: add admit answer believe borrow cost declare demand expect feel imagine know notice pay perform practise proclaim read remember sing survive understand win write accelerate bang bend boil break burn change close cook cool crack decrease drop dry end expand fly improve increase match melt open ring rip rock roll shatter shut slam smash snap spill split spread start stop stretch swing tilt turn wake ask attack catch choose climb drink eat help kick knit miss outline pack paint plan prescribe pull remain steal suck warn wash The results for the UBC experiment are very similar. If the median is used, the number of FPs and FNs are evenly balanced. This is because the median threshold is, by definition, taken midway between the test items arranged in order of their similarity scores. There are an even number of items on either side of the decision point, and an even number of positive and negative candidates in our test sample. Thus, the errors on either side of the decision point are equal in number. For both base cuts, there are a larger number of false positives than false negatives when the mean is used. The mean produces a higher accuracy than the median, but gives an increase in false positives. Many false positives arise where the preferences at both target slots are near neighbours in WordNet. For example, this occurred for eat and drink. There verbs have a high probability mass (around 0.7) under the entity class in both target slots, since both people and types of food occur under this class. In cases like these, the probability distributions at the RBC, and frequently the UBC, are not sufficiently distinctive. The polysemy of the verbs may provide another explanation for the large quantity of false positives. The scFs and data of different senses should not ideally be combined, at least not for coarse grained sense distinctions. We tested the false positive and true negative candidates to see if there was a relationship between the polysemy of a verb and its misclassification. The number of senses (according to WordNet) was used to indicate the polysemy of a verb. The Mann Whitney U test was performed on the verbs found to be true negative and false positive using the RBC. A significant relationship was not found between participation and misclassification. Both groups had an average of 5 senses per verb. This is not to say that distinguishing verb senses would not improve performance, provided that there was sufficient data. However, verb polysemy does not appear to be a major source of error, from our preliminary analysis. In many cases, such as read which was classified both by the judges, and the system as a negative candidate, the predominant sense of the verb provides the majority of the data. Alternate senses, for example, The book reads well, often do not contribute enough data so as to give rise to a large proportion of errors. Finding an appropriate inventory of senses would be difficult, since we would not wish to separate related senses which occur as alternate variants of one another. The inventory would therefore require knowledge of the phenomena that we are endeavouring to acquire automatically. To show that our method will work for other RSAS, we use the conative. Our sample size is rather small since we are limited by the number of positive candidates in the corpus having sufficient frequency for both scFs. The sparse data problem is acute when we look at alternations with specific prepositions. A sample of 12 verbs (6 positive and 6 negative) remained after the selection process outlined above. For this small sample we obtained a significant result (p = 0.02) with a mean accuracy of 67% and a median accuracy of 83%. On this occasion, the median performed better than the mean. More data is required to see if this difference is significant. This experiment was conducted using the same data as that used in the previous subsection. In this experiment, we used a similarity score on the argument heads directly, instead of generalizing the argument heads to WordNet classes. The venn diagram in figure 3 shows a subset of the lemmas at the transitive and intransitive scFs for the verb break. The lemma based similarity measure is termed lemma overlap (LO) and is given in equation 3, where A and B represent the target slots. LO is the size of the intersection of the multisets of argument heads at the target slots, divided by the size of the smaller of the two multisets. The intersection of two multisets includes duplicate items only as many times as the item is in both sets. For example, if one slot contained the argument heads (person, person, person, child, man, spokeswoman), and the other slot contained {person, person, child, chair, collection}, then the intersection would be {person, person, child}, and LO would be t . This measure ranges between zero (no overlap) and I (where one set is a proper subset of that at the other slot). Using the Mann Whitney U test on the LO scores, we obtained a z score of 2.00. This is significant to the 95% level, a lower level than that for the classbased experiments. The results using the mean and median of the LO scores are shown in table 3. Performance is lower than that for the class-based experiments. The outcome for the individual verbs using the mean as a threshold was:add admit answer borrow choose climb cost declare demand drink eat feel imagine notice outline pack paint perform plan practise prescribe proclaim read remain sing steal suck survive understand wash win write accelerate bang break cook crack decrease drop expand flood land march repeat rip rock shatter Interestingly, the errors for the LO measure tend to be false negatives, rather than false positives. The LO measure is much more conservative than the approach using the Tcms. In this case the median threshold produces better results. For the conative alternation, the lemma based method does not show a significant relationship between participation and the LO scores. Moreover, there is no difference between the sums of the ranks of the two groups for the Mann Whitney U test. The mean produces an accuracy of 58% whilst the median produces an accuracy of 50%. There has been some recent interest in observing alternations in corpora (McCarthy and Korhonen, 1998; Lapata, 1999) and predicting related verb classifications (Stevenson and Merlo, 1999). Earlier work by Resnik (1993) demonstrated a link between selectional preference strength and participation in alternations where the direct object is omitted. Resnik used syntactic information from the bracketing within the Penn Treebank corpus. Research into the identification of other diathesis alternations has been advanced by the availability of automatic syntactic processing. Most work using corpus evidence for verb classification has relied on a priori knowledge in the form of linguistic cues specific to the phenomena being observed (Lapata, 1999; Stevenson and Merlo, 1999). Our approach, whilst being applicable only to RsAs, does not require human input specific to the alternation at hand. Lapata (1999) identifies participation in the dative and benefactive alternations. Lapata's strategy is to identify participants using a shallow parser and various linguistic and semantic cues, which are specified manually for these two alternations. PP attachments are resolved using Hindle and Rooth's (1993) lexical association score. Compound nouns, which could be mistaken for the double object construction, were filtered using the log-likelihood ratio test. The semantic cues were obtained by manual analysis. The relative frequency of a SCF for a verb, compared to the total frequency of the verb, was used for filtering out erroneous scFs. Lapata does not report recall and precision figures against a gold standard. The emphasis is on the phenomena actually evident in the corpus data. Many of the verbs listed in Levin as taking the alternation were not observed with this alternation in the corpus data. This amounted to 44% of the verbs for the benefactive, and 52% for the dative. These figures only take into account the verbs for which at least one of the scFs were observed. 54% of the verbs listed for the dative and benefactive by Levin were not acquired with either of the target scFs. Conversely, many verbs not listed in Levin were identified as taking the benefactive or dative alternation using Lapata's criteria. Manual analysis of these verbs revealed 18 false positives out of 52 candidates. Stevenson and Merlo (1999) use syntactic and lexical cues for classifying 60 verbs in three verb classes: unergative, unaccusative and verbs with an optional direct object. These three classes were chosen because a few well defined features, specified a priori, can distinguish the three groups. Twenty verbs from Levin's classification were used in each class. They were selected by virtue of having sufficient frequency in a combined corpus (from the Brown and the wsJ) of 65 million words. The verbs were also chosen for having one predominant intended sense in the corpus. Stevenson and Merlo used four linguistically motivated features to distinguish these groups. Counts from the corpus data for each of the four features were normalised to give a score on a scale of 1 to 100. One feature was the causative non-causative distinction. For this feature, a measure similar to our LO measure was used. The four features were identified in the corpus using automatic POS tagging and parsing of the data. The data for half of the verbs in each class was subject to manual scrutiny, after initial automatic processing. The rest of the data was produced fully automatically. The verbs were classified automatically using the four features. The accuracy of automatic classification was 52% using all four features, compared to a baseline of 33%. The best result was obtained using a combination of three features. This gave an accuracy of 66%. McCarthy and Korhonen (1998) proposed a method for identifying asAs using MDL. This method relied on an estimation of the cost of using Tcms to encode the argument head data at a target slot. The sum of the costs for the two target slots was compared to the cost of a TCM for encoding the union of the argument head data over the two slots. Results are reported for the causative alternation with 15 verbs. This method depends on there being similar quantities of data at the alternating slots, otherwise the data at the more frequent slot overwhelms the data at the less frequent slot. However, many alternations involve scFs with substantially different relative frequencies, especially when one SCF is specific to a particular preposition. We carried out some experiments using the MDL method and our Tcms. For the causative, we used a sample of 110 verbs and obtained 63% accuracy. For the conative, a sample of 16 verbs was used and this time accuracy was only 56%. Notably, only one negative decision was made because of the disparate frame frequencies, which reduces the cost of combining the argument head data. We have discovered a significant relationship between the similarity of selectional preferences at the target slots, and participation in the causative and conative alternations. A threshold, such as the mean or median can be used to obtain a level of accuracy well above the baseline. A lemma based similarity score does not always indicate a significant relationship and generally produces a lower accuracy. There are patterns of diathesis behaviour among verb groups (Levin, 1993). Accuracy may be improved by considering several alternations collectively, rather than in isolation. Complementary techniques to identify alternations, for example (Resnik, 1993), might be combined with ours. Although we have reported results on only two RSAS, our method is applicable to other such alternations. Furthermore, such application requires no human endeavour, apart from that required for evaluation. However, a considerably larger corpus would be required to overcome the sparse data problem for other RSA alternations.","We have discovered a significant relationship between the similarity of selectional preferences at the target slots, and participation in the causative and conative alternations. A threshold, such as the mean or median can be used to obtain a level of accuracy well above the baseline. A lemma based similarity score does not always indicate a significant relationship and generally produces a lower accuracy. There are patterns of diathesis behaviour among verb groups (Levin, 1993). Accuracy may be improved by considering several alternations collectively, rather than in isolation. Complementary techniques to identify alternations, for example (Resnik, 1993), might be combined with ours. Although we have reported results on only two RSAS, our method is applicable to other such alternations. Furthermore, such application requires no human endeavour, apart from that required for evaluation. However, a considerably larger corpus would be required to overcome the sparse data problem for other RSA alternations."
44,"We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain.","We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain. This paper presents three trainable systems for surface natural language generation (NLG). Surface NLG, for our purposes, consists of generating a grammatical natural language phrase that expresses the meaning of an input semantic representation. The systems take a &quot;corpus-based&quot; or &quot;machinelearning&quot; approach to surface NLG, and learn to generate phrases from semantic input by statistically analyzing examples of phrases and their corresponding semantic representations. The determination of the content in the semantic representation, or &quot;deep&quot; generation, is not discussed here. Instead, the systems assume that the input semantic representation is fixed and only deal with how to express it in natural language. This paper discusses previous approaches to surface NLG, and introduces three trainable systems for surface NLG, called NLG1, NLG2, and NLG3. Quantitative evaluation of experiments in the air travel domain will also be discussed. Templates are the easiest way to implement surface NLG. A template for describing a flight noun phrase in the air travel domain might be flight departing from $city-fr at $time-dep and arriving in $city-to at $time-arr where the words starting with &quot;$&quot; are actually variables — representing the departure city, and departure time, the arrival city, and the arrival time, respectively— whose values will be extracted from the environment in which the template is used. The approach of writing individual templates is convenient, but may not scale to complex domains in which hundreds or thousands of templates would be necessary, and may have shortcomings in maintainability and text quality (e.g., see (Reiter, 1995) for a discussion). There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996), KPML (Bateman, 1996), MUMBLE (Meteer et al., 1987), and RealPro (Lavoie and Rambow, 1997), which produce natural language text from an abstract semantic representation. These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text. The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998). The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation. In contrast, (Langkilde and Knight, 1998) uses corpus-derived statistical knowledge to rank plausible hypotheses from a grammarbased surface generation component. In trainable surface NLG, the goal is to learn the mapping from semantics to words that would otherwise need to be specified in a grammar or knowledge base. All systems in this paper use attribute-value pairs as a semantic representation, which suffice as a representation for a limited domain like air travel. For example, the set of attribute-value pairs { $cityfr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } represent the meaning of the noun phrase &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot;. The goal, more specifically, is then to learn the optimal attribute ordering and lexical choice for the text to be generated from the attribute-value pairs. For example, the NLG system should automatically decide if the attribute ordering in &quot;flights to New York in the evening&quot; is better or worse than the ordering in &quot;flights in the evening to New York&quot;. Furthermore, it should automatically decide if the lexical choice in &quot;flights departing to New York&quot; is better or worse than the choice in &quot;flights leaving to New York&quot;. The motivation for a trainable surface generator is to solve the above two problems in a way that reflects the observed usage of language in a corpus, but without the manual effort needed to construct a grammar or knowledge base. All the trainable NLG systems in this paper assume the existence of a large corpus of phrases in which the values of interest have been replaced with their corresponding attributes, or in other words, a corpus of generation templates. Figure 1 shows a sample of training data, where only words marked with a &quot;8&quot; are attributes. All of the NLG systems in this paper work in two steps as shown in Table 2. The systems NLG1, NLG2 and NLG3 all implement step 1; they produce a sequence of words intermixed with attributes, i.e., a template, from the the attributes alone. The values are ignored until step 2, when they replace their corresponding attributes in the phrase produced by step 1. The surface generation model NLG1 simply chooses the most frequent template in the training data that corresponds to a given set of attributes. Its performance is intended to serve as a baseline result to the more sophisticated models discussed later. Specifically, nlgi(A) returns the phrase that corresponds to the attribute set A: [empty string] TA = where TA are the phrases that have occurred with A in the training data, and where C(phrase, A) is the training data frequency of the natural language phrase phrase and the set of attributes A. NLG1 will fail to generate anything if A is a novel combination of attributes. The surface generation system NLG2 assumes that the best choice to express any given attribute-value set is the word sequence with the highest probability that mentions all of the input attributes exactly once. When generating a word, it uses local information, captured by word n-grams, together with certain non-local information, namely, the subset of the original attributes that remain to be generated. The local and non-local information is integrated with use of features in a maximum entropy probability model, and a highly pruned search procedure attempts to find the best scoring word sequence according to the model. The probability model in NLG2 is a conditional distribution over V U * stop*, where V is the generation vocabulary and where *stop* is a special &quot;stop&quot; symbol. The generation vocabulary V consists of all the words seen in the training data. The form of the maximum entropy probability model is identical to the one used in (Berger et al., 1996; Ratnaparkhi, 1998): where wi ranges over V U *stop* and {wi-i wi-2, attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase. The h, where f3 (a, b) E {0,1}, are called features and capture any information in the history that might be useful for estimating P(wt iwi-1, wi-2, attri). The features used in NLG2 are described in the next section, and the feature weights ai, obtained from the Improved Iterative Scaling algorithm (Berger et al., 1996), are set to maximize the likelihood of the training data. The probability of the sequence W = wn, given the attribute set A, (and also given that its length is n) is: The feature patterns, used in NLG2 are shown in Table 3. The actual features are created by matching the patterns over the training data, e.g., an actual feature derived from the word hi-gram template might be: f(w1,tvi-1,w2-2,attri). 01 if w = from and wi-i = flight and $city — fr E attri otherwise Input to Step 1: 1 $city-fr, $city-to, $time-dep, $date-dep 1 Output of Step 1: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot; Input to Step 2: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot;, $city-fr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } Output of Step 2: &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot; Low frequency features involving word n—grams tend to be unreliable; the NLG2 system therefore only uses features which occur K times or more in the training data. The search procedure attempts to find a word sequence wi wn of any length n <M for the input attribute set A such that and where M is an heuristically set maximum phrase length. The search is similar to a left-to-right breadthfirst-search, except that only a fraction of the word sequences are considered. More specifically, the search procedure implements the recurrence: VVNa = top(N,{wlw E V}) 147.mi-1-1 = top(N,next(WN,i)) The set WN,i is the top N scoring sequences of length i, and the expression next(WN,i) returns all sequences w1 w,+1 such that wi w, E WN,, and wi-Fi E V U *stop*. The expression top(N,next(WN,i)) finds the top N sequences in next(WN,i). During the search, any sequence that ends with *stop* is removed and placed in the set of completed sequences. If N completed hypotheses are discovered, or if WN,m is computed, the search terminates. Any incomplete sequence which does not satisfy condition (3) is discarded and any complete sequence that does not satisfy condition (2) is also discarded. When the search terminates, there will be at most N completed sequences, of possibly differing lengths. Currently, there is no normalization for different lengths, i.e., all sequences of length n < M are equiprobable: NLG2 chooses the best answer to express the attribute set A as follows: where Wnig2 are the completed word sequences that satisfy the conditions of the NLG2 search described above. NLG3 addresses a shortcoming of NLG2, namely that the previous two words are not necessarily the best informants when predicting the next word. Instead, NLG3 assumes that conditioning on syntactically related words in the history will result on more accurate surface generation. The search procedure in NLG3 generates a syntactic dependency tree from top-to-bottom instead of a word sequence from leftto-right, where each word is predicted in the context of its syntactically related parent, grandparent, and siblings. NLG3 requires a corpus that has been annotated with tree structure like the sample dependency tree shown in Figure 1. The probability model for NLG3, shown in Figure 2, conditions on the parent, the two closest siblings, the direction of the child relative to the parent, and the attributes that remain to be generated. Just as in NLG2, p is a distribution over V U *stop*, and the Improved Iterative Scaling algorithm is used to find the feature weights a3. The expression chi(w) denotes the ith closest child to the headword w, par(w) denotes the parent of the headword w, dir E {left, right} denotes the direction of the child relative to the parent, and attr,i denotes the attributes that remain to be generated in the tree when headword w is predicting its ith child. For example, in Figure 1, if w =&quot;flights&quot;, then chi (w) =&quot;evening&quot; when generating the left children, and chl(w) =&quot;from&quot; when generating the right children. As shown in Figure 3, the probability of a dependency tree that expresses an attribute set A can be found by computing, for each word in the tree, the probability of generating its left children and then its right children.' In this formulation, the left children are generated independently from the right children. As in NLG2, NLG3 assumes the uniform distribution for the length probabilities Pr(# of left children = n) and Pr(# of right children = n) up to a certain maximum length M' = 10. The feature patterns for NLG3 are shown in Table 4. As before, the actual features are created by matching the patterns over the training data. The features in NLG3 have access to syntactic information whereas the features in NLG2 do not. Low frequency features involving word n—grams tend to be unreliable; the NLG3 system therefore only uses features which occur K times or more in the training data. Furthermore, if a feature derived from Table 4 looks at a particular word chi (w) and attribute a, we only allow it if a has occurred as a descendent of 1-We use a dummy ROOT node to generate the top most head word of the phrase chi(w) in some dependency tree in the training set. As an example, this condition allows features that look at chi(w) =&quot;to&quot; and $city-toE attr,i but disallows features that look at chi(w) =&quot;to&quot; and $cityfrE The idea behind the search procedure for NLG3 is similar to the search procedure for NLG2, namely, to explore only a fraction of the possible trees by continually sorting and advancing only the top N trees at any given point. However, the dependency trees are not built left-to-right like the word sequences in NLG2; instead they are built from the current head (which is initially the root node) in the following order: As before, any incomplete trees that have generated a particular attribute twice, as well as completed trees that have not generated a necessary attribute are discarded by the search. The search terminates when either N complete trees or N trees of the maximum length M are discovered. NLG3 chooses the best answer to express the attribute set A as follows: where Tn193 are the completed dependency trees that satisfy the conditions of the NLG3 search described above. The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain. The annotation scheme used a total of 26 attributes to represent flights. The training set consisted of 6000 templates describing flights while the test set consisted of 1946 templates describing flights. All systems used the same training set, and were tested on the attribute sets extracted from the phrases in the test set. For example, if the test set contains the template &quot;flights to $city-to leaving at $time-dep&quot;, the surface generation systems will be told to generate a phrase for the attribute set { $city-to, $time-dep }. The output of NLG3 on the attribute set { $city-to, $city-fr, $time-dep } is shown in Table 9. There does not appear to be an objective automatic evaluation method2 for generated text that correlates with how an actual person might judge the output. Therefore, two judges — the author and a colleague — manually evaluated the output of all three systems. Each judge assigned each phrase from each of the three systems one of the following rankings: OK: Tense or agreement is wrong, but word choice is correct. (These errors could be corrected by post-processing with a morphological analyzer.) Bad: Words are missing or extraneous words are present No Output: The system failed to produce any output While there were a total 1946 attribute sets from the test examples, the judges only needed to evaluate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data. Subjective evaluation of generation output is 2Measuring word overlap or edit distance between the system's output and a &quot;reference&quot; set would be an automatic scoring method. We believe that such a method does not accurately measure the correctness or grammaticality of the text. not ideal, but is arguably superior than an automatic evaluation that fails to correlate with human linguistic judgement. The results of the manual evaluation, as well as the values of the search and feature selection parameters for all systems, are shown in Tables 5, 6, 7, and 8. (The values for N, M, and K were determined by manually evaluating the output of the 4 or 5 most common attribute sets in the training data). The weighted results in Tables 5 and 6 account for multiple occurrences of attribute sets, whereas the unweighted results in Tables 7 and 8 count each unique attribute set once, i.e., { $city-fr Scity-to } is counted 741 times in the weighted results but once in the unweighted results. Using the weighted results, which represent testing conditions more realistically than the unweighted results, both judges found an improvement from NLG1 to NLG2, and from NLG2 to NLG3. NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong). NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data. NLG1 has no chance of generating anything for 3% of the data — it fails completely on novel attribute sets. Using the unweighted results, both judges found an improvement from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase in accuracy from NLG2 to NLG3. The unweighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases. The NLG2 and NLG3 systems automatically attempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets. There Probability 0.107582 0.00822441 0.00564712 0.00343372 0.0012465 Generated Text $time-dep flights from $city-fr to $city-to $time-dep flights between $city-fr and $city-to $time-dep flights $city-fr to $city-to flights from $city-fr to $city-to at $time-dep $time-dep flights from $city-fr to to $city-to Table 9: Sample output from NLG3. (Dependency tree structures are not shown.) Typical values for attributes: $time-dep = &quot;10 a.m.&quot;, $city-fr = &quot;New York&quot;, $city-to = &quot;Miami&quot; is some additional cost associated with producing the syntactic dependency annotation necessary for NLG3, but virtually no additional cost is associated with NLG2, beyond collecting the data itself and identifying the attributes. The trainable surface NLG systems in this paper differ from grammar-based systems in how they determine the attribute ordering and lexical choice. NLG2 and NLG3 automatically determine attribute ordering by simultaneously searching multiple orderings. In grammar-based approaches, such preferences need to be manually encoded. NLG2 and NLG3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with a given attribute and local context, whereas (Elhadad et al., 1997) uses a rule-based approach to decide the word choice. While trainable approaches avoid the expense of crafting a grammar to determine attribute ordering and lexical choice, they are less accurate than grammar-based approaches. For short phrases, accuracy is typically 100% with grammar-based approaches since the grammar writer can either correct or add a rule to generate the phrase of interest once an error is detected. Whereas with NLG2 and NLG3, one can tune the feature patterns, search parameters, and training data itself, but there is no guarantee that the tuning will result in 100% generation accuracy. Our approach differs from the corpus-based surface generation approaches of (Langkilde and Knight, 1998) and (Berger et al., 1996). (Langkilde and Knight, 1998) maps from semantics to words with a concept ontology, grammar, and lexicon, and ranks the resulting word lattice with corpus-based statistics, whereas NLG2 and NLG3 automatically learn the mapping from semantics to words from a corpus. (Berger et al., 1996) describes a statistical machine translation approach that generates text in the target language directly from the source text. NLG2 and NLG3 are also statistical learning approaches but generate from an actual semantic representation. This comparison suggests that statistical MT systems could also generate text from an &quot;interlingua&quot;, in a way similar to that of knowledgebased translation systems. We suspect that our statistical generation approach should perform accurately in domains of similar complexity to air travel. In the air travel domain, the length of a phrase fragment to describe an attribute is usually only a few words. Domains which require complex and lengthy phrase fragments to describe a single attribute will be more challenging to model with features that only look at word n-grams for n E {2, 3}. Domains in which there is greater ambiguity in word choice will require a more thorough search, i.e., a larger value of N, at the expense of CPU time and memory. Most importantly, the semantic annotation scheme for air travel has the property that it is both rich enough to accurately represent meaning in the domain, but simple enough to yield useful corpus statistics. Our approach may not scale to domains, such as freely occurring newspaper text, in which the semantic annotation schemes do not have this property. Our current approach has the limitation that it ignores the values of attributes, even though they might strongly influence the word order and word choice. This limitation can be overcome by using features on values, so that NLG2 and NLG3 might discover — to use a hypothetical example — that &quot;flights leaving $city-fr&quot; is preferred over &quot;flights from $city-fr&quot; when $city-fr is a particular value, such as &quot;Miami&quot;. This paper presents the first systems (known to the author) that use a statistical learning approach to produce natural language text directly from a semantic representation. Information to solve the attribute ordering and lexical choice problems— which would normally be specified in a large handwritten grammar— is automatically collected from data with a few feature patterns, and is combined via the maximum entropy framework. NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy. We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data.","This paper presents the first systems (known to the author) that use a statistical learning approach to produce natural language text directly from a semantic representation. Information to solve the attribute ordering and lexical choice problems— which would normally be specified in a large handwritten grammar— is automatically collected from data with a few feature patterns, and is combined via the maximum entropy framework. NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy. We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data."
45,"Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown in Figure 2. a human reduces the sentence to which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3. The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed. Suppose the program reduces sentence to can be translated similarly to the annotated tree shown in Figure 4. Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions. Two out of the five decisions agree (they are D--÷B and D—>E), so the rate is 2/5 (40%). The rate defined as: man and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions 313 Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases. 3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged. We tested the program on the rest 100 sentences. Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%. If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%. We also computed the success rate of program's decisions on particular types of phrases. For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%. We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase. One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed. Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs. On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%. The probabilities we computed from the training corpus covered 58% of instances in the test corpus. When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge. Some of the errors made by the system result from the errors by the syntactic parser. We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors. There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing. One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing. For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached. Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase. We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions. The other reason is that parsing errors do not always result in reduction errors. For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example. 4 Discussion and related work The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article. We can tailor the reduction system to queries-based summarization. In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries. We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence. In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information. Ideally, the sentence reduction module should interact with other modules in a summarization system. It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score). It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules. Some researchers suggested removing phrases or clauses from sentences for certain applications. (Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. researchers worked on the text simplifica- 314 tion problem, which usually involves in simplifying text but not removing any phrases. For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general. The difference between these studies on text simplification and our system is that a text simplification system usually not from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences. 5 Conclusions and future work We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization. The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence. The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed. Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis. We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system. The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans. In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system. Acknowledgment This material is based upon work supported by the National Science Foundation under Grant No. IRI 96-19124 and IRI 96-18797. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not","Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown in Figure 2. a human reduces the sentence to which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3. The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed. Suppose the program reduces sentence to can be translated similarly to the annotated tree shown in Figure 4. Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions. Two out of the five decisions agree (they are D--÷B and D—>E), so the rate is 2/5 (40%). The rate defined as: man and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions 313 Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases. 3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged. We tested the program on the rest 100 sentences. Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%. If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%. We also computed the success rate of program's decisions on particular types of phrases. For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%. We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase. One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed. Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs. On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%. The probabilities we computed from the training corpus covered 58% of instances in the test corpus. When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge. Some of the errors made by the system result from the errors by the syntactic parser. We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors. There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing. One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing. For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached. Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase. We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions. The other reason is that parsing errors do not always result in reduction errors. For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example. 4 Discussion and related work The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article. We can tailor the reduction system to queries-based summarization. In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries. We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence. In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information. Ideally, the sentence reduction module should interact with other modules in a summarization system. It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score). It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules. Some researchers suggested removing phrases or clauses from sentences for certain applications. (Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. researchers worked on the text simplifica- 314 tion problem, which usually involves in simplifying text but not removing any phrases. For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general. The difference between these studies on text simplification and our system is that a text simplification system usually not from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences. 5 Conclusions and future work We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization. The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence. The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed. Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis. We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system. The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans. In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system. Acknowledgment This material is based upon work supported by the National Science Foundation under Grant No. IRI 96-19124 and IRI 96-18797. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not Current automatic summarizers usually rely on sentence extraction to produce summaries. Human professionals also often reuse the input documents to generate summaries; however, rather than simply extracting sentences and stringing them together, as most current summarizers do, humans often &quot;edit&quot; the extracted sentences in some way so that the resulting summary is concise and coherent. We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences (Jing and McKeown, 1999; Jing and McKeown, 2000). We call the operation of removing extraneous phrases from an extracted sentence sentence reduction. It is one of the most effective operations that can be used to edit the extracted sentences. Reduction can remove material at any granularity: a word, a prepositional phrase, a gerund, a to-infinitive or a clause. We use the term &quot;phrase&quot; here to refer to any of the above components that can be removed in reduction. The following example shows an original sentence and its reduced form written by a human professional: Original sentence: When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. Reduced sentence by humans: The V-chip will give parents a device to block out programs they don't want their children to see. We implemented an automatic sentence reduction system. Input to the reduction system includes extracted sentences, as well as the original document. Output of reduction are reduced forms of the extracted sentences, which can either be used to produce summaries directly, or be merged with other sentences. The reduction system uses multiple sources of knowledge to make reduction decisions, including syntactic knowledge, context, and statistics computed from a training corpus. We evaluated the system against the output of human professionals. The program achieved a success rate of 81.3%, meaning that 81.3% of reduction decisions made by the system agreed with those of humans. Sentence reduction improves the conciseness of automatically generated summaries, making it concise and on target. It can also improve the coherence of generated summaries, since extraneous phrases that can potentially introduce incoherece are removed. We collected 500 sentences and their corresponding reduced forms written by humans, and found that humans reduced the length of these 500 sentences by 44.2% on average. This indicates that a good sentence reduction system can improve the conciseness of generated summaries significantly. In the next section, we describe the sentence reduction algorithm in details. In Section 3, we introduce the evaluation scheme used to access the performance of the system and present evaluation results. In Section 4, we discuss other applications of sentence reduction, the interaction between reduction and other modules in a summarization system, and related work on sentence simplication. Finally, we The goal of sentence reduction is to &quot;reduce without major loss&quot;; that is, we want to remove as many extraneous phrases as possible from an extracted sentence so that it can be concise, but without detracting from the main idea the sentence conveys. Ideally, we want to remove a phrase from an extracted sentence only if it is irrelevant to the main topic. To achieve this, the system relies on multiple sources of knowledge to make reduction decisions. We first introduce the resources in the system and then describe the reduction algorithm. (1) The corpus. One of the key features of the system is that it uses a corpus consisting of original sentences and their corresponding reduced forms written by humans for training and testing purpose. This corpus was created using an automatic program we have developed to automatically analyze human-written abstracts. The program, called the decomposition program, matches phrases in a human-written summary sentence to phrases in the original document (Jing and McKeown, 1999). The human-written abstracts were collected from the free daily news service &quot;Communicationsrelated headlines&quot;, provided by the Benton Foundation (http://www.benton.org). The articles in the corpus are news reports on telecommunication related issues, but they cover a wide range of topics, such as law, labor, and company mergers. database to date. It provides lexical relations between words, including synonymy, antonymy, meronymy, entailment (e.g., eat —> chew), or causation (e.g., kill --* die). These lexical links are used to identify the focus in the local context. (4) The syntactic parser. We use the English Slot Grammar(ESG) parser developed at IBM (McCord, 1990) to analyze the syntactic structure of an input sentence and produce a sentence parse tree. The ESG parser not only annotates the syntactic category of a phrase (e.g., &quot;np&quot; or &quot;vp&quot;), it also annotates the thematic role of a phrase (e.g., &quot;subject&quot; or &quot;object&quot;). There are five steps in the reduction program: Step 1: Syntactic parsing. We first parse the input sentence using the ESG parser and produce the sentence parse tree. The operations in all other steps are performed based on this parse tree. Each following step annotates each node in the parse tree with additional information, such as syntactic or context importance, which are used later to determine which phrases (they are represented as subtrees in a parse tree) can be considered extraneous and thus removed. Step 2: Grammar checking. In this step, we determine which components of a sentence must not be deleted to keep the sentence grammatical. To do this, we traverse the parse tree produced in the first step in top-down order and mark, for each node in the parse tree, which of its children are grammatically obligatory. We use two sources of knowledge for this purpose. One source includes simple, linguistic-based rules that use the thematic role structure produced by the ESG parser. For instance, for a sentence, the main verb, the subject, and the object(s) are essential if they exist, but a prepositional phrase is not; for a noun phrase, the head noun is essential, but an adjective modifier of the head noun is not. The other source we rely on is the large-scale lexicon we described earlier. The information in the lexicon is used to mark the obligatory arguments of verb phrases. For example, for the verb &quot;convince&quot;, the lexicon has the following entry: This entry indicates that the verb &quot;convince&quot; can be followed by a noun phrase and a prepositional phrase starting with the preposition &quot;of' (e.g., he convinced me of his innocence). It can also be followed by a noun phrase and a to-infinitive phrase (e.g., he convinced me to go to the party). This information prevents the system from deleting the &quot;of&quot; prepositional phrase or the to-infinitive that is part of the verb phrase. At the end of this step, each node in the parse tree — including both leaf nodes and intermediate nodes — is annotated with a value indicating whether it is grammatically obligatory. Note that whether a node is obligatory is relative to its parent node only. For example, whether a determiner is obligatory is relative to the noun phrase it is in; whether a prepositional phrase is obligatory is relative to the sentence or the phrase it is in. Step 3: Context information. In this step, the system decides which components in the sentence are most related to the main topic being discussed. To measure the importance of a phrase in the local context, the system relies on lexical links between words. The hypothesis is that the more connected a word is with other words in the local context, the more likely it is to be the focus of the local context. We link the words in the extracted sentence with words in its local context, if they are repetitions, morphologically related, or linked in WordNet through one of the lexical relations. The system then computes an importance score for each word in the extracted sentence, based on the number of links it has with other words and the types of links. The formula for computing the context importance score for a word w is as follows: Here, i represents the different types of lexical relations the system considered, including repetition, inflectional relation, derivational relation, and the lexical relations from WordNet. We assigned a weight to each type of lexical relation, represented by Li in the formula. Relations such as repetition or inflectional relation are considered more important and are assigned higher weights, while relations such as hypernym are considered less important and assigned lower weights. NU (w) in the formula represents the number of a particular type of lexical links the word w has with words in the local context. After an importance score is computed for each word, each phrase in the 'sentence gets a score by adding up the scores of its children nodes in the parse tree. This score indicates how important the phrase is in the local context. Step 4: Corpus evidence. The program uses a corpus consisting of sentences reduced by human professionals and their corresponding original sentences to compute how likely humans remove a certain phrase. The system first parsed the sentences in the corpus using ESG parser. It then marked which subtrees in these parse trees (i.e., phrases in the sentences) were removed by humans. Using this corpus of marked parse trees, we can compute how likely a subtree is removed from its parent node. For example, we can compute the probability that the &quot;when&quot; temporal clause is removed when the main verb is &quot;give&quot;, represented as Prob(&quot;when-clause is removed&quot; I &quot;v=give&quot;), or the probability that the to-infinitive modifier of the head noun &quot;device&quot; is removed, represented as Prob(&quot;to-infinitive modifier is removed&quot; I&quot;n=device&quot;). These probabilities are computed using Bayes's rule. For example, the probability that the &quot;when&quot; temporal clause is removed when the main verb is &quot;give&quot;, Prob(&quot;when-clause is removed&quot; I &quot;v=give&quot;), is computed as the product of Prob( &quot;v=give&quot; I &quot;when-clause is removed&quot;) (i.e., the probability that the main verb is &quot;give&quot; when the &quot;when&quot; clause is removed) and Prob(&quot;when-clause is removed&quot;) (i.e., the probability that the &quot;when&quot; clause is removed), divided by Prob(&quot;v=give&quot;) (i.e., the probability that the main verb is &quot;give&quot;). Besides computing the probability that a phrase is removed, we also compute two other types of probabilities: the probability that a phrase is reduced (i.e., the phrase is not removed as a whole, but some components in the phrase are removed), and the probability that a phrase is unchanged at all (i.e., neither removed nor reduced). These corpus probabilities help us capture human practice. For example, for sentences like &quot;The agency reported that ...&quot; , &quot;The other source says that ...&quot; , &quot;The new study suggests that ...&quot; , the thatclause following the say-verb (i.e., report, say, and suggest) in each sentence is very rarely changed at all by professionals. The system can capture this human practice, since the probability that that-clause of the verb say or report being unchanged at all will be relatively high, which will help the system to avoid removing components in the that-clause. These corpus probabilities are computed beforehand using a training corpus. They are then stored in a table and loaded at running time. Step 5: Final Decision. The final reduction decisions are based on the results from all the earlier steps. To decide which phrases to remove, the system traverses the sentence parse tree, which now have been annotated with different types of information from earlier steps, in the top-down order and decides which subtrees should be removed, reduced or unchanged. A subtree (i.e., a phrase) is removed only if it is not grammatically obligatory, not the focus of the local context (indicated by a low importance score), and has a reasonable probability of being removed by humans. Figure 1 shows sample output of the reduction program. The reduced sentences produced by humans are also provided for comparison. We define a measure called success rate to evaluate the performance of our sentence reduction program. Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. The success rate computes the percentage of system's reduction decisions that agree with those of humans. We compute the success rate in the following way. The reduction process can be considered as a series of decision-making process along the edges of a sentence parse tree. At each node of the parse tree, both the human and the program make a decision whether to remove the node or to keep it. If a node is removed, the subtree with that node as the root is removed as a whole, thus no decisions are needed for the descendants of the removed node. If the node is kept, we consider that node as the root and repeat this process. Suppose we have an input sentence (ABCDEFGH), which has a parse tree shown in Figure 2. Suppose a human reduces the sentence to (ABDGH), which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3. The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed. Suppose the program reduces the sentence to (BCD), which can be translated similarly to the annotated tree shown in Figure 4. We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions. Two out of the five decisions agree (they are D--÷B and D—>E), so the success rate is 2/5 (40%). The success rate is defined as: # of edges along which the human and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases. In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged. We tested the program on the rest 100 sentences. Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%. If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%. We also computed the success rate of program's decisions on particular types of phrases. For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%. We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase. One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed. Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs. On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%. The probabilities we computed from the training corpus covered 58% of instances in the test corpus. When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge. Some of the errors made by the system result from the errors by the syntactic parser. We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors. There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing. One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing. For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached. Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase. We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions. The other reason is that parsing errors do not always result in reduction errors. For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example. The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article. We can tailor the reduction system to queries-based summarization. In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries. We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence. In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information. Ideally, the sentence reduction module should interact with other modules in a summarization system. It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score). It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules. Some researchers suggested removing phrases or clauses from sentences for certain applications. (Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. Other researchers worked on the text simplification problem, which usually involves in simplifying text but not removing any phrases. For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general. The difference between these studies on text simplification and our system is that a text simplification system usually does not remove anything from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences. We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization. The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence. The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed. Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis. We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system. The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans. In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system.","We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization. The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence. The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed. Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis. We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system. The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans. In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system."
46,"This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.","This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts. Let fi,i denote the frequency of word j in sentence i. The similarity between a pair of sentences :1:, y For short text segments, the absolute value of sim(x, y) is unreliable. An additional occurrence of a common word (reflected in the numerator) causes a disproportionate increase in sim(x, y) unless the denominator (related to segment length) is large. Thus, in the context of text segmentation where a segment has typically < 100 informative tokens, one can only use the metric to estimate the order of similarity between sentences, e.g. a is more similar to b than c. Furthermore, language usage varies throughout a document. For instance, the introduction section of a document is less cohesive than a section which is about a particular topic. Consequently, it is inappropriate to directly compare the similarity values from different regions of the similarity matrix. In non-parametric statistical analysis, one compares the rank of data sets when the qualitative behaviour is similar but the absolute quantities are unreliable. We present a ranking scheme which is an adaptation of that described in (O'Neil and Denos, 1992). 'The contrast of the image has been adjusted to highlight the image features. Each value in the similarity matrix is replaced by its rank in the local region. The rank is the number of neighbouring elements with a lower similarity value. Figure 2 shows an example of image ranking using a 3 x 3 rank mask with output range {0, 8). For segmentation, we used a 11 x 11 rank mask. The output is expressed as a ratio r (equation 2) to circumvent normalisation problems (consider the cases when the rank mask is not contained in the image). # of elements with a lower value To demonstrate the effect of image ranking, the process was applied to the matrix shown in figure 1 to produce figure 32. Notice the contrast has been improved significantly. Figure 4 illustrates the more subtle effects of our ranking scheme. r(x) is the rank (1 x 11 mask) of (x) which is a sine wave with decaying mean, amplitude and frequency (equation 3). The final process determines the location of the topic boundaries. The method is based on Reynar's maximisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993). A text segment is defined by two sentences i, j (inclusive). This is represented as a square region along the diagonal of the rank matrix. Let si,j denote the sum of the rank values in a segment and ai,j = (j —i +1)2 be the inside area. B = {b1, ...,197-4 is a list of in coherent text segments. sk and ak refers to the sum of rank and area of segment k in B. D is the inside density of B (see equation 4). ak To initialise the process, the entire document is placed in B as one coherent text segment. Each step of the process splits one of the segments in B. The split point is a potential boundary which maximises D. Figure 5 shows a working example. The number of segments to generate, in, is determined automatically. Den) is the inside density of n segments and SD(n) , Den) Den-1) is the gradient. For a document with b potential boundaries, b steps of divisive clustering generates {D(1), ...,D(b+1)} and {bD(2), oD(b+1)} (see figure 6 and 7). An unusually large reduction in 6D suggests the optiinal clustering has been obtained3 (see n = 10 in the threshold, p+c x to dD (c= 1.2 works well in practice) The running time of each step is dominated by the computation of sk. Given si,j is constant, our algorithm pre-computes all the values to improve speed performance. The procedure computes the values along diagonals, starting from the main diagonal and works towards the corner. The method has a complexity of order 171-5.n2. Let ri,j refer to the rank value in the rank matrix R and S to the sum of rank matrix. Given R of size n X 77,, S is computed in three steps (see equation 5). Figure 8 shows the result of applying this procedure to the rank matrix in figure 5. The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997). Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998). We assume a good algorithm is one that finds the most prominent topic boundaries. An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first n sentences of a randomly selected document from the Brown corpus'. A sample is characterised by the range of n. The corpus was generated by an automatic procedure5. Table 1 presents the corpus statistics. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6. Segmentation accuracy is measured by the error metric (equation 6, fa false alarms) proposed in (Beeferman et al., 1999). Low error probability indicates high accuracy. Other performance measures include the popular precision and recall metric (PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997). The problems associated with these metrics are discussed in (Beeferman et al., 1999). Five degenerate algorithms define the baseline for the experiments. B„ does not propose any boundaries. B„ reports all potential boundaries as real boundaries. B„ partitions the sample into regular segments. B(i.,?) randomly selects any number of boundaries as real boundaries. B(r,b) randomly selects b boundaries as real boundaries. The accuracy of the last two algorithms are computed analytically. We consider the status of in potential boundaries as a bit string (1 -4 topic boundary). The terms p(iniss) awl p(fa) in equation 6 corresponds to p(samelk) and p(difflk) = 1 -p(samelk). Equation 7, 8 and 9 gives the general form of p(samelk), B(r,?) and Berm, respectively'. Table 2 presents the experimental results. The values in row two and three, four and five are not actually the same. However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992). We compare three versions of the TextTiling algorithm (Hearst, 1994). H94(c,d) is Hearst's C implementation with default parameters. H94(e.7.) uses the recommended parameters k = 6, w = 20. H94(3,,,) is my implementation of the algorithm. Experimental result (table 3) shows H94(,,d) and H94(,,) are more accurate than H94(j,,,). We suspect this is due to the use of a different stopword list and stemming algorithm. Five versions of Reynar's optimisation algorithm (Reynar, 1998) were evaluated. R98 and R98(7-„rn) are exact implementations of his maximisation and minimisation algorithm. R98(8,,08) is my version of the maximisation algorithm which uses the cosine coefficient instead of dot density for measuring similarity. It incorporates the optimisations described in section 3.4. R98(,„,d0t) is the modularised version of R98 for experimenting with different similarity measures. R98(,,,,„) uses a variant of Kozima's semantic similarity measure (Kozima, 1993) to compute block similarity. Word similarity is a function of word cooccurrence statistics in the given document. Words that belong to the same sentence are considered to be related. Given the co-occurrence frequencies f (wi, wi), the transition probability matrix t is computed by equation 10. Equation 11 defines our spread activation scheme. s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix. x = 5 was used in the experiment. Experimental result (table 4) shows the cosine coefficient and our spread activation method improved segmentation accuracy. The speed optimisations significantly reduced the execution time. We compare three versions of Segmenter (Kan et at, 1998). K98(p) is the original Perl implementation of the algorithm (version 1.6). K98(i) is my implementation of the algorithm. K98(j,,i) is a version of K98(i) which uses a document specific chain breaking strategy. The distribution of link distances are used to identify unusually long links. The threshold is a function p + c x VT, of the mean p and variance We found c = 1 works well in practice. Table 5 summarises the experimental results. K98 performed performed significantly better than K98(J,). This is due to the use of a different part-of-speech tagger and shallow parser. The difference in speed is largely due to the programming languages and term clustering strategies. Our chain breaking strategy improved accuracy (compare K98(i) with K98(j,a))• Two versions of our algorithm were developed, C99 and C99(b). The former is an exact implementation of the algorithm described in this paper. The latter is given the expected number of topic segments for fair comparison with R98. Both algorithms used a 11 x 11 ranking mask. The first experiment focuses on the impact of our automatic termination strategy on C99(b) (table 6). C99(b) is marginally more accurate than C99. This indicates our automatic termination strategy is effective but not optimal. The minor reduction in speed performance is acceptable. The second experiment investigates the effect of different ranking mask size on the performance of C99 (table 7). Execution time increases with mask size. A 1 x 1 ranking mask reduces all the elements in the rank matrix to zero. Interestingly, the increase in ranking mask size beyond 3 x 3 has insignificant effect on segmentation accuracy. This suggests the use of extrema for clustering has a greater impact on accuracy than linearising the similarity scores (figure 4). Experimental result (table 8) shows our algorithm C99 is more accurate than existing algorithms. A two-fold increase in accuracy and seven-fold increase in speed was achieved (compare C99(b) with R98). If one disregards segmentation accuracy, H94 has the best algorithmic performance (linear). C99, K98 and R98 are all polynomial time algorithms. The significance of our results has been confirmed by both t-test and KS-test. A segmentation algorithm has two key elements, a, clustering strategy and a similarity measure. Our results show divisive clustering (R98) is more precise than sliding window (H94) and lexical chains (K98) for locating topic boundaries. Four similarity measures were examined. The cosine coefficient (R98(s,„0) and dot density measure (R98(m,doo ) yield similar results. Our spread activation based semantic measure (R98(„,sa)) improved accuracy. This confirms that although Kozima's approach (Kozima, 1993) is computationally expensive, it does produce more precise segmentation. The most significant improvement was due to our ranking scheme which linearises the cosine coefficient,. Our experiments demonstrate that given insufficient data, the qualitative behaviour of the cosine measure is indeed more reliable than the actual values. Although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark. It would be interesting to compare C99 with the multi-source method described in (Beeferman et al., 1999) using the TDT corpus. We would also like to develop a linear time and multi-source version of the algorithm.","A segmentation algorithm has two key elements, a, clustering strategy and a similarity measure. Our results show divisive clustering (R98) is more precise than sliding window (H94) and lexical chains (K98) for locating topic boundaries. Four similarity measures were examined. The cosine coefficient (R98(s,„0) and dot density measure (R98(m,doo ) yield similar results. Our spread activation based semantic measure (R98(„,sa)) improved accuracy. This confirms that although Kozima's approach (Kozima, 1993) is computationally expensive, it does produce more precise segmentation. The most significant improvement was due to our ranking scheme which linearises the cosine coefficient,. Our experiments demonstrate that given insufficient data, the qualitative behaviour of the cosine measure is indeed more reliable than the actual values. Although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark. It would be interesting to compare C99 with the multi-source method described in (Beeferman et al., 1999) using the TDT corpus. We would also like to develop a linear time and multi-source version of the algorithm."
47,"and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguistics,","and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguistics, Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (ChitraD and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratnaparkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR. The Penn treebank contains a great deal of additional syntactic and semantic information from which to gather statistics; reproducing more of this information automatically is a goal which has so far been mostly ignored. This paper details a process by which some of this information—the function tags— may be recovered automatically. In the Penn treebank, there are 20 tags (figure 1) that can be appended to constituent labels in order to indicate additional information about the syntactic or semantic role of the constituent. We have divided them into four categories (given in figure 2) based on those in the bracketing guidelines (Bies et al., 1995). A constituent can be tagged with multiple tags, but never with two tags from the same category.1 In actuality, the case where a constituent has tags from all four categories never happens, but constituents with three tags do occur (rarely). At a high level, we can simply say that having the function tag information for a given text is useful just because any further information would help. But specifically, there are distinct advantages for each of the various categories. Grammatical tags are useful for any application trying to follow the thread of the text—they find the 'who does what' of each clause, which can be useful to gain information about the situation or to learn more about the behaviour of the words in the sentence. The form/function tags help to find those constituents behaving in ways not conforming to their labelled type, as well as further clarifying the behaviour of adverbial phrases. Information retrieval applications specialising in describing events, as with a number of the MUC applications, could greatly benefit from some of these in determining the where-when-why of things. Noting a topicalised constituent could also prove useful to these applications, and it might also help in discourse analysis, or pronoun resolution. Finally, the 'miscellaneous' tags are convenient at various times; particularly the CLR 'closely related' tag, which among other things marks phrasal verbs and prepositional ditransitives. To our knowledge, there has been no attempt so far to recover the function tags in parsing treebank text. In fact, we know of only one project that used them at all: (Collins, 1997) defines certain constituents as complements based on a combination of label and function tag information. This boolean condition is then used to train an improved parser. We have found it useful to define our statistical model in terms of features. A 'feature', in this context, is a boolean-valued function, generally over parse tree nodes and either node labels or lexical items. Features can be fairly simple and easily read off the tree (e.g. 'this node's label is X', 'this node's parent's label is Y'), or slightly more complex (`this node's head's partof-speech is Z'). This is concordant with the usage in the maximum entropy literature (Berger et al., 1996). When using a number of known features to guess an unknown one, the usual procedure is to calculate the value of each feature, and then essentially look up the empirically most probable value for the feature to be guessed based on those known values. Due to sparse data, some of the features later in the list may need to be ignored; thus the probability of an unknown feature value would be estimated as where P refers to an empirically observed probability. Of course, if features 1 through i only co-occur a few times in the training, this value may not be reliable, so the empirical probability is usually smoothed: The values for Ai can then be determined according to the number of occurrences of features 1 through i together in the training. One way to think about equation 1 (and specifically, the notion that j will depend on the values of Ii fn) is as follows: We begin with the prior probability of f. If we have data indicating P(fIfi), we multiply in that likelihood, while dividing out the original prior. If we have data for P( fl f2), we multiply that in while dividing out the P (f in) term. This is repeated for each piece of feature data we have; at each point, we are adjusting the probability we already have estimated. If knowledge about feature fi makes f more likely than with just fi_i, the term where fi is added will be greater than one and the running probability will be adjusted upward. This gives us the new probability shown in equation 3, which is exactly equivalent to equation 1 since everything except the last numerator cancels out of the equation. The value of j is chosen such that features Ii fi are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems. Smoothing is performed on this equation exactly as before: each term is interpolated between the empirical value and the prior estimated probability, according to a value of Ai that estimates confidence. But aside from perhaps providing a new way to think about the problem, equation 3 is not particularly useful as it is—it is exactly the same as what we had before. Its real usefulness comes, as shown in (Charniak, 1999), when we move from the notion of a feature chain to a feature tree. These feature chains don't capture everything we'd like them to. If there are two independent features that are each relatively sparse but occasionally carry a lot of information, then putting one before the other in a chain will effectively block the second from having any effect, since its information is (uselessly) conditioned on the first one, whose sparseness will completely dilute any gain. What we'd really like is to be able to have a feature tree, whereby we can condition those two sparse features independently on one common predecessor feature. As we said before, equation 3 represents, for each feature the probability of f based on fi and all its predecessors, divided by the probability of f based only on the predecessors. In the chain case, this means that the denominator is conditioned on every feature from 1 to i — 1, but if we use a feature tree, it is conditioned only on those features along the path to the root of the tree. A notable issue with feature trees as opposed to feature chains is that the terms do not all cancel out. Every leaf on the tree will be represented in the numerator, and every fork in the tree (from which multiple nodes depend) will be represented at least once in the denominator. For example: in figure 3 we have a small feature tree that has one target feature and four conditioning features. Features b and d are independent of each other, but each depends on a; c depends directly only on b. The unsmoothed version of the corresponding equation would be which, after cancelling of terms and smoothing, results in Note that strictly speaking the result is not a probability distribution. It could be made into one with an appropriate normalisation—the so-called partition function in the maximumentropy literature. However, if the independence assumptions made in the derivation of equation 4 are good ones, the partition function will be close to 1.0. We assume this to be the case for our feature trees. Now we return the discussion to function tagging. There are a number of features that seem tar et feature to condition strongly for one function tag or another; we have assembled them into the feature tree shown in figure 4.2 This figure should be relatively self-explanatory, except for the notion of an 'alternate head'; currently, an alternate head is only defined for prepositional phrases, and is the head of the object of the prepositional phrase. This data is very important in distinguishing, for example, 'by John' (where John might be a logical subject) from 'by next year' (a temporal modifier) and 'by selling it' (an adverbial indicating manner). In the training phase of our experiment, we gathered statistics on the occurrence of function tags in sections 2-21 of the Penn treebank. Specifically, for every constituent in the treebank, we recorded the presence of its function tags (or lack thereof) along with its conditioning information. From this we calculated the empirical probabilities of each function tag referenced in section 2 of this paper. Values of A were determined using EM on the development corpus (treebank section 24). To test, then, we simply took the output of our parser on the test corpus (treebank section 23), and applied a postprocessing step to add function tags. For each constituent in the tree, we calculated the likelihood of each function tag according to the feature tree in figure 4, and for each category (see figure 2) we assigned the most likely function tag (which might be the null tag). 2The reader will note that the 'features' listed in the tree are in fact not boolean-valued; each node in the given tree can be assumed to stand for a chain of boolean features, one per potential value at that node, exactly one of which will be true. To evaluate our results, we first need to determine what is 'correct'. The definition we chose is to call a constituent correct if there exists in the correct parse a constituent with the same start and end points, label, and function tag (or lack thereof). Since we treated each of the four function tag categories as a separate feature for the purpose of tagging, evaluation was also done on a per-category basis. The denominator of the accuracy measure should be the maximum possible number we could get correct. In this case, that means excluding those constituents that were already wrong in the parser output; the parser we used attains 89% labelled precision-recall, so roughly 11% of the constituents are excluded from the function tag accuracy evaluation. (For reference, we have also included the performance of our function tagger directly on treebank parses; the slight gain that resulted is discussed below.) Another consideration is whether to count non-tagged constituents in our evaluation. On the one hand, we could count as correct any constituent with the correct tag as well as any correctly non-tagged constituent, and use as our denominator the number of all correctlylabelled constituents. (We will henceforth refer to this as the 'with-null' measure.) On the other hand, we could just count constituents with the correct tag, and use as our denominators the total number of tagged, correctly-labelled constituents. We believe the latter number (`nonull') to be a better performance metric, as it is not overwhelmed by the large number of untagged constituents. Both are reported below. There are, it seems, two reasonable baselines for this and future work. First of all, most constituents in the corpus have no tags at all, so obviously one baseline is to simply guess no tag for any constituent. Even for the most common type of function tag (grammatical), this method performs with 87% accuracy. Thus the with-null accuracy of a function tagger needs to be very high to be significant here. The second baseline might be useful in examining the no-null accuracy values (particularly the recall): always guess the most common tag in a category. This means that every constituent gets labelled with `--SBJ-TMP-TPC-CLR' (meaning that it is a topicalised temporal subject that is 'closely related' to its verb). This combination of tags is in fact entirely illegal by the treebank guidelines, but performs adequately for a baseline. The precision is, of course, abysmal, for the same reasons the first baseline did so well; but the recall is (as one might expect) substantial. The performances of the two baseline measures are given in Table 1. In table 2, we give the results for each category. The first column is the with-null accuracy, and the precision and recall values given are the nonull accuracy, as noted in section 4. Grammatical tagging performs the best of the four categories. Even using the more difficult no-null accuracy measure, it has a 96% accuracy. This seems to reflect the fact that grammatical relations can often be guessed based on constituent labels, parts of speech, and highfrequency lexical items, largely avoiding sparsedata problems. Topicalisation can similarly be guessed largely on high-frequency information, and performed almost as well (93%). On the other hand, we have the form/function tags and the 'miscellaneous' tags. These are characterised by much more semantic information, and the relationships between lexical items are very important, making sparse data a real problem. All the same, it should be noted that the performance is still far better than the baselines. The feature tree given in figure 4 is by no means the only feature tree we could have used. Indeed, we tried a number of different trees on the development corpus; this tree gave among the best overall results, with no category performing too badly. However, there is no reason to use only one feature tree for all four categories; the best results can be got by using a separate tree for each one. One can thus achieve slight (one to three point) gains in each category. The overall performance, given in table 3, appears promising. With a tagging accuracy of about 87%, various information retrieval and knowledge base applications can reasonably expect to extract useful information. The performance given in the first row is (like all previously given performance values) the function-tagger's performance on the correctlylabelled constituents output by our parser. For comparison, we also give its performance when run directly on the original treebank parse; since the parser's accuracy is about 89%, working directly with the treebank means our statistics are over roughly 12% more constituents. This second version does slightly better. The main reason that tagging does worse on the parsed version is that although the constituent itself may be correctly bracketed and labelled, its exterior conditioning information can still be incorrect. An example of this that actually occurred in the development corpus (section 24 of the treebank) is the 'that' clause in the phrase 'can swallow the premise that the rewards for such ineptitude are six-figure salaries', correctly diagrammed in figure 5. The function tagger gave this SBAR an ADV tag, indicating an unspecified adverbial function. This seems extremely odd, given that its conditioning information (nodes circled in the figure) clearly show that it is part of an NP, and hence probably modifies the preceding NN. Indeed, the statistics give the probability of an ADV tag in this conditioning environment as vanishingly small. However, this was not the conditioning information that the tagger received. The parser had instead decided on the (incorrect) parse in figure 6. As such, the tagger's decision makes much more sense, since an SBAR under two VPs whose heads are VB and MD is rather likely to be an ADV. (For instance, the 'although' clause of the sentence 'he can help, although he doesn't want to.' has exactly the conditioning environment given in figure 6, except that its predecessor is a comma; and this SBAR would be correctly tagged ADV.) The SBAR itself is correctly bracketed and labelled, so it still gets counted in the statistics. Happily, this sort of case seems to be relatively rare. Another thing that lowers the overall performance somewhat is the existence of error and inconsistency in the treebank tagging. Some tags seem to have been relatively easy for the human treebank taggers, and have few errors. Other tags have explicit caveats that, however welljustified, proved difficult to remember for the taggers—for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, '[LGS] attaches to the NP object of by and not to the PP node itself.' (Bies et al., 1995) Each mistagging in the test corpus can cause up to two spurious errors, one in precision and one in recall. Still another source of difficulty comes when the guidelines are vague or silent on a specific issue. To return to logical subjects, it is clear that `the loss' is a logical subject in 'The company was hurt by the loss', but what about in 'The company was unperturbed by the loss'? In addition, a number of the function tags are authorised for 'metaphorical use', but what exactly constitutes such a use is somewhat inconsistently marked. It is as yet unclear just to what degree these tagging errors in the corpus are affecting our results.","There are, it seems, two reasonable baselines for this and future work. First of all, most constituents in the corpus have no tags at all, so obviously one baseline is to simply guess no tag for any constituent. Even for the most common type of function tag (grammatical), this method performs with 87% accuracy. Thus the with-null accuracy of a function tagger needs to be very high to be significant here. The second baseline might be useful in examining the no-null accuracy values (particularly the recall): always guess the most common tag in a category. This means that every constituent gets labelled with `--SBJ-TMP-TPC-CLR' (meaning that it is a topicalised temporal subject that is 'closely related' to its verb). This combination of tags is in fact entirely illegal by the treebank guidelines, but performs adequately for a baseline. The precision is, of course, abysmal, for the same reasons the first baseline did so well; but the recall is (as one might expect) substantial. The performances of the two baseline measures are given in Table 1. In table 2, we give the results for each category. The first column is the with-null accuracy, and the precision and recall values given are the nonull accuracy, as noted in section 4. Grammatical tagging performs the best of the four categories. Even using the more difficult no-null accuracy measure, it has a 96% accuracy. This seems to reflect the fact that grammatical relations can often be guessed based on constituent labels, parts of speech, and highfrequency lexical items, largely avoiding sparsedata problems. Topicalisation can similarly be guessed largely on high-frequency information, and performed almost as well (93%). On the other hand, we have the form/function tags and the 'miscellaneous' tags. These are characterised by much more semantic information, and the relationships between lexical items are very important, making sparse data a real problem. All the same, it should be noted that the performance is still far better than the baselines. The feature tree given in figure 4 is by no means the only feature tree we could have used. Indeed, we tried a number of different trees on the development corpus; this tree gave among the best overall results, with no category performing too badly. However, there is no reason to use only one feature tree for all four categories; the best results can be got by using a separate tree for each one. One can thus achieve slight (one to three point) gains in each category. The overall performance, given in table 3, appears promising. With a tagging accuracy of about 87%, various information retrieval and knowledge base applications can reasonably expect to extract useful information. The performance given in the first row is (like all previously given performance values) the function-tagger's performance on the correctlylabelled constituents output by our parser. For comparison, we also give its performance when run directly on the original treebank parse; since the parser's accuracy is about 89%, working directly with the treebank means our statistics are over roughly 12% more constituents. This second version does slightly better. The main reason that tagging does worse on the parsed version is that although the constituent itself may be correctly bracketed and labelled, its exterior conditioning information can still be incorrect. An example of this that actually occurred in the development corpus (section 24 of the treebank) is the 'that' clause in the phrase 'can swallow the premise that the rewards for such ineptitude are six-figure salaries', correctly diagrammed in figure 5. The function tagger gave this SBAR an ADV tag, indicating an unspecified adverbial function. This seems extremely odd, given that its conditioning information (nodes circled in the figure) clearly show that it is part of an NP, and hence probably modifies the preceding NN. Indeed, the statistics give the probability of an ADV tag in this conditioning environment as vanishingly small. However, this was not the conditioning information that the tagger received. The parser had instead decided on the (incorrect) parse in figure 6. As such, the tagger's decision makes much more sense, since an SBAR under two VPs whose heads are VB and MD is rather likely to be an ADV. (For instance, the 'although' clause of the sentence 'he can help, although he doesn't want to.' has exactly the conditioning environment given in figure 6, except that its predecessor is a comma; and this SBAR would be correctly tagged ADV.) The SBAR itself is correctly bracketed and labelled, so it still gets counted in the statistics. Happily, this sort of case seems to be relatively rare. Another thing that lowers the overall performance somewhat is the existence of error and inconsistency in the treebank tagging. Some tags seem to have been relatively easy for the human treebank taggers, and have few errors. Other tags have explicit caveats that, however welljustified, proved difficult to remember for the taggers—for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, '[LGS] attaches to the NP object of by and not to the PP node itself.' (Bies et al., 1995) Each mistagging in the test corpus can cause up to two spurious errors, one in precision and one in recall. Still another source of difficulty comes when the guidelines are vague or silent on a specific issue. To return to logical subjects, it is clear that `the loss' is a logical subject in 'The company was hurt by the loss', but what about in 'The company was unperturbed by the loss'? In addition, a number of the function tags are authorised for 'metaphorical use', but what exactly constitutes such a use is somewhat inconsistently marked. It is as yet unclear just to what degree these tagging errors in the corpus are affecting our results."
48,"1993). We that part of speech tagging and word alignment could have an important role in glossary construction for translation. Glossaries are extremely important for translation. How would Microsoft, or some other software vendor, want the term &quot;Character menu&quot; to be translated in their manuals? Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text. In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one. It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button. Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs. A glossary is a list of terms and their translations.' We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents. The first task will be referred to as the monolingual task and the second as the bilingual task. How should a glossary be constructed? Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming","1993). We that part of speech tagging and word alignment could have an important role in glossary construction for translation. Glossaries are extremely important for translation. How would Microsoft, or some other software vendor, want the term &quot;Character menu&quot; to be translated in their manuals? Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text. In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one. It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button. Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs. A glossary is a list of terms and their translations.' We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents. The first task will be referred to as the monolingual task and the second as the bilingual task. How should a glossary be constructed? Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming The statistical corpus-based renaissance in computational linguistics has produced a number of interesting technologies, including part-of-speech tagging and bilingual word alignment. Unfortunately, these technologies are still not as widely deployed in practical applications as they might be. Part-ofspeech taggers are used in a few applications, such as speech synthesis (Sproat et al., 1992) and question answering (Kupiec, 1993b). Word alignment is newer, found only in a few places (Gale and Church, 1991a; Brown et al., 1993; Dagan et al., 1993). It is used at IBM for estimating parameters of their statistical machine translation prototype (Brown et al., 1993). We suggest that part of speech tagging and word alignment could have an important role in glossary construction for translation. Glossaries are extremely important for translation. How would Microsoft, or some other software vendor, want the term &quot;Character menu&quot; to be translated in their manuals? Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text. In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one. It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button. Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs. A glossary is a list of terms and their translations.' We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents. The first task will be referred to as the monolingual task and the second as the bilingual task. How should a glossary be constructed? Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming process, as the introduction to Hann's (1992, P. 8) text on technical translation indicates: Contrary to popular opinion, the job of a technical translator has little in common with other linguistic professions, such as literature translation, foreign correspondence or interpreting. Apart from an expert knowledge of both languages..., all that is required for the latter professions is a few general dictionaries, whereas a technical translator needs a whole library of specialized dictionaries, encyclopedias and 'The source and target fields are standard, though many other fields can also be found, e.g., usage notes, part of speech constraints, comments, etc. technical literature in both languages; he is more concerned with the exact meanings of terms than with stylistic considerations and his profession requires certain 'detective' skills as well as linguistic and literary ones. Beginners in this profession have an especially hard time... This book attempts to meet this requirement. Unfortunately, the academic prescriptions are often too expensive for commercial practice. Translators need just-in-time glossaries. They cannot afford to do a lot of background reading and &quot;detective&quot; work when they are being paid by the word. They need something more practical. We propose a tool, termight, that automates some of the more tedious and laborious aspects of terminology research. The tool relies on part-of-speech tagging and word-alignment technologies to extract candidate terms and translations. It then sorts the extracted candidates and presents them to the user along with reference concordance lines, supporting efficient construction of glossaries. The tool is currently being used by the translators at AT&T Business Translation Services (formerly AT&T Language Line Services). Termight may prove useful in contexts other than human-based translation. Primarily, it can support customization of machine translation (MT) lexicons to a new domain. In fact, the arguments for constructing a job-specific glossary for human-based translation may hold equally well for an MT-based process, emphasizing the need for a productivity tool. The monolingual component of termight can be used to construct terminology lists in other applications, such as technical writing, book indexing, hypertext linking, natural language interfaces, text categorization and indexing in digital libraries and information retrieval (Salton, 1988; Cherry, 1990; Harding, 1982; Bourigault, 1992; Damerau, 1993), while the bilingual component can be useful for information retrieval in multilingual text collections (Landauer and Littman, 1990). Although part-of-speech taggers have been around for a while, there are relatively few practical applications of this technology. The monolingual task appears to be an excellent candidate. As has been noticed elsewhere (Bourigault, 1992; Justeson and Katz, 1993), most technical terms can be found by looking for multiword noun phrases that satisfy a rather restricted set of syntactic patterns. We follow Justeson and Katz (1993) who emphasize the importance of term frequency in selecting good candidate terms. An expert terminologist can then skim the list of candidates to weed out spurious candidates and cliches. Very simple procedures of this kind have been remarkably successful. They can save an enormous amount of time over the current practice of reading the document to be translated, focusing on tables, figures, index, table of contents and so on, and writing down terms that happen to catch the translator's eye. This current practice is very laborious and runs the risk of missing many important terms. Termight uses a part of speech tagger (Church, 1988) to identify a list of candidate terms which is then filtered by a manual pass. We have found, however, that the manual pass dominates the cost of the monolingual task, and consequently, we have tried to design an interactive user interface (see Figure 1) that minimizes the burden on the expert terminologist. The terminologist is presented with a list of candidate terms, and corrects the list with a minimum number of key strokes. The interface is designed to make it easy for the expert to pull up evidence from relevant concordance lines to help identify incorrect candidates as well as terms that are missing from the list. A single key-press copies the current candidate term, or the content of any marked emacs region, into the upper-left screen. The candidates are sorted so that the better ones are found near the top of the list, and so that related candidates appear near one another. Candidate terms. The list of candidate terms contains both multi-word noun phrases and single words. The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech (Church, 1988). The set of syntactic patterns is considered as a parameter and can be adopted to a specific domain by the user. Currently our patterns match only sequences of nouns, which seem to yield the best hit rate in our environment. Single-word candidates are defined by taking the list of all words that occur in the document and do not appear in a standard stop-list of &quot;noise&quot; words. Grouping and sorting of terms. The list of candidate terms is sorted to group together all noun phrase terms that have the same head word (as in Figure 1), which is simply the last word of the term for our current set of noun phrase patterns. The order of the groups in the list is determined by decreasing frequency of the head word in the document, which usually correlates with the likelihood that this head word is used in technical terms. Sorting within groups. Under each head word the terms are sorted alphabetically according to reversed order of the words. Sorting in this order reflects the order of modification in simple English noun phrases and groups together terms that denote different modifications of a more general term (see point one space to the right._ To point to the place you want the object_ point to the last_ character you point to the beginrting of the fde, sele insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion insertion_ point to the end of the file._ insertion point where you want the page break and for example the terms default paper size, paper size and size in Figure 1). Concordance lines. To decide whether a candidate term is indeed a term, and to identify multiword terms that are missing from the candidate list, one must view relevant lines of the document. For this purpose we present a concordance line for each occurrence of a term (a text line centered around the term). If, however, a term, ti, (like 'point') is contained in a longer term, t2, (like 'insertion point' or 'decimal point') then occurrences of t2 are not displayed for ti. This way, the occurrences of a general term (or a head word) are classified into disjoint sets corresponding to more specific terms, leaving only unclassified occurrences under the general term. In the case of 'point', for example, five specific terms are identified that account for 61 occurrences of 'point', and accordingly, for 61 concordance lines. Only 20 concordance lines are displayed for the word 'point' itself, and it is easy to identify in them 5 occurrences of the term 'starting point', which is missing from the candidate list (because 'starting' is tagged as a verb). To facilitate scanning, concordance lines are sorted so that all occurrences of identical preceding contexts of the head word, like 'starting', are grouped together. Since all the words of the document, except for stop list words, appear in the candidate list as single-word terms it is guaranteed that every term that was missed by the automatic procedure will appear in the concordance lines. In summary, our algorithm performs the following steps: Using the monolingual component, a terminologist at AT&T Business Translation Services constructs terminology lists at the impressive rate of 150-200 terms per hour. For example, it took about 10 hours to construct a list of 1700 terms extracted from a 300,000 word document. The tool has at least doubled the rate of constructing terminology lists, which was previously performed by simpler lexicographic tools. Alternative proposals are likely to miss important but infrequent terms/translations such as 'Format Disk dialog box' and 'Label Disk dialog box' which occur just once. In particular, mutual information (Church and Hanks, 1990; Wu and Su, 1993) and other statistical methods such as (Smadja, 1993) and frequency-based methods such as (Justeson and Katz, 1993) exclude infrequent phrases because they tend to introduce too much noise. We have found that frequent head words are likely to generate a number of terms, and are therefore more important for the glossary (a &quot;productivity&quot; criterion). Consider the frequent head word box. In the Microsoft Windows manual, for example, almost any type of box is a technical term. By sorting on the frequency of the headword, we have been able to find many infrequent terms, and have not had too much of a problem with noise (at least for common headwords). Another characteristic of previous work is that each candidate term is scored independently of other terms. We score a group of related terms rather than each term at a time. Future work may enhance our simple head-word frequency score and may take into account additional relationships between terms, including common words in modifying positions. Termight uses a part-of-speech tagger to identify candidate noun phrases. Justeson and Katz (1993) only consult a lexicon and consider all the possible parts of speech of a word. In particular, every word that can be a noun according to the lexicon is considered as a noun in each of its occurrences. Their method thus yields some incorrect noun phrases that will not be proposed by a tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors. Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most alignment work was concerned with alignment at the sentence level. Algorithms for the more difficult task of word alignment were proposed in (Gale and Church, 1991a; Brown et al., 1993; Dagan et al., 1993) and were applied for parameter estimation in the IBM statistical machine translation system (Brown et al., 1993). Previously translated texts provide a major source of information about technical terms. As Isabelle (1992) argues, &quot;Existing translations contain more solutions to more translation problems than any other existing resource.&quot; Even if other resources, such as general technical dictionaries, are available it is important to verify the translation of terms in previously translated documents of the same customer (or domain) to ensure consistency across documents. Several translation workstations provide sentence alignment and allow the user to search interactively for term translations in aligned archives (e.g. (Ogden and Gonzales, 1993)). Some methods use sentence alignment and additional statistics to find candidate translations of terms (Smadja, 1992; van der Eijk, 1993). We suggest that word level alignment is better suitable for term translation. The bilingual component of termight gets as input a list of source terms and a bilingual corpus aligned at the word level. We have been using the output of word_align, a robust alignment program that proved useful for bilingual concordancing of noisy texts (Dagan et al., 1993). Word_align produces a partial mapping between the words of the two texts, skipping words that cannot be aligned at a given confidence level (see Figure 2). For each occurrence of a source term, termight identifies a candidate translation based on the alignment of its words. The candidate translation is defined as the sequence of words between the first and last target positions that are aligned with any of the words of the source term. In the example of Figure 2 the candidate translation of Optional Parameters box is zone Parametres optionnels, since zone and optionnels are the first and last French words that are aligned with the words of the English term. Notice that in this case the candidate translation is correct even though the word Parameters is aligned incorrectly. In other cases alignment errors may lead to an incorrect candidate translation for a specific occurrence of the term. It is quite likely, however, that the correct translation, or at least a string that overlaps with it, will be identified in some occurrences of the term. Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order. The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3). The user views the concordances to verify correct candidates or to find translations that are missing from the candidate list. The latter task becomes especially easy when a candidate overlaps with the correct translation, directing the attention of the user to the concordance lines of this particular candidate, which are likely to be aligned correctly. A single key-stroke copies a verified candidate translation, or a translation identified as a marked emacs region in a concordance line, into the appropriate place in the glossary. We evaluated the bilingual component of termight in translating a glossary of 192 terms found in the English and German versions of a technical manual. The correct answer was often the first choice (40%) or the second choice (7%) in the candidate list. For the remaining 53% of the terms, the correct answer was always somewhere in the concordances. Using the interface, the glossary was translated at a rate of about 100 terms per hour. Smadja (1992) and van der Eijk (1993) describe term translation methods that use bilingual texts that were aligned at the sentence level. Their methods find likely translations by computing statistics on term cooccurrence within aligned sentences and selecting source-target pairs with statistically significant associations. We found that explicit word alignments enabled us to identify translations of infrequent terms that would not otherwise meet statistical significance criteria. If the words of a term occur at least several times in the document (regardless of the term frequency) then word_align is likely to align them correctly and termight will identify the correct translation. If only some of the words of a term are frequent then termight is likely to identify a translation that overlaps with the correct one, directing the user quickly to correctly aligned concordance lines. Even if all the words of the term were not aligned by word_align it is still likely that most concordance lines are aligned correctly based on other words in the near context. Termight motivates future improvements in word alignment quality that will increase recall and precision of the candidate list. In particular, taking into account local syntactic structures and phrase boundaries will impose more restrictions on alignments of complete terms. Finally, termight can be extended for verifying translation consistency at the proofreading (editing) step of a translation job, after the document has been translated. For example, in an English-German document pair the tool identified the translation of the term Controls menu as Menu Steuerung in 4 out of 5 occurrences. In the fifth occurrence word_align failed to align the term correctly because another translation, Steuernienu, was uniquely used, violating the consistency requirement. Termight, or a similar tool, can thus be helpful in identifying inconsistent translations. We have shown that terminology research provides a good application for robust natural language technology, in particular for part-of-speech tagging and word-alignment algorithms. Although the output of these algorithms is far from perfect, it is possible to extract from it useful information that is later corrected and augmented by a user. Our extraction algorithms emphasize completeness, and identify also infrequent candidates that may not meet some of the statistical significance criteria proposed in the literature. To make the entire process efficient, however, it is necessary to analyze the user's work process and provide interfaces that support it. In many cases, improving the way information is presented to the user may have a larger effect on productivity than improvements in the underlying natural language technology. In particular, we have found the following to be very effective: As the need for efficient knowledge acquisition tools becomes widely recognized, we hope that this experience with termight will be found useful for other text-related systems as well.","We have shown that terminology research provides a good application for robust natural language technology, in particular for part-of-speech tagging and word-alignment algorithms. Although the output of these algorithms is far from perfect, it is possible to extract from it useful information that is later corrected and augmented by a user. Our extraction algorithms emphasize completeness, and identify also infrequent candidates that may not meet some of the statistical significance criteria proposed in the literature. To make the entire process efficient, however, it is necessary to analyze the user's work process and provide interfaces that support it. In many cases, improving the way information is presented to the user may have a larger effect on productivity than improvements in the underlying natural language technology. In particular, we have found the following to be very effective: As the need for efficient knowledge acquisition tools becomes widely recognized, we hope that this experience with termight will be found useful for other text-related systems as well."
49,"We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment. 1 Desiderata Many words are ambiguous in their part of speech. For example, &quot;tag&quot; can be a noun or a verb. However, when a word appears in the context of other words, the ambiguity is often reduced: in &quot;a tag is a part-of-speech label,&quot; the &quot;tag&quot; can only be a noun. A tagger is a system that uses context to assign parts of speech to words. Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora. Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text. For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: corpora contain ungrammatical constructions, isolated phrases (such as titles), and nonlinguistic data (such as tables). Corpora are also likely to contain words that are unknown to the tagger. It is desirable that a tagger deal gracefully with these situations. a tagger is to be used to analyze arbitrarily large corpora, it must be efficient—performing in time linear in the number of words tagged. Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres. A should attempt to assign the correct part-of-speech tag to every word encountered. A should be able to take advantage of linguistic insights. One should be able to correct errors by supplying appropriate priori &quot;hints.&quot; It should be possible to give different hints for different corpora. effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal. 2 Methodology 2.1 Background Several different approaches have been used for building text taggers. Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and KuEera, 1982]. TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Koskenniemi, 1990]. Statistical methods have also been used (e.g., [DeRose, [Garside al., These provide the capability of resolving ambiguity on the basis of most likely interpretation. A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words. Two types of training (i.e., parameter estimation) have been used with this model. The first makes use of a tagged training corpus. Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986]. At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model. Church uses the tagged Brown corpus for training [Church, 1988]. These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972]. Under this regime the model is a Markov model as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger [Jelinek, 1985]. Parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980]. Kupiec used word equivclasses (referred to here as classes) on parts of speech, to pool data from individual words [Kupiec, 1989b]. The most common words are still represented individually, as sufficient data exist for robust estimation. 133 However all other words are represented according to the set of possible categories they can assume. In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992]. To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). In [Kupiec, 1989a], networks are used to selectively augment the context in a basic firstorder model, rather than using uniformly second-order dependencies. 2.2 Our approach We next describe how our choice of techniques satisfies the listed in section 1. The use of an complete flexibility in the choice of training corpora. Text from any desired domain can be used, and a tagger can be tailored for use with a particular text database by training on a portion of that database. Lexicons containing alternative tag sets can be easily accommodated without any need for re-labeling the training corpus, affording further flexibility in the use of specialized tags. As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal effort, even for other languages, such as French (e.g., [Kupiec, 1992]). The use of ambiguity classes and a first-order model reduces the number of parameters to be estimated without significant reduction in accuracy (discussed in section 5). This also enables a tagger to be reliably trained using only moderate amounts of text. We have produced reasonable results training on as few as 3,000 sentences. Fewer parameters also reduce the time required for training. Relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are already accommodated. Vocabulary independence is achieved by predicting categories for words not in the lexicon, using both context and suffix information. Probabilities corresponding to category sequences that never occurred in the training data are assigned small, non-zero values, ensuring that the model will accept any sequence of tokens, while still providing the most likely tagging. By using the fact that words are typically associated with only a few part-ofspeech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3). 3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specgiven in [Levinson et with special attention to space and time efficiency issues. Only first-order modeling is addressed and will be presumed for the remainder of this discussion. 3.1 Formalism brief, an a doubly stochastic process that generates sequence of symbols ={Sl,S2, , 1 <i< where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise).' Th Markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities A -= 1 < a, the probability of moving from state i to state of initial probabilities H = {70 1 < i < is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by of symbol probabilities 1 < j < < < M = IW I is the probability ( symbol given that the Markov process is i In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context. Given the parameters .4, Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to of ambiguity classes. In the following, identified with the number of possible .tags, and W wil the set of all ambiguity classes. Applying an HMM consists of two tasks: estimating ti parameters A, a training set; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities eft+1. (i) = bi(St+i) < t < T — 1, ( = for all the backward prob bilities, i3(i) = — 1 < t < 1, ( 1.1 1 for all forward probabili the joint probability of the sequence up to tir t, S2, , the event that the Markov pr is in state i at time the backwa is the probability of seeing the sequen , ST} that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N =","We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment. 1 Desiderata Many words are ambiguous in their part of speech. For example, &quot;tag&quot; can be a noun or a verb. However, when a word appears in the context of other words, the ambiguity is often reduced: in &quot;a tag is a part-of-speech label,&quot; the &quot;tag&quot; can only be a noun. A tagger is a system that uses context to assign parts of speech to words. Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora. Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text. For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: corpora contain ungrammatical constructions, isolated phrases (such as titles), and nonlinguistic data (such as tables). Corpora are also likely to contain words that are unknown to the tagger. It is desirable that a tagger deal gracefully with these situations. a tagger is to be used to analyze arbitrarily large corpora, it must be efficient—performing in time linear in the number of words tagged. Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres. A should attempt to assign the correct part-of-speech tag to every word encountered. A should be able to take advantage of linguistic insights. One should be able to correct errors by supplying appropriate priori &quot;hints.&quot; It should be possible to give different hints for different corpora. effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal. 2 Methodology 2.1 Background Several different approaches have been used for building text taggers. Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and KuEera, 1982]. TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Koskenniemi, 1990]. Statistical methods have also been used (e.g., [DeRose, [Garside al., These provide the capability of resolving ambiguity on the basis of most likely interpretation. A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words. Two types of training (i.e., parameter estimation) have been used with this model. The first makes use of a tagged training corpus. Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986]. At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model. Church uses the tagged Brown corpus for training [Church, 1988]. These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972]. Under this regime the model is a Markov model as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger [Jelinek, 1985]. Parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980]. Kupiec used word equivclasses (referred to here as classes) on parts of speech, to pool data from individual words [Kupiec, 1989b]. The most common words are still represented individually, as sufficient data exist for robust estimation. 133 However all other words are represented according to the set of possible categories they can assume. In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992]. To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). In [Kupiec, 1989a], networks are used to selectively augment the context in a basic firstorder model, rather than using uniformly second-order dependencies. 2.2 Our approach We next describe how our choice of techniques satisfies the listed in section 1. The use of an complete flexibility in the choice of training corpora. Text from any desired domain can be used, and a tagger can be tailored for use with a particular text database by training on a portion of that database. Lexicons containing alternative tag sets can be easily accommodated without any need for re-labeling the training corpus, affording further flexibility in the use of specialized tags. As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal effort, even for other languages, such as French (e.g., [Kupiec, 1992]). The use of ambiguity classes and a first-order model reduces the number of parameters to be estimated without significant reduction in accuracy (discussed in section 5). This also enables a tagger to be reliably trained using only moderate amounts of text. We have produced reasonable results training on as few as 3,000 sentences. Fewer parameters also reduce the time required for training. Relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are already accommodated. Vocabulary independence is achieved by predicting categories for words not in the lexicon, using both context and suffix information. Probabilities corresponding to category sequences that never occurred in the training data are assigned small, non-zero values, ensuring that the model will accept any sequence of tokens, while still providing the most likely tagging. By using the fact that words are typically associated with only a few part-ofspeech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3). 3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specgiven in [Levinson et with special attention to space and time efficiency issues. Only first-order modeling is addressed and will be presumed for the remainder of this discussion. 3.1 Formalism brief, an a doubly stochastic process that generates sequence of symbols ={Sl,S2, , 1 <i< where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise).' Th Markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities A -= 1 < a, the probability of moving from state i to state of initial probabilities H = {70 1 < i < is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by of symbol probabilities 1 < j < < < M = IW I is the probability ( symbol given that the Markov process is i In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context. Given the parameters .4, Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to of ambiguity classes. In the following, identified with the number of possible .tags, and W wil the set of all ambiguity classes. Applying an HMM consists of two tasks: estimating ti parameters A, a training set; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities eft+1. (i) = bi(St+i) < t < T — 1, ( = for all the backward prob bilities, i3(i) = — 1 < t < 1, ( 1.1 1 for all forward probabili the joint probability of the sequence up to tir t, S2, , the event that the Markov pr is in state i at time the backwa is the probability of seeing the sequen , ST} that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N = Many words are ambiguous in their part of speech. For example, &quot;tag&quot; can be a noun or a verb. However, when a word appears in the context of other words, the ambiguity is often reduced: in &quot;a tag is a part-of-speech label,&quot; the word &quot;tag&quot; can only be a noun. A part-of-speech tagger is a system that uses context to assign parts of speech to words. Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora. Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text. For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: Robust Text corpora contain ungrammatical constructions, isolated phrases (such as titles), and nonlinguistic data (such as tables). Corpora are also likely to contain words that are unknown to the tagger. It is desirable that a tagger deal gracefully with these situations. Efficient If a tagger is to be used to analyze arbitrarily large corpora, it must be efficient—performing in time linear in the number of words tagged. Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres. Accurate A tagger should attempt to assign the correct part-of-speech tag to every word encountered. Tunable A tagger should be able to take advantage of linguistic insights. One should be able to correct systematic errors by supplying appropriate a priori &quot;hints.&quot; It should be possible to give different hints for different corpora. Reusable The effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal. Several different approaches have been used for building text taggers. Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and KuEera, 1982]. TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Koskenniemi, 1990]. Statistical methods have also been used (e.g., [DeRose, 1988], [Garside et al., 1987]). These provide the capability of resolving ambiguity on the basis of most likely interpretation. A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words. Two types of training (i.e., parameter estimation) have been used with this model. The first makes use of a tagged training corpus. Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986]. At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model. Church uses the tagged Brown corpus for training [Church, 1988]. These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972]. Under this regime the model is called a hidden Markov model (HMM), as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger [Jelinek, 1985]. Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980]. Kupiec used word equivalence classes (referred to here as ambiguity classes) based on parts of speech, to pool data from individual words [Kupiec, 1989b]. The most common words are still represented individually, as sufficient data exist for robust estimation. However all other words are represented according to the set of possible categories they can assume. In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992]. To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). In [Kupiec, 1989a], networks are used to selectively augment the context in a basic firstorder model, rather than using uniformly second-order dependencies. We next describe how our choice of techniques satisfies the criteria listed in section 1. The use of an HMM permits complete flexibility in the choice of training corpora. Text from any desired domain can be used, and a tagger can be tailored for use with a particular text database by training on a portion of that database. Lexicons containing alternative tag sets can be easily accommodated without any need for re-labeling the training corpus, affording further flexibility in the use of specialized tags. As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal effort, even for other languages, such as French (e.g., [Kupiec, 1992]). The use of ambiguity classes and a first-order model reduces the number of parameters to be estimated without significant reduction in accuracy (discussed in section 5). This also enables a tagger to be reliably trained using only moderate amounts of text. We have produced reasonable results training on as few as 3,000 sentences. Fewer parameters also reduce the time required for training. Relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are already accommodated. Vocabulary independence is achieved by predicting categories for words not in the lexicon, using both context and suffix information. Probabilities corresponding to category sequences that never occurred in the training data are assigned small, non-zero values, ensuring that the model will accept any sequence of tokens, while still providing the most likely tagging. By using the fact that words are typically associated with only a few part-ofspeech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3). The hidden Markov modeling component of our tagger is implemented as an independent module following the specification given in [Levinson et al., 1983], with special attention to space and time efficiency issues. Only first-order modeling is addressed and will be presumed for the remainder of this discussion. In brief, an HMM is a doubly stochastic process that generates sequence of symbols S ={Sl,S2, , ST}, S,EW 1 <i< T, where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise).' Th Markov process captures the notion of sequence depen dency and is described by a set of N states, a matrix c transition probabilities A -= la,11 1 < i, j < N where a, is the probability of moving from state i to state j, and vector of initial probabilities H = {70 1 < i < N where 71 is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by matrix of symbol probabilities B {kik} 1 < j < N an 1 < k < M where M = IW I and kik is the probability ( generating symbol sk given that the Markov process is i state j.2 In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context. Given the parameters .4, and II, hidden Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to sequence of ambiguity classes. In the following, N can identified with the number of possible .tags, and W wil the set of all ambiguity classes. Applying an HMM consists of two tasks: estimating ti model parameters A, B and H from a training set; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities where ot1(i) = ribi(51) for all i; and the backward prob bilities, where OT(j) = 1 for all j. The forward probabili oet(i) is the joint probability of the sequence up to tir t, {S1, S2, , St}, and the event that the Markov pr cess is in state i at time t. Similarly, the backwa probability Ot(j) is the probability of seeing the sequen {St+i, St+2, , ST} given that the Markov process is state i at time t. It follows that the probability of t entire sequence is for any tin the range 1 <t < T — 1.3 'For an introduction to hidden Markov modeling see [F biner and Juang, 1986]. Given an initial choice for the parameters A, B, and H the expected number of transitions, , from state i to state j conditioned on the observation sequence S may be computed as follows: rescale. One approach premultiplies the a and 0 probabilities with an accumulating product depending on t [Levinson et al., 1983]. Let ei1(i) = ai(i) and define and 1 . = —(Y1(001(0. (5) In summary, to find maximum likelihood estimates for A, B, and H, via the Baum-Welch algorithm, one chooses some starting values, applies equations 3-5 to compute new values, and then iterates until convergence. It can be shown that this algorithm will converge, although possibly to a non-global maximum [Baum, 19721. Once a model has been estimated, selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S. An efficient dynamic programming procedure, known as the Viterbi algorithm [Viterbi, 1967], arranges for this computation to proceed in time proportional to T. Suppose V = {v(t)} 1 < t < T is a state sequence that generates S, then the probability that V generates S is, To find the most probable such sequence we start by defining 01(i) = rabi(Si) for 1 < i < N and then perform the recursion for 2 < t < T and 1 < j < N. The crucial observation is that for each time t and each state i one need only consider the most probable sequence arriving at state i at time t. The probability of the most probable sequence is maxi<,<N[OT(i)] while the sequence itself can be reconstructed by defining v(T) = max<N OT(i) and v(t - 1) = Ike(qt) for T > t > 2. The Baum-Welch algorithm (equations 1-5) and the Viterbi algorithm (equation 6) involve operations on products of numbers constrained to be between 0 and 1. Since these products can easily underflow, measures must be taken to Now define &t(i) = cicit(i) and use a' in place of a in equation 1 to define & for the next iteration: Note that Ein_i eet(i) = 1 for 1 < t < T. Similarly, let OT(i) = OT(i) and define $t(i) = ct4t(i) for T > t > 1 where The scaled backward and forward probabilities, & and 0-, can be exchanged for the unscaled probabilities in equations 3-5 without affecting the value of the ratios. To see this, note that &t(i) = Cat(i) and 0t(i) where Now, in terms of the scaled probabilities, equation 5, for example, can be seen to be unchanged: A slight difficulty occurs in equation 3 that can be cured by the addition of a new term, ct+i, in each product of the upper sum: Numerical instability in the Viterbi algorithm can be ameliorated by operating on a logarithmic scale [Levinson et al., 1983]. That is, one maximizes the log probability of each sequence of state transitions, Care must be taken with zero probabilities. However, this can be elegantly handled through the use of IEEE negative infinity [P754, 1981]. As can be seen from equations 1-5, the time cost of training is 0(TN2). Similarly, as given in equation 6, the Viterbi algorithm is also 0(TN2). However, in part-of-speech tagging, the problem structure dictates that the matrix of symbol probabilities B is sparsely populated. That is, 0 if the ambiguity class corresponding to symbol j includes the part-of-speech tag associated with state i. In practice, the degree of overlap between ambiguity classes is relatively low; some tokens are assigned unique tags, and hence have only one non-zero symbol probability. The sparseness of B leads one to consider restructuring equations 1-6 so a check for zero symbol probability can obviate the need for further computation. Equation 1 is already conveniently factored so that the dependence on bj(St+i ) is outside the inner sum. Hence, if k is the average number of non-zero entries in each row of B, the cost of computing equation 1 can be reduced to 0(kTN). Equations 2-4 can be similarly reduced by switching the order of iteration. For example, in equation 2, rather than for a given t computing ot(i) for each i one at a time, one can accumulate terms for all i in parallel. The net effect of this rewriting is to place a b3(St+i) = 0 check outside the innermost iteration. Equations 3 and 4 submit to a similar approach. Equation 5 is already only 0(N). Hence, the overall cost of training can be reduced to 0(kTN), which, in our experience, amounts to an order of magnitude speedup. 4 The time complexity of the Viterbi algorithm can also be reduced to 0(kTN) by noting that b3 (S) can be factored out of the maximization of equation 6. Adding up the sizes of the probability matrices A, B, and H, it is easy to see that the storage cost for directly representing one model is proportional to N(N M 1). Running the Baum-Welch algorithm requires storage for the sequence of observations, the a and 3 probabilities, the vector {c}, and copies of the A and B matrices (since the originals cannot be overwritten until the end of each iteration). Hence, the grand total of space required for training is proportional to T +2N(T N M + 1). Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T. Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a]. The training set can be considered one long sequence, it which case T is very large indeed, or it can be broken up into a number of smaller sequences at convenient boundaries. In first-order hidden Markov modeling, the stochastic process effectively restarts at unambiguous tokens, such as sentence and paragraph markers, hence these tokens are convenient points at which to break the training set. If the Baum-Welch algorithm is run separately (from the same starting point) on each piece, the resulting trained models must be recombined in some way. One obvious approach is simply to average. However, this fails if any two 'An equivalent approach maintains a mapping from states i to non-zero symbol probabilities and simply avoids, in the inner iteration, computing products which must be zero [Kupiec, 1992]. states are indistinguishable (in the sense that they had the same transition probabilities and the same symbol probabilities at start), because states are then not matched across trained models. It is therefore important that each state have a distinguished role, which is relatively easy to achieve in part-of-speech tagging. Our implementation of the Baum-Welch algorithm breaks up the input into fixed-sized pieces of training text. The Baum-Welch algorithm is then run separately on each piece and the results are averaged together. Running the Viterbi algorithm requires storage for the sequence of observations, a vector of current maxes, a scratch array of the same size, and a matrix of i,b indices, for a total proportional to T + N(2 +T) and a grand total (including the model) of T N(N + M +T +3). Again, N and M are fixed. However, T need not be longer than a single sentence, since, as was observed above, the HMM, and hence the Viterbi algorithm, restarts at sentence boundaries. An HMM for part-of-speech tagging can be tuned in a variety of ways. First, the choice of tagset and lexicon determines the initial model. Second, empirical and a priori information can influence the choice of starting values for the Baum-Welch algorithm. For example, counting instances of ambiguity classes in running text allows one to assign non-uniform starting probabilities in A for a particular tag's realization as a particular ambiguity class. Alternatively, one can state a priori that a particular ambiguity class is most likely to be the reflection of some subset of its component tags. For example, if an ambiguity class consisting of the open class tags is used for unknown words, one may encode the fact that most unknown words are nouns or proper nouns by biasing the initial probabilities in B. Another biasing of starting values can arises from noting that some tags are unlikely to be followed by others. For example, the lexical item &quot;to&quot; maps to an ambiguity class containing two tags, infinitive-marker and to-aspreposition, neither of which occurs in any other ambiguity class. If nothing more were stated, the HMM would have two states which were indistinguishable. This can be remedied by setting the initial transition probabilities from infinitive-marker to strongly favor transitions to such states as verb-uninflected and adverb. Our implementation allows for two sorts of biasing of starting values: ambiguity classes can be annotated with favored tags; and states can be annotated with favored transitions. These biases may be specified either as sets or as set complements. Biases are implemented by replacing the disfavored probabilities with a small constant (machine epsilon) and redistributing mass to the other possibilities. This has the effect of disfavoring the indicated outcomes without disallowing them; sufficient converse data can rehabilitate these values. In support of this and other work, we have developed a system architecture for text access [Cutting et al., 1991]. This architecture defines five components for such systems: corpus, which provides text in a generic manner; analysis, which extracts terms from the text; index which stores term occurrence statistics; and search, which utilizes these statistics to resolve queries. The part-of-speech tagger described here is implemented as an analysis module. Figure 1 illustrates the overall architecture, showing the tagger analysis implementation in detail. The tagger itself has a modular architecture, isolating behind standard protocols those elements which may vary, enabling easy substitution of alternate implementations. Also illustrated here are the data types which flow between tagger components. As an analysis implementation, the tagger must generate terms from text. In this context, a term is a word stem annotated with part of speech. Text enters the analysis sub-system where the first processing module it encounters is the tokenizer, whose duty is to convert text (a sequence of characters) into a sequence of tokens. Sentence boundaries are also identified by the tokenizer and are passed as reserved tokens. The tokenizer subsequently passes tokens to the lexicon. Here tokens are converted into a set of stems, each annotated with a part-of-speech tag. The set of tags identifies an ambiguity class. The identification of these classes is also the responsibility of the lexicon. Thus the lexicon delivers a set of stems paired with tags, and an ambiguity class. The training module takes long sequences of ambiguity classes as input. It uses the Baum-Welch algorithm to produce a trained HMM, an input to the tagging module. Training is typically performed on a sample of the corpus at hand, with the trained HMM being saved for subsequent use on the corpus at large. The tagging module buffers sequences of ambiguity classes between sentence boundaries. These sequences are disambiguated by computing the maximal path through the HMM with the Viterbi algorithm. Operating at sentence granularity provides fast throughput without loss of accuracy, as sentence boundaries are unambiguous. The resulting sequence of tags is used to select the appropriate stems. Pairs of stems and tags are subsequently emitted. The tagger may function as a complete analysis component, providing tagged text to search and indexing components, or as a sub-system of a more elaborate analysis, such as phrase recognition. The problem of tokenization has been well addressed by much work in compilation of programming languages. The accepted approach is to specify token classes with regular expressions. These may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens [Aho et al., 1986, Lesk, 19751. In the context of tagging, we require at least two token classes: sentence boundary and word. Other classes may include numbers, paragraph boundaries and various sorts of punctuation (e.g., braces of various types, commas). However, for simplicity, we will henceforth assume only words and sentence boundaries are extracted. Just as with programming languages, with text it is not always possible to unambiguously specify the required token classes with regular expressions. However the addition of a simple lookahead mechanism which allows specification of right context ameliorates this [Aho et al., 1986, Lesk, 1975]. For example, a sentence boundary in English text might be identified by a period, followed bywhitespace, followed by an uppercase letter. However the uppercase letter must not be consumed, as it is the first component of the next token. A lookahead mechanism allows us to specify in the sentence-boundary regular expression that the final character matched should not be considered a part of the token. This method meets our stated goals for the overall system. It is efficient, requiring that each character be examined only once (modulo lookahead). It is easily parameterizable, providing the expressive power to concisely define accurate and robust token classes. The lexicon module is responsible for enumerating parts of speech and their associated stems for each word it is given. For the English word &quot;does,&quot; the lexicon might return &quot;do, verb&quot; and &quot;doe, plural-noun.&quot; It is also responsible for identifying ambiguity classes based upon sets of tags. We have employed a three-stage implementation: First, we consult a manually-constructed lexicon to find stems and parts of speech. Exhaustive lexicons of this sort are expensive, if not impossible, to produce. Fortunately, a small set of words accounts for the vast majority of word occurences. Thus high coverage can be obtained without prohibitive effort. Words not found in the manually constructed lexicon are generally both open class and regularly inflected. As a second stage, a language-specific method can be employed to guess ambiguity classes for unknown words. For many languages (e.g., English and French), word suffixes provide strong cues to words' possible categories. Probabalistic predictions of a word's category can be made by analyzing suffixes in untagged text [Kupiec, 1992, Meteer et al., 1991]. As a final stage, if a word is not in the manually constructed lexicon, and its suffix is not recognized, a default ambiguity class is used. This class typically contains all the open class categories in the language. Dictionaries and suffix tables are both efficiently implementable as letter trees, or tries [Knuth, 1973], which require that each character of a word be examined only once during a lookup. In this section, we detail how our tagger meets the desiderata that we outlined in section 1. The system is implemented in Common Lisp [Steele, 1990]. All timings reported are for a Sun SPARCStation2. The English lexicon used contains 38 tags (M = 38) and 174 ambiguity classes (N = 174). Training was performed on 25,000 words in articles selected randomly from Grolier's Encyclopedia. Five iterations of training were performed in a total time of 115 CPU seconds. Following is a time breakdown by component: Training: average pseconds per token tokenizer lexicon 1 iteration 5 iterations total 640 400 680 3400 4600 Tagging was performed on 115,822 words in a collection of articles by the journalist Dave Barry. This required a total of of 143 CPU seconds. The time breakdown for this was as follows: Tagging: average pseconds per token tokenizer lexicon Viterbi total 604 388 233 1235 It can be seen from these figures that training on a new corpus may be accomplished in a matter of minutes, and that tens of megabytes of text may then be tagged per hour. When using a lexicon and tagset built from the tagged text of the Brown corpus [Francis and KuEera, 1982], training on one half of the corpus (about 500,000 words) and tagging the other, 96% of word instances were assigned the correct tag. Eight iterations of training were used. This level of accuracy is comparable to the best achieved by other taggers [Church, 1988, Merialdo, 1991]. The Brown Corpus contains fragments and ungrammaticalities, thus providing a good demonstration of robustness. A tagger should be tunable, so that systematic tagging errors and anomalies can be addressed. Similarly, it is important that it be fast and easy to target the tagger to new genres and languages, and to experiment with different tagsets reflecting different insights into the linguistic phenomena found in text. In section 3.5, we describe how the HMM implementation itself supports tuning. In addition, our implementation supports a number of explicit parameters to facilitate tuning and reuse, including specification of lexicon and training corpus. There is also support for a flexible tagset. For example, if we want to collapse distinctions in the lexicon, such as those between positive, comparative, and superlative adjectives, we only have to make a small change in the mapping from lexicon to tagset. Similarly, if we wish to make finer grain distinctions than those available in the lexicon, such as case marking on pronouns, there is a simple way to note such exceptions. We have used the tagger in a number of applications. We describe three applications here: phrase recognition; word sense disambiguation; and grammatical function assignment. These projects are part of a research effort to use shallow analysis techniques to extract content from unrestricted text. We have constructed a system that recognizes simple phrases when given as input the sequence of tags for a sentence. There are recognizers for noun phrases, verb groups adverbial phrases, and prepositional phrases. Each of these phrases comprises a contiguous sequence of tags that sat is. fies a simple grammar. For example, a noun phrase can be a unary sequence containing a pronoun tag or an arbitrar. ily long sequence of noun and adjective tags, possibly pre. ceded by a determiner tag and possibly with an embeddec possessive marker. The longest possible sequence is founc (e.g., &quot;the program committee&quot; but not &quot;the program') Conjunctions are not recognized as part of any phrase; for example, in the fragment &quot;the cats and dogs,&quot; &quot;the cats&quot; and &quot;dogs&quot; will be recognized as two noun phrases. Prepositional phrase attachment is not performed at this stage of processing. This approach to phrase recognition in some cases captures only parts of some phrases; however, our approach minimizes false positives, so that we can rely on the recognizers' results. Part-of-speech tagging in and of itself is a useful tool in lexical disambiguation; for example, knowing that &quot;dig&quot; is being used as a noun rather than as a verb indicates the word's appropriate meaning. But many words have multiple meanings even while occupying the same part of speech. To this end, the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm [Hearst, 1991]. The algorithm (known as CatchWord) performs supervised training over a large text corpus, gathering lexical, orthographic, and simple syntactic evidence for each sense of the ambiguous noun. After a period of training, Catch Word classifies new instances of the noun by checking its context against that of previously observed instances and choosing the sense for which the most evidence is found. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. Initial tests resulted in accuracies of around 90% for nouns with strongly distinct senses. This algorithm uses the tagger in two ways: (i) to determine the part of speech of the target word (filtering out the non-noun usages) and (ii) as a step in the phrase recognition analysis of the context surrounding the noun. The phrase recognizers also provide input to a system, Sopa [Sibun, 1991], which recognizes nominal arguments of verbs, specifically, Subject, Object, and Predicative Arguments. Sopa does not rely on information (such as arity or voice) specific to the particular verbs involved. The first step in assigning grammatical functions is to partition the tag sequence of each sentence into phrases. The phrase types include those mentioned in section 6.1, additional types to account for conjunctions, complementizers, and indicators of sentence boundaries, and an &quot;unknown&quot; type. After a sentence has been partitioned, each simple noun phrase is examined in the context of the phrase to its left and the phrase to its right. On the basis of this local context and a set of rules, the noun phrase is marked as a syntactic Subject, Object, Predicative, or is not marked at all. A label of Predicative is assigned only if it can be determined that the governing verb group is a form of a predicating verb (e.g., a form of &quot;be&quot;). Because this cannot always be determined, some Predicatives are labeled Objects. If a noun phrase is labeled, it is also annotated as to whether the governing verb is the closest verb group to the right or to the left. The algorithm has an accuracy of approximately 80% in assigning grammatical functions.","We have used the tagger in a number of applications. We describe three applications here: phrase recognition; word sense disambiguation; and grammatical function assignment. These projects are part of a research effort to use shallow analysis techniques to extract content from unrestricted text. We have constructed a system that recognizes simple phrases when given as input the sequence of tags for a sentence. There are recognizers for noun phrases, verb groups adverbial phrases, and prepositional phrases. Each of these phrases comprises a contiguous sequence of tags that sat is. fies a simple grammar. For example, a noun phrase can be a unary sequence containing a pronoun tag or an arbitrar. ily long sequence of noun and adjective tags, possibly pre. ceded by a determiner tag and possibly with an embeddec possessive marker. The longest possible sequence is founc (e.g., &quot;the program committee&quot; but not &quot;the program') Conjunctions are not recognized as part of any phrase; for example, in the fragment &quot;the cats and dogs,&quot; &quot;the cats&quot; and &quot;dogs&quot; will be recognized as two noun phrases. Prepositional phrase attachment is not performed at this stage of processing. This approach to phrase recognition in some cases captures only parts of some phrases; however, our approach minimizes false positives, so that we can rely on the recognizers' results. Part-of-speech tagging in and of itself is a useful tool in lexical disambiguation; for example, knowing that &quot;dig&quot; is being used as a noun rather than as a verb indicates the word's appropriate meaning. But many words have multiple meanings even while occupying the same part of speech. To this end, the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm [Hearst, 1991]. The algorithm (known as CatchWord) performs supervised training over a large text corpus, gathering lexical, orthographic, and simple syntactic evidence for each sense of the ambiguous noun. After a period of training, Catch Word classifies new instances of the noun by checking its context against that of previously observed instances and choosing the sense for which the most evidence is found. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. Initial tests resulted in accuracies of around 90% for nouns with strongly distinct senses. This algorithm uses the tagger in two ways: (i) to determine the part of speech of the target word (filtering out the non-noun usages) and (ii) as a step in the phrase recognition analysis of the context surrounding the noun. The phrase recognizers also provide input to a system, Sopa [Sibun, 1991], which recognizes nominal arguments of verbs, specifically, Subject, Object, and Predicative Arguments. Sopa does not rely on information (such as arity or voice) specific to the particular verbs involved. The first step in assigning grammatical functions is to partition the tag sequence of each sentence into phrases. The phrase types include those mentioned in section 6.1, additional types to account for conjunctions, complementizers, and indicators of sentence boundaries, and an &quot;unknown&quot; type. After a sentence has been partitioned, each simple noun phrase is examined in the context of the phrase to its left and the phrase to its right. On the basis of this local context and a set of rules, the noun phrase is marked as a syntactic Subject, Object, Predicative, or is not marked at all. A label of Predicative is assigned only if it can be determined that the governing verb group is a form of a predicating verb (e.g., a form of &quot;be&quot;). Because this cannot always be determined, some Predicatives are labeled Objects. If a noun phrase is labeled, it is also annotated as to whether the governing verb is the closest verb group to the right or to the left. The algorithm has an accuracy of approximately 80% in assigning grammatical functions."
50,"We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer.","We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer. There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals. Certainly one factor contributing to this gap is that automatic systems can not always correctly identify the important topics of an article. Another factor, however, which has received little attention, is that automatic summarizers have poor text generation techniques. Most automatic summarizers rely on extracting key sentences or paragraphs from an article to produce a summary. Since the extracted sentences are disconnected in the original article, when they are strung together, the resulting summary can be inconcise, incoherent, and sometimes even misleading. We present a cut and paste based text summarization technique, aimed at reducing the gap between automatically generated summaries and human-written abstracts. Rather than focusing on how to identify key sentences, as do other researchers, we study how to generate the text of a summary once key sentences have been extracted. The main idea of cut and paste summarization is to reuse the text in an article to generate the summary. However, instead of simply extracting sentences as current summarizers do, the cut and paste system will &quot;smooth&quot; the extracted sentences by editing them. Such edits mainly involve cutting phrases and pasting them together in novel ways. The key features of this work are: ing operations. We identified six operations that can be used alone or together to transform extracted sentences into sentences in human-written abstracts. The operations were identified based on manual and automatic comparison of human-written abstracts and the original articles. Examples include sentence reduction, sentence combination, syntactic transformation, and lexical paraphrasing. (2) Development of an automatic system to perform cut and paste operations. Two operations - sentence reduction and sentence combination - are most effective in transforming extracted sentences into summary sentences that are as concise and coherent as in human-written abstracts. We implemented a sentence reduction module that removes extraneous phrases from extracted sentences, and a sentence combination module that merges the extracted sentences or the reduced forms resulting from sentence reduction. Our sentence reduction model determines what to cut based on multiple sources of information, including syntactic knowledge, context, and statistics learned from corpus analysis. It improves the conciseness of extracted sentences, making them concise and on target. Our sentence combination module implements combination rules that were identified by observing examples written by human professionals. It improves the coherence of extracted sentences. sentences. The cut and paste technique we propose here is a new computational model which we based on analysis of human-written abstracts. To do this analysis, we developed an automatic system that can match a phrase in a human-written abstract to the corresponding phrase in the article, identifying its most likely location. This decomposition program allows us to analyze the construction of sentences in a human-written abstract. Its results have been used to train and test the sentence reduction and sentence combination module. In Section 2, we discuss the cut and paste technique in general, from both a professional and computational perspective. We also describe the six cut and paste operations. In Section 3, we describe the system architecture. The major components of the system, including sentence reduction, sentence combination, decomposition, and sentence selection, are described in Section 4. The evaluation results are shown in Section 5. Related work is discussed in Section 6. Finally, we conclude and discuss future work. Professionals take two opposite positions on whether a summary should be produced by cutting and pasting the original text. One school of scholars is opposed; &quot;(use) your own words... Do not keep too close to the words before you&quot;, states an early book on abstracting for American high school students (Thurber, 1924). Another study, however, shows that professional abstractors actually rely on cutting and pasting to produce summaries: &quot;Their professional role tells abstractors to avoid inventing anything. They follow the author as closely as possible and reintegrate the most important points of a document in a shorter text&quot; (Endres-Niggemeyer et al., 1998). Some studies are somewhere in between: &quot;summary language may or may not follow that of author's&quot; (Fidel, 1986). Other guidelines or books on abstracting (ANSI, 1997; Cremmins, 1982) do not discuss the issue. Our cut and paste based summarization is a computational model; we make no claim that humans use the same cut and paste operations. We manually analyzed 30 articles and their corresponding human-written summaries; the articles and their summaries come from different domains ( 15 general news reports, 5 from the medical domain, 10 from the legal domain) and the summaries were written by professionals from different organizations. We found that reusing article text for summarization is almost universal in the corpus we studied. We defined six operations that can be used alone, sequentially, or simultaneously to transform selected sentences from an article into the corresponding summary sentences in its human-written abstract: Remove extraneous phrases from a selected sentence, as in the following example 1: 'All the examples in this section were produced by human professionals Document sentence: When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. Summary sentence: The V-chip will give parents a device to block out programs they don't want their children to see. The deleted material can be at any granularity: a word, a phrase, or a clause. Multiple components can be removed. Merge material from several sentences. It can be used together with sentence reduction, as illustrated in the following example, which also uses paraphrasing: Text Sentence 1: But it also raises serious questions about the privacy of such highly personal information wafting about the digital world. Text Sentence 2: The issue thus fits squarely into the broader debate about privacy and security on the internet, whether it involves protecting credit card number or keeping children from offensive information. Summary sentence: But it also raises the issue of privacy of such personal information and this issue hits the head on the nail in the broader debate about privacy and security on the internet. In both sentence reduction and combination, syntactic transformations may be involved. For example, the position of the subject in a sentence may be moved from the end to the front. Replace phrases with their paraphrases. For instance, the summaries substituted point out with note, and fits squarely into with a more picturesque description hits the head on the nail in the previous examples. (5) generalization or specification Replace phrases or clauses with more general or specific descriptions. Examples of generalization and specification include: Generalization: &quot;a proposed new law that would require Web publishers to obtain parental consent before collecting personal information from children&quot; &quot;legislation to protect children's privacy on-line&quot; Specification: &quot;the White House's top drug official&quot; -4 &quot;Gen. Barry R. McCaffrey, the White House's top drug official&quot; Change the order of extracted sentences. For instance, place an ending sentence in an article at the beginning of an abstract. In human-written abstracts, there are, of course, sentences that are not based on cut and paste, but completely written from scratch. We used our decomposition program to automatically analyze 300 human-written abstracts, and found that 19% of sentences in the abstracts were written from scratch. There are also other cut and paste operations not listed here due to their infrequent occurrence. The architecture of our cut and paste based text summarization system is shown in Figure 1. Input to the system is a single document from any domain. In the first stage, extraction, key sentences in the article are identified, as in most current summarizers. In the second stage, cut and paste based generation, a sentence reduction module and a sentence combination module implement the operations we observed in human-written abstracts. The cut and paste based component receives as input not only the extracted key sentences, but also the original article. This component can be ported to other single-document summarizers to serve as the generation component, since most current summarizers extract key sentences - exactly what the extraction module in our system does. Other resources and tools in the summarization system include a corpus of articles and their humanwritten abstracts, the automatic decomposition program, a syntactic parser, a co-reference resolution system, the WordNet lexical database, and a largescale lexicon we combined from multiple resources. The components in dotted lines are existing tools or resources; all the others were developed by ourselves. The main focus of our work is on decomposition of summaries, sentence reduction, and sentence combination. We also describe the sentence extraction module, although it is not the main focus of our work. The decomposition program, see (Jing and McKeown, 1999) for details, is used to analyze the construction of sentences in human-written abstracts. The results from decomposition are used to build the training and testing corpora for sentence reduction and sentence combination. The decomposition program answers three questions about a sentence in a human-written abstract: (1) Is the sentence constructed by cutting and pasting phrases from the input article? (2) If so, what phrases in the sentence come from the original article? (3) Where in the article do these phrases come from? We used a Hidden Markov Model (Baum, 1972) solution to the decomposition problem. We first mathematically formulated the problem, reducing it to a problem of finding, for each word in a summary sentence, a document position that it most likely comes from. The position of a word in a document is uniquely identified by the position of the sentence where the word appears, and the position of the word within the sentence. Based on the observation of cut and paste practice by humans, we produced a set of general heuristic rules. Sample heuristic rules include: two adjacent words in a summary sentence are most likely to come from two adjacent words in the original document; adjacent words in a summary sentence are not very likely to come from sentences that are far apart in the original document. We use these heuristic rules to create a Hidden Markov Model. The Viterbi algorithm (Viterbi, 1967) is used to efficiently find the most likely document position for each word in the summary sentence. Figure 2 shows sample output of the program. For the given summary sentence, the program correctly identified that the sentence was combined from four sentences in the input article. It also divided the summary sentence into phrases and pinpointed the exact document origin of each phrase. A phrase in the summary sentence is annotated as (FNUM:SNUM actual-text), where FNUM is the sequential number of the phrase and SNUM is the number of the document sentence where the phrase comes from. SNUM = -1 means that the component does not come from the original document. The phrases in the document sentences are annotated as (FNUM a c tu a 1- text). The task of the sentence reduction module, described in detail in (Jing, 2000), is to remove extraneous phrases from extracted sentences. The goal of reduction is to &quot;reduce without major loss&quot;; that is, we want to remove as many extraneous phrases as possible from an extracted sentence so that it can be concise, but without detracting from the main idea that the sentence conveys. Ideally, we want to remove a phrase from an extracted sentence only if it is irrelavant to the main topic. Our reduction module makes decisions based on multiple sources of knowledge: Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. by adding up the scores of its children nodes in the parse tree. This score indicates how important the phrase is to the main topic in discussion. The phrases we remove from an extracted sentence include clauses, prepositional phrases, gerunds, and to-infinitives. The result of sentence reduction is a shortened version of an extracted sentence 2. This shortened text can be used directly as a summary, or it can be fed to the sentence combination module to be merged with other sentences. Figure 3 shows two examples produced by the reduction program. The corresponding sentences in human-written abstracts are also provided for comparison. 21t is actually also possible that the reduction program decides no phrase in a sentence should be removed, thus the result of reduction is the same as the input. To build the combination module, we first manually analyzed a corpus of combination examples produced by human professionals, automatically created by the decomposition program, and identified a list of combination operations. Table 1 shows the combination operations. To implement a combination operation, we need to do two things: decide when to use which combination operation, and implement the combining actions. To decide when to use which operation, we analyzed examples by humans and manually wrote a set of rules. Two simple rules are shown in Figure 4. Sample outputs using these two simple rules are shown in Figure 5. We are currently exploring using machine learning techniques to learn the combination rules from our corpus. The implementation of the combining actions involves joining two parse trees, substituting a subtree with another, or adding additional nodes. We implemented these actions using a formalism based on Tree Adjoining Grammar (Joshi, 1987). The extraction module is the front end of the summarization system and its role is to extract key sentences. Our method is primarily based on lexical relations. First, we link words in a sentence with other words in the article through repetitions, morphological relations, or one of the lexical relations encoded in WordNet, similar to step 2 in sentence reduction. An importance score is computed for each word in a sentence based on the number of lexical links it has with other words, the type of links, and the directions of the links. After assigning a score to each word in a sentence, we then compute a score for a sentence by adding up the scores for each word. This score is then normalIF: ((a person or an organization is mentioned the first time) and (the full name or the full description of the person or the organization exists somewhere in the original article but is missing in the summary)) THEN: replace the phrase with the full name plus the full description IF: ((two sentences are close to each other in the original article) and (their subjects refer to the same entity) and (at least one of the sentences is the reduced form resulting from sentence reduction)) THEN: merge the two sentences by removing the subject in the second sentence, and then combining it with the first sentence using connective &quot;and&quot;. ized over the number of words a sentence contains. The sentences with high scores are considered important. The extraction system selects sentences based on the importance computed as above, as well as other indicators, including sentence positions, cue phrases, and tf*idf scores. Our evaluation includes separate evaluations of each module and the final evaluations of the overall system. We evaluated the decomposition program by two experiments, described in (Jing and McKeown, 1999). In the first experiment, we selected 50 human-written abstracts, consisting of 305 sentences in total. A human subject then read the decomposition results of these sentences to judge whether they are correct. 93.8% of the sentences were correctly decomposed. In the second experiment, we tested the system in a summary alignment task. We ran the decomposition program to identify the source document sentences that were used to construct the sentences in human-written abstracts. Human subjects were also asked to select the document sentences that are semantic-equivalent to the sentences in the abstracts. We compared the set of sentences identified by the program with the set of sentences selected by the majority of human subjects, which is used as the gold standard in the computation of precision and recall. The program achieved an average 81.5% precision, 78.5% recall, and 79.1% f-measure for 10 documents. The average performance of 14 human judges is 88.8% precision, 84.4% recall, and 85.7% f-measure. Recently, we have also tested the system on legal documents (the headnotes used by Westlaw company), and the program works well on those documents too. The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts. 400 sentences were used to compute corpus probabilities and 100 sentences were used for testing. The results show that 81.3% of the reduction decisions made by the system agreed with those of humans. The humans reduced the length of the 500 sentences by 44.2% on average, and the system reduced the length of the 100 test sentences by 32.7%. The evaluation of sentence combination module is not as straightforward as that of decomposition or reduction since combination happens later in the pipeline and it depends on the output from prior The new measure is an echo of the original bad idea, blurred just enough to cloud prospects both for enforcement and for court review. Unlike the 1996 act, this one applies only to commercial Web sites - thus sidestepping conversation deemed &quot;indecent&quot; by somebody somewhere. The new version also replaces the vague &quot;indecency&quot; standard, to which the court objected, with the better-defined one of material ruled &quot;harmful to minors.&quot; Combined sentences: The new measure is an echo of the original bad idea. The new version applies only to commercial web sites and replaces the vague &quot;indecency&quot; standard with the better-defined one of material ruled &quot;harmful to minors.&quot; modules. To evaluate just the combination component, we assume that the system makes the same reduction decision as humans and the co-reference system has a perfect performance. This involves manual tagging of some examples to prepare for the evaluation; this preparation is in progress. The evaluation of sentence combination will focus on the accessment of combination rules. The overall system evaluation includes both intrinsic and extrinsic evaluation. In the intrinsic evaluation, we asked human subjects to compare the quality of extraction-based summaries and their revised versions produced by our sentence reduction and combination modules. We selected 20 documents; three different automatic summarizers were used to generate a summary for each document, producing 60 summaries in total. These summaries are all extraction-based. We then ran our sentence reduction and sentence combination system to revise the summaries, producing a revised version for each summary. We presented human subjects with the full documents, the extraction-based summaries, and their revised versions, and asked them to compare the extraction-based summaries and their revised versions. The human subjects were asked to score the conciseness of the summaries (extractionbased or revised) based on a scale from 0 to 10 the higher the score, the more concise a summary is. They were also asked to score the coherence of the summaries based on a scale from 0 to 10. On average, the extraction-based summaries have a score of 4.2 for conciseness, while the revised summaries have a score of 7.9 (an improvement of 88%). The average improvement for the three systems are 78%, 105%, and 88% respectively. The revised summaries are on average 41% shorter than the original extractionbased summaries. For summary coherence, the average score for the extraction-based summaries is 3.9, while the average score for the revised summaries is 6.1 (an improvement of 56%). The average improvement for the three systems are 69%, 57%, and 53% respectively. We are preparing a task-based evaluation, in which we will use the data from the Summarization Evaluation Conference (Mani et al., 1998) and compare how our revised summaries can influence humans' performance in tasks like text categorization and ad-hoc retrieval. (Mani et al., 1999) addressed the problem of revising summaries to improve their quality. They suggested three types of operations: elimination, aggregation, and smoothing. The goal of the elimination operation is similar to that of the sentence reduction operation in our system. The difference is that while elimination always removes parentheticals, sentenceinitial PPs and certain adverbial phrases for every extracted sentence, our sentence reduction module aims to make reduction decisions according to each case and removes a sentence component only if it considers it appropriate to do so. The goal of the aggregation operation and the smoothing operation is similar to that of the sentence combination operation in our system. However, the combination operations and combination rules that we derived from corpus analysis are significantly different from those used in the above system, which mostly came from operations in traditional natural language generation. This paper presents a novel architecture for text summarization using cut and paste techniques observed in human-written abstracts. In order to automatically analyze a large quantity of human-written abstracts, we developed a decomposition program. The automatic decomposition allows us to build large corpora for studying sentence reduction and sentence combination, which are two effective operations in cut and paste. We developed a sentence reduction module that makes reduction decisions using multiple sources of knowledge. We also investigated possible sentence combination operations and implemented the combination module. A sentence extraction module was developed and used as the front end of the summarization system. We are preparing the task-based evaluation of the overall system. We also plan to evaluate the portability of the system by testing it on another corpus. We will also extend the system to query-based summarization and investigate whether the system can be modified for multiple document summarization.","This paper presents a novel architecture for text summarization using cut and paste techniques observed in human-written abstracts. In order to automatically analyze a large quantity of human-written abstracts, we developed a decomposition program. The automatic decomposition allows us to build large corpora for studying sentence reduction and sentence combination, which are two effective operations in cut and paste. We developed a sentence reduction module that makes reduction decisions using multiple sources of knowledge. We also investigated possible sentence combination operations and implemented the combination module. A sentence extraction module was developed and used as the front end of the summarization system. We are preparing the task-based evaluation of the overall system. We also plan to evaluate the portability of the system by testing it on another corpus. We will also extend the system to query-based summarization and investigate whether the system can be modified for multiple document summarization."
51,"We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and !as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.","We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and !as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). On first glance, it may appear that using a short list, of sentence-final punctuation marks, such as ., ?, and !, is sufficient. However, these punctuation marks are not used exclusively to mark sentence breaks. For example, embedded quotations may contain any of the sentence-ending punctuation marks and . is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations. Both ! and ? are somewhat less ambiguous *The authors would like to acknowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH0494-G-0426 and NSF grant SBR89-20230. but appear in proper names and may be used multiple times for emphasis to mark a single sentence boundary. Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above. However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties. Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995)). For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D.0 is followed by only a single . in The president lives in Washington, D.C.). As a result, we believe that manually writing rules is not a good approach. Instead, we present a solution based on a maximum entropy model which requires a few hints about what. information to use and a corpus annotated with sentence boundaries. The model trains easily and performs comparably to systems that require vastly more information. Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds. To our knowledge, there have been few papers about identifying sentence boundaries. The most recent work will be described in (Palmer and Hearst, To appear). There is also a less detailed description of Palmer and Hearst's system, SATZ, in (Palmer and Hearst, 1994).' The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries. The neural network achieves 98.5% accuracy on a corpus of Wall Street Journal We recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here. articles using a lexicon which includes part-of-speech (POS) tag information. By increasing the quantity ol training data and decreasing the size of their test corpus, Palmer and Hearst achieved performance of ! )8.9% with the neural network. They obtained similar results using the decision tree. All the results we will present for our algorithms are on their initial, larger test corpus. In (Riley, 1989), Riley describes a decision-tree based approach to the problem. His performance on I he Brown corpus is 99.8%, using a model learned from a. corpus of 25 million words. Liberman and Church suggest in (Liberma.n and Church, 1992) that a. system could be quickly built to divide newswire text into sentences with a nearly negligible error rate, but do not actually build such a system. We present two systems for identifying sentence boundaries. One is targeted at high performance and uses some knowledge about the structure of English financial newspaper text which may not be applicable to text from other genres or in other languages. The other system uses no domain-specific knowledge and is aimed at being portable across English text genres and Roman alphabet languages. Potential sentence boundaries are identified by scanning the text for sequences of characters separated by whitespace (tokens) containing one of the symbols !, . or ?. We use information about the token containing the potential sentence boundary, as well as contextual information about the tokens immediately to the left and to the right. We also conducted tests using wider contexts, but performance did not improve. We call the token containing the symbol which marks a putative sentence boundary the Candidate. 'Hie portion of the Candidate preceding the potential sentence boundary is called the Prefix and the portion following it is called the Suffix. The system that focused on maximizing performance used the following hints, or contextual &quot;templates&quot;: The templates specify only the form of the information. The exact information used by the maximum entropy model for the potential sentence boundary marked by . in Corp. in Example 1 would be: PreviousWordIsCapitalized, Prefix= Corp, Suffix=NULL, PrefixFeature=CorporateDesignator. The highly portable system uses only the identity of the Candidate and its neighboring words, and a list of abbreviations induced from the training data.2 Specifically, the &quot;templates&quot; used are: The information this model would use for Example 1 would be: PreviousWord=ANLP, FollowingWord=chairmon, Prefix=Corp, Suffix=NULL, PrefixFeature=InducedAbbreviation. The abbreviation list is automatically produced from the training data, and the contextual questions are also automatically generated by scanning the training data. with question templates. As a. result, no hand-crafted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres. The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratna.parkhi, 1996). For each potential sentence boundary token (., ?, and ! ), we estimate a. joint, probability distribution p of the token and its surrounding context, both of which are denoted by c, occurring as an actual sentence boundary. The distribution is given by: p(b, c) = Ir „,,,.f-(b„c), where b e no, yes}, where the cri's are the unknown parameters of the model, and where each aj corresponds to a fi, or a feature. Thus the probability of seeing an actual sentence boundary in the context c is given by p(yes, c). The contextual information deemed useful for sentence-boundary detection, which. we described earlier, must be encoded using features. For example, a useful feature might be: This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary. Therefore the parameter corresponding to this feature will hopefully boost the probability p(no, c) if the Prefix is Mr. The parameters are chosen to maximize the likelihood of the training data, using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm. The model also can be viewed under the Maximum Entropy framework, in which we choose a distribution p that maximizes the entropy H (p) where /:5(b, c) is the observed distribution of sentenceboundaries and contexts in the training data. As a result, the model in practice tends not to commit towards a particular outcome (yes or no) unless it has seen sufficient evidence for that outcome; it is maximally uncertain beyond meeting the evidence. All experiments use a simple decision rule to classify each potential sentence boundary: a potential sentence boundary is an actual sentence boundary if and only if p(yesic) > .5, where and where c is the context including the potential sentence boundary. We trained our system on 39441 sentences (898737 words) of Wall Street Journal text from sections 00 through 24 of the second release of the Penn Treebank3 (Marcus, Santorini, and Marcinkiewicz, 1993). We corrected punctuation mistakes and erroneous sentence boundaries in the training data. Performance figures for our best performing system, which used a hand-crafted list of honorifics and corporate designators, are shown in Table 1. The first test set, WSJ, is Palmer and Hearst's initial test data and the second is the entire Brown corpus. We present the Brown corpus performance to show the importance of training on the genre of text. on which testing will be performed. Table 1 also shows the number of sentences in each corpus, the number of candidate punctuation marks, the accuracy over potential sentence boundaries, the number of false positives and the number of false negatives. Performance on the WSJ corpus was, as we expected, higher than performance on the Brown corpus since we trained the model on financial newspaper text. Possibly more significant. than the system's performance is its portability to new domains and languages. A trimmed down system which used no information except that derived from the training corpus performs nearly as well, and requires no resources other than a training corpus. Its performance on the same two corpora is shown in Table 2. Since 39441 training sentences is considerably more than might exist in a new domain or a language other than English, we experimented with the quantity of training data required to maintain performance. Table 3 shows performance on the WSJ corpus as a. function of training set size using the best performing system and the more portable system. As can seen from the table, performance degrades a.s the quantity of training data decreases, but even with only 500 example sentences performance is beter than the baselines of 64.00/0 if a. sentence boundary is guessed at every potential site and 78.4%, if only token-final instances of sentence-ending punctuation are assumed to be boundaries. We have described an approach to identifying sentence boundaries which performs comparably to other state-of-the-art systems that require vastly more resources. For example, Riley's performance on the Brown corpus is higher than ours, but his system is trained on the Brown corpus and uses thirty times as much data as our system. Also, Palmer L Hearst's system requires POS tag information, which limits its use to those genres or languages for which there are either POS tag lexica or POS tag annotated corpora that could be used to train automatic taggers. In comparison, our system does not require POS ta.gs or any supporting resources beyond the sentence-boundary annotated corpus. It is therefore easy and inexpensive to retrain this system for different genres of text in English and text in other Roman-alphabet languages. Furthermore, we showed tha.t a small training corpus is sufficient for good performance, and we estimate that annotating enough data to achieve good performance would require only several hours of work, in comparison to the many hours required to generate POS tag and lexical probabilities.","We have described an approach to identifying sentence boundaries which performs comparably to other state-of-the-art systems that require vastly more resources. For example, Riley's performance on the Brown corpus is higher than ours, but his system is trained on the Brown corpus and uses thirty times as much data as our system. Also, Palmer L Hearst's system requires POS tag information, which limits its use to those genres or languages for which there are either POS tag lexica or POS tag annotated corpora that could be used to train automatic taggers. In comparison, our system does not require POS ta.gs or any supporting resources beyond the sentence-boundary annotated corpus. It is therefore easy and inexpensive to retrain this system for different genres of text in English and text in other Roman-alphabet languages. Furthermore, we showed tha.t a small training corpus is sufficient for good performance, and we estimate that annotating enough data to achieve good performance would require only several hours of work, in comparison to the many hours required to generate POS tag and lexical probabilities."
52,"This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination. The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms.","This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination. The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms. Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Com binatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentencewhich are then manipulated by the parser. Supertag ging was introduced for LTAG as a way of increasingparsing efficiency by reducing the number of struc tures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003).Supertagging accuracy is relatively high for man ually constructed LTAGs (Bangalore and Joshi,1999). However, for LTAGs extracted automati cally from the Penn Treebank, performance is much lower (Chen et al, 1999; Chen et al, 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al, 2000). In this paper we demonstratethat CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an auto matically extracted grammar, but also offers several practical advantages. Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse. Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number. The supertagger is also crucial for increasing thespeed of the parser. We show that spectacular in creases in speed can be obtained, without affectingaccuracy or coverage, by tightly integrating the su pertagger with the CCG grammar and parser. To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis. We also demonstrate how extra constraints on the category combinations, and the application of beam search using the parsing model, can further increase parsing speed.This is the first work we are aware of to succes fully integrate a supertagger with a full parser which uses a lexicalised grammar automatically extractedfrom the Penn Treebank. We also report signifi cantly higher parsing speeds on newspaper text than any previously reported for a full wide-coverage parser. Our results confirm that wide-coverage CCG parsing is feasible for many large-scale NLP tasks. Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories to gether using CCG?s combinatory rules.1 The first stage can be accomplished by simply assigning to each word all categories from the word?s entry in the lexicon (Hockenmaier, 2003). 1See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. The WSJ is a publication that I enjoy reading NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP Figure 1: Example sentence with CCG lexical categories frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00 cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat 1 1 225 0 0 12 (0.03%) 12 (0.6%) 10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%) Table 1: Statistics for the lexical category setAn alternative is to use a statistical tagging approach to assign one or more categories. A statisti cal model can be used to determine the most likelycategories given the word?s context. The advantage of this supertagging approach is that the number of categories assigned to each word can be re duced, with a correspondingly massive reduction in the number of derivations. Bangalore and Joshi (1999) use a standard Markov model tagger to assign LTAG elementarytrees to words. Here we use the Maximum En tropy models described in Curran and Clark (2003). An advantage of the Maximum Entropy approachis that it is easy to encode a wide range of poten tially useful information as features; for example,Clark (2002) has shown that POS tags provide use ful information for supertagging. The next section describes the set of lexical categories used by our supertagger and parser. 2.1 The Lexical Category Set. The set of lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of CCG normal-form deriva tions derived semi-automatically from the PennTreebank. Following Clark (2002), we apply a fre quency cutoff to the training set, only using thosecategories which appear at least 10 times in sections 2-21. Figure 1 gives an example sentence su pertagged with the correct CCG lexical categories. Table 1 gives the number of different category types and shows the coverage on training (seen) anddevelopment (unseen) data (section 00 from CCGbank). The table also gives statistics for the com plete set containing every lexical category type inCCGbank.2 These figures show that using a fre quency cutoff can significantly reduce the size of the category set with only a small loss in coverage. 2The numbers differ slightly from those reported in Clark (2002) since a newer version of CCGbank is being used here. Clark (2002) compares the size of grammarsextracted from CCGbank with automatically extracted LTAGs. The grammars of Chen and Vijay Shanker (2000) contain between 2,000 and 9,000 tree frames, depending on the parameters used inthe extraction process, significantly more elemen tary structures than the number of lexical categories derived from CCGbank. We hypothesise this is a key factor in the higher accuracy for supertaggingusing a CCG grammar compared with an automati cally extracted LTAG. 2.2 The Tagging Model. The supertagger uses probabilities p(y|x) where y is a lexical category and x is a context. The conditional probabilities have the following log-linear form: p(y|x) = 1 Z(x)e ? i ?i fi(y,x) (1) where fi is a feature, ?i is the corresponding weight, and Z(x) is a normalisation constant. The context is a 5-word window surrounding the target word. Features are defined for each word in the window and for the POS tag of each word. Curran and Clark(2003) describes the model and explains how Gen eralised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. Theper-word accuracy is between 91 and 92% on un seen data in CCGbank; however, Clark (2002) shows this is not high enough for integration into a parser since the large number of incorrect categories results in a significant loss in coverage. Clark (2002) shows how the models in (1) can be used to define a multi-tagger which can assign more than one category to a word. For each word inthe sentence, the multi-tagger assigns all those cat ? CATS/ ACC SENT ACC SENT WORD ACC (POS) ACC 0.1 1.4 97.0 62.6 96.4 57.4 0.075 1.5 97.4 65.9 96.8 60.6 0.05 1.7 97.8 70.2 97.3 64.4 0.01 2.9 98.5 78.4 98.2 74.2 0.01k=100 3.5 98.9 83.6 98.6 78.9 0 21.9 99.1 84.8 99.0 83.0 Table 2: Supertagger accuracy on section 00 egories whose probability according to (1) is within some factor, ?, of the highest probability category for the word. We follow Clark (2002) in ignoring the featuresbased on the previously assigned categories; there fore every tagging decision is local and the Viterbi algorithm is not required. This simple approach has the advantage of being very efficient, and we findthat it is accurate enough to enable highly accu rate parsing. However, a method which used theforward-backward algorithm to sum over all possi ble sequences, or some other method which took into account category sequence information, may well improve the results. For words seen at least k times in the trainingdata, the tagger can only assign categories appear ing in the word?s entry in the tag dictionary. Eachentry in the tag dictionary is a list of all the cate gories seen with that word in the training data. For words seen less than k times, we use an alternative dictionary based on the word?s POS tag: the tagger can only assign categories that have been seen with the POS tag in the training data. A value of k = 20was used in this work, and sections 2-21 of CCG bank were used as training data.Table 2 gives the per-word accuracy (acc) on sec tion 00 for various values of ?, together with the average number of categories per word. The sent acc column gives the precentage of sentences whose words are all supertagged correctly. The figures for ? = 0.01k=100 correspond to a value of 100 for thetag dictionary parameter k. The set of categories as signed to a word is considered correct if it contains the correct category. The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran andClark (2003) tagger. The drop in accuracy is ex pected given the importance of POS tags as features. The figures for ? = 0 are obtained by assigning all categories to a word from the word?s entry in the tag dictionary. For words which appear less than 20 times in the training data, the dictionary based on the word?s POS tag is used. The table demonstrates the significant reduction in the average number of categories that can be achieved through the use of a supertagger. To give one example, the number of categories in the tag dictionary?s entry for the wordis is 45 (only considering categories which have appeared at least 10 times in the training data). However, in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., the supertag ger correctly assigns 1 category to is for ? = 0.1, and 3 categories for ? = 0.01. The parser is described in detail in Clark and Curran (2004). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG. In this paper weuse the normal-form model, which defines proba bilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence. Featuresare defined in terms of the local trees in the derivation, including lexical head information and word word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. The feature set we use is from the best performing normal-form model in Clark and Curran (2004). For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm. The dependency relations are de fined in terms of the argument slots of CCG lexical categories. Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. 3.1 Model Estimation. In Clark and Curran (2004) we describe a discrim inative method for estimating the parameters of a log-linear parsing model. The estimation method maximises the following objective function: L?(?) = L(?) ?G(?) (2) = log m ? j=1 P?(d j|S j) ? n ? i=1 ?2i 2?2The data consists of sentences S 1, . . . , S m, to gether with gold standard normal-form derivations, d1, . . . , dm. L(?) is the log-likelihood of model ?, and G(?) is a Gaussian prior term used to avoid overfitting (n is the number of features; ?i is the weight for feature fi; and ? is a parameter of theGaussian). The objective function is optimised using L-BFGS (Nocedal and Wright, 1999), an iterative algorithm from the numerical optimisation lit erature.The algorithm requires the gradient of the objective function, and the value of the objective function, at each iteration. Calculation of these val ues requires all derivations for each sentence in the training data. In Clark and Curran (2004) wedescribe efficient methods for performing the cal culations using packed charts. However, a very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in Clark and Curran (2003) we report a memory usage of 30 GB. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster. The need for large high-performance computing resources is a disadvantage of our earlier approach.In the next section we show how use of the supertag ger, combined with normal-form constraints on thederivations, can significantly reduce the memory re quirements for the model estimation. Since the training data contains the correct lexicalcategories, we ensure the correct category is as signed to each word when generating the packed charts for model estimation. Whilst training theparser, the supertagger can be thought of as supply ing a number of plausible but incorrect categoriesfor each word; these, together with the correct cat egories, determine the parts of the parse space that are used in the estimation process. We would like to keep the packed charts as small as possible, but not lose accuracy in the resulting parser. Section 4.2discusses the use of various settings on the supertag ger. The next section describes how normal-form constraints can further reduce the derivation space. 4.1 Normal-Form Constraints. As well as the supertagger, we use two additional strategies for reducing the derivation space. Thefirst, following Hockenmaier (2003), is to only al low categories to combine if the combination hasbeen seen in sections 2-21 of CCGbank. For exam ple, NP/NP could combine with NP/NP accordingto CCG?s combinatory rules (by forward composi tion), but since this particular combination does not appear in CCGbank the parser does not allow it.The second strategy is to use Eisner?s normal form constraints (Eisner, 1996). The constraints SUPERTAGGING/PARSING USAGE CONSTRAINTS DISK MEMORY ? = 0.01 ? 0.05 ? 0.1 17 GB 31 GB CCGbank constraints 13 GB 23 GB Eisner constraints 9 GB 16 GB ? = 0.05 ? 0.1 2 GB 4 GB Table 3: Space requirements for model training dataprevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application. Eis ner only deals with a grammar without type-raising,and so the constraints do not guarantee a normal form parse when using a grammar extracted from CCGbank. However, the constraints are still useful in restricting the derivation space. As far as we are aware, this is the first demonstration of the utility of such constraints for a wide-coverage CCG parser. 4.2 Results (Space Requirements). Table 3 shows the effect of different supertagger set tings, and the normal-form constraints, on the size of the packed charts used for model estimation. The disk usage is the space taken on disk by the charts,and the memory usage is the space taken in memory during the estimation process. The training sen tences are parsed using a number of nodes from a 64-node Beowulf cluster.3 The time taken to parse the training sentences depends on the supertagging and parsing constraints, and the number of nodes used, but is typically around 30 minutes. The first row of the table corresponds to using the least restrictive ? value of 0.01, and reverting to ? = 0.05, and finally ? = 0.1, if the chart size exceeds some threshold. The threshold was set at300,000 nodes in the chart. Packed charts are created for approximately 94% of the sentences in sec tions 2-21 of CCGbank. The coverage is not 100%because, for some sentences, the parser cannot pro vide an analysis, and some charts exceed the node limit even at the ? = 0.1 level. This strategy was used in our earlier work (Clark and Curran, 2003) and, as the table shows, results in very large charts.Note that, even with this relaxed setting on the su pertagger, the number of categories assigned to each word is only around 3 on average. This suggests that it is only through use of the supertagger that we are able to estimate a log-linear parsing model on all of the training data at all, since without it the memory 3The figures in the table are estimates based on a sample of the nodes in the cluster. requirements would be far too great, even for the entire 64-node cluster.4 The second row shows the reduction in size if the parser is only allowed to combine categorieswhich have combined in the training data. This sig nificantly reduces the number of categories created using the composition rules, and also prevents thecreation of unlikely categories using rule combina tions not seen in CCGbank. The results show thatthe memory and disk usage are reduced by approx imately 25% using these constraints. The third row shows a further reduction in size when using the Eisner normal-form constraints. Even with the CCGbank rule constraints, theparser still builds many non-normal-form derivations, since CCGbank does contain cases of compo sition and type-raising. (These are used to analysesome coordination and extraction cases, for example.) The combination of the two types of normal form constraints reduces the memory requirements by 48% over the original approach. In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies. The final row corresponds to a more restrictive setting on the supertagger, in which a value of ? = 0.05 is used initially and ? = 0.1 is used if thenode limit is exceeded. The two types of normal form constraints are also used. In Clark and Curran(2004) we show that using this more restrictive set ting has a small negative impact on the accuracy of the resulting parser (about 0.6 F-score over labelled dependencies). However, the memory requirement for training the model is now only 4 GB, a reduction of 87% compared with the original approach. The previous section showed how to combine the supertagger and parser for the purpose of creating training data, assuming the correct category for each word is known. In this section we describe our approach to tightly integrating the supertagger and parser for parsing unseen data. Our previous approach to parsing unseen data (Clark et al, 2002; Clark and Curran, 2003) wasto use the least restrictive setting of the supertagger which still allows a reasonable compromise be tween speed and accuracy. Our philosophy was to give the parser the greatest possibility of finding thecorrect parse, by giving it as many categories as pos sible, while still retaining reasonable efficiency.4Another possible solution would be to use sampling meth ods, e.g. Osborne (2000). SUPERTAGGING/PARSING TIME SENTS WORDS CONSTRAINTS SEC /SEC /SEC ? = 0.01? 0.1 3 523 0.7 16 CCGbank constraints 1 181 2.0 46 Eisner constraints 995 2.4 55 ? = 0.1? 0.01k=100 608 3.9 90 CCGbank constraints 124 19.4 440 Eisner constraints 100 24.0 546 Parser beam 67 35.8 814 94% coverage 49 49.0 1 114 Parser beam 46 52.2 1 186 Oracle 18 133.4 3 031 Table 4: Parse times for section 23 The problem with this approach is that, for some sentences, the number of categories in the chart still gets extremely large and so parsing is unacceptably slow. Hence we applied a limit to the number of categories in the chart, as in the previous section,and reverted to a more restrictive setting of the su pertagger if the limit was exceeded. We first used a value of ? = 0.01, and then reverted to ? = 0.05, and finally ? = 0.1. In this paper we take the opposite approach: westart with a very restrictive setting of the supertag ger, and only assign more categories if the parser cannot find an analysis spanning the sentence. In this way the parser interacts much more closely with the supertagger. In effect, the parser is using the grammar to decide if the categories provided by thesupertagger are acceptable, and if not the parser re quests more categories. The parser uses the 5 levels given in Table 2, starting with ? = 0.1 and moving through the levels to ? = 0.01k=100 . The advantage of this approach is that parsing speeds are much higher. We also show that our new approach slightly increases parsing accuracy over the previous method. This suggests that, given our current parsing model, it is better to rely largely on the supertagger to provide the correct categoriesrather than use the parsing model to select the cor rect categories from a very large derivation space. 5.1 Results (Parse Times). The results in this section are all using the best per forming normal-form model in Clark and Curran (2004), which corresponds to row 3 in Table 3. All experiments were run on a 2.8 GHZ Intel Xeon P4 with 2 GB RAM. Table 4 gives parse times for the 2,401 sentences in section 23 of CCGbank. The final two columns give the number of sentences, and the number of ? CATS/ 0.1 FIRST 0.01 FIRST WORD PARSES % PARSES % 0.1 1.4 1689 88.4 0 0.0 0.075 1.5 43 2.3 7 0.4 0.05 1.7 51 2.7 39 2.0 0.01 2.9 79 4.1 1816 95.1 0.01k=100 3.5 33 1.7 33 1.7 NO SPAN 15 0.8 15 0.8 Table 5: Supertagger ? levels used on section 00words, parsed per second. For all of the figures re ported on section 23, unless stated otherwise, the parser is able to provide an analysis for 98.5% of the sentences. The parse times and speeds include the failed sentences, but do not include the time takenby the supertagger; however, the supertagger is ex tremely efficient, and takes less than 6 seconds to supertag section 23, most of which consists of load time for the Maximum Entropy model. The first three rows correspond to our strategy ofearlier work by starting with the least restrictive set ting of the supertagger. The first value of ? is 0.01; if the parser cannot find a spanning analysis, this ischanged to ? = 0.01k=100; if the node limit is ex ceeded (for these experiments set at 1,000,000), ? is changed to 0.05. If the node limit is still exceeded, ? is changed to 0.075, and finally 0.1. The second row has the CCGbank rule restriction applied, and the third row the Eisner normal-form restrictions.The next three rows correspond to our new strat egy of starting with the least restrictive setting of thesupertagger (? = 0.1), and moving through the set tings if the parser cannot find a spanning analysis. The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%. The new strategy also has a spectacular impact on the speed compared with the old strategy, reducing the times by 83% without the normal-form constraints and 90% with the constraints. The 94% coverage row corresponds to using only the first two supertagging levels; the parser ignores the sentence if it cannot get an analysis at the ? = 0.05 level. The percentage of sentences without an analysis is now 6%, but the parser is extremely fast,processing almost 50 sentences a second. This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for ex ample, for which large amounts of data are required. The oracle row shows the parser speed when it is provided with only the correct lexical categories.The parser is extremely fast, and in Clark and Cur ran (2004) we show that the F-score for labelled dependencies is almost 98%. This demonstratesthe large amount of information in the lexical categories, and the potential for improving parser ac curacy and efficiency by improving the supertagger. Finally, the first parser beam row corresponds to the parser using a beam search to further reduce thederivation space. The beam search works by prun ing categories from the chart: a category can only be part of a derivation if its beam score is within some factor, ?, of the highest scoring category forthat cell in the chart. Here we simply use the ex ponential of the inside score of a category as the beam score; the inside score for a category c is the sum over all sub-derivations dominated by c of the weights of the features in those sub-derivations (see Clark and Curran (2004).5The value of ? that we use here reduces the accu racy of the parser on section 00 by a small amount (0.3% labelled F-score), but has a significant impacton parser speed, reducing the parse times by a fur ther 33%. The final parser beam row combines thebeam search with the fast, reduced coverage config uration of the parser, producing speeds of over 50 sentences per second. Table 5 gives the percentage of sentences which are parsed at each supertagger level, for both the new and old parsing strategies. The results show that, for the old approach, most of the sentences areparsed using the least restrictive setting of the supertagger (? = 0.01); conversely, for the new ap proach, most of the sentences are parsed using the most restrictive setting (? = 0.1). As well as investigating parser efficiency, we have also evaluated the accuracy of the parser onsection 00 of CCGbank, using both parsing strate gies together with the normal-form constraints. Thenew strategy increases the F-score over labelled de pendencies by approximately 0.5%, leading to the figures reported in Clark and Curran (2004). 5.2 Comparison with Other Work. The only other work we are aware of to investigate the impact of supertagging on parsing efficiency is the work of Sarkar et al (2000) for LTAG. Sarkar etal. did find that LTAG supertagging increased pars ing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse. The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours.5Multiplying by an estimate of the outside score may im prove the efficacy of the beam. Kaplan et al (2004) report high parsing speedsfor a deep parsing system which uses an LFG gram mar: 1.9 sentences per second for 560 sentencesfrom section 23 of the Penn Treebank. They also re port speeds for the publicly available Collins parser (Collins, 1999): 2.8 sentences per second for the same set. The best speeds we have reported for the CCG parser are an order of magnitude faster.","The previous section showed how to combine the supertagger and parser for the purpose of creating training data, assuming the correct category for each word is known. In this section we describe our approach to tightly integrating the supertagger and parser for parsing unseen data. Our previous approach to parsing unseen data (Clark et al, 2002; Clark and Curran, 2003) wasto use the least restrictive setting of the supertagger which still allows a reasonable compromise be tween speed and accuracy. Our philosophy was to give the parser the greatest possibility of finding thecorrect parse, by giving it as many categories as pos sible, while still retaining reasonable efficiency.4Another possible solution would be to use sampling meth ods, e.g. Osborne (2000). SUPERTAGGING/PARSING TIME SENTS WORDS CONSTRAINTS SEC /SEC /SEC ? = 0.01? 0.1 3 523 0.7 16 CCGbank constraints 1 181 2.0 46 Eisner constraints 995 2.4 55 ? = 0.1? 0.01k=100 608 3.9 90 CCGbank constraints 124 19.4 440 Eisner constraints 100 24.0 546 Parser beam 67 35.8 814 94% coverage 49 49.0 1 114 Parser beam 46 52.2 1 186 Oracle 18 133.4 3 031 Table 4: Parse times for section 23 The problem with this approach is that, for some sentences, the number of categories in the chart still gets extremely large and so parsing is unacceptably slow. Hence we applied a limit to the number of categories in the chart, as in the previous section,and reverted to a more restrictive setting of the su pertagger if the limit was exceeded. We first used a value of ? = 0.01, and then reverted to ? = 0.05, and finally ? = 0.1. In this paper we take the opposite approach: westart with a very restrictive setting of the supertag ger, and only assign more categories if the parser cannot find an analysis spanning the sentence. In this way the parser interacts much more closely with the supertagger. In effect, the parser is using the grammar to decide if the categories provided by thesupertagger are acceptable, and if not the parser re quests more categories. The parser uses the 5 levels given in Table 2, starting with ? = 0.1 and moving through the levels to ? = 0.01k=100 . The advantage of this approach is that parsing speeds are much higher. We also show that our new approach slightly increases parsing accuracy over the previous method. This suggests that, given our current parsing model, it is better to rely largely on the supertagger to provide the correct categoriesrather than use the parsing model to select the cor rect categories from a very large derivation space. 5.1 Results (Parse Times). The results in this section are all using the best per forming normal-form model in Clark and Curran (2004), which corresponds to row 3 in Table 3. All experiments were run on a 2.8 GHZ Intel Xeon P4 with 2 GB RAM. Table 4 gives parse times for the 2,401 sentences in section 23 of CCGbank. The final two columns give the number of sentences, and the number of ? CATS/ 0.1 FIRST 0.01 FIRST WORD PARSES % PARSES % 0.1 1.4 1689 88.4 0 0.0 0.075 1.5 43 2.3 7 0.4 0.05 1.7 51 2.7 39 2.0 0.01 2.9 79 4.1 1816 95.1 0.01k=100 3.5 33 1.7 33 1.7 NO SPAN 15 0.8 15 0.8 Table 5: Supertagger ? levels used on section 00words, parsed per second. For all of the figures re ported on section 23, unless stated otherwise, the parser is able to provide an analysis for 98.5% of the sentences. The parse times and speeds include the failed sentences, but do not include the time takenby the supertagger; however, the supertagger is ex tremely efficient, and takes less than 6 seconds to supertag section 23, most of which consists of load time for the Maximum Entropy model. The first three rows correspond to our strategy ofearlier work by starting with the least restrictive set ting of the supertagger. The first value of ? is 0.01; if the parser cannot find a spanning analysis, this ischanged to ? = 0.01k=100; if the node limit is ex ceeded (for these experiments set at 1,000,000), ? is changed to 0.05. If the node limit is still exceeded, ? is changed to 0.075, and finally 0.1. The second row has the CCGbank rule restriction applied, and the third row the Eisner normal-form restrictions.The next three rows correspond to our new strat egy of starting with the least restrictive setting of thesupertagger (? = 0.1), and moving through the set tings if the parser cannot find a spanning analysis. The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%. The new strategy also has a spectacular impact on the speed compared with the old strategy, reducing the times by 83% without the normal-form constraints and 90% with the constraints. The 94% coverage row corresponds to using only the first two supertagging levels; the parser ignores the sentence if it cannot get an analysis at the ? = 0.05 level. The percentage of sentences without an analysis is now 6%, but the parser is extremely fast,processing almost 50 sentences a second. This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for ex ample, for which large amounts of data are required. The oracle row shows the parser speed when it is provided with only the correct lexical categories.The parser is extremely fast, and in Clark and Cur ran (2004) we show that the F-score for labelled dependencies is almost 98%. This demonstratesthe large amount of information in the lexical categories, and the potential for improving parser ac curacy and efficiency by improving the supertagger. Finally, the first parser beam row corresponds to the parser using a beam search to further reduce thederivation space. The beam search works by prun ing categories from the chart: a category can only be part of a derivation if its beam score is within some factor, ?, of the highest scoring category forthat cell in the chart. Here we simply use the ex ponential of the inside score of a category as the beam score; the inside score for a category c is the sum over all sub-derivations dominated by c of the weights of the features in those sub-derivations (see Clark and Curran (2004).5The value of ? that we use here reduces the accu racy of the parser on section 00 by a small amount (0.3% labelled F-score), but has a significant impacton parser speed, reducing the parse times by a fur ther 33%. The final parser beam row combines thebeam search with the fast, reduced coverage config uration of the parser, producing speeds of over 50 sentences per second. Table 5 gives the percentage of sentences which are parsed at each supertagger level, for both the new and old parsing strategies. The results show that, for the old approach, most of the sentences areparsed using the least restrictive setting of the supertagger (? = 0.01); conversely, for the new ap proach, most of the sentences are parsed using the most restrictive setting (? = 0.1). As well as investigating parser efficiency, we have also evaluated the accuracy of the parser onsection 00 of CCGbank, using both parsing strate gies together with the normal-form constraints. Thenew strategy increases the F-score over labelled de pendencies by approximately 0.5%, leading to the figures reported in Clark and Curran (2004). 5.2 Comparison with Other Work. The only other work we are aware of to investigate the impact of supertagging on parsing efficiency is the work of Sarkar et al (2000) for LTAG. Sarkar etal. did find that LTAG supertagging increased pars ing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse. The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours.5Multiplying by an estimate of the outside score may im prove the efficacy of the beam. Kaplan et al (2004) report high parsing speedsfor a deep parsing system which uses an LFG gram mar: 1.9 sentences per second for 560 sentencesfrom section 23 of the Penn Treebank. They also re port speeds for the publicly available Collins parser (Collins, 1999): 2.8 sentences per second for the same set. The best speeds we have reported for the CCG parser are an order of magnitude faster."
53,"This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.","This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems. More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993). We would now propose that HMM's have successfully been applied to the problem of name-finding. We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &quot;Nymble&quot;. To our knowledge, Nymble out-performs the best published results of any other learning name-finder. Furthermore, it performs at or above the 90% accuracy level, often considered &quot;near-human performance&quot;. The system arose from the NE task as specified in the last Message Understanding Conference (MUC), where organization names, person names, location names, times, dates, percentages and money amounts were to be delimited in text using SGML-markup. We will describe the various models employed, the methods for training these models and the method for &quot;decoding&quot; on test data (the term &quot;decoding&quot; borrowed from the speech recognition community, since one goal of traversing an HMM is to recover the hidden state sequence). To date, we have successfully trained and used the model on both English and Spanish, the latter for MET, the multi-lingual entity task. The basic premise of the approach is to consider the raw text encountered when decoding as though it had passed through a noisy channel, where it had been originally marked with named entities.' The job of the generative model is to model the original process which generated the name-class–annotated words, before they went through the noisy channel. More formally, we must find the most likely sequence of name-classes (NC) given a sequence of words (W): Pr(NC I W) (2.1) In order to treat this as a generative model (where it generates the original, name-class–annotated words), and since the a priori probability of the word sequence—the denominator—is constant for any given sentence, we can maxi-mize Equation 2.2 by maximizing the numerator alone. I See (Cover and Thomas, 1991), ch. 2, for an excellent overview of the principles of information theory. (2.2) Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995). For every new language and every new class of new information to spot, one has to write a new set of rules to cover the new language and to cover the new class of information. A finite-state pattern rule attempts to match against a sequence of tokens (words), in much the same way as a general regular expression matcher. In addition to these finitestate pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995). The atomic elements of information extraction— indeed, of language as a whole—could be considered the who, where, when and how much in a sentence. A name-finder performs what is known as surface- or lightweight-parsing, delimiting sequences of tokens that answer these important questions. It can be used as the first step in a chain of processors: a next level of processing could relate two or more named entities, or perhaps even give semantics to that relationship using a verb. In this way, further processing could discover the &quot;what&quot; and &quot;how&quot; of a sentence or body of text. Furthermore, name-finding can be useful in its own right: an Internet query system might use namefinding to construct more appropriately-formed queries: &quot;When was Bill Gates born?&quot; could yield the query &quot;Bill Gates&quot;+born. Also, name-finding can be directly employed for link analysis and other information retrieval problems. We will present the model twice, first in a conceptual and informal overview, then in a moredetailed, formal description of it as a type of HMM. The model bears resemblance to Scott Miller's novel work in the Air Traffic Information System (ATIS) task, as documented in (Miller et al., 1994). Figure 3.1 is a pictorial overview of our model. Informally, we have an ergodic HMM with only eight internal states (the name classes, including the NOT-A-NAME class), with two special states, the START- and END-OF-SENTENCE states. Within each of the name-class states, we use a statistical bigram language model, with the usual one-word-per-state emission. This means that the number of states in each of the name-class states is equal to the vocabulary size, I VI . The generation of words and name-classes proceeds in three steps: These three steps are repeated until the entire observed word sequence is generated. Using the Viterbi algorithm, we efficiently search the entire space of all possible name-class assignments, maximizing the numerator of Equation 2.2, Pr(W, NC). Informally, the construction of the model in this manner indicates that we view each type of &quot;name&quot; to be its own language, with separate bigram probabilities for generating its words. While the number of word-states within each name-class is equal to I VI , this &quot;interior&quot; bigram language model is ergodic, i.e., there is a probability associated with every one of the 1V12transitions. As a parameterized, trained model, if such a transition were never observed, the model &quot;backs off' to a less-powerful model, as described below, in §3.3.3 on p. 4. Throughout most of the model, we consider words to be ordered pairs (or two-element vectors), composed of word and word-feature, denoted (w, f). The word feature is a simple, deterministic computation performed on each word as it is added to or feature computation is an extremely small part of the implementation, at roughly ten lines of code. Also, most of the word features are used to distinguish types of numbers, which are language-independent.2 The rationale for having such features is clear: in Roman languages, capitalization gives good evidence of names.3 This section describes the model formally, discussing the transition probabilities to the wordstates, which &quot;generate&quot; the words of each name-class. As with most trained, probabilistic models, we looked up in the vocabulary. It produces one of the fourteen values in Table 3.1. These values are computed in the order listed, so that in the case of non-disjoint feature-classes, such as containsDigitAndAlpha and containsDigitAndDash, the former will take precedence. The first eight features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates. The rest of the features distinguish types of capitalization and all other words (such as punctuation marks, which are separate tokens). In particular, the f irstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that allCaps and capPeriod are computed before f irstWord, and therefore take precedence). The word feature is the one part of this model which is language-dependent. Fortunately, the word have a most accurate, most powerful model, which will &quot;back off' to a less-powerful model when there is insufficient training, and ultimately back-off to unigram probabilities. In order to generate the first word, we must make a transition from one name-class to another, as well as calculate the likelihood of that word. Our intuition was that a word preceding the start of a name-class (such as &quot;Mr.&quot;, &quot;President&quot; or other titles preceding the PERSON name-class) and the word following a name-class would be strong indicators of the subsequent and preceding name-classes, respectively. 2 Non-english languages tend to use the comma and period in the reverse way in which English does, i.e., the comma is a decimal point and the period separates groups of three digits in large numbers. However, the re-ordering of the precedence of the two relevant word-features had little effect when decoding Spanish, so they were left as is. 3 Although Spanish has many lower-case words in organization names. See §4.1 on p. 6 for more details. Accordingly, the probabilitiy for generating the first word of a name-class is factored into two parts: Pr(NC I NC_,, w_1) Pr((w,f)firs, I NC, NC_,). (3.1) The top level model for generating all but the first word in a name-class is Pr((w, NC). (3.2) There is also a magical &quot;+end+&quot; word, so that the probability may be computed for any current word to be the final word of its name-class, i.e., Pr((+end+, o the r) I(w, f)find' NC). (3.3) As one might imagine, it would be useless to have the first factor in Equation 3.1 be conditioned off of the +end+ word, so the probability is conditioned on the previous real word of the previous name-class, i.e., we compute W-1 = last observed word otherwise NC , = START - OF - SENTENCE (3.4) Note that the above probability is not conditioned on the word-feature of w_1, the intuition of which is that in the cases where the previous word would help the model predict the next name-class, the world feature—capitalization in particular—is not important: &quot;Mr.&quot; is a good indicator of the next word beginning the PERSON name-class, regardless of capitalization, especially since it is almost never seen as &quot;mr.&quot;. The calculation of the above probabilities is straightforward, using events/sample-size: where c() represents the number of times the events occurred in the training data (the count). Ideally, we would have sufficient training (or at least one observation of!) every event whose conditional probability we wish to calculate. Also, ideally, we would have sufficient samples of that upon which each conditional probability is conditioned, e.g., for Pr(NC I NC 1, w_,), we would like to have seen sufficient numbers of NC_,, w1. Unfortunately, there is rarely enough training data to compute accurate probabilities when &quot;decoding&quot; on new data. 3. 3. 3.1 Unknown Words The vocabulary of the system is built as it trains. Necessarily, then, the system knows about all words for which it stores bigram counts in order to compute the probabilities in Equations 3.1 – 3.3. The question arises how the system should deal with unknown words, since there are three ways in which they can appear in a bigram: as the current word, as the previous word or as both. A good answer is to train a separate, unknown word–model off of held-out data, to gather statistics of unknown words occurring in the midst of known words. Typically, one holds out 10-20% of one's training for smoothing or unknown word–training. In order to overcome the limitations of a small amount of training data—particularly in Spanish—we hold out 50% of our data to train the unknown word– model (the vocabulary is built up on the first 50%), save these counts in training data file, then hold out the other 50% and concatentate these bigram counts with the first unknown word–training file. This way, we can gather likelihoods of an unknown word appearing in the bigram using all available training data. This approach is perfectly valid, as we art trying to estimate that which we have not legitimately seen in training. When decoding, if either word of the bigram is unknown, the model used to estimate the probabilities of Equations 3.1-3 is the unknown word model, otherwise it is the model from the normal training. The unknown word–model can be viewed as a first level of back-off, therefore, since it is used as a backup model when an unknown word is encountered, and is necessarily not as accurate as the bigram model formed from the actual training. 3. 3. 3.2 Further Back-off Models and Smoothing Whether a bigram contains an unknown word or not, it is possible that either model may not have seen this bigram, in which case the model backs off to a less-powerful, less-descriptive model. Table 3.2 shows a graphic illustration of the back-off scheme: The weight for each back-off model is computed onthe-fly, using the following formula: If computing Pr(XIY), assign weight of A to the direct computation (using one of the formulae of §3.3.2) and a weight of (1 — A) to the back-off model, where (3.8) where &quot;old c(Y)&quot; is the sample size of the model from which we are backing off. This is a rather simple method of smoothing, which tends to work well when there are only three or four levels of back-off.4 This method also overcomes the problem when a back-off model has roughly the same amount of training as the current model, via the first factor of Equation 3.8, which essentially ignores the back-off model and puts all the weight on the primary model, in such an equi-trained situation. As an example—disregarding the first factor—if we saw the bigram &quot;come hither&quot; once in training and we saw &quot;come here&quot; three times, and nowhere else did we see the word &quot;come&quot; in the NOT-A-NAME class, when computing Pr(&quot;hither&quot; I &quot;come&quot;, NOT-A-NAME), we would back off to the unigram probability Pr(&quot;hither&quot; I NOT-A-NAME) with a weight of , since the number of unique outcomes for the word-state for &quot;come&quot; would be two, and the total number of times &quot;come&quot; had been the preceding word in a bigram would be four (a 4 Any more levels of back-off might require a more sophisticated smoothing technique, such as deleted interpolation. No matter what smoothing technique is used, one must remember that smoothing is the art of estimating the probability of that which is unknown (i.e., not seen in training). Unlike a traditional HMM, the probability of generating a particular word is 1 for each word-state inside each of the name-class states. An alternative— and more traditional—model would have a small number of states within each name-class, each having, perhaps, some semantic signficance, e.g., three states in the PERSON name-class, representing a first, middle and last name, where each of these three states would have some probability associated with emitting any word from the vocabulary. We chose to use a bigram language model because, while less semantically appealing, such n-gram language models work remarkably well in practice. Also, as a first research attempt, an n-gram model captures the most general significance of the words in each name-class, without presupposing any specifics of the structure of names, a la the PERSON name-class example, above. More important, either approach is mathematically valid, as long as all transitions out of a given state sum to one. All of this modeling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence, thereby &quot;decoding&quot; the original sequence of name-classes. The number of possible state sequences for N states in an ergodic model for a sentence of m words is Alm, but, using dynamic programming and an appropriate merging of multiple theories when they converge on a particular state—the Viterbi decoding algorithm—a sentence can be &quot;decoded&quot; in time linear to the number of tokens in the sentence, 0(m) (Viterbi, 1967). Since we are interested in recovering the name-class state sequence, we pursue eight theories at every given step of the algorithm. Initially, the word-feature was not in the model; instead the system relied on a third-level back-off partof-speech tag, which in turn was computed by our stochastic part-of-speech tagger. The tags were taken at face value: there were not k-best tags; the system treated the part-of-speech tagger as a &quot;black box&quot;. Although the part-of-speech tagger used capitalization to help it determine proper-noun tags, this feature was only implicit in the model, and then only after two levels of back-off! Also, the capitalization of a word was submerged in the muddiness of part-of-speech tags, which can &quot;smear&quot; the capitalization probability mass over several tags. Because it seemed that capitalization would be a good name-predicting feature, and that it should appear earlier in the model, we eliminated the reliance on part-of-speech altogether, and opted for the more direct, word-feature model described above, in §3. Originally, we had a very small number of features, indicating whether the word was a number, the first word of a sentence, all uppercase, inital-capitalized or lower-case. We then expanded the feature set to its current state in order to capture more subtleties related mostly to numbers; due to increased performance (although not entirely dramatic) on every test, we kept the enlarged feature set. Contrary to our expectations (which were based on our experience with English), Spanish contained many examples of lower-case words in organization and location names. For example, departamento (&quot;Department&quot;) could often start an organization name, and adjectival place-names, such as coreana (&quot;Korean&quot;) could appear in locations and by convention are not capitalized. The entire system is implemented in C++, atop a &quot;home-brewed&quot;, general-purpose class library, providing a rapid code-compile-train-test cycle. In fact, many NLP systems suffer from a lack of software and computer-science engineering effort: runtime efficiency is key to performing numerous experiments, which, in turn, is key to improving performance. A system may have excellent performance on a given task, but if it takes long to compile and/or run on test data, the rate of improvement of that system will be miniscule compared to that which can run very efficiently. On a Sparc20 or SGI Indy with an appropritae amount of RAM, Nymble can compile in 10 minutes, train in 5 minutes and run at 6MB/hr. There were days in which we had as much as a 15% reduction in error rate, to borrow the performance measure used by the speech community, where error rate = 100% — Fmeasure. (See §4.3 for the definition of F-measure.) In this section we report the results of evaluating the final version of the learning software. We report the results for English and for Spanish and then the results of a set of experiments to determine the impact of the training set size on the algorithm's performance in both English and Spanish. For each language, we have a held-out development test set and a held-out, blind test set. We only report results on the blind test set for each respective language. The scoring program measures both precision =I recall, terms borrowed from the information-retrieval community, where number of correct responses and number responses number of correct responses number correct in key Put informally, recall measures the number of &quot;hits&quot; vs. the number of possible correct answers as specified in the key file, whereas precision measures how many answers were correct ones compared to the number of answers delivered. These two measures of performance combine to form one measure of performance, the F-measure, which is computed by the weighted harmonic mean of precision and recall: (/32 +1)RP where if represents the relative weight of recall to precision (and typically has the value 1). To our knowledge, our learned name-finding system has achieved a higher F-measure than any other learned system when compared to state-of-the-art manual (rule-based) systems on similar data. Our test set of English data for reporting results is that of the MUC-6 test set, a collection of 30 WSJ documents (we used a different test set during development). Our Spanish test set is that used for MET, comprised of articles from the news agency AFP. Table 4.1 illustrates Nymble's performance as compared to the best reported scores for each category. (4.2) With any learning technique one of the important questions is how much training data is required to get acceptable performance. More generally how does performance vary as the training set size is increased or decreased? We ran a sequence of experiments in English and in Spanish to try to answer this question for the final model that was implemented. For English, there were 450,000 words of training data. By that we mean that the text of the document itself (including headlines but not including SGML tags) was 450,000 words long. Given this maximum size of training available to us, we successfully divided the training material in half until we were using only one eighth of the original training set size or a training set of 50,000 words for the smallest experiment. To give a sense of the size of 450,000 words, that is roughly half the length of one edition of the Wall Street Journal. The results are shown in a histogram in Figure 4.1 below. The positive outcome of the experiment is that half as much training data would have given almost equivalent performance. Had we used only one quarter of the data or approximately 100,000 words, performance would have degraded slightly, only about 1-2 percent. Reducing the training set size to 50,000 words would have had a more significant decrease in the performance of the system; however, the performance is still impressive even with such a small training set. On the other hand, the result also shows that merely annotating more data will not yield dramatic improvement in the performance. With increased training data it would be possible to use even more detailed models that require more data and could achieve significantly improved overall system performance with those more detailed models. For Spanish we had only 223,000 words of training data. We also measured the performance of the system with half the training data or slightly more than 100,000 words of text. Figure 4.2 shows the results. There is almost no change in performance by using as little as 100,000 words of training data. Therefore the results in both languages were comparable. As little as 100,000 words of training data produces performance nearly comparable to handcrafted systems. While our initial results have been quite favorable, there is still much that can be done potentially to improve performance and completely close the gap between learned and rule-based name-finding systems. We would like to incorporate the following into the current model:","While our initial results have been quite favorable, there is still much that can be done potentially to improve performance and completely close the gap between learned and rule-based name-finding systems. We would like to incorporate the following into the current model:"
54,"trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, New York, May.","trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, New York, May. Text processing applications, such as machine translation systems, information retrieval systems or natural-language understanding systems, need to identify multi-word expressions that refer to proper names of people, organizations, places, laws and other entities. When encountering Mrs. Candy Hill in input text, for example, a machine translation system should not attempt to look up the translation of candy and hill, but should translate Mrs. to the appropriate personal title in the target language and preserve the rest of the name intact. Similarly, an information retrieval system should not attempt to expand Candy to all of its morphological variants or suggest synonyms (Wacholder et al. 1994). The need to identify proper names has two aspects: the recognition of known names and the discovery of new names. Since obtaining and maintaining a name database requires significant effort, many applications need to operate in the absence of such a resource. Without a database, names need to be discovered in the text and linked to entities they refer to. Even where name databases exist, text needs to be scanned for new names that are formed when entities, such as countries or commercial companies, are created, or for unknown names which become important when the entities they refer to become topical. This situation is the norm for dynamic applications such as news providing services or Internet information indexing. The next Section describes the different types of proper name ambiguities we have observed. Section 3 discusses the role of context and world knowledge in their disambiguation; Section 4 describes the process of name discovery as implemented in Nominator, a module for proper name recognition developed at the IBM T.J. Watson Research Center. Sections 5-7 elaborate on Nominator's disambiguation heuristics. Name identification requires resolution of a subset of the types of structural and semantic ambiguities encountered in the analysis of nouns and noun phrases (NPs) in natural language processing. Like common nouns, ((Jensen and Binot 1987), (Hindle and Rooth 1993) and (Brill and Resnick 1994)), proper names exhibit structural ambiguity in prepositional phrase (PP) attachment and in conjunction scope. A PP may be attached to the preceding NP and form part of a single large name, as in NP [Midwest Center PP[for NP[Computer Research]]]. Alternatively it may be independent of the preceding NP, as in NP[Carnegie Hall] PP[for NP[Irwin Berlin]], where for separates two distinct names, Carnegie Hall and Irwin Berlin. As with PP-attachment of common noun phrases, the ambiguity is not always resolved, even in human sentence parsing (cf. the famous example I saw the girl in the park with the telescope). The location of an organization, for instance, could be part of its name (City University of New York) or an phrases. The components of Victoria and Albert Museum and IBM and Bell Laboratories look identical; however, and is part of the name of the museum in the first example, but a conjunction joining two computer company names in the second. Although this problem is well known, a search of the computational literature shows that few solutions have been proposed, perhaps because the conjunct ambiguity problem is harder than PP attachment (though see (Agarwal and Boggess 1992) for a method of conjunct identification that relies on syntactic category and semantic label). Similar structural ambiguity exists with respect to the possessive pronoun, which may indicate a relationship between two names (e.g., Israel's Shimon Peres) or may constitute a component of a single name (e.g., Donoghue's Money Fund Report). The resolution of structural ambiguity such as PP attachment and conjunction scope is required in order to automatically establish the exact boundaries of proper names. Once these boundaries have been established, there is another type of well-known structural ambiguity, involving the internal structure of the proper name. For example, Professor of Far Eastern Art John Blake is parsed as [[Professor [of Far Eastern Art]] John Blake] whereas Professor Art Klein is [[Professor] Art Klein]. Proper names also display semantic ambiguity. Identification of the type of proper nouns resembles the problem of sense disambiguation for common nouns where, for instance, state taken out of context may refer either to a government body or the condition of a person or entity. A name variant taken out of context may be one of many types, e.g., Ford by itself could be a person (Gerald Ford), an organization (Ford Motors), a make of car (Ford), or a place (Ford, Michigan). Entity-type ambiguity is quite common, as places are named after famous people and companies are named after their owners or locations. In addition, naming conventions are sometimes disregarded by people who enjoy creating novel and unconventional names. A store named Mr. Tall and a woman named April Wednesday (McDonald 1993) come to mind. Like common nouns, proper nouns exhibit systematic metonymy: United States refers either to a geographical area or to the political body which governs this area; Wall Street Journal refers to the printed object, its content, and the commercial entity that produces it. In addition, proper names resemble definite noun phrases in that their intended referent may be ambiguous. The man may refer to more than one male individual previously mentioned in the discourse or present in the non-linguistic context; J. Smith may similarly refer to more than one individual named Joseph Smith, John Smith, Jane Smith, etc. Semantic ambiguity of names is very common because of the standard practice of using shorter names to stand for longer ones. Shared knowledge and context are crucial disambiguation factors. Paris, usually refers to the capital of France, rather than a city in Texas or the Trojan prince, but in a particular context, such as a discussion of Greek mythology, the presumed referent changes. Beyond the ambiguities that proper names share with common nouns, some ambiguities are particular to names: noun phrases may be ambiguous between a name reading and a common noun phrase, as in Candy, the person's name, versus candy the food, or The House as an organization versus a house referring to a building. In English, capitalization usually disambiguates the two, though not at sentence beginnings: at the beginning of a sentence, the components and capitalization patterns of New Coke and New Sears are identical; only world knowledge informs us that New Coke is a product and Sears is a company. Furthermore, capitalization does not always disambiguate names from non-names because what constitutes a name as opposed to a • non-name is not always clear. According to (Quirk et al. 1972) names, which consist of proper nouns (classified into personal names like Shakespeare, temporal names like Monday, or geographical names like Australia) have 'unique' reference. Proper nouns differ in their linguistic behavior from common nouns in that they mostly do not take determiners or have a plural form. However, some names do take determiners, as in The New York Times; in this case, they &quot;are perfectly regular in taking the definite article since they are basically premodified count nouns... The difference between an ordinary common noun and an ordinary common noun turned name is that the unique reference of the name has been institutionalized, as is made overt in writing by initial capital letter.&quot; Quirk et al. 's description of names seems to indicate that capitalized words like Egyptian (an adjective) or Frenchmen (a noun referring to a set of individuals) are not names. It leaves capitalized sequences like Minimum Alternative Tax, Annual Report, and Chairman undetermined as to whether or not they are names. All of these ambiguities must be dealt with if proper names are to be identified correctly. In the rest of the paper we describe the resources and heuristics we have designed and implemented in Nominator and the extent to which they resolve these ambiguities. In general, two types of resources are available for disambiguation: context and world knowledge. Each of these can be exploited along a continuum, from 'cheaper' to computationally and manually more expensive usage. 'Cheaper' models, which include no context or world knowledge, do very little disambiguation. More 'expensive' models, which use full syntactic parsing, discourse models, inference and reasoning, require computational and human resources that may not always be available, as when massive amounts of text have to be rapidly processed on a regular basis. In addition, given the current state of the art, full parsing and extensive world knowledge would still not yield complete automatic ambiguity resolution. In designing Nominator, we have tried to achieve a balance between high accuracy and speed by adopting a model which uses minimal context and world knowledge. Nominator uses no syntactic contextual information. It applies a set of heuristics to a list of (multi-word) strings, based on patterns of capitalization, punctuation and location within the sentence and the document. This design choice differentiates our approach from that of several similar projects. Most proper name recognizers that have been reported on in print either take as input text tagged by part-of-speech (e.g., the systems of (Paik et al. 1993) and (Mani et al. 1993)) or perform syntactic and/or morphological analysis on all words, including capitalized ones, that are part of candidate proper names (e.g., (Coates-Stephens 1993) and (McDonald 1993)). Several (e.g., (McDonald 1993), (Mani et al. 1993), (Paik et al. 1993) and (Cowie et al. 1992)) look in the local context of the candidate proper name for external information such as appositives (e.g., in a sequence such as Robin Clark, president of Clark Co.) or for human-subject verbs (e.g., say, plan) in order to determine the category of the candidate proper name. Nominator does not use this type of external context. Instead, Nominator makes use of a different kind of contextual information — proper names cooccuring in the document. It is a fairly standard convention in an edited document for one of the first references to an entity (excluding a reference in the title) to include a relatively full form of its name. In a kind of discourse anaphora, other references to the entity take the form of shorter, more ambiguous variants. Nominator identifies the referent of the full form (see below) and then takes advantage of the discourse context provided by the list of names to associate shorter more ambiguous name occurrences with their intended referents. In terms of world knowledge, the most obvious resource is a database of known names. In fact, this is what many commercially available name identification applications use (e.g., Hayes 1994). A reliable database provides both accuracy and efficiency, if fast look-up methods are incorporated. A database also has the potential to resolve structural ambiguity; for example, if IBM and Apple Computers are listed individually in the database but IBM and Apple Computers is not, it may indicate a conjunction of two distinct names. A database may also contain default world knowledge information: e.g., with no other over-riding information, it may be safe to assume that the string McDonald's refers to an organization. But even if an existing database is reliable, names that are not yet in it must be discovered and information in the database must be over-ridden when appropriate. For example, if a new name such as IBM Credit Corp. occurs in the text but not in the database, while IBM exists in the database, automatic identification of IBM should be blocked in favor of the new name IBM Credit Corp. If a name database exists, Nominator can take advantage of it. However, our goal has been to design Nominator to function optimally in the absence of such a resource. In this case, Nominator consults a small authority file which contains information on about 3000 special 'name words' and their relevant lexical features. Listed are personal titles (e.g., Mr., King), organizational identifiers (including strong identifiers such as Inc. and weaker domain identifiers such as Arts) and names of large places (e.g., Los Angeles, California, but not Scarsdale, N.Y.). Also listed are exception words, such as upper-case lexical items that are unlikely to be single-word proper names (e.g., Very, I or TV) and lower-case lexical items (e.g., and and van) that can be parts of proper names. In addition, the authority file contains about 20,000 first names. Our choice of disambiguation resources makes Nominator fast and robust. The precision and recall of Nominator, operating without a database of pre-existing proper names, is in the 90's while the processing rate is over 40Mg of text per hour on a RISC/6000 machine. (See (Ravin and Wacholder 1996) for details.) This efficient processing has been achieved at the cost of limiting the extent to which the program can 'understand' the text being analyzed and resolve potential ambiguity. Many wordsequences that are easily recognized by human readers as names are ambiguous for Nominator, given the restricted set of tools available to it. In cases where Nominator cannot resolve an ambiguity with relatively high confidence, we follow the principle that 'noisy information' is to be preferred to data omitted, so that no information is lost. In ambiguous cases, the module is designed to make conservative decisions, such as including non-names or non-name parts in otherwise valid name sequences. It assigns weak types such as ?HUMAN or fails to assign a type if the available information is not sufficient. In this section, we give an overview of the process by which Nominator identifies and classifies proper names. Nominator's first step is to build a list of candidate names for a document. Next, 'splitting' heuristics are applied to all candidate names for the purpose of breaking up complex names into smaller ones. Finally, Nominator groups together name vanants that refer to the same entity. After information about names and their referents has been extracted from individual documents, an aggregation process combines the names collected from all the documents into a dictionary, or database of names, representative of the document collection. (For more details on the process, see (Ravin and Wacholder 1996)). We illustrate the process of name discovery with an excerpt taken from a Wall Street Journal article in the TIPSTER CD-ROM collection (NIST 1993). Paragraph breaks are omitted to conserve space. ... The professional conduct of lawyers in other jurisdictions is guided by American Bar Association rules or by state bar ethics codes, none of which permit non-lawyers to be partners in law firms. The ABA has steadfastly reserved the title of partner and partnership perks (which include getting a stake of the firm's profit) for those with law degrees. But Robert Jordan, a partner at Steptoe & Johnson who took the lead in drafting the new district bar code, said the ABA's rules were viewed as &quot;too restrictive&quot; by lawyers here. &quot;The practice of law in Washington is very different from what it is in Dubuque,&quot; he said. ... Some of these non-lawyer employees are paid at partners' levels. Yet, not having the partner title &quot;makes non-lawyers working in law firms second-class citizens,&quot; said Mr. Jordan of Steptoe & Johnson. ... Before the text is processed by Nominator, it is analyzed into tokens — sentences, words, tags, and punctuation elements. Nominator forms a candidate name list by scanning the tokenized document and collecting sequences of capitalized tokens (or words) as well as some special lower-case tokens, such as conjunctions and prepositions. The list of candidate names extracted from the sample document contains: Each candidate name is examined for the presence of conjunctions, prepositions or possessive 's. A set of heuristics is applied to determine whether each candidate name should be split into smaller independent names. For example, Mr. Jordan of Steptoe & Johnson is split into Mr. Jordan and Steptoe & Johnson. Finally, Nominator links together variants that refer to the same entity. Because of standard English-language naming conventions, Mr. Jordan is grouped with Robert Jordan. ABA is grouped with American Bar Association as a possible abbreviation of the longer name. Each linked group is categorized by an entity type and assigned a 'canonical name' as its identifier. The canonical name is the fullest, least ambiguous label that can be used to refer to the entity. It may be one of the variants found in the document or it may be constructed from components of different ones As the links are formed, each group is assigned a type. In the sample output shown below, each canonical name is followed by its entity type and by the variants linked to it. After the whole document collection has been processed, linked groups are merged across documents and their variants combined. Thus, if in one document President Clinton was a variant of William Clinton, while in another document Governor Clinton was a variant of William Clinton, both are treated as variants of an aggregated William Clinton group. In this minimal sense, Nominator uses the larger context of the document collection to 'learn' more variants for a given name. In the following sections we describe how ambiguity is resolved as part of the name discovery process. We identify three indicators of potential structural ambiguity, prepositions, conjunctions and possessive pronouns, which we refer to as 'ambiguous operators'. In order to determine whether 'splitting' should occur, a name sequence containing an ambiguous operator is divided into three segments — the operator, the substring to its left and the substring to its right. The splitting process applies a set of heuristics based on patterns of capitalization, lexical features and the relative 'scope' of operators (see below) to name sequences containing these operators to determine whether or not they should be split into smaller names. We can describe the splitting heuristics as determining the scope of ambiguous operators, by analogy to the standard linguistic treatment of quantifiers. From Nominator's point of view, all three operator types behave in similar ways and often interact when they co-occur in the same name sequence, as in New York's MOMA and the Victoria and Albert Museum in London. The scope of ambiguous operators also interacts with the 'scope' of NP-heads, if we define the scope of NP-heads as the constituents they dominate. For example, in Victoria and Albert Museum, the conjunction is within the scope of the lexical head Museum because Museum is a noun that can take PP modification (Museum of Natural History) and hence pre-modification (Natural History Museum). Since pre-modifiers can contain conjunctions (Japanis within the scope of the noun, and so the name is not split. Although the same relationship holds between the lexical head Laboratories and the conjunction and in IBM and Bell Laboratories, another heuristic takes precedence, one whose condition requires splitting a string if it contains an acronym immediately to the left or to the right of the ambiguous operator. It is not possible to determine relative scope strength for all the combinations of different operators. Contradictory examples abound: Gates of Microsoft and Gerstner of IBMsuggests stronger scope of and over of, The Department of German Languages and Literature suggests the opposite. Since it is usually the case that a right-hand operator has stronger scope over a left-hand one, we evaluate strings containing operators from right to left. To illustrate, New York's MOMA and the Victoria and Albert Museum in London is first evaluated for splitting on in. Since the left and right substrings do not satisfy any conditions, we proceed to the next operator on the left — and. Because of the strong scope of Museum, as mentioned above, no splitting occurs. Next, the second and from the right is evaluated. It causes a split because it is immediately preceded by an all-capitalized word. We have found this simple typographical heuristic to be powerful and surprisingly accurate. Ambiguous operators form recursive structures and so the splitting heuristics apply recursively to name sequences until no more splitting conditions hold. New York's MOMA is further split at 's because of a heuristic that checks for place names on the left of a possessive pronoun or a comma. Victoria and Albert Museum in London remains intact. Nominator's other heuristics resemble those discussed above in that they check for typographical patterns or for the presence of particular name types to the left or right of certain operators. Some heuristics weigh the relative scope strength in the substrings on either side of the operator. If the scope strength is similar, the string is split. We have observed that this type of heuristic works quite well. Thus, the string The Natural History Museum and The Board of Education is split at and because each of its substrings contains a strong-scope NP-head (as we define it) with modifiers within its scope. These two substrings are better balanced than the substrings of The Food and Drug Administration where the left substring does not contain a strong-scope NP-head while the right one does (Administration). Because of the principle that noisy data is preferable to loss of information, Nominator does not split names if relative strength cannot be determined. As a result, there occur in Nominator's output certain 'names' such as American Television 6 Commu Special treatment is required for words in sentenceinitial position, which may be capitalized because they are part of a proper name or simply because they are sentence initial. While the heuristics for splitting names are linguistically motivated and rule-governed, the heuristics for handling sentence-initial names are based on patterns of word occurrence in the document. When all the names have been collected and split, names containing sentence-initial words are compared to other names on the list. If the sentence-initial candidate name also occurs as a non-sentence-initial name or as a substring of it, the candidate name is assumed to be valid and is retained. Otherwise, it is removed from the list. For example, if White occurs at sentence-initial position and also as a substring of another name (e.g., Mr. White) it is kept. If it is found only in sentence-initial position (e.g., White paint is ...), White is discarded. A more difficult situation arises when a sentenceinitial candidate name contains a valid name that begins at the second word of the string. If the preceding word is an adverb, a pronoun, a verb or a preposition, it can safely be discarded. Thus a sentence beginning with Yesterday Columbia yields Columbia as a name. But cases involving other parts of speech remain unresolved. If they are sentenceinitial, Nominator accepts as names both New Sears and New Coke; it also accepts sentence-initial Five Reagan as a variant of President Reagan, if the two co-occur in a document. In a typical document, a single entity may be referred to by many name variants which differ in their degree of potential ambiguity. As noted above, Paris and Washington are highly ambiguous out of context but in well edited text they are often disambiguated by the occurrence of a single unambiguous variant in the same document. Thus, Washington is likely to co-occur with either President Washington or Washington, D.C., but not with both. Indeed, we have observed that if several unambiguous variants do co-occur, as in documents that mention both the owner of a company and the company named after the owner, the editors refrain from using a variant that is ambiguous with respect to both. To disambiguate highly ambiguous variants then, we link them to unambiguous ones occurring within the same document. Nominator cycles through the list of names, identifying 'anchors', or variant names that unambiguously refer to certain entity types. When an anchor is identified, the list of name candidates is scanned for ambiguous variants that could refer to the same entity. They are linked to the anchor. Our measure of ambiguity is very pragmatic. It is based on the confidence scores yielded by heuristics that analyze a name and determine the entity types it can refer to. If the heuristic for a certain entity type (a person, for example) results in a high condifence score (highly confident that this is a person name), we determine that the name unambiguously refers to this type. Otherwise, we choose the highest score obtained by the various heuristics. A few simple indicators can unambiguously determine the entity type of a name, such as Mr. for a person or Inc. for an organization. More commonly, however, several pieces of positive and negative evidence are accumulated in order to make this judgement. We have defined a set of obligatory and optional components for each entity type. For a human name, these components include a professional title (e.g., Attorney General), a personal title (e.g., Dr.), a first name, middle name, nickname, last name, and suffix (e.g., Jr.). The combination of the various components is inspected. Some combinations may result in a high negative score — highly confident that this cannot be a person name. For example, if the name lacks a personal title and a first name, and its last name is listed as an organization word (e.g., Department) in the authority list, it receives a high negative score. This is the case with Justice Department or Frank Sinatra Building. The same combination but with a last name that is not a listed organization word results in a low positive score, as for Justice Johnson or Frank Sinatra. The presence or absence of a personal title is also important for determining confidence: If present, the result is a high confidence score (e.g., Mrs. Ruth Lake); No personal title with a known first name results in a low positive confidence score (e.g., Ruth Lake, Beverly Hills); and no personal title with an unknown first name results in a zero score (e.g., Panorama Lake). By the end of the analysis process, Justice Department has a high negative score for person and a low positive score for organization, resulting in its classification as an organization. Beverly Hills, by contrast, has low positive scores both for place and for person. Names with low or zero scores are first tested as possible variants of names with high positive scores. However, if they are incompatible with any, they are assigned a weak entity type. Thus in the absence of any other evidence in the document, Beverly Hills is classified as a ?PERSON. (?PERSON is preferred over ?PLACE as it tends to be the correct choice most of the time.) This analysis of course can be over-ridden by a name database listing Beverly Hills as a place. Further disambiguation may be possible during aggregation across documents. As mentioned before, during aggregation, linked groups from different documents are merged if their canonical forms are identical. As a rule, their entity types should be identical as well, to prevent a merge of Boston (PLACE) and Boston (ORG). Weak entity types, however, are allowed to merge with stronger entity types. Thus, Jordan Hills (?PERSON) from one document is aggregated with Jordan Hills (PERSON) from another, where there was sufficient evidence, such as Mr. Hills, to make a firmer decision. An evaluation of an earlier version of Nominator, was performed on 88 Wall Street Journal documents (NIST 1993) that had been set aside for testing. We chose the Wall Street Journal corpus because it follows standard stylistic conventions, especially capitalization, which is essential for Nominator to work. Nominator's performance deteriorates if other conventions are not consistently followed. A linguist manually identified 2426 occurrences of proper names, which reduced to 1354 unique tokens. Of these, Nominator correctly identified the boundaries of 91% (1230/1354). The precision rate was 92% for the 1409 names Nominator identified (1230/1409). In terms of semantic disambiguation, Nominator failed to assign an entity type to 21% of the names it identified. This high percentage is due to a decision not to assign a type if the confidence measure is too low. The payoff of this choice is a very high precision rate — 99 % — for the assignment of semantic type to those names that were disambiguated. (See (Ravin and Wacholder 1996) for details. The main reason that names remain untyped is insufficent evidence in the document. If IBM, for example, occurs in a document without International Business Machines, Nominator does not type it; rather, it lets later processes inspect the local context for further clues. These processess form part of the Talent tool set under development at the T.J. Watson Research Center. They take as their input text processed by Nominator and further disambiguate untyped names appearing in certain contexts, such as an appositive, e.g., president of CitiBank Corp. Other untyped names, such as Star Bellied Sneetches or George Melloan's Business World, are neither people, places, organizations nor any of the other legal or financial entities we categorize into. Many of these uncategorized names are titles of articles, books and other works of art that we currently do not handle.","An evaluation of an earlier version of Nominator, was performed on 88 Wall Street Journal documents (NIST 1993) that had been set aside for testing. We chose the Wall Street Journal corpus because it follows standard stylistic conventions, especially capitalization, which is essential for Nominator to work. Nominator's performance deteriorates if other conventions are not consistently followed. A linguist manually identified 2426 occurrences of proper names, which reduced to 1354 unique tokens. Of these, Nominator correctly identified the boundaries of 91% (1230/1354). The precision rate was 92% for the 1409 names Nominator identified (1230/1409). In terms of semantic disambiguation, Nominator failed to assign an entity type to 21% of the names it identified. This high percentage is due to a decision not to assign a type if the confidence measure is too low. The payoff of this choice is a very high precision rate — 99 % — for the assignment of semantic type to those names that were disambiguated. (See (Ravin and Wacholder 1996) for details. The main reason that names remain untyped is insufficent evidence in the document. If IBM, for example, occurs in a document without International Business Machines, Nominator does not type it; rather, it lets later processes inspect the local context for further clues. These processess form part of the Talent tool set under development at the T.J. Watson Research Center. They take as their input text processed by Nominator and further disambiguate untyped names appearing in certain contexts, such as an appositive, e.g., president of CitiBank Corp. Other untyped names, such as Star Bellied Sneetches or George Melloan's Business World, are neither people, places, organizations nor any of the other legal or financial entities we categorize into. Many of these uncategorized names are titles of articles, books and other works of art that we currently do not handle."
55,"In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.","In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.","In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate."
56,"In order to respond correctly to a free form factual question given a large collection of texts, one needs to un derstand the question to a level that allows determiningsome of the constraints the question imposes on a pos sible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach toquestion classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes. We show accurate results on a large col lection of free-form questions used in TREC 10.","In order to respond correctly to a free form factual question given a large collection of texts, one needs to un derstand the question to a level that allows determiningsome of the constraints the question imposes on a pos sible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach toquestion classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes. We show accurate results on a large col lection of free-form questions used in TREC 10. Open-domain question answering (Lehnert, 1986; Harabagiu et al, 2001; Light et al, 2001) and storycomprehension (Hirschman et al, 1999) have become important directions in natural language pro cessing. Question answering is a retrieval task morechallenging than common search engine tasks be cause its purpose is to find an accurate and conciseanswer to a question rather than a relevant docu ment. The difficulty is more acute in tasks such as story comprehension in which the target text is less likely to overlap with the text in the questions. For this reason, advanced natural language techniques rather than simple key term extraction are needed.One of the important stages in this process is analyz ing the question to a degree that allows determining the ?type? of the sought after answer. In the TRECcompetition (Voorhees, 2000), participants are requested to build a system which, given a set of En glish questions, can automatically extract answers (a short phrase) of no more than 50 bytes from a5-gigabyte document library. Participants have re Research supported by NSF grants IIS-9801638 and ITR IIS 0085836 and an ONR MURI Award. alized that locating an answer accurately hinges on first filtering out a wide range of candidates (Hovy et al, 2001; Ittycheriah et al, 2001) based on some categorization of answer types. This work develops a machine learning approach to question classification (QC) (Harabagiu et al, 2001; Hermjakob, 2001). Our goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answeringprocess. For example, when considering the question Q: What Canadian city has the largest popula tion?, the hope is to classify this question as havinganswer type city, implying that only candidate an swers that are cities need consideration.Based on the SNoW learning architecture, we develop a hierarchical classifier that is guided by a lay ered semantic hierarchy of answer types and is able to classify questions into fine-grained classes. Wesuggest that it is useful to consider this classifica tion task as a multi-label classification and find that it is possible to achieve good classification results(over 90%) despite the fact that the number of dif ferent labels used is fairly large, 50. We observe thatlocal features are not sufficient to support this accu racy, and that inducing semantic features is crucial for good performance. The paper is organized as follows: Sec. 2 presents the question classification problem; Sec. 3 discusses the learning issues involved in QC and presents ourlearning approach; Sec. 4 describes our experimen tal study. We define Question Classification(QC) here to be the task that, given a question, maps it to one of k classes, which provide a semantic constraint on the sought-after answer1. The intension is that this 1We do not address questions like ?Do you have a light??, which calls for an action, but rather only factual Wh-questions. classification, potentially with other constraints on the answer, will be used by a downstream process which selects a correct answer from among several candidates.A question classification module in a question an swering system has two main requirements. First, it provides constraints on the answer types that allow further processing to precisely locate and verify theanswer. Second, it provides information that downstream processes may use in determining answer se lection strategies that may be answer type specific,rather than uniform. For example, given the ques tion ?Who was the first woman killed in the Vietnam War?? we do not want to test every noun phrase in a document to see whether it provides an answer. At the very least, we would like to know that the target of this question is a person, thereby reducingthe space of possible answers significantly. The fol lowing examples, taken from the TREC 10 question collection, exhibit several aspects of this point. Q: What is a prism? Identifying that the target of this question is a definition, strategies that are specific fordefinitions (e.g., using predefined templates) may be use ful. Similarly, in: Q: Why is the sun yellow? Identifying that this question asks for a reason, may lead to using a specific strategy for reasons.The above examples indicate that, given that dif ferent answer types may be searched using different strategies, a good classification module may helpthe question answering task. Moreover, determin ing the specific semantic type of the answer couldalso be beneficial in locating the answer and veri fying it. For example, in the next two questions, knowing that the targets are a city or country willbe more useful than just knowing that they are loca tions. Q: What Canadian city has the largest population? Q: Which country gave New York the Statue of Liberty?However, confined by the huge amount of man ual work needed for constructing a classifier for a complicated taxonomy of questions, most questionanswering systems can only perform a coarse clas sification for no more than 20 classes. As a result, existing approaches, as in (Singhal et al, 2000), have adopted a small set of simple answer entitytypes, which consisted of the classes: Person, Location, Organization, Date, Quantity, Duration, Lin ear Measure. The rules used in the classification were of the following forms: ? If a query starts with Who or Whom: type Person. If a query starts with Where: type Location. If a query contains Which or What, the head noun phrase determines the class, as for What X questions.While the rules used have large coverage and rea sonable accuracy, they are not sufficient to supportfine-grained classification. One difficulty in supporting fine-grained classification is the need to ex tract from the questions finer features that require syntactic and semantic analysis of questions, and possibly, many of them. The approach we adoptedis a multi-level learning approach: some of our fea tures rely on finer analysis of the questions that are outcomes of learned classifiers; the QC module then applies learning with these as input features. 2.1 Classification Standard. Earlier works have suggested various standards of classifying questions. Wendy Lehnert?s conceptual taxonomy (Lehnert, 1986), for example, proposesabout 13 conceptual classes including causal antecedent, goal orientation, enablement, causal consequent, verification, disjunctive, and so on. How ever, in the context of factual questions that are of interest to us here, conceptual categories do notseem to be helpful; instead, our goal is to se mantically classify questions, as in earlier work on TREC (Singhal et al, 2000; Hovy et al, 2001; Harabagiu et al, 2001; Ittycheriah et al, 2001). The key difference, though, is that we attempt todo that with a significantly finer taxonomy of answer types; the hope is that with the semantic an swer types as input, one can easily locate answercandidates, given a reasonably accurate named en tity recognizer for documents. 2.2 Question Hierarchy. We define a two-layered taxonomy, which repre sents a natural semantic classification for typicalanswers in the TREC task. The hierarchy con tains 6 coarse classes (ABBREVIATION, ENTITY,DESCRIPTION, HUMAN, LOCATION and NU MERIC VALUE) and 50 fine classes, Table 1 showsthe distribution of these classes in the 500 ques tions of TREC 10. Each coarse class contains anon-overlapping set of fine classes. The motiva tion behind adding a level of coarse classes is that of compatibility with previous work?s definitions, andcomprehensibility. We also hoped that a hierarchi cal classifier would have a performance advantage over a multi-class classifier; this point, however is not fully supported by our experiments. Class # Class # ABBREV. 9 description 7 abb 1 manner 2 exp 8 reason 6 ENTITY 94 HUMAN 65 animal 16 group 6 body 2 individual 55 color 10 title 1 creative 0 description 3 currency 6 LOCATION 81 dis.med. 2 city 18 event 2 country 3 food 4 mountain 3 instrument 1 other 50 lang 2 state 7 letter 0 NUMERIC 113 other 12 code 0 plant 5 count 9 product 4 date 47 religion 0 distance 16 sport 1 money 3 substance 15 order 0 symbol 0 other 12 technique 1 period 8 term 7 percent 3 vehicle 4 speed 6 word 0 temp 5 DESCRIPTION 138 size 0 definition 123 weight 4 Table 1: The distribution of 500 TREC 10 questions over the question hierarchy. Coarse classes (in bold) are followed by their fine class refinements. 2.3 The Ambiguity Problem. One difficulty in the question classification task is that there is no completely clear boundary between classes. Therefore, the classification of a specific question can be quite ambiguous. Consider 1. What is bipolar disorder?. 2. What do bats eat?. Question 1 could belong to definition or dis ease medicine; Question 2 could belong to food,plant or animal; And Question 3 could be a numeric value or a definition. It is hard to catego rize those questions into one single class and it islikely that mistakes will be introduced in the down stream process if we do so. To avoid this problem,we allow our classifiers to assign multiple class la bels for a single question. This method is better than only allowing one label because we can apply all the classes in the later precessing steps without any loss. 3 Learning a Question Classifier. Using machine learning methods for question clas sification is advantageous over manual methods forseveral reasons. The construction of a manual clas sifier for questions is a tedious task that requiresthe analysis of a large number of questions. More over, mapping questions into fine classes requiresthe use of lexical items (specific words) and there fore an explicit representation of the mapping may be very large. On the other hand, in our learning approach one can define only a small number of ?types? of features, which are then expanded in adata-driven way to a potentially large number of features (Cumby and Roth, 2000), relying on the abil ity of the learning process to handle it. It is hard to imagine writing explicitly a classifier that depends on thousands or more features. Finally, a learnedclassifier is more flexible to reconstruct than a man ual one because it can be trained on a new taxonomy in a very short time.One way to exhibit the difficulty in manually con structing a classifier is to consider reformulations of a question: What tourist attractions are there in Reims? What are the names of the tourist attractions in Reims? What do most tourists visit in Reims? What attracts tourists to Reims? What is worth seeing in Reims? All these reformulations target the same answertype Location. However, different words and syntactic structures make it difficult for a manual clas sifier based on a small set of rules to generalize well and map all these to the same answer type. Good learning methods with appropriate features, on the other hand, may not suffer from the fact that the number of potential features (derived from wordsand syntactic structures) is so large and would gen eralize and classify these cases correctly. 3.1 A Hierarchical ClassifierQuestion classification is a multi-class classification. A question can be mapped to one of 50 pos sible classes (We call the set of all possible class labels for a given question a confusion set (Golding and Roth, 1999)). Our learned classifier is based on the SNoW learning architecture (Carlson et al, 1999; Roth, 1998)2 where, in order to allow the classifier to output more than one class label, wemap the classifier?s output activation into a condi tional probability of the class labels and threshold it. The question classifier makes use of a sequence of two simple classifiers (Even-Zohar and Roth, 2001), each utilizing the Winnow algorithm within SNoW. The first classifies questions into coarse classes (Coarse Classifier) and the second into fineclasses (Fine Classifier). A feature extractor automatically extracts the same features for each clas sifier. The second classifier depends on the first in2Freely available at http://L2R.cs.uiuc.edu/cogcomp/cc software.html ABBR, ENTITY,DESC,HUMAN,LOC,NUM ABBR, ENTITY ENTITY, HUMAN ENTITY, LOC,NUM DESC Coarse Classifier Fine Classifier abb,exp ind, plant date abb, animal, food, plant? food,plant, ind,group? food, plant, city, state? definition, reason,? Map coarse classes to fine classes C0 C1 C2 C3 abb,def animal,food all possible subsets of C0 wih size = 5 all possible subsets of C2 with size =5 Figure 1: The hierarchical classifier that its candidate labels are generated by expanding the set of retained coarse classes from the first into a set of fine classes; this set is then treated as the confusion set for the second classifier.Figure 1 shows the basic structure of the hierar chical classifier. During either the training or the testing stage, a question is processed along one path top-down to get classified. The initial confusion set of any question is C 0 = fc 1 ; c 2 ; : : : ; c n g, the set of all the coarse classes. The coarse classifier determines a set of preferred labels, C 1 = Coarse Classifier(C 0 ), C 1  C 0 so that jC 1 j  5. Then each coarse class label in C 1 is expanded to a fixed set of fine classesdetermined by the class hierarchy. That is, sup pose the coarse class c i is mapped into the set c i = ff i1 ; f i2 ; : : : ; f im g of fine classes, then C 2 = S c i 2C 1 c i . The fine classifier determines a set of. preferred labels, C 3 = Fine Classifier(C 2 ) so that C 3  C 2 and jC 3 j  5. C 1 and C 3are the ul timate outputs from the whole classifier which are used in our evaluation. 3.2 Feature Space. Each question is analyzed and represented as a listof features to be treated as a training or test exam ple for learning. We use several types of features and investigate below their contribution to the QC accuracy. The primitive feature types extracted for eachquestion include words, pos tags, chunks (non overlapping phrases) (Abney, 1991), named entities,head chunks (e.g., the first noun chunk in a sen tence) and semantically related words (words that often occur with a specific question class). Over these primitive features (which we call ?sensors?) we use a set of operators to composemore complex features, such as conjunctive (n grams) and relational features, as in (Cumby and Roth, 2000; Roth and Yih, 2001). A simple script that describes the ?types? of features used, (e.g., conjunction of two consecutive words and their postags) is written and the features themselves are ex tracted in a data driven way. Only ?active? features are listed in our representation so that despite the large number of potential features, the size of each example is small. Among the 6 primitive feature types, pos tags, chunks and head chunks are syntactic features while named entities and semantically related words are semantic features. Pos tags are extracted using a SNoW-based pos tagger (Even-Zohar and Roth, 2001). Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001). The named entity classifier isalso learned and makes use of the same technol ogy developed for the chunker (Roth et al, 2002).The ?related word? sensors were constructed semi automatically. Most question classes have a semantically related word list. Features will be extracted for this class ifa word in a question belongs to the list. For exam ple, when ?away?, which belongs to a list of words semantically related to the class distance, occurs inthe sentence, the sensor Rel(distance) will be ac tive. We note that the features from these sensors are different from those achieved using named entitysince they support more general ?semantic catego rization? and include nouns, verbs, adjectives rather than just named entities. For the sake of the experimental comparison, wedefine six feature sets, each of which is an incre mental combination of the primitive feature types. That is, Feature set 1 (denoted by Word) contains word features; Feature set 2 (Pos) contains featurescomposed of words and pos tags and so on; The fi nal feature set, Feature set 6 (RelWord) contains all the feature types and is the only one that containsthe related words lists. The classifiers will be experimented with different feature sets to test the influ ence of different features. Overall, there are about 200; 000 features in the feature space of RelWorddue to the generation of complex features over sim ple feature types. For each question, up to a couple of hundreds of them are active. 3.3 Decision Model. For both the coarse and fine classifiers, the same decision model is used to choose class labels for a question. Given a confusion set and a question, SNoW outputs a density over the classes derived from the activation of each class. After ranking the classes in the decreasing order of density values, we have the possible class labels C = fc 1 ; c 2 ; : : : ; c n g, with their densities P = fp 1 ; p 2 ; : : : ; p n g (where, P n 1 p i = 1, 0  p i 1, 1  i  n). As dis cussed earlier, for each question we output the first k classes (1  k  5), c 1 ; c 2 ; : : : c kwhere k satis fies, k = min(argmin t ( t X 1 p i  T ); 5) (1) T is a threshold value in [0,1]. If we treat p i as the probability that a question belongs to Class i, the decision model yields a reasonable probabilistic interpretation. We use T = 0:95 in the experiments. We designed two experiments to test the accuracy ofour classifier on TREC questions. The first experi ment evaluates the contribution of different featuretypes to the quality of the classification. Our hi erarchical classifier is trained and tested using oneof the six feature sets defined in Sect. 3.2 (we re peated the experiments on several different trainingand test sets). In the second experiment, we evaluate the advantage we get from the hierarchical clas sifier. We construct a multi-class classifier only for fine classes. This flat classifier takes all fine classes as its initial confusion set and classifies a questioninto fine classes directly. Its parameters and deci sion model are the same as those of the hierarchicalone. By comparing this flat classifier with our hi erarchical classifier in classifying fine classes, we hope to know whether the hierarchical classifier hasany advantage in performance, in addition to the ad vantages it might have in downstream processing and comprehensibility. 4.1 Data. Data are collected from four sources: 4,500 English questions published by USC (Hovy et al, 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set3.These questions were manually labeled accord ing to our question hierarchy. Although we allow multiple labels for one question in our classifiers, in our labeling, for simplicity, we assigned exactly 3The annotated data and experimental results are available from http://L2R.cs.uiuc.edu/cogcomp/one label to each question. Our annotators were requested to choose the most suitable class accord ing to their own understanding. This methodology might cause slight problems in training, when the labels are ambiguous, since some questions are not treated as positive examples for possible classes as they should be. In training, we divide the 5,500 questions from the first three sources randomly into 5 training sets of 1,000, 2,000, 3,000, 4,000 and 5,500 questions. All 500 TREC 10 questions are used as the test set. 4.2 Evaluation. In this paper, we count the number of correctly clas sified questions by two different precision standards P 1 and P 5 . Suppose k. ilabels are output for the i th question (k i  5) and are ranked in a decreasing order according to their density values. We define I ij = f 1; if the correct label of the ith question is output in rank j; 0; otherwise: (2) Then, P 1 = P m i=1 I i1 =m and P 5 = P m i=1 P k i j=1 I ij =m where m is the total number of test examples. P 1corresponds to the usual defini tion of precision which allows only one label for each question, while P 5 allows multiple labels. P 5reflects the accuracy of our classifier with respect to later stages in a question answering sys tem. As the results below show, although questionclasses are still ambiguous, few mistakes are intro duced by our classifier in this step. 4.3 Experimental Results. Performance of the hierarchical classifier Table 2 shows the P 5precision of the hierarchi cal classifier when trained on 5,500 examples andtested on the 500 TREC 10 questions. The re sults are quite encouraging; question classification is shown to be solved effectively using machine learning techniques. It also shows the contribution of the feature sets we defined. Overall, we get a98.80% precision for coarse classes with all the fea tures and 95% for the fine classes. P =5 Word Pos Chunk NE Head RelWord Coarse 92.00 96.60 97.00 97.00 97.80 98.80 Fine 86.00 86.60 87.60 88.60 89.40 95.00Table 2: Classification results of the hierarchical clas sifier on 500 TREC 10 questions. Training is done on 5,500 questions. Columns show the performance for difference feature sets and rows show the precision forcoarse and fine classes, resp. All the results are evalu ated using P 5 . Inspecting the data carefully, we can observe the significant contribution of the features constructed based on semantically related words sensors. It is interesting to observe that this improvement is even more significant for fine classes. No. Train Test P 1 P =5 1 1000 500 83.80 95.60 2 2000 500 84.80 96.40 3 3000 500 91.00 98.00 4 4000 500 90.80 98.00 Table 3: Classification accuracy for coarse classes ondifferent training sets using the feature set RelWord. Re sults are evaluated using P 1 and P 5 . No. Train Test P 1 P =5 1 1000 500 71.00 83.80 2 2000 500 77.80 88.20 3 3000 500 79.80 90.60 4 4000 500 80.00 91.20Table 4: Classification accuracy for fine classes on different training sets using the feature set RelWord. Re sults are evaluated using P 1 and P 5 . Tables 3 and 4 show the P 1 and P 5 accuracyof the hierarchical classifier on training sets of dif ferent sizes and exhibit the learning curve for this problem.We note that the average numbers of labels out put by the coarse and fine classifiers are 1.54 and 2.05 resp., (using the feature set RelWord and 5,500 training examples), which shows the decision model is accurate as well as efficient. Comparison of the hierarchical and the flat classifier The flat classifier consists of one classifier which isalmost the same as the fine classifier in the hierar chical case, except that its initial confusion set is the whole set of fine classes. Our original hope was that the hierarchical classifier would have a better performance, given that its fine classifier only needs to deal with a smaller confusion set. However, it turns out that there is a tradeoff between this factor and the inaccuracy, albeit small, of the coarse levelprediction. As the results show, there is no perfor mance advantage for using a level of coarse classes, and the semantically appealing coarse classes do not contribute to better performance. Figure 2 give some more intuition on the flat vs. hierarchical issue. We define the tendency of Class i to be confused with Class j as follows: D ij = Err ij  2=(N i + N j ); (3) where (when using P 1 ), Err ij is the number ofquestions in Class i that are misclassified as belong P 1 Word Pos Chunk NE Head RelWord h 77.60 78.20 77.40 78.80 78.80 84.20 f 52.40 77.20 77.00 78.40 76.80 84.00 P =5 Word Pos Chunk NE Head RelWord h 86.00 86.60 87.60 88.60 89.40 95.00 f 83.20 86.80 86.60 88.40 89.80 95.60 Table 5: Comparing accuracy of the hierarchical (h) and flat (f) classifiers on 500 TREC 10 question; training is done on 5,500 questions. Results are shown for different feature sets using P 1 and P 5 . Fine Classes 1?50 Fi ne C la ss es 1 ?5 0 2 24 28 32 37 50 2 24 28 32 37 50 Figure 2: The gray?scale map of the matrix D[n,n]. The color of the small box in position (i,j) denotes D ij . The. larger D ij is, the darker the color is. The dotted lines separate the 6 coarse classes. ing to Class j, and N i ; N jare the numbers of ques tions in Class i and j resp. Figure 2 is a gray-scale map of the matrix D[n,n]. D[n,n] is so sparse that most parts of the graph areblank. We can see that there is no good cluster ing of fine classes mistakes within a coarse class,which explains intuitively why the hierarchical clas sifier with an additional level coarse classes does not work much better. 4.4 Discussion and Examples. We have shown that the overall accuracy of our clas sifier is satisfactory. Indeed, all the reformulation questions that we exemplified in Sec. 3 have been correctly classified. Nevertheless, it is constructive to consider some cases in which the classifier fails.Below are some examples misclassified by the hier archical classifier.What French ruler was defeated at the battle of Water loo? The correct label is individual, but the classifier, failing to relate the word ?ruler? to a person, since it was not in any semantic list, outputs event. What is the speed hummingbirds fly ? The correct label is speed, but the classifier outputs animal. Our feature sensors fail to determine that the focus of the question is ?speed?. This example illustrates the necessity of identifying the question focus by analyzing syntactic structures. What do you call a professional map drawer ? The classifier returns other entities instead ofequivalent term. In this case, both classes are ac ceptable. The ambiguity causes the classifier not to output equivalent term as the first choice.","We designed two experiments to test the accuracy ofour classifier on TREC questions. The first experi ment evaluates the contribution of different featuretypes to the quality of the classification. Our hi erarchical classifier is trained and tested using oneof the six feature sets defined in Sect. 3.2 (we re peated the experiments on several different trainingand test sets). In the second experiment, we evaluate the advantage we get from the hierarchical clas sifier. We construct a multi-class classifier only for fine classes. This flat classifier takes all fine classes as its initial confusion set and classifies a questioninto fine classes directly. Its parameters and deci sion model are the same as those of the hierarchicalone. By comparing this flat classifier with our hi erarchical classifier in classifying fine classes, we hope to know whether the hierarchical classifier hasany advantage in performance, in addition to the ad vantages it might have in downstream processing and comprehensibility. 4.1 Data. Data are collected from four sources: 4,500 English questions published by USC (Hovy et al, 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set3.These questions were manually labeled accord ing to our question hierarchy. Although we allow multiple labels for one question in our classifiers, in our labeling, for simplicity, we assigned exactly 3The annotated data and experimental results are available from http://L2R.cs.uiuc.edu/cogcomp/one label to each question. Our annotators were requested to choose the most suitable class accord ing to their own understanding. This methodology might cause slight problems in training, when the labels are ambiguous, since some questions are not treated as positive examples for possible classes as they should be. In training, we divide the 5,500 questions from the first three sources randomly into 5 training sets of 1,000, 2,000, 3,000, 4,000 and 5,500 questions. All 500 TREC 10 questions are used as the test set. 4.2 Evaluation. In this paper, we count the number of correctly clas sified questions by two different precision standards P 1 and P 5 . Suppose k. ilabels are output for the i th question (k i  5) and are ranked in a decreasing order according to their density values. We define I ij = f 1; if the correct label of the ith question is output in rank j; 0; otherwise: (2) Then, P 1 = P m i=1 I i1 =m and P 5 = P m i=1 P k i j=1 I ij =m where m is the total number of test examples. P 1corresponds to the usual defini tion of precision which allows only one label for each question, while P 5 allows multiple labels. P 5reflects the accuracy of our classifier with respect to later stages in a question answering sys tem. As the results below show, although questionclasses are still ambiguous, few mistakes are intro duced by our classifier in this step. 4.3 Experimental Results. Performance of the hierarchical classifier Table 2 shows the P 5precision of the hierarchi cal classifier when trained on 5,500 examples andtested on the 500 TREC 10 questions. The re sults are quite encouraging; question classification is shown to be solved effectively using machine learning techniques. It also shows the contribution of the feature sets we defined. Overall, we get a98.80% precision for coarse classes with all the fea tures and 95% for the fine classes. P =5 Word Pos Chunk NE Head RelWord Coarse 92.00 96.60 97.00 97.00 97.80 98.80 Fine 86.00 86.60 87.60 88.60 89.40 95.00Table 2: Classification results of the hierarchical clas sifier on 500 TREC 10 questions. Training is done on 5,500 questions. Columns show the performance for difference feature sets and rows show the precision forcoarse and fine classes, resp. All the results are evalu ated using P 5 . Inspecting the data carefully, we can observe the significant contribution of the features constructed based on semantically related words sensors. It is interesting to observe that this improvement is even more significant for fine classes. No. Train Test P 1 P =5 1 1000 500 83.80 95.60 2 2000 500 84.80 96.40 3 3000 500 91.00 98.00 4 4000 500 90.80 98.00 Table 3: Classification accuracy for coarse classes ondifferent training sets using the feature set RelWord. Re sults are evaluated using P 1 and P 5 . No. Train Test P 1 P =5 1 1000 500 71.00 83.80 2 2000 500 77.80 88.20 3 3000 500 79.80 90.60 4 4000 500 80.00 91.20Table 4: Classification accuracy for fine classes on different training sets using the feature set RelWord. Re sults are evaluated using P 1 and P 5 . Tables 3 and 4 show the P 1 and P 5 accuracyof the hierarchical classifier on training sets of dif ferent sizes and exhibit the learning curve for this problem.We note that the average numbers of labels out put by the coarse and fine classifiers are 1.54 and 2.05 resp., (using the feature set RelWord and 5,500 training examples), which shows the decision model is accurate as well as efficient. Comparison of the hierarchical and the flat classifier The flat classifier consists of one classifier which isalmost the same as the fine classifier in the hierar chical case, except that its initial confusion set is the whole set of fine classes. Our original hope was that the hierarchical classifier would have a better performance, given that its fine classifier only needs to deal with a smaller confusion set. However, it turns out that there is a tradeoff between this factor and the inaccuracy, albeit small, of the coarse levelprediction. As the results show, there is no perfor mance advantage for using a level of coarse classes, and the semantically appealing coarse classes do not contribute to better performance. Figure 2 give some more intuition on the flat vs. hierarchical issue. We define the tendency of Class i to be confused with Class j as follows: D ij = Err ij  2=(N i + N j ); (3) where (when using P 1 ), Err ij is the number ofquestions in Class i that are misclassified as belong P 1 Word Pos Chunk NE Head RelWord h 77.60 78.20 77.40 78.80 78.80 84.20 f 52.40 77.20 77.00 78.40 76.80 84.00 P =5 Word Pos Chunk NE Head RelWord h 86.00 86.60 87.60 88.60 89.40 95.00 f 83.20 86.80 86.60 88.40 89.80 95.60 Table 5: Comparing accuracy of the hierarchical (h) and flat (f) classifiers on 500 TREC 10 question; training is done on 5,500 questions. Results are shown for different feature sets using P 1 and P 5 . Fine Classes 1?50 Fi ne C la ss es 1 ?5 0 2 24 28 32 37 50 2 24 28 32 37 50 Figure 2: The gray?scale map of the matrix D[n,n]. The color of the small box in position (i,j) denotes D ij . The. larger D ij is, the darker the color is. The dotted lines separate the 6 coarse classes. ing to Class j, and N i ; N jare the numbers of ques tions in Class i and j resp. Figure 2 is a gray-scale map of the matrix D[n,n]. D[n,n] is so sparse that most parts of the graph areblank. We can see that there is no good cluster ing of fine classes mistakes within a coarse class,which explains intuitively why the hierarchical clas sifier with an additional level coarse classes does not work much better. 4.4 Discussion and Examples. We have shown that the overall accuracy of our clas sifier is satisfactory. Indeed, all the reformulation questions that we exemplified in Sec. 3 have been correctly classified. Nevertheless, it is constructive to consider some cases in which the classifier fails.Below are some examples misclassified by the hier archical classifier.What French ruler was defeated at the battle of Water loo? The correct label is individual, but the classifier, failing to relate the word ?ruler? to a person, since it was not in any semantic list, outputs event. What is the speed hummingbirds fly ? The correct label is speed, but the classifier outputs animal. Our feature sensors fail to determine that the focus of the question is ?speed?. This example illustrates the necessity of identifying the question focus by analyzing syntactic structures. What do you call a professional map drawer ? The classifier returns other entities instead ofequivalent term. In this case, both classes are ac ceptable. The ambiguity causes the classifier not to output equivalent term as the first choice."
57,"This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme. The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).","This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme. The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels). There has been a steadily increasing interest in syntactic parsing based on dependency analysis in re cent years. One important reason seems to be thatdependency parsing offers a good compromise be tween the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other. Thus, whereas a complete dependency structure provides a fully disambiguated analysisof a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed determin istically with reasonable accuracy. Deterministicmethods for dependency parsing have now been ap plied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). For English, the interest in dependency parsing has been weaker than for other languages. To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English,the Penn Treebank (Marcus et al, 1993), is anno tated primarily with constituent analysis. On the other hand, the best available parsers trained on thePenn Treebank, those of Collins (1997) and Charniak (2000), use statistical models for disambigua tion that make crucial use of dependency relations. Moreover, the deterministic dependency parser of Yamada and Matsumoto (2003), when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000). The parser described in this paper is similar to that of Yamada and Matsumoto (2003) in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the twoapproaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linearin the size of the input, while the algorithm of Ya mada and Matsumoto is quadratic in the worst case. Another difference is that Yamada and Matsumoto use support vector machines (Vapnik, 1995), whilewe instead rely on memory-based learning (Daele mans, 1999). Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types. As far as we know, this makesit different from all previous systems for dependency parsing applied to the Penn Treebank (Eis ner, 1996; Yamada and Matsumoto, 2003), althoughthere are systems that extract labeled grammatical relations based on shallow parsing, e.g. Buchholz (2002). The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over sup port vector machines, since we require a multi-class classifier. Even though it is possible to use SVMfor multi-class classification, this can get cumber some when the number of classes is large. (For the The   ? DEP finger-pointing   ? NP-SBJ has already   ? ADVP begun   ? VP . ?   DEP Figure 1: Dependency graph for English sentenceunlabeled dependency parser of Yamada and Matsumoto (2003) the classification problem only in volves three classes.) The parsing methodology investigated here haspreviously been applied to Swedish, where promis ing results were obtained with a relatively smalltreebank (approximately 5000 sentences for train ing), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004).1 However, since there are no comparable resultsavailable for Swedish, it is difficult to assess the significance of these findings, which is one of the reasons why we want to apply the method to a bench mark corpus such as the the Penn Treebank, even though the annotation in this corpus is not ideal for labeled dependency parsing.The paper is structured as follows. Section 2 describes the parsing algorithm, while section 3 ex plains how memory-based learning is used to guidethe parser. Experimental results are reported in sec tion 4, and conclusions are stated in section 5. In dependency parsing the goal of the parsing pro cess is to construct a labeled dependency graph of the kind depicted in Figure 1. In formal terms, we define dependency graphs as follows: 1. Let R = {r1, . . . , rm} be the set of permissible. dependency types (arc labels). 2. A dependency graph for a string of words W = w1? ?wn is a labeled directed graph D = (W,A), where (a) W is the set of nodes, i.e. word tokens in the input string, (b) A is a set of labeled arcs (wi, r, wj) (wi, wj ? W , r ? R), (c) for every wj ? W , there is at most one arc (wi, r, wj) ? A.1The attachment score only considers whether a word is as signed the correct head; the labeled accuracy score in additionrequires that it is assigned the correct dependency type; cf. sec tion 4. acyclic, projective and connected. For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004). Parser configurations are rep resented by triples ?S, I,A?, where S is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph. (Since in a dependencygraph the set of nodes is given by the input tokens, only the arcs need to be represented explicitly.) Given an input string W , the parser is initial ized to ?nil,W, ??2 and terminates when it reaches a configuration ?S,nil, A? (for any list S and set ofarcs A). The input string W is accepted if the de pendency graph D = (W,A) given at termination is well-formed; otherwise W is rejected. Given an arbitrary configuration of the parser, there are four possible transitions to the next configuration (where t is the token on top of the stack, n is the next input token, w is any word, and r, r? R): 1. Left-Arc: In a configuration ?t|S,n|I,A?, if. there is no arc (w, r, t) ? A, extend A with(n, r?, t) and pop the stack, giving the configu ration ?S,n|I,A?{(n, r?, t)}?. 2. Right-Arc: In a configuration ?t|S,n|I,A?, if. there is no arc (w, r, n) ? A, extend A with (t, r?, n) and push n onto the stack, giving the configuration ?n|t|S,I,A?{(t, r?, n)}?. is an arc (w, r, t)?A, pop the stack, giving the configuration ?S,I,A?. n onto the stack, giving the configuration ?n|S,I,A?. 2We use nil to denote the empty list and a|A to denote a list with head a and tail A. TH.POS   ? T.DEP . . . TL.POS   ? TL.DEP . . . T.POS T.LEX   ? TR.DEP . . . TR.POS . . . NL.POS   ? NL.DEP . . . N.POS N.LEX L1.POS L2.POS L3.POS T = Top of the stack N = Next input token TL = Leftmost dependent of T TR = Rightmost dependent of T NL = Leftmost dependent of N Li = Next plus i input token X.LEX = Word form of X X.POS = Part-of-speech of X X.DEP = Dependency type of X Figure 2: Parser state featuresAfter initialization, the parser is guaranteed to ter minate after at most 2n transitions, given an input string of length n (Nivre, 2003). Moreover, the parser always constructs a dependency graph that isacyclic and projective. This means that the depen dency graph given at termination is well-formed if and only if it is connected (Nivre, 2003). Otherwise, it is a set of connected components, each of which is a well-formed dependency graph for a substring of the original input.The transition system defined above is nondeterministic in itself, since several transitions can often be applied in a given configuration. To con struct deterministic parsers based on this system,we use classifiers trained on treebank data in or der to predict the next transition (and dependency type) given the current configuration of the parser. In this way, our approach can be seen as a form ofhistory-based parsing (Black et al, 1992; Mager man, 1995). In the experiments reported here, we use memory-based learning to train our classifiers. 3 Memory-Based Learning. Memory-based learning and problem solving is based on two fundamental principles: learning is thesimple storage of experiences in memory, and solv ing a new problem is achieved by reusing solutionsfrom similar previously solved problems (Daele mans, 1999). It is inspired by the nearest neighborapproach in statistical pattern recognition and arti ficial intelligence (Fix and Hodges, 1952), as well as the analogical modeling approach in linguistics(Skousen, 1989; Skousen, 1992). In machine learning terms, it can be characterized as a lazy learning method, since it defers processing of input un til needed and processes input by combining stored data (Aha, 1997). Memory-based learning has been successfully applied to a number of problems in natural languageprocessing, such as grapheme-to-phoneme conver sion, part-of-speech tagging, prepositional-phraseattachment, and base noun phrase chunking (Daele mans et al, 2002). Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). For the experiments reported in this paper, we have used the software package TiMBL (TilburgMemory Based Learner), which provides a vari ety of metrics, algorithms, and extra functions on top of the classical k nearest neighbor classification kernel, such as value distance metrics and distance weighted class voting (Daelemans et al, 2003).The function we want to approximate is a map ping f from configurations to parser actions, where each action consists of a transition and (except for Shift and Reduce) a dependency type: f : Config ? {LA,RA,RE,SH} ? (R ? {nil}) Here Config is the set of all configurations and R is the set of dependency types. In order to make theproblem tractable, we approximate f with a func tion f? whose domain is a finite space of parser states, which are abstractions over configurations. For this purpose we define a number of features that can be used to define different models of parser state. Figure 2 illustrates the features that are used to define parser states in the present study. The two central elements in any configuration are the token on top of the stack (T) and the next input token(N), the tokens which may be connected by a de pendency arc in the next configuration. For these tokens, we consider both the word form (T.LEX, N.LEX) and the part-of-speech (T.POS, N.POS), as assigned by an automatic part-of-speech tagger ina preprocessing phase. Next, we consider a selection of dependencies that may be present in the cur rent arc relation, namely those linking T to its head (TH) and its leftmost and rightmost dependent (TL, TR), and that linking N to its leftmost dependent (NL),3 considering both the dependency type (arclabel) and the part-of-speech of the head or depen dent. Finally, we use a lookahead of three tokens, considering only their parts-of-speech. We have experimented with two different statemodels, one that incorporates all the features depicted in Figure 2 (Model 1), and one that ex cludes the parts-of-speech of TH, TL, TR, NL (Model 2). Models similar to model 2 have been found towork well for datasets with a rich annotation of de pendency types, such as the Swedish dependency treebank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). Model 1 can be expected towork better for datasets with less informative dependency annotation, such as dependency trees ex tracted from the Penn Treebank, where the extra part-of-speech features may compensate for the lack of information in arc labels. The learning algorithm used is the IB1 algorithm (Aha et al, 1991) with k = 5, i.e. classification basedon 5 nearest neighbors.4 Distances are measured us ing the modified value difference metric (MVDM) (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) for instances with a frequency of at least 3 (andthe simple overlap metric otherwise), and classifica tion is based on distance weighted class voting with inverse distance weighting (Dudani, 1976). Thesesettings are the result of extensive experiments partially reported in Nivre et al (2004). For more infor mation about the different parameters and settings, see Daelemans et al (2003). 4 Experiments. The data set used for experimental evaluation is the standard data set from the Wall Street Journal section of the Penn Treebank, with sections 2?21 3Given the parsing algorithm, N can never have a head or a right dependent in the current configuration.4In TiMBL, the value of k in fact refers to k nearest dis tances rather than k nearest neighbors, which means that, evenwith k = 1, the nearest neighbor set can contain several instances that are equally distant to the test instance. This is dif ferent from the original IB1 algorithm, as described in Aha et al. (1991). used for training and section 23 for testing (Collins,1999; Charniak, 2000). The data has been converted to dependency trees using head rules (Magerman, 1995; Collins, 1996). We are grateful to Ya mada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used byCollins (1999). This permits us to make exact com parisons with the parser of Yamada and Matsumoto (2003), but also the parsers of Collins (1997) and Charniak (2000), which are evaluated on the same data set in Yamada and Matsumoto (2003).One problem that we had to face is that the standard conversion of phrase structure trees to de pendency trees gives unlabeled dependency trees, whereas our parser requires labeled trees. Since the annotation scheme of the Penn Treebank does notinclude dependency types, there is no straightfor ward way to derive such labels. We have therefore experimented with two different sets of labels, none of which corresponds to dependency types in a strict sense. The first set consists of the function tags forgrammatical roles according to the Penn II annota tion guidelines (Bies et al, 1995); we call this set G.The second set consists of the ordinary bracket la bels (S, NP, VP, etc.), combined with function tags for grammatical roles, giving composite labels such as NP-SBJ; we call this set B. We assign labels to arcs by letting each (non-root) word that heads aphrase P in the original phrase structure have its in coming edge labeled with the label of P (modulo the set of labels used). In both sets, we also includea default label DEP for arcs that would not other wise get a label. This gives a total of 7 labels in the G set and 50 labels in the B set. Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP. We use the following metrics for evaluation: 1. Unlabeled attachment score (UAS): The pro-. portion of words that are assigned the correct head (or no head if the word is a root) (Eisner, 1996; Collins et al, 1999). 2. Labeled attachment score (LAS): The pro-. portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). 3. Dependency accuracy (DA): The proportion. of non-root words that are assigned the correct head (Yamada and Matsumoto, 2003). 4. Root accuracy (RA): The proportion of root. words that are analyzed as such (Yamada and Matsumoto, 2003).","n onto the stack, giving the configuration ?n|S,I,A?. 2We use nil to denote the empty list and a|A to denote a list with head a and tail A. TH.POS   ? T.DEP . . . TL.POS   ? TL.DEP . . . T.POS T.LEX   ? TR.DEP . . . TR.POS . . . NL.POS   ? NL.DEP . . . N.POS N.LEX L1.POS L2.POS L3.POS T = Top of the stack N = Next input token TL = Leftmost dependent of T TR = Rightmost dependent of T NL = Leftmost dependent of N Li = Next plus i input token X.LEX = Word form of X X.POS = Part-of-speech of X X.DEP = Dependency type of X Figure 2: Parser state featuresAfter initialization, the parser is guaranteed to ter minate after at most 2n transitions, given an input string of length n (Nivre, 2003). Moreover, the parser always constructs a dependency graph that isacyclic and projective. This means that the depen dency graph given at termination is well-formed if and only if it is connected (Nivre, 2003). Otherwise, it is a set of connected components, each of which is a well-formed dependency graph for a substring of the original input.The transition system defined above is nondeterministic in itself, since several transitions can often be applied in a given configuration. To con struct deterministic parsers based on this system,we use classifiers trained on treebank data in or der to predict the next transition (and dependency type) given the current configuration of the parser. In this way, our approach can be seen as a form ofhistory-based parsing (Black et al, 1992; Mager man, 1995). In the experiments reported here, we use memory-based learning to train our classifiers. 3 Memory-Based Learning. Memory-based learning and problem solving is based on two fundamental principles: learning is thesimple storage of experiences in memory, and solv ing a new problem is achieved by reusing solutionsfrom similar previously solved problems (Daele mans, 1999). It is inspired by the nearest neighborapproach in statistical pattern recognition and arti ficial intelligence (Fix and Hodges, 1952), as well as the analogical modeling approach in linguistics(Skousen, 1989; Skousen, 1992). In machine learning terms, it can be characterized as a lazy learning method, since it defers processing of input un til needed and processes input by combining stored data (Aha, 1997). Memory-based learning has been successfully applied to a number of problems in natural languageprocessing, such as grapheme-to-phoneme conver sion, part-of-speech tagging, prepositional-phraseattachment, and base noun phrase chunking (Daele mans et al, 2002). Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). For the experiments reported in this paper, we have used the software package TiMBL (TilburgMemory Based Learner), which provides a vari ety of metrics, algorithms, and extra functions on top of the classical k nearest neighbor classification kernel, such as value distance metrics and distance weighted class voting (Daelemans et al, 2003).The function we want to approximate is a map ping f from configurations to parser actions, where each action consists of a transition and (except for Shift and Reduce) a dependency type: f : Config ? {LA,RA,RE,SH} ? (R ? {nil}) Here Config is the set of all configurations and R is the set of dependency types. In order to make theproblem tractable, we approximate f with a func tion f? whose domain is a finite space of parser states, which are abstractions over configurations. For this purpose we define a number of features that can be used to define different models of parser state. Figure 2 illustrates the features that are used to define parser states in the present study. The two central elements in any configuration are the token on top of the stack (T) and the next input token(N), the tokens which may be connected by a de pendency arc in the next configuration. For these tokens, we consider both the word form (T.LEX, N.LEX) and the part-of-speech (T.POS, N.POS), as assigned by an automatic part-of-speech tagger ina preprocessing phase. Next, we consider a selection of dependencies that may be present in the cur rent arc relation, namely those linking T to its head (TH) and its leftmost and rightmost dependent (TL, TR), and that linking N to its leftmost dependent (NL),3 considering both the dependency type (arclabel) and the part-of-speech of the head or depen dent. Finally, we use a lookahead of three tokens, considering only their parts-of-speech. We have experimented with two different statemodels, one that incorporates all the features depicted in Figure 2 (Model 1), and one that ex cludes the parts-of-speech of TH, TL, TR, NL (Model 2). Models similar to model 2 have been found towork well for datasets with a rich annotation of de pendency types, such as the Swedish dependency treebank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). Model 1 can be expected towork better for datasets with less informative dependency annotation, such as dependency trees ex tracted from the Penn Treebank, where the extra part-of-speech features may compensate for the lack of information in arc labels. The learning algorithm used is the IB1 algorithm (Aha et al, 1991) with k = 5, i.e. classification basedon 5 nearest neighbors.4 Distances are measured us ing the modified value difference metric (MVDM) (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) for instances with a frequency of at least 3 (andthe simple overlap metric otherwise), and classifica tion is based on distance weighted class voting with inverse distance weighting (Dudani, 1976). Thesesettings are the result of extensive experiments partially reported in Nivre et al (2004). For more infor mation about the different parameters and settings, see Daelemans et al (2003). 4 Experiments. The data set used for experimental evaluation is the standard data set from the Wall Street Journal section of the Penn Treebank, with sections 2?21 3Given the parsing algorithm, N can never have a head or a right dependent in the current configuration.4In TiMBL, the value of k in fact refers to k nearest dis tances rather than k nearest neighbors, which means that, evenwith k = 1, the nearest neighbor set can contain several instances that are equally distant to the test instance. This is dif ferent from the original IB1 algorithm, as described in Aha et al. (1991). used for training and section 23 for testing (Collins,1999; Charniak, 2000). The data has been converted to dependency trees using head rules (Magerman, 1995; Collins, 1996). We are grateful to Ya mada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used byCollins (1999). This permits us to make exact com parisons with the parser of Yamada and Matsumoto (2003), but also the parsers of Collins (1997) and Charniak (2000), which are evaluated on the same data set in Yamada and Matsumoto (2003).One problem that we had to face is that the standard conversion of phrase structure trees to de pendency trees gives unlabeled dependency trees, whereas our parser requires labeled trees. Since the annotation scheme of the Penn Treebank does notinclude dependency types, there is no straightfor ward way to derive such labels. We have therefore experimented with two different sets of labels, none of which corresponds to dependency types in a strict sense. The first set consists of the function tags forgrammatical roles according to the Penn II annota tion guidelines (Bies et al, 1995); we call this set G.The second set consists of the ordinary bracket la bels (S, NP, VP, etc.), combined with function tags for grammatical roles, giving composite labels such as NP-SBJ; we call this set B. We assign labels to arcs by letting each (non-root) word that heads aphrase P in the original phrase structure have its in coming edge labeled with the label of P (modulo the set of labels used). In both sets, we also includea default label DEP for arcs that would not other wise get a label. This gives a total of 7 labels in the G set and 50 labels in the B set. Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP. We use the following metrics for evaluation: 1. Unlabeled attachment score (UAS): The pro-. portion of words that are assigned the correct head (or no head if the word is a root) (Eisner, 1996; Collins et al, 1999). 2. Labeled attachment score (LAS): The pro-. portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). 3. Dependency accuracy (DA): The proportion. of non-root words that are assigned the correct head (Yamada and Matsumoto, 2003). 4. Root accuracy (RA): The proportion of root. words that are analyzed as such (Yamada and Matsumoto, 2003)."
58,"We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.","We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata. The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction. In particular, we focus on several methodological issues concerning the annotation of non-configurational languages. In section 2, we examine the appropriateness of existing annotation schemes. On the basis of these considerations, we formulate several additional requirements. A formalism complying with these requirements is described in section 3. Section 4 deals with the treatment of selected phenomena. For a description of the annotation tool see section 5. Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods. Realworld texts annotated with different strata of linguistic information can be used for grammar induction. The data-drivenness of this approach presents a clear advantage over the traditional, idealised notion of competence grammar. Corpora annotated with syntactic structures are commonly referred to as trctbank.5. Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained. Theory-independence: Annotations should not be influenced by theory-specific considerations. Nevertheless, different theory-specific representations shall be recoverable from the annotation, cf. (Marcus et al., 1994). Multi-stratal representation: Clear separation of different description levels is desirable. Data-drivenness: The scheme must provide representational means for all phenomena occurring in texts. Disambiguation is based on human processing skills (cf. (Marcus et. al., 1994), (Sampson, 1995), (Black et. al. , 1996)). The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies. The underlying argument SirlteilITC is not represented directly, but can be recovered from the tree and trace-filler annotations. Syntactic category is encoded in node labels. Grammatical functions constitute a complex label system (cf. (Bies et al., 1995), (Sampson, 1995)). Part-of-Speech is annotated at word level. Thus the context-free constituent backbone plays a pivotal role in the annotation scheme. Due to the substantial differences between existing models of constituent structure, the question arises of how the theory independencf requirement, can be satisfied. At, this point the importance of the underlying argument structure is emphasised (cf. (Lehmann et al., 1996), (Marcus et al., 1994), (Sampson, 1995)). Treebanks of the format, described in the above section have been designed for English. Therefore, the solutions they offer are not always optimal for other language types. As for free word order languages, the following features may cause problems: sition between the two poles. In light of these facts, serious difficulties can be expected arising from the structural component of the existing formalisms. Due to the frequency of discontinuous constituents in non-configurational languages, the filler-trace mechanism would be used very often, yielding syntactic trees fairly different from the underlying predicate-argument structures. Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes. This hybrid representation makes the structure less transparent, and therefore more difficult to annotate. Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious. Since most methods of handling discontinuous constituents make the formalism more powerful, the efficiency of processing deteriorates, too. An alternative solution is to make argument structure the main structural component of the formalism. This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union, extraposition, diverse second-position phenomena). Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, 'flat' trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions, e.g. every syntactic structure has a unique head. Thus, notions such as head should be distinguished at the level of syntactic functions rather than structures. This requirement speaks against the traditional sort of dependency trees, in which heads a,re represented as non-terminal nodes, cf. (Hudson, 1984). A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen, dais er 'vein! Such a word order independent representation has the advantage of all structural information being encoded in a single data structure. A uniform representation of local and non-local dependencies makes the structure more transparent'. We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node. Argument structure, represented in terms of unordered trees. Grammatical functions, encoded in edge labels, e.g. SB (subject), MO (modifier), HD (head). Syntactic categories, expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals. A structure for (2) is shown in fig. 2. (2) schade, daB kein Arzt anwesend ist, der pity that no doctor present is who sich a.uskennt is competent 'Pity that no competent doctor is here' Note that the root node does not have a head descendant (HD) as the sentence is a predicative construction consisting of a subject (SB) and a predicate (PD) without a copula. The subject is itself a sentence in which the copula (zA) does occur and is assigned the tag HD'. The tree resembles traditional constituent structures. The difference is its word order independence: structural units (&quot;phrases&quot;) need not be contiguous substrings. For instance, the extraposed relative clause (RC) is still treated as part of the subject NP. As the annotation scheme does not distinguish different bar levels or any similar intermediate categories, only a small set of node labels is needed (currently 16 tags, S, NP, AP ...). Due to the rudimentary character of the argument structure representations, a great deal of information has to be expressed by grammatical functions. Their further classification must reflect different kinds of linguistic information: morphology (e.g., case, inflection), category, dependency type (complementation vs. modification), thematic role, etc.' However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation. In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714. While in the first phase each annotator has to annotate structures as well as categories and functions, the refinement call be done separately for each representation level. During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc. Modifiers are assigned the label MO (further classification with respect to thematic roles is planned). Separate labels are defined for dependencies that do not fit the complement/modifier dichotomy, e.g., pre- (GL) and postnominal genitives (GR). Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD. Morphological information: Another set of labels represents morphological information. PM stands for morphological particle, a label for German infinitival Z7t and superlative am. Separable verb prefixes are labeled SVP. During the second annotation stage, the annotation is enriched with information about thematic roles, quantifier scope and anaphoric reference. As already mentioned, this is done separately for each of the three information areas. A phrase or a lexical item can perform multiple functions in a sentence. Consider (qui verbs where the subject of the infinitival VP is not realised syntactically, but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt,n). In such cases, an additional edge is drawn from the embedded VP node to the controller, thus changing the syntactic tree into a graph. We call such additional edges secondary links and represent them as dotted lines, see fig. 4, showing the structure of (3). As theory-independence is one of our objectives, the annotation scheme incorporates a number of widely accepted linguistic analyses, especially in the area of verbal, adverbial and adjectival syntax. However, some other standard analysts turn out to be problematic, mainly due to the partial, idealised character of competence grammars, which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions, appositions, temporal expressions, etc. In the following paragraphs, we give annotations for a number of such phenomena.. Most linguistic theories treat NPs as structures headed by a unique lexical item (noun). However, this idealised model needs several additional assumptions in order to account for such important phenomena as complex nominal NP components (cf. (4)) or nominalised adjectives (cf. (5)). In (4), different theories make different headedness predictions. In (5), either a lexical nominalisation rule for the adjective Gliickliche is stipulated, or the existence of an empty nominal head. Moreover, the so-called DP analysis views the article der as the head of the phrase. Further differences concern the attachment of the degree modifier sehr. Because of the intended theory-independence of the scheme, we annotate only the common minimum. We distinguish an NP kernel consisting of determiners, adjective phrases and nouns. All components of this kernel are assigned the label NK and treated as sibling nodes. The difference between the particular NK's lies in the positional and part-of-speech information, which is also sufficient to recover theory-specific structures from our `underspecified' representations. For instance, the first, determiner among the NK's can be treated as the specifier of the phrase. The head of the phrase can be determined in a similar way according to theory-specific assumptions. In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC). They are all treated as siblings of NK's regardless of their position (in situ or extraposed). Adjunct attachment often gives rise to structural ambiguities or structural uncertainty. However, full or partial disambiguation takes place in context, and the annotators do not consider unrealistic readings. In addition, we have adopted a simple convention for those cases in which context information is insufficient, for total disambiguation: the highest possible attachment, site is chosen. A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable, cf. focus particles such as only or also. If the scope of such a word does not directly correspond to a tree node, the word is attached to the lowest node dominating all subconstituents appearing in its scope. A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language, i.e. phenomena such as coordination and ellipsis. Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types, we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links. Fig. 3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here, where C stands for coordination, and VP for the actual category. This extra marking makes it easy to distinguish between 'normal' and coordinated categories. Multiple coordination as well as enumerations are annotated in the same way. An explicit coordinating conjunction need not be present. Structure-sharing is expressed using secondary links. The development of linguistically interpreted corpora presents a laborious and time-consuming task. In order to make the annotation process more efficient, extra effort has been put. into the development of an annotation tool. The tool supports immediate graphical feedback and automatic error checking. Since our scheme permits crossing edges, visualisation as bracketing and indentation would be insufficient.. Instead, the complete structure should be represented. The tool should also permit a convenient handling of node and edge labels. In particular, variable tagsets and label collections should be allowed. As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages. In the first phase, the main functionality for building and displaying unordered trees is supplied. In the second phase, secondary links and additional structural functions are supported. The implementation of the first phase as described in the following paragraphs is completed. As keyboard input, is more efficient than mouse input (cf. (Lehmann et al., 1996)) most effort, has been put in developing an efficient keyboard interface. Menus are supported as a useful way of getting help on commands and labels. In addition to pure annotation, we can attach comments to structures. Figure 1 shows a screen dump of the tool. The largest part of the window contains the graphical representation of the structure being annotated. The following commands are available: The three tagsets used by the annotation tool (for words, phrases, and edges) are variable and are stored together with the corpus. This allows easy modification if needed. The tool checks the appropriateness of the input. For the implementation, we used Tcl/Tk Version 4.1. The corpus is stored in a SQL database. The degree of automation increases with the amount of data available. Sentences annotated in previous steps are used as training material for further processing. We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated. This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above). Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). For a phrase Q with children of type T„..., Ta and grammatical functions G„...,GA., we use the lexical probabilities and the contextual (trigram) probabilities The lexical and contextual probabilities are determined separately for each type of phrase. During annotation, the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure, i.e., we calculate argma.x11 PQ (Ti 1Z-1, Ti.-2) PQ (Gi ITi). To keep the human annotator from missing errors made by the tagger, we additionally calculate the strongest competitor for each label G. If its probability is close to the winner (closeness is defined by a threshold on the quotient), the assignment is regarded as unreliable, and the annotator is asked to confirm the assignment. For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%). The procedure was repeated 10 times with different. partitionings. The tagger rates 90% of all assignments as reliable and carries them out fully automatically. Accuracy for these cases is 97%. Most errors are due to wrong identification of the subject and different kinds of objects in sentences and VPs. Accuracy of the unreliable 10% of assignments is 75%, i.e., the annotator has to alter the choice in 1 of 4 cases when asked for confirmation. Overall accuracy of the tagger is 95%. Owing to the partial automation, the average annotation efficiency improves by 25% (from around 4 minutes to 3 minutes per sentence). As the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects. These differences can be illustrated by a comparison with the Penn Treebank annotation scheme. The following features of our formalism are then of particular importance: The current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future. We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order. In general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories. As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data., interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research. In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts. Syntactically annotated corpora of German have been missing until now. In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created. We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. Since the combinatorics of syntactic constructions creates a. demand for very large corpora., efficiency of annotation is an important. criterion for the success of the developed methodology and tools. Our annotation tool supplies efficient manipulation and immediate visualization of argument structures. Partial automation included in the current version significantly reduces the manna.1 effort. Its extension is subject to further investigations.","As the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects. These differences can be illustrated by a comparison with the Penn Treebank annotation scheme. The following features of our formalism are then of particular importance: The current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future. We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order. In general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories. As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data., interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research. In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts. Syntactically annotated corpora of German have been missing until now. In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created. We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. Since the combinatorics of syntactic constructions creates a. demand for very large corpora., efficiency of annotation is an important. criterion for the success of the developed methodology and tools. Our annotation tool supplies efficient manipulation and immediate visualization of argument structures. Partial automation included in the current version significantly reduces the manna.1 effort. Its extension is subject to further investigations."
59,An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses forlarge treebank grammars and long input sen tences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.,"An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses forlarge treebank grammars and long input sen tences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one. Large context-free grammars extracted from tree banks achieve high coverage and accuracy, but they are difficult to parse with because of their massive ambiguity. The application of standard chart-parsing techniques often fails due to excessive memory and runtime requirements.Treebank grammars are mostly used as probabilis tic grammars and users are usually only interested in the best analysis, the Viterbi parse. To speed up Viterbi parsing, sophisticated search strategies havebeen developed which find the most probable anal ysis without examining the whole set of possible analyses (Charniak et al, 1998; Klein and Manning,2003a). These methods reduce the number of gener ated edges, but increase the amount of time needed for each edge. The parser described in this paper follows a contrary approach: instead of reducing the number of edges, it minimises the costs of building edges in terms of memory and runtime.The new parser, called BitPar, is based on a bit vector implementation (cf. (Graham et al, 1980)) of the well-known Cocke-Younger-Kasami (CKY) algorithm (Kasami, 1965; Younger, 1967). It buildsa compact ?parse forest? representation of all anal yses in two steps. In the first step, a CKY-style recogniser fills the chart with constituents. In the second step, the parse forest is built top-down from the chart. Viterbi parses are computed in four steps. Again, the first step is a CKY recogniser which is followed by a top-down filtering of the chart, the bottom-up computation of the Viterbi probabilities, and the top-down extraction of the best parse.The rest of the paper is organised as follows: Sec tion 2 explains the transformation of the grammar to Chomsky normal form. The following sectionsdescribe the recogniser algorithm (Sec. 3), improvements of the recogniser by means of bit-vector op erations (Sec. 4), and the generation of parse forests(Sec. 5), and Viterbi parses (Sec. 6). Section 7 discusses the advantages of the new architecture, Sec tion 8 describes experimental results, and Section 9 summarises the paper.","Large context-free grammars extracted from tree banks achieve high coverage and accuracy, but they are difficult to parse with because of their massive ambiguity. The application of standard chart-parsing techniques often fails due to excessive memory and runtime requirements.Treebank grammars are mostly used as probabilis tic grammars and users are usually only interested in the best analysis, the Viterbi parse. To speed up Viterbi parsing, sophisticated search strategies havebeen developed which find the most probable anal ysis without examining the whole set of possible analyses (Charniak et al, 1998; Klein and Manning,2003a). These methods reduce the number of gener ated edges, but increase the amount of time needed for each edge. The parser described in this paper follows a contrary approach: instead of reducing the number of edges, it minimises the costs of building edges in terms of memory and runtime.The new parser, called BitPar, is based on a bit vector implementation (cf. (Graham et al, 1980)) of the well-known Cocke-Younger-Kasami (CKY) algorithm (Kasami, 1965; Younger, 1967). It buildsa compact ?parse forest? representation of all anal yses in two steps. In the first step, a CKY-style recogniser fills the chart with constituents. In the second step, the parse forest is built top-down from the chart. Viterbi parses are computed in four steps. Again, the first step is a CKY recogniser which is followed by a top-down filtering of the chart, the bottom-up computation of the Viterbi probabilities, and the top-down extraction of the best parse.The rest of the paper is organised as follows: Sec tion 2 explains the transformation of the grammar to Chomsky normal form. The following sectionsdescribe the recogniser algorithm (Sec. 3), improvements of the recogniser by means of bit-vector op erations (Sec. 4), and the generation of parse forests(Sec. 5), and Viterbi parses (Sec. 6). Section 7 discusses the advantages of the new architecture, Sec tion 8 describes experimental results, and Section 9 summarises the paper."
60,"Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we showthat an NE recognizer based on Support Vector Ma chines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster.This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selec tion method and an efficient training method.","Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we showthat an NE recognizer based on Support Vector Ma chines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster.This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selec tion method and an efficient training method. Named Entity (NE) recognition is a task in whichproper nouns and numerical information in a docu ment are detected and classified into categories suchas person, organization, and date. It is a key technol ogy of Information Extraction and Open-Domain Question Answering (Voorhees and Harman, 2000). We are building a trainable Open-Domain Question Answering System called SAIQA-II. In this paper, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores thanconventional systems. SVMs have given high per formance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). However, it turned out that off-the-shelf SVM classifiers are too inefficient for NE recognition. The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001)) can process several kilobytes in a second. The major reason is the inefficiency of SVM classifiers. There are otherreports on the slowness of SVM classifiers. Another SVM-based NE recognizer (Yamada and Mat sumoto, 2001) is 0.8 sentences/sec on a Pentium III 933 MHz PC. An SVM-based part-of-speech (POS). tagger (Nakagawa et al, 2001) is 20 tokens/sec on an Alpha 21164A 500 MHz processor. It is difficult to use such slow systems in practical applications. In this paper, we present a method that makes the NE system substantially faster. This method can also be applied to other tasks in natural languageprocessing such as chunking and POS tagging. Another problem with SVMs is its incomprehensibil ity. It is not clear which features are important or how they work. The above method is also useful for finding useless features. We also mention a method to reduce training time. 1.1 Support Vector Machines. Suppose we have a set of training data for a two class problem:     , where   ffflfi is a feature vector of the ffi -th sample in the training data and   !$#%# is the label forthe sample. The goal is to find a decision func tion that accurately predicts for unseen  . A non-linear SVM classifier gives a decision function ( ) * sign ,+-) for an input vector  where +-) .* / 0 21)3 546879: !6; Here, () *=!$# means  is a member of a cer tain class and () $* # means  is not a mem ber. 7 s are called support vectors and are repre sentatives of training examples.  is the numberof support vectors. Therefore, computational com plexity of +?) is proportional to  . Support vectorsand other constants are determined by solving a cer tain quadratic programming problem. 4687@ is akernel that implicitly maps vectors into a higher di mensional space. Typical kernels use dot products: 4687@ A*CBED7@ . A polynomial kernel of degree Fis given by BG? *HI#J!KG L . We can use vari MM M M N M M M M M M M M M N M O O O O O N O O O O O O O O O O O O M : positive example, O : negative example N M , N O : support vectors Figure 1: Support Vector Machine ous kernels, and the design of an appropriate kernel for a particular application is an important research issue.Figure 1 shows a linearly separable case. The de cision hyperplane defined by +-) P*RQ separatespositive and negative examples by the largest mar gin. The solid line indicates the decision hyperplaneand two parallel dotted lines indicate the margin be tween positive and negative examples. Since such aseparating hyperplane may not exist, a positive pa rameter S is introduced to allow misclassifications. See Vapnik (1995). 1.2 SVM-based NE recognition. As far as we know, the first SVM-based NE system was proposed by Yamada et al (2001) for Japanese.His system is an extension of Kudo?s chunking sys tem (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. In theirsystem, every word in a sentence is classified sequentially from the beginning or the end of a sen tence. However, since Yamada has not compared it with other methods under the same conditions, it is not clear whether his NE system is better or not. Here, we show that our SVM-based NE system ismore accurate than conventional systems. Our sys tem uses the Viterbi search (Allen, 1995) instead of sequential determination.For training, we use ?CRL data?, which was prepared for IREX (Information Retrieval and Extrac tion Exercise1, Sekine and Eriguchi (2000)). It has about 19,000 NEs in 1,174 articles. We also use additional data by Isozaki (2001). Both datasets are based on Mainichi Newspaper?s 1994 and 1995 CD-ROMs. We use IREX?s formal test data calledGENERAL that has 1,510 named entities in 71 ar ticles from Mainichi Newspaper of 1999. Systems are compared in terms of GENERAL?s F-measure 1http://cs.nyu.edu/cs/projects/proteus/irexwhich is the harmonic mean of ?recall? and ?preci sion? and is defined as follows. Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.We developed an SVM-based NE system by following our NE system based on maximum entropy (ME) modeling (Isozaki, 2001). We sim ply replaced the ME model with SVM classifiers.The above datasets are processed by a morphological analyzer ChaSen 2.2.12. It tokenizes a sen tence into words and adds POS tags. ChaSen uses about 90 POS tags such as common-noun and location-name. Since most unknown words are proper nouns, ChaSen?s parameters for unknownwords are modified for better results. Then, a char acter type tag is added to each word. It uses 17character types such as all-kanji and small integer. See Isozaki (2001) for details. Now, Japanese NE recognition is solved by theclassification of words (Sekine et al, 1998; Borth wick, 1999; Uchimoto et al, 2000). For instance, the words in ?President George Herbert Bush saidClinton is . . . are classified as follows: ?President? = OTHER, ?George? = PERSON-BEGIN, ?Her bert? = PERSON-MIDDLE, ?Bush? = PERSON-END, ?said? = OTHER, ?Clinton? = PERSON-SINGLE, ?is? = OTHER. In this way, the first word of a person?s name is labeled as PERSON-BEGIN. The last word is labeled as PERSON-END. Other words in the nameare PERSON-MIDDLE. If a person?s name is expressed by a single word, it is labeled as PERSON SINGLE. If a word does not belong to any namedentities, it is labeled as OTHER. Since IREX de fines eight NE classes, words are classified into 33 ( *UTWVEX!K# ) categories.Each sample is represented by 15 features be cause each word has three features (part-of-speech tag, character type, and the word itself), and two preceding words and two succeeding words are also used for context dependence. Although infrequent features are usually removed to prevent overfitting, we use all features because SVMs are robust. Each sample is represented by a long binary vector, i.e., a sequence of 0 (false) and 1 (true). For instance, ?Bush? in the above example is represented by a 2http://chasen.aist-nara.ac.jp/ vector P*YG[Z\#^]_ G[Z `a] described below. Only 15 elements are 1. bdcfe8ghji // Current word is not ?Alice? bdc klghme // Current word is ?Bush? bdc nghji // Current word is not ?Charlie? : bdcfe^opikpqpghme // Current POS is a proper noun bdcfe^opinipghji // Current POS is not a verb : bdc nqre^sre ghji // Previous word is not ?Henry? bdc nqre^skghme // Previous word is ?Herbert? :Here, we have to consider the following problems. First, SVMs can solve only a two-class problem. Therefore, we have to reduce the above multi class problem to a group of two-class problems. Second, we have to consider consistency among word classes in a sentence. For instance, a word classified as PERSON-BEGIN should be followed by PERSON-MIDDLE or PERSON-END. It impliesthat the system has to determine the best combina tions of word classes from numerous possibilities.Here, we solve these problems by combining exist ing methods. There are a few approaches to extend SVMs to cover t -class problems. Here, we employ the ?oneclass versus all others? approach. That is, each clas sifier (%u ) is trained to distinguish members of a class v from non-members. In this method, two or more classifiers may give !$# to an unseen vector or no classifier may give !$# . One common way to avoid such situations is to compare + u ) values and to choose the class index v of the largest + u ) . The consistency problem is solved by the Viterbi search. Since SVMs do not output probabilities, we use the SVM+sigmoid method (Platt, 2000). That is, we use a sigmoid function wxG? J*y#zI#{! |l}~ {G to map + u ) to a probability-like value. The output of the Viterbi search is adjusted by a postprocessor for wrong word boundaries. The adjustment rules are also statistically determined (Isozaki, 2001). 1.3 Comparison of NE recognizers. We use a fixed value ?* #Q9Q . F-measures are not very sensitive to  unless  is too small. Whenwe used 1,038,986 training vectors, GENERAL?s F measure was 89.64% for ?*?Q?# and 90.03% for 6*?#Q9Q . We employ the quadratic kernel ( F *Y? ) because it gives the best results. Polynomial kernels of degree 1, 2, and 3 resulted in 83.03%, 88.31%, F-measure (%) ? ? RG+DT ? ? ME ? ? SVM 0 20 40 60 80 100 120 CRL data ???E? ?^??:??? 76 78 80 82 84 86 88 90 Number of NEs in training data ( ?? ) Figure 2: F-measures of NE systems and 87.04% respectively when we used 569,994 training vectors. Figure 2 compares NE recognizers in terms ofGENERAL?s F-measures. ?SVM? in the figure in dicates F-measures of our system trained by Kudo?s TinySVM-0.073 with S?*?Q?# . It attained 85.04% when we used only CRL data. ?ME? indicates our ME system and ?RG+DT? indicates a rule-basedmachine learning system (Isozaki, 2001). According to this graph, ?SVM? is better than the other sys tems.However, SVM classifiers are too slow. Fa mous SVM-Light 3.50 (Joachims, 1999) took 1.2 days to classify 569,994 vectors derived from 2 MB documents. That is, it runs at only 19 bytes/sec. TinySVM?s classifier seems best optimized among publicly available SVM toolkits, but it still works at only 92 bytes/sec. In this section, we investigate the cause of this in efficiency and propose a solution. All experiments are conducted for training data of 569,994 vectors. The total size of the original news articles was 2 MB and the number of NEs was 39,022. According to the definition of +-) , a classifier has to process support vectors for each  . Table 1 shows  s for different word classes. According to this table, classi fication of one word requires  ?s dot products with 228,306 support vectors in 33 classifiers. Therefore, the classifiers are very slow. We have never seensuch large  s in SVM literature on pattern recogni tion. The reason for the large  s is word features. Inother domains such as character recognition, dimen 3http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM sion ` is usually fixed. However, in the NE task, ` increases monotonically with respect to the size of the training data. Since SVMs learn combinations of features,  tends to be very large. This tendencywill hold for other tasks of natural language pro cessing, too. Here, we focus on the quadratic kernel BG * I#!?G ? that yielded the best score in the above experiments. Suppose ?* G[Z\#^]_ G[Z `a] hasonly ? (=15) non-zero elements. The dot prod uct of  and 7  * 5?  Z\#^]_ ?  Z `] is given by ? fi ? 1) G[Z??  Z?? ] . Hence, I#!??D?7  ? *?#!W? fi 0 ? 1) G?Z??  Z???]!? fi 0 ? 1) G?Z??  Z???] ?  We can rewrite +-) as follows. fi 0 ? 1) _?  Z?? ]?G[Z???]?!m? ? Z???]?G[Z???] fi.?  0 ? 1) fi 0 ? 1 ??  ???rZ??? B@]?G[Z?? ]?G?Z?B@]_ where ? ? / ?1) 3 ? ??Z?? / ?1) 3 5? Z?? ]_ ? ? Z?? ]?* ? / ?1) 3 ??p8Z?? ]??% ?P?rZ?? B@]?* ? ? / ?1) 3  ?  Z??  Z?B@]_ For binary vectors, it can be simplified as +-) .*?? 0 ??,?9? ?l? 1) _?C?  Z???] 0 ?-?,????%? ?9? 1) ? ? Z?? B@]  where ? ?  Z???]?* ?  Z???] !m? ? Z???]Y* ? 0  ???5? ?l? 1) 3   ???9Z??? B@]?* ? 0  ?,???_? ?l? 1 ????? 1) 3   Now, +?) can be given by summing up ? ?  Z???] for every non-zero element G?Z?? ] and ? ? Z?? B@] for every non-zero pair G?Z?? ]?G[Z?B@] . Accordingly, we only need to add #W!???!??j?R?# z%? (=121) con stants to get +-) . Therefore, we can expect thismethod to be much faster than a na??ve implementa tion that computes tens of thousands of dot products at run time. We call this method ?XQK? (eXpand the Quadratic Kernel). Table 1 compares TinySVM and XQK in terms of CPU time taken to apply 33 classifiers to process the training data. Classes are sorted by  . Small numbers in parentheses indicate the initializationtime for reading support vectors 7   and allocat ing memory. XQK requires a longer initialization time in order to prepare ? ?  and ??? For instance,TinySVM took 11,490.26 seconds (3.2 hours) in to tal for applying OTHER?s classifier to all vectors in the training data. Its initialization phase took 2.13 seconds and all vectors in the training data were classified in 11,488.13 ( *=#9#%X?%Q??9????x?#p? ) sec onds. On the other hand, XQK took 225.28 secondsin total and its initialization phase took 174.17 sec onds. Therefore, 569,994 vectors were classified in51.11 seconds. The initialization time can be disre garded because we can reuse the above coefficents. Consequently, XQK is 224.8 (=11,488.13/51.11) times faster than TinySVM for OTHER. TinySVM took 6 hours to process all the word classes, whereas XQK took only 17 minutes. XQK is 102 times faster than SVM-Light 3.50 which took 1.2 days. XQK makes the classifiers faster, but mem ory requirement increases from ? ? / ?1) ?  to ? ? / ?1) ?  ?  !fl# z%?r where ? (=15) is the num ber of non-zero elements in 7  . Therefore, removal. of useless features would be beneficial. Conven tional SVMs do not tell us how an individual feature works because weights are given not to features but to 4687  . However, the above weights ( ? ?  and ??? ) clarify how a feature or a feature pair works. We can use this fact for feature selection after the training. We simplify ( ) by removing all features ? that satisfy ?? } 8??? Z?? ]?f??? } ? ?????rZ??? B@]?f ?? } ? ???P?rZ?B- ?]?? K??? The largest ? that does not change the number of misclassifications for the training data is found by using the binary searchfor each word class. We call this method ?XQKFS? (XQK with Feature Selection). This approx imation slightly degraded GENERAL?s F-measure from 88.31% to 88.03%.Table 2 shows the reduction of features that ap pear in support vectors. Classes are sorted by the numbers of original features. For instance, OTHERhas 56,220 features in its support vectors. Accord ing to the binary search, its performance did notchange even when the number of features was re duced to 21,852 at ?*KQ?Qr?9?r?%? Table 1: Reduction of CPU time (in seconds) by XQK word class  TinySVM (init) XQK (init) speed up SVM-Light OTHER 64,970 11,488.13 (2.13) 51.11 (174.17) 224.8 29,986.52 ARTIFACT-MIDDLE 14,171 1,372.85 (0.51) 41.32 (14.98) 33.2 6,666.26 LOCATION-SINGLE 13,019 1,209.29 (0.47) 38.24 (11.41) 31.6 6,100.54 ORGANIZ..-MIDDLE 12,050 987.39 (0.44) 37.93 (11.70) 26.0 5,570.82 : : : : : : TOTAL 228,306 21,754.23 (9.83) 1,019.20 (281.28) 21.3 104,466.31 Table 2: Reduction of features by XQK-FS word class number of features number of non-zero weights seconds OTHER 56,220 ? 21,852 (38.9%) 1,512,827 ? 892,228 (59.0%) 42.31 ARTIFIFACT-MIDDLE 22,090 ? 4,410 (20.0%) 473,923 ? 164,632 (34.7%) 30.47 LOCATION-SINGLE 17,169 ? 3,382 (19.7%) 366,961 ? 123,808 (33.7%) 27.72 ORGANIZ..-MIDDLE 17,123 ? 9,959 (58.2%) 372,784 ? 263,695 (70.7%) 31.02 ORGANIZ..-END 15,214 ? 3,073 (20.2%) 324,514 ? 112,307 (34.6%) 26.87 : : : : TOTAL 307,721 ? 75,455 (24.5%) 6,669,664 ? 2,650,681 (39.7%) 763.10 The total number of features was reduced by 75%and that of weights was reduced by 60%. The ta ble also shows CPU time for classification by the selected features. XQK-FS is 28.5 (=21754.23/ 763.10) times faster than TinySVM. Although the reduction of features is significant, the reduction of CPU time is moderate, because most of the reducedfeatures are infrequent ones. However, simple re duction of infrequent features without consideringweights damages the system?s performance. For instance, when we removed 5,066 features that ap peared four times or less in the training data, themodified classifier for ORGANIZATION-END misclassified 103 training examples, whereas the origi nal classifier misclassified only 19 examples. On theother hand, XQK-FS removed 12,141 features with out an increase in misclassifications for the training data. XQK can be easily extended to a more generalquadratic kernel BG? ?*??vl??!?v  G ? and to nonbinary sparse vectors. XQK-FS can be used to se lect useful features before training by other kernels. As mentioned above, we conducted an experiment for the cubic kernel ( F *?? ) by using all features.When we trained the cubic kernel classifiers by us ing only features selected by XQK-FS, TinySVM?s classification time was reduced by 40% because  was reduced by 38%. GENERAL?s F-measure was slightly improved from 87.04% to 87.10%. Onthe other hand, when we trained the cubic ker nel classifiers by using only features that appeared three times or more (without considering weights), TinySVM?s classification time was reduced by only 14% and the F-measure was slightly degraded to86.85%. Therefore, we expect XQK-FS to be use ful as a feature selection method for other kernels when such kernels give much better results than the quadratic kernel. Since training of 33 classifiers also takes a longtime, it is difficult to try various combinations of pa rameters and features. Here, we present a solution for this problem. In the training time, calculation of B???Dr   B??$Dr ?  B??D@  for various  ? s is dominant. Conventional systems save time by caching the results. By analyzing TinySVM?s classifier, we found that they can be calculated more efficiently. For sparse vectors, most SVM classifiers (e.g., SVM-Light) use a sparse dot product algorithm (Platt, 1999) that compares non-zero elements of  and those of 7  to get BED7  in +-) . However,  is common to all dot products in B?D7   BD 7/ . Therefore, we can implement a faster classifierthat calculates them concurrently. TinySVM?s clas sifier prepares a list fi2si Z?? ] that contains all 7  s whose ? -th coordinates are not zero. In addition, counters for ?D%7  p ?D%7 / are prepared because dot products of binary vectors are integers. Then, for each non-zero G[Z?? ] , the counters are incremented for all 7   fi2si Z???] By checking only members of fi2si Z?? ] for non-zero G[Z?? ] , the classifier is not bothered by fruitless cases: G?Z?? ]?*?Q ?8Z???]??*YQ orG[Z???]W?*?Q ? ?Z???]?*yQ . Therefore, TinySVM?s clas sifier is faster than other classifiers. This method is applicable to any kernels based on dot products. For the training phase, we can build fi2si ? Z???] that contains all   s whose ? -th coordinates are notzero. Then, B??D   B???D  can be efficiently calculated because ?? is common. This im provement is effective especially when the cache is small and/or the training data is large. When we used a 200 MB cache, the improved system took only 13 hours for training by the CRL data, while TinySVM and SVM-Light took 30 hours and 46hours respectively for the same cache size. Al though we have examined other SVM toolkits, we could not find any system that uses this approach in the training phase. The above methods can also be applied to othertasks in natural language processing such as chunk ing and POS tagging because the quadratic kernels give good results. Utsuro et al (2001) report that a combination of two NE recognizers attained F = 84.07%, butwrong word boundary cases are excluded. Our system attained 85.04% and word boundaries are auto matically adjusted. Yamada (Yamada et al, 2001) also reports that F*?? is best. Although his sys tem attained F = 83.7% for 5-fold cross-validation of the CRL data (Yamada and Matsumoto, 2001), our system attained 86.8%. Since we followedIsozaki?s implementation (Isozaki, 2001), our system is different from Yamada?s system in the fol lowing points: 1) adjustment of word boundaries, 2)ChaSen?s parameters for unknown words, 3) char acter types, 4) use of the Viterbi search. For efficient classification, Burges and Scho?lkopf (1997) propose an approximation method that uses ?reduced set vectors? instead of support vectors. Since the size of the reduced set vectors is smaller than  , classifiers become more efficient, but the computational cost to determine the vectors is verylarge. Osuna and Girosi (1999) propose two meth ods. The first method approximates +-) by support vector regression, but this method is applicable onlywhen S is large enough. The second method reformulates the training phase. Our approach is sim pler than these methods. Downs et al (Downs et al, 2001) try to reduce the number of support vectors by using linear dependence. We can also reduce the run-time complexity of a multi-class problem by cascading SVMs in the form of a binary tree (Schwenker, 2001) or a directacyclic graph (Platt et al, 2000). Yamada and Mat sumoto (2001) applied such a method to their NEsystem and reduced its CPU time by 39%. This ap proach can be combined with our SVM classifers.NE recognition can be regarded as a variablelength multi-class problem. For this kind of prob lem, probability-based kernels are studied for more theoretically well-founded methods (Jaakkola and Haussler, 1998; Tsuda et al, 2001; Shimodaira et al., 2001).","The above methods can also be applied to othertasks in natural language processing such as chunk ing and POS tagging because the quadratic kernels give good results. Utsuro et al (2001) report that a combination of two NE recognizers attained F = 84.07%, butwrong word boundary cases are excluded. Our system attained 85.04% and word boundaries are auto matically adjusted. Yamada (Yamada et al, 2001) also reports that F*?? is best. Although his sys tem attained F = 83.7% for 5-fold cross-validation of the CRL data (Yamada and Matsumoto, 2001), our system attained 86.8%. Since we followedIsozaki?s implementation (Isozaki, 2001), our system is different from Yamada?s system in the fol lowing points: 1) adjustment of word boundaries, 2)ChaSen?s parameters for unknown words, 3) char acter types, 4) use of the Viterbi search. For efficient classification, Burges and Scho?lkopf (1997) propose an approximation method that uses ?reduced set vectors? instead of support vectors. Since the size of the reduced set vectors is smaller than  , classifiers become more efficient, but the computational cost to determine the vectors is verylarge. Osuna and Girosi (1999) propose two meth ods. The first method approximates +-) by support vector regression, but this method is applicable onlywhen S is large enough. The second method reformulates the training phase. Our approach is sim pler than these methods. Downs et al (Downs et al, 2001) try to reduce the number of support vectors by using linear dependence. We can also reduce the run-time complexity of a multi-class problem by cascading SVMs in the form of a binary tree (Schwenker, 2001) or a directacyclic graph (Platt et al, 2000). Yamada and Mat sumoto (2001) applied such a method to their NEsystem and reduced its CPU time by 39%. This ap proach can be combined with our SVM classifers.NE recognition can be regarded as a variablelength multi-class problem. For this kind of prob lem, probability-based kernels are studied for more theoretically well-founded methods (Jaakkola and Haussler, 1998; Tsuda et al, 2001; Shimodaira et al., 2001)."
61,"of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within projections; X1 XO Argl... ArgN). more, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class lemmas of arguments, such as suband so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar—the proportion of sentences for which at least one analysis is found—is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized and treated specially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: -i) = (1 Ipatterns_f or_i n! n, p) = mi(p im The probability of the event happening m or more times is: n,p) = i=m -i)) the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have observed for the verb to be in class 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: • most sentences will not allow the application of all possible rules of English complementation; • some sentences will be unambiguous even given indeterminacy of the (1993:249-253) provides a detailed explanation and justification for the use of this measure. fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. • many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; • arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output for certain classes more often than others; and even a highest ranked for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- (Meyers at., who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation — Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garat., total of 1.2 million words—and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 'patterns' The binomial distribution gives the probability of an with probability exactly m times out of n attempts: 359 successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to automated report of precision of correct subcategorization classes to all classes found) of correct classes found in the dictionary entry). However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, predict that occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus for examples of further classes we judge valid, in which take a and infinitive complement, as in seems to to be insane, a passive participle, as in depressed. comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from dictionaries. All classes for exemplified in the corpus data, but for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. Lexicon Evaluation Figure 2 gives the raw results for the merged entries and corpus analysis on each verb. It shows the of positives correct classes proby our system, positives incorrect proposed by our system, and negatives classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Dictionary (14 verbs) Corpus (7 verbs) Precision Recall 65.7% 76.6% 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin 100.0% believe 66.7% cause 100.0% give 70.0% seem 75.0% swing 83.3% Mean 81.4% Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs). The frequency distribution of classes is highly skewed: for example for there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by because the binomial filter always rejects classes hypothesised on the basis of such little evidence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed. We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n < m in the system ranking that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the relative frequencies of classes output by the system. For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak in the systeni. There are only 13 negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other there are 67 negatives by an mean of 7.1 examples which should, ide- 360 Merged TP FP Entry Corpus TP FP Data No. of FN FN Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall have been accepted by the filter, and 11 should have been rejected. The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class. 3.3 Parsing Evaluation In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)—although the experiment does not in any way rely on the Mean Recall Precision crossings 'Baseline' Lexicalised 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Gral., see figure Next, we colwords in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). the existing test corpus this is not statisti- 'Carroll & Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. 361 significant at the 95% level p = if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. 4 Related Work Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. al. utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for verbs to the entries given in the Advanced Dictionary of Current English 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available. 5 Conclusions and Further Work The experiment and comparison reported above suggests that our more comprehensive subcategorization class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with comparable accuracy to extant systems. We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount. The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev & Briscoe, 1987).","of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within projections; X1 XO Argl... ArgN). more, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class lemmas of arguments, such as suband so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar—the proportion of sentences for which at least one analysis is found—is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized and treated specially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: -i) = (1 Ipatterns_f or_i n! n, p) = mi(p im The probability of the event happening m or more times is: n,p) = i=m -i)) the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have observed for the verb to be in class 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: • most sentences will not allow the application of all possible rules of English complementation; • some sentences will be unambiguous even given indeterminacy of the (1993:249-253) provides a detailed explanation and justification for the use of this measure. fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. • many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; • arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output for certain classes more often than others; and even a highest ranked for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- (Meyers at., who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation — Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garat., total of 1.2 million words—and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 'patterns' The binomial distribution gives the probability of an with probability exactly m times out of n attempts: 359 successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to automated report of precision of correct subcategorization classes to all classes found) of correct classes found in the dictionary entry). However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, predict that occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus for examples of further classes we judge valid, in which take a and infinitive complement, as in seems to to be insane, a passive participle, as in depressed. comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from dictionaries. All classes for exemplified in the corpus data, but for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. Lexicon Evaluation Figure 2 gives the raw results for the merged entries and corpus analysis on each verb. It shows the of positives correct classes proby our system, positives incorrect proposed by our system, and negatives classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Dictionary (14 verbs) Corpus (7 verbs) Precision Recall 65.7% 76.6% 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin 100.0% believe 66.7% cause 100.0% give 70.0% seem 75.0% swing 83.3% Mean 81.4% Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs). The frequency distribution of classes is highly skewed: for example for there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by because the binomial filter always rejects classes hypothesised on the basis of such little evidence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed. We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n < m in the system ranking that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the relative frequencies of classes output by the system. For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak in the systeni. There are only 13 negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other there are 67 negatives by an mean of 7.1 examples which should, ide- 360 Merged TP FP Entry Corpus TP FP Data No. of FN FN Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall have been accepted by the filter, and 11 should have been rejected. The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class. 3.3 Parsing Evaluation In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)—although the experiment does not in any way rely on the Mean Recall Precision crossings 'Baseline' Lexicalised 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Gral., see figure Next, we colwords in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). the existing test corpus this is not statisti- 'Carroll & Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. 361 significant at the 95% level p = if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. 4 Related Work Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. al. utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for verbs to the entries given in the Advanced Dictionary of Current English 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available. 5 Conclusions and Further Work The experiment and comparison reported above suggests that our more comprehensive subcategorization class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with comparable accuracy to extant systems. We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount. The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev & Briscoe, 1987). Predicate subcategorization is a key component of a lexical entry, because most, if not all, recent syntactic theories 'project' syntactic structure from the lexicon. Therefore, a wide-coverage parser utilizing such a lexicalist grammar must have access to an accurate and comprehensive dictionary encoding (at a minimum) the number and category of a predicate's arguments and ideally also information about control with predicative arguments, semantic selection preferences on arguments, and so forth, to allow the recovery of the correct predicate-argument structure. If the parser uses statistical techniques to rank analyses, it is also critical that the dictionary encode the relative frequency of distinct subcategorization classes for each predicate. Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the COMLEX Syntax dictionary, Grishman et al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda et at. (1993). In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et at. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classes—a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which demonstrates that this system is capable of acquiring the subcategorization classes of verbs and the relative frequencies of these classes with comparable accuracy to the less ambitious extant systems. We achieve this performance by exploiting a more sophisticated robust statistical parser which yields complete though 'shallow' parses, a more comprehensive subcategorization class classifier, and a priori estimates of the probability of membership of these classes. We also describe a small-scale experiment which demonstrates that subcategorization class frequency information for individual verbs can be used to improve parsing accuracy. The system consists of the following six components which are applied in sequence to sentences containing a specific predicate in order to retrieve a set of subcategorization classes for that predicate: For example, building entries for attribute, and given that one of the sentences in our data was (la), the tagger and lemmatizer return (lb). (lb) is parsed successfully by the probabilistic LR parser, and the ranked analyses are returned. Then the patternset extractor locates the subanalyses containing attribute and constructs a patternset. The highest ranked analysis and pattern for this example are shown in Figure 12. Patterns encode the value of the VSUBCAT feature from the VP rule and the head lemma(s) of each argument. In the case of PP (P2) arguments, the pattern also encodes the value of PSUBCAT from the PP rule and the head lemma(s) of its complement(s). In the next stage of processing, patterns are classified, in this case giving the subcategorization class corresponding to transitive plus PP with non-finite clausal complement. The system could be applied to corpus data by first sorting sentences into groups containing instances of a specified predicate, but we use a different strategy since it is more efficient to tag, lemmatize and parse a corpus just once, extracting patternsets for all predicates in each sentence; then to classify the patterns in all patternsets; and finally, to sort and recombine patternsets into sets of patternsets, one set for each distinct predicate containing patternsets of just the patterns relevant to that predicate. The tagger, lemmatizer, grammar and parser have been described elsewhere (see previous references), so we provide only brief relevant details here, concentrating on the description of the components of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within X1 projections; X1 XO Argl... ArgN). Furthermore, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar—the proportion of sentences for which at least one analysis is found—is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized and treated specially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: The probability of the event happening m or more times is: Thus P(m,n,p(v -i)) is the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have been observed for the verb to be in class i3. Our approach to acquiring subcategorization classes is predicated on the following assumptions: probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicographers (Meyers et at., 1994), who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garside et at., 1987)—a total of 1.2 million words—and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting The binomial distribution gives the probability of an event with probability p happening exactly m times out of n attempts: successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to an automated report of type precision (percentage of correct subcategorization classes to all classes found) and recall (percentage of correct classes found in the dictionary entry). However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to seem, where there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, both predict that seem will occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus data for seem contains examples of further classes which we judge valid, in which seem can take a PP[tol and infinitive complement, as in he seems to me to be insane, and a passive participle, as in he seemed depressed. This comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from machine-readable dictionaries. All classes for seem are exemplified in the corpus data, but for ask, for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. Figure 2 gives the raw results for the merged entries and corpus analysis on each verb. It shows the number of true positives (TP), correct classes proposed by our system, false positives (FP), incorrect classes proposed by our system, and false negatives (FN), correct classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs). The frequency distribution of the classes is highly skewed: for example for believe, there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by the system because the binomial filter always rejects classes hypothesised on the basis of such little evidence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed. We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n < m in the system ranking that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the relative frequencies of classes output by the system. For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak link in the systeni. There are only 13 true negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other hand, there are 67 false negatives supported by an estimated mean of 7.1 examples which should, ideally, have been accepted by the filter, and 11 false positives which should have been rejected. The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class. In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)—although the experiment does not in any way rely on the parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Grishman et al., 1992); see figure 65. Next, we collected all words in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). Over the existing test corpus this is not statistically significant at the 95% level (paired t-test, 1.21, 249 df, p = 0.11)—although if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. Ushioda et al. (1993) utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for the verbs to the entries given in the Oxford Advanced Learner's Dictionary of Current English (Hornby, 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available.","Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. Ushioda et al. (1993) utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for the verbs to the entries given in the Oxford Advanced Learner's Dictionary of Current English (Hornby, 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available."
62,"This paper presents an unsupervised method forassembling semantic knowledge from a part-of speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntacticrelationships. We focus on the symmetric relationship between pairs of nouns which occur to gether in lists. An incremental cluster-building algorithm using this part of the graph achieves82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific am biguities as distinct components in the graph surrounding an ambiguous word.","This paper presents an unsupervised method forassembling semantic knowledge from a part-of speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntacticrelationships. We focus on the symmetric relationship between pairs of nouns which occur to gether in lists. An incremental cluster-building algorithm using this part of the graph achieves82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific am biguities as distinct components in the graph surrounding an ambiguous word. Semantic knowledge for particular domains isincreasingly important in NLP. Many applications such as Word-Sense Disambiguation, In formation Extraction and Speech Recognitionall require lexicons. The coverage of handbuilt lexical resources such as WordNet (Fellbaum, 1998) has increased dramatically in re cent years, but leaves several problems andchallenges. Coverage is poor in many criti cal, rapidly changing domains such as current affairs, medicine and technology, where much time is still spent by human experts employed to recognise and classify new terms. Mostlanguages remain poorly covered in compari son with English. Hand-built lexical resourceswhich cannot be automatically updated can of ten be simply misleading. For example, using WordNet to recognise that the word apple refers to a fruit or a tree is a grave error in the many situations where this word refers to a computer manufacturer, a sense which WordNet does notcover. For NLP to reach a wider class of appli cations in practice, the ability to assemble andupdate appropriate semantic knowledge auto matically will be vital. This paper describes a method for arranging semantic information into a graph (Bolloba?s, 1998), where the nodes are words and the edges(also called links) represent relationships be tween words. The paper is arranged as follows. Section 2 reviews previous work on semanticsimilarity and lexical acquisition. Section 3 de scribes how the graph model was built from the PoS-tagged British National Corpus. Section 4 describes a new incremental algorithm used to build categories of words step by step from thegraph model. Section 5 demonstrates this algo rithm in action and evaluates the results againstWordNet classes, obtaining state-of-the-art re sults. Section 6 describes how the graph modelcan be used to recognise when words are polysemous and to obtain groups of words represen tative of the different senses. Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity. The underlying claim is that words which are semantically similar occurwith similar distributions and in similar con texts (Miller and Charles, 1991).The main results to date in the field of au tomatic lexical acquisition are concerned withextracting lists of words reckoned to belong to gether in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). Roark and Charniak de scribe a ?generic algorithm? for extracting suchlists of similar words using the notion of seman tic similarity, as follows (Roark and Charniak, 1998, ?1). 1. For a given category, choose a small. set of exemplars (or ?seed words?) 2. Count co-occurrence of words and. seed words within a corpus these counts to select new seed words 4. Return to step 2 and iterate n times. 5. Use a figure of merit to rank words. for category membership and output a ranked list Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. Like the algorithm we present in Section 5, the similarity measure (or ?figure ofmerit?) used in these cases was based on co occurrence in lists. Both of these works evaluated their resultsby asking humans to judge whether items generated were appropriate members of the cate gories sought. Riloff and Shepherd (1997) also give some credit for ?related words? (for example crash might be regarded as being related to the category vehicles). One problem with these techniques is the danger of ?infections? once any incorrect or out-of-category word has been admitted, theneighbours of this word are also likely to be ad mitted. In Section 4 we present an algorithmwhich goes some way towards reducing such in fections. The early results have been improved upon byRiloff and Jones (1999), where a ?mutual boot strapping? approach is used to extract words in particular semantic categories and expression patterns for recognising relationships betweenthese words for the purposes of information extraction. The accuracy achieved in this experiment is sometimes as high as 78% and is there fore comparable to the results reported in this paper. Another way to obtain word-senses directly from corpora is to use clustering algorithms on feature-vectors (Lin, 1998; Schu?tze, 1998).Clustering techniques can also be used to discriminate between different senses of an ambiguous word. A general problem for such cluster ing techniques lies in the question of how many clusters one should have, i.e. how many senses are appropriate for a particular word in a given domain (Manning and Schu?tze, 1999, Ch 14). Lin?s approach to this problem (Lin, 1998) isto build a ?similarity tree? (using what is in ef fect a hierarchical clustering method) of words related to a target word (in this case the word duty). Different senses of duty can be discerned as different sub-trees of this similarity tree. Wepresent a new method for word-sense discrimi nation in Section 6. PoS-tagged Corpus In this section we describe how a graph ? a collection of nodes and links ? was built to represent the relationships between nouns. Themodel was built using the British National Cor pus which is automatically tagged for parts of speech. Initially, grammatical relations between pairsof words were extracted. The relationships ex tracted were the following: ? Noun (assumed to be subject) Verb ? Verb Noun (assumed to be object) ? Adjective Noun? Noun Noun (often the first noun is modify ing the second) ? Noun and/or Noun The last of these relationships often occurs when the pair of nouns is part of a list. Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). In this paper we too focus on nouns co-occurring in lists. This is be cause the noun and/or noun relationship is the only symmetric relationship in our model, andsymmetric relationships are much easier to ma nipulate than asymmetric ones. Our full graph contains many directed links between words of different parts of speech. Initial experiments with this model show considerable promise but are at too early a stage to be reported upon yet.Thus the graph used in most of this paper repre sents only nouns. Each node represents a noun and two nodes have a link between them if they co-occur separated by the conjunctions and or or, and each link is weighted according to the number of times the co-occurrence is observed.Various cutoff functions were used to deter mine how many times a relationship must be observed to be counted as a link in the graph. A well-behaved option was to take the top nneighbours of each word, where n could be determined by the user. In this way the link weighting scheme was reduced to a link-ranking scheme. One consequence of this decision was that links to more common words were preferred over links to rarer words. This decision may have effectively boosted precision at the expense of recall, because the preferred links are to fairlycommon and (probably) more stable words. Re search is need to reveal theoretically motivatedor experimentally optimal techniques for select ing the importance to assign to each link ? the choices made in this area so far are often of an ad hoc nature. The graph used in the experiments described has 99,454 nodes (nouns) and 587,475 links. There were roughly 400,000 different types tagged as nouns in the corpus, so the graph model represents about one quarter of these nouns, including most of the more common ones. Extracting Categories of Similar Words In this section we describe a new algorithm for adding the ?most similar node? to an existingcollection of nodes in a way which incremen tally builds a stable cluster. We rely entirelyupon the graph to deduce the relative importance of relationships. In particular, our algo rithm is designed to reduce so-called ?infections?(Roark and Charniak, 1998, ?3) where the inclu sion of an out-of-category word which happens to co-occur with one of the category words can significantly distort the final list. Here is the process we use to select and add the ?most similar node? to a set of nodes: Definition 1 Let A be a set of nodes and let N(A), the neighbours of A, be the nodes which are linked to any a ? A. (So N(A) = ? a?AN(a).) The best new node is taken to be the node b ? N(A)\A with the highest proportion of links to N(A). More precisely, for each u ? N(A)\A, let the affinity between u and A be given by the ratio |N(u) ?N(A)| |N(u)| . The best new node b ? N(A) \ A is the node which maximises this affinity score. This algorithm has been built into an on-line demonstration where the user inputs a givenseed word and can then see the cluster of re lated words being gradually assembled. The algorithm is particularly effective atavoiding infections arising from spurious co occurrences and from ambiguity. Consider, forexample, the graph built around the word ap ple in Figure 6. Suppose that we start with the seed-list apple, orange, banana. However many times the string ?Apple and Novell? occurs in the corpus, the novell node will not be addedto this list because it doesn?t have a link to or ange, banana or any of their neighbours except for apple. One way to summarise the effect of this decision is that the algorithm adds words to clusters depending on type frequency rather than token frequency. This avoids spurious links due to (for example) particular idioms rather than geniune semantic similarity. In this section we give examples of lexical cat egories extracted by our method and evaluatethem against the corresponding classes in Word Net. 5.1 Methodology. Our methodology is as follows. Consider an intuitive category of objects such as musical instruments. Define the ?WordNet class? or ?WordNet category? of musical instruments tobe the collection of synsets subsumed in Word Net by the musical instruments synset. Take a ?protypical example? of a musical instrument, such as piano. The algorithm defined in (1) gives a way of finding the n nodes deemed to be most closely related to the piano node. Thesecan then be checked to see if they are members of the WordNet class of musical instru ments. This method is easier to implement and less open to variation than human judgements. While WordNet or any other lexical resource isnot a perfect arbiter, it is hoped that this exper iment procedure is both reliable and repeatable. The ten classes of words chosen were crimes, places, tools, vehicles, musical instruments, clothes, diseases, body parts, academic subjects and foodstuffs. The classes were chosen beforethe experiment was carried out so that the re sults could not be massaged to only use thoseclasses which gave good results. (The first 4 cat egories are also used by (Riloff and Shepherd, 1997) and (Roark and Charniak, 1998) and so were included for comparison.) Having chosen these classes, 20 words were retrieved using asingle seed-word chosen from the class in ques tion. This list of words clearly depends on the seed word chosen. While we have tried to optimise this choice, it depends on the corpus and thethe model. The influence of semantic Proto type Theory (Rosch, 1988) is apparent in this process, a link we would like to investigate in more detail. It is possible to choose an optimal seed word for a particular category: it should be possible to compare these optimal seed wordswith the ?prototypes? suggested by psychologi cal experiments (Mervis and Rosch, 1981). 5.2 Results. The results for a list of ten classes and proto typical words are given in Table 1. Words which are correct members of the classes sought arein Roman type: incorrect results are in italics. The decision between correctness and in correctness was made on a strict basis for thesake of objectivity and to enable the repeata bility of the experiment: words which are in WordNet were counted as correct results only if they are actual members of the WordNet class in question. Thus brigandage is not regarded as a crime even though it is clearly an act ofwrongdoing, orchestra is not regarded as a musical instrument because it is a collection of in struments rather than a single instrument, etc. The only exceptions we have made are the terms wynd and planetology (marked in bold), whichare not in WordNet but are correct nonethe less. These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. (It has been pointed out that many polysemous words may occur in several classes, making the task easier because for many words there are several classes which our algorithm would give credit for.)With these conditions, our algorithm re trieves only 36 incorrect terms out of a total of 200, giving an accuracy of 82%. 5.3 Analysis. Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), whoreport average accuracies of 17% and 35% re spectively. (Our results are also slightly better than those reported by Riloff and Jones (1999)). Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performedusing MUC-4 and the Wall Street Journal cor pus (some 30 million words). Our model was built using the British National Corpus (100 million words). On the other hand, our modelwas built using only a part-of-speech tagged cor pus. The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that ?parsing is invaluable?. Our results clearly indicate that a large PoS-tagged corpusmay be much better for automatic lexical ac quisition than a small fully-parsed corpus. This claim could of course be tested by comparing techniques on the same corpus.To evaluate the advantage of using PoS infor mation, we compared the graph model with asimilarity thesaurus generated using Latent Se mantic Indexing (Manning and Schu?tze, 1999, Ch 15), a ?bag-of-words? approach, on the samecorpus. The same number of nouns was re trieved for each class using the graph model and LSI. The LSI similarity thesaurus obtained an accuracy of 31%, much less than the graph model?s 82%. This is because LSI retrieves words which are related by context but are not in the same class: for example, the neighbours of piano found using LSI cosine-similarity on the BNC corpus include words such as composer,music, Bach, concerto and dance, which are re lated but certainly not in the same semantic class.The incremental clustering algorithm of Def inition (1) works well at preventing ?infections? Class Seed Word Neighbours Produced by Graph Model crimes murder crime theft arson importuning incest fraud larceny parricideburglary vandalism indecency violence offences abuse brig andage manslaughter pillage rape robbery assault lewdness places park path village lane viewfield church square road avenue garden castle wynd garage house chapel drive crescent home place cathedral street tools screwdriver chisel naville nail shoulder knife drill matchstick morgenthau gizmo hand knee elbow mallet penknife gallie leg arm sickle bolster hammer vehicle conveyance train tram car driver passengers coach lorry truck aeroplane coons plane trailer boat taxi pedestrians vans vehicles jeep bus buses helicopter musical instruments piano fortepiano orchestra marimba clarsach violin cizek viola oboeflute horn bassoon culbone mandolin clarinet equiluz contra bass saxophone guitar cello clothes shirt chapeaubras cardigan trousers breeches skirt jeans boots pair shoes blouse dress hat waistcoat jumper sweater coat cravat tie leggings diseases typhoid malaria aids polio cancer disease atelectasis illnesses cholerahiv deaths diphtheria infections hepatitis tuberculosis cirrho sis diptheria bronchitis pneumonia measles dysentery body parts stomach head hips thighs neck shoulders chest back eyes toes breasts knees feet face belly buttocks haws ankles waist legs academic subjectsphysics astrophysics philosophy humanities art religion science politics astronomy sociology chemistry history theology eco nomics literature maths anthropology culture mathematics geography planetology foodstuffs cake macaroons confectioneries cream rolls sandwiches croissant buns scones cheese biscuit drinks pastries tea danish butter lemonade bread chocolate coffee milk Table 1: Classes of similar words given by the graph model. and keeping clusters within one particular class. The notable exception is the tools class, where the word hand appears to introduce infection. In conclusion, it is clear that the graph modelcombined with the incremental clustering algo rithm of Definition 1 performs better than mostprevious methods at the task of automatic lex ical acquisition.","In this section we give examples of lexical cat egories extracted by our method and evaluatethem against the corresponding classes in Word Net. 5.1 Methodology. Our methodology is as follows. Consider an intuitive category of objects such as musical instruments. Define the ?WordNet class? or ?WordNet category? of musical instruments tobe the collection of synsets subsumed in Word Net by the musical instruments synset. Take a ?protypical example? of a musical instrument, such as piano. The algorithm defined in (1) gives a way of finding the n nodes deemed to be most closely related to the piano node. Thesecan then be checked to see if they are members of the WordNet class of musical instru ments. This method is easier to implement and less open to variation than human judgements. While WordNet or any other lexical resource isnot a perfect arbiter, it is hoped that this exper iment procedure is both reliable and repeatable. The ten classes of words chosen were crimes, places, tools, vehicles, musical instruments, clothes, diseases, body parts, academic subjects and foodstuffs. The classes were chosen beforethe experiment was carried out so that the re sults could not be massaged to only use thoseclasses which gave good results. (The first 4 cat egories are also used by (Riloff and Shepherd, 1997) and (Roark and Charniak, 1998) and so were included for comparison.) Having chosen these classes, 20 words were retrieved using asingle seed-word chosen from the class in ques tion. This list of words clearly depends on the seed word chosen. While we have tried to optimise this choice, it depends on the corpus and thethe model. The influence of semantic Proto type Theory (Rosch, 1988) is apparent in this process, a link we would like to investigate in more detail. It is possible to choose an optimal seed word for a particular category: it should be possible to compare these optimal seed wordswith the ?prototypes? suggested by psychologi cal experiments (Mervis and Rosch, 1981). 5.2 Results. The results for a list of ten classes and proto typical words are given in Table 1. Words which are correct members of the classes sought arein Roman type: incorrect results are in italics. The decision between correctness and in correctness was made on a strict basis for thesake of objectivity and to enable the repeata bility of the experiment: words which are in WordNet were counted as correct results only if they are actual members of the WordNet class in question. Thus brigandage is not regarded as a crime even though it is clearly an act ofwrongdoing, orchestra is not regarded as a musical instrument because it is a collection of in struments rather than a single instrument, etc. The only exceptions we have made are the terms wynd and planetology (marked in bold), whichare not in WordNet but are correct nonethe less. These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. (It has been pointed out that many polysemous words may occur in several classes, making the task easier because for many words there are several classes which our algorithm would give credit for.)With these conditions, our algorithm re trieves only 36 incorrect terms out of a total of 200, giving an accuracy of 82%. 5.3 Analysis. Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), whoreport average accuracies of 17% and 35% re spectively. (Our results are also slightly better than those reported by Riloff and Jones (1999)). Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performedusing MUC-4 and the Wall Street Journal cor pus (some 30 million words). Our model was built using the British National Corpus (100 million words). On the other hand, our modelwas built using only a part-of-speech tagged cor pus. The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that ?parsing is invaluable?. Our results clearly indicate that a large PoS-tagged corpusmay be much better for automatic lexical ac quisition than a small fully-parsed corpus. This claim could of course be tested by comparing techniques on the same corpus.To evaluate the advantage of using PoS infor mation, we compared the graph model with asimilarity thesaurus generated using Latent Se mantic Indexing (Manning and Schu?tze, 1999, Ch 15), a ?bag-of-words? approach, on the samecorpus. The same number of nouns was re trieved for each class using the graph model and LSI. The LSI similarity thesaurus obtained an accuracy of 31%, much less than the graph model?s 82%. This is because LSI retrieves words which are related by context but are not in the same class: for example, the neighbours of piano found using LSI cosine-similarity on the BNC corpus include words such as composer,music, Bach, concerto and dance, which are re lated but certainly not in the same semantic class.The incremental clustering algorithm of Def inition (1) works well at preventing ?infections? Class Seed Word Neighbours Produced by Graph Model crimes murder crime theft arson importuning incest fraud larceny parricideburglary vandalism indecency violence offences abuse brig andage manslaughter pillage rape robbery assault lewdness places park path village lane viewfield church square road avenue garden castle wynd garage house chapel drive crescent home place cathedral street tools screwdriver chisel naville nail shoulder knife drill matchstick morgenthau gizmo hand knee elbow mallet penknife gallie leg arm sickle bolster hammer vehicle conveyance train tram car driver passengers coach lorry truck aeroplane coons plane trailer boat taxi pedestrians vans vehicles jeep bus buses helicopter musical instruments piano fortepiano orchestra marimba clarsach violin cizek viola oboeflute horn bassoon culbone mandolin clarinet equiluz contra bass saxophone guitar cello clothes shirt chapeaubras cardigan trousers breeches skirt jeans boots pair shoes blouse dress hat waistcoat jumper sweater coat cravat tie leggings diseases typhoid malaria aids polio cancer disease atelectasis illnesses cholerahiv deaths diphtheria infections hepatitis tuberculosis cirrho sis diptheria bronchitis pneumonia measles dysentery body parts stomach head hips thighs neck shoulders chest back eyes toes breasts knees feet face belly buttocks haws ankles waist legs academic subjectsphysics astrophysics philosophy humanities art religion science politics astronomy sociology chemistry history theology eco nomics literature maths anthropology culture mathematics geography planetology foodstuffs cake macaroons confectioneries cream rolls sandwiches croissant buns scones cheese biscuit drinks pastries tea danish butter lemonade bread chocolate coffee milk Table 1: Classes of similar words given by the graph model. and keeping clusters within one particular class. The notable exception is the tools class, where the word hand appears to introduce infection. In conclusion, it is clear that the graph modelcombined with the incremental clustering algo rithm of Definition 1 performs better than mostprevious methods at the task of automatic lex ical acquisition."
63,"We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies.","We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies. We address here the problem of Base NP translation, in which for a given Base Noun Phrase in a source language (e.g., ?information age? in English), we are to find out its possible translation(s) in a target language (e.g., ? in Chinese). We define a Base NP as a simple and non-recursive noun phrase. In many cases, Base NPs represent holistic and non-divisible concepts, and thus accurate translation of them from one language to another is extremely important in applications like machine translation, cross language information retrieval, and foreign language writing assistance. In this paper, we propose a new method for Base NP translation, which contains two steps: (1) translation candidate collection, and (2) translation selection. In translation candidate collection, for a given Base NP in the source language, we look for its translation candidates in the target language. To do so, we use a word-to-word translation dictionary and corpus data in the target language on the web. In translation selection, we determine the possible translation(s) from among the candidates. We use non-parallel corpus data in the two languages on the web and employ one of the two methods which we have developed. In the first method, we view the problem as that of classification and employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm. We will use ?EM-NBC-Ensemble? to denote this method, hereafter. In the second method, we view the problem as that of calculating similarities between context vectors and use TF-IDF vectors also constructed with the EM Algorithm. We will use ?EM-TF-IDF? to denote this method. Experimental results indicate that our method is very effective, and the coverage and top 3 accuracy of translation at the final stage are 91.4% and 79.8%, respectively. The results are significantly better than those of the baseline methods relying on existing technologies. The higher performance of our method can be attributed to the enormity of the web data used and the employment of the EM Algorithm. 2.1 Translation with Non-parallel. Corpora A straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., Brown et al 1993). Parallel corpora are, however, difficult to obtain in practice. To deal with this difficulty, a number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000). Within these methods, it is usually assumed that a number of translation candidates for a word or phrase are given (or can be easily collected) and the problem is focused on translation selection. All of the proposed methods manage to find out the translation(s) of a given word or phrase, on the basis of the linguistic phenomenon that the contexts of a translation tend to be similar to the contexts of the given word or phrase. Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF vector), in which one element corresponds to one word in the contexts. In translation selection, they select the translation candidates whose context vectors are the closest to that of the given word or phrase. Since the context vector of the word or phrase to be translated corresponds to words in the source language, while the context vector of a translation candidate corresponds to words in the target language, and further the words in the source language and those in the target language have a many-to-many relationship (i.e., translation ambiguities), it is necessary to accurately transform the context vector in the source language to a context vector in the target language before distance calculation. The vector-transformation problem was not, however, well-resolved previously. Fung and Yee assumed that in a specific domain there is only one-to-one mapping relationship between words in the two languages. The assumption is reasonable in a specific domain, but is too strict in the general domain, in which we presume to perform translation here. A straightforward extension of Fung and Yee?s assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping). This approach, however, has a drawback of losing information in vector transformation, as will be described. For other methods using non-parallel corpora, see also (Tanaka and Iwasaki, 1996; Kikui, 1999, Koehn and Kevin 2000; Sumita 2000; Nakagawa 2001; Gao et al 2001). 2.2 Translation Using Web Data. Web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data). Recently, a new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., Brill et al 2001). Nagata et al(2001), for example, proposed to collect partial parallel corpus data on the web to create a translation dictionary. They observed that there are many partial parallel corpora between English and Japanese on the web, and most typically English translations of Japanese terms (words or phrases) are parenthesized and inserted immediately after the Japanese terms in documents written in Japanese. Our method for Base NP translation comprises of two steps: translation candidate collection and translation selection. In translation candidate collection, we look for translation candidates of a given Base NP. In translation selection, we find out possible translation(s) from the translation candidates. In this paper, we confine ourselves to translation of noun-noun pairs from English to Chinese; our method, however, can be extended to translations of other types of Base NPs between other language pairs. 3.1 Translation Candidate Collection. We use heuristics for translation candidate collection. Figure 1 illustrates the process of collecting Chinese translation candidates for an English Base NP ?information age? with the heuristics. 1. Input ?information age?;. 2. Consult English-Chinese word translation dictionary:. information ->  age - (how old somebody is)  (historical era)  (legal adult hood) 3. Compositionally create translation candidates in. Chinese:  obtain the document frequencies of them (i.e., numbers of documents containing them):  10000  10  0 5. Output candidates having non-zero document. frequencies and the document frequencies:  10000  10 Figure 1. Translation candidate collection 3.2 Translation Selection --. EM-NBC-Ensemble We view the translation selection problem as that of classification and employ EM-NBC-Ensemble to perform the task. For the ease of explanation, we first describe the algorithm of using only EM-NBC and next extend it to that of using EM-NBC-Ensemble. Basic Algorithm Let e~ denote the Base NP to be translated and C~ the set of its translation candidates (phrases). Suppose that kC =|~| . Let c~ represent a random variable on C~ . Let E denote a set of words in English, and C a set of words in Chinese. Suppose that nCmE == ||and|| . Let e represent a random variable on E and c a random variable on C. Figure 2 describes the algorithm. Input: e~ , C~ , contexts containing e~ , contexts containing all Cc ~~ ? ; 1. create a frequency vector )),(,),(),(( 21 mefefef L ),,1(, miEei L=? using contexts containing e~ ; transforming the vector into )),(,),(),(( 21 nEEE cfcfcf L ),,1(, niCci L=? , using a translation dictionary and the EM algorithm; 2. for each ( Cc ~~ ? ){ estimate with Maximum Likelihood Estimation the prior probability )~(cP using contexts containing all Cc ~~ ? ; create a frequency vector )),(,),(),(( 21 ncfcfcf L ),,1(, niCci L=? using contexts containing c~ ; normalize the frequency vector , yielding ),,1(,)),~|(,),~|(),~|(( 21 niCcccPccPccP in LL =? ; calculate the posterior probability )|~( DcP with EM-NBC (generally EM-NBC-Ensemble), where ),,1(,)),(,),(),(( 21 niCccfcfcf inEEE LL =?=D 3. Sort Cc ~~ ? in descending order of )|~( DcP ;. Output: the top sorted results Figure 2. Algorithm of EM-NBC-Ensemble Context Information As input data, we use ?contexts? in English which contain the phrase to be translated. We also use contexts in Chinese which contain the translation candidates. Here, a context containing a phrase is defined as the surrounding words within a window of a predetermined size, which window covers the phrase. We can easily obtain the data by searching for them on the web. Actually, the contexts containing the candidates are obtained at the same time when we conduct translation candidate collection (Step 4 in Figure 1). EM Algorithm We define a relation between E and C as CER ?? , which represents the links in a translation dictionary. We further define }),(|{ Rceec ?=? At Step 1, we assume that all the instances in ))(),..,(),(( 21 mefefef are independently generated according to the distribution defined as: ? ? = Cc cePcPeP )|()()( (1) We estimate the parameters of the distribution by using the Expectation and Maximization (EM) Algorithm (Dempster et al, 1977). Initially, we set for all Cc ? || 1)( C cP = , ?? ?= c c c e e ceP if,0 if,|| 1 )|( Next, we estimate the parameters by iteratively updating them, until they converge (cf., Figure 3). Finally, we calculate )(cf E for all Cc ? as:? = Ee E efcPcf )()()( (2) In this way, we can transform the frequency vector in English ))(),..,(),(( 21 mefefef into a vector in Chinese ))(),..,(),(( 21 nEEE cfcfcf=D . Prior Probability Estimation At Step 2, we approximately estimate the prior probability )~(cP by using the document frequencies of the translation candidates. The data are obtained when we conduct candidate collection (Step 4 in Figure 1). Ee Ee Cc ecPef ecPef ceP ecPefcP cePcP cePcP ecP )|()( )|()()|( )|()()(StepM )|()( )|()()|(StepE Figure 3. EM Algorithm EM-NBC At Step 2, we use an EM-based Na?ve Bayesian Classifier (EM-NBC) to select the candidates c~ whose posterior probabilities are the largest: ?? )~|(log)()~(logmaxarg )|~(maxarg ~ ~ ~ ~ ccPcfcP cP Cc E Cc Cc D (3) Equation (3) is based on Bayes? rule and the assumption that the data in D are independently generated from CcccP ?),~|( . In our implementation, we use an equivalent ?? )~|(log)()~(logminarg ~ ~ ccPcfcP Cc E Cc ? (4) where 1?? is an additional parameter used to emphasize the prior information. If we ignore the first term in Equation (4), then the use of one EM-NBC turns out to select the candidate whose frequency vector is the closest to the transformed vector D in terms of KL divergence (cf., Cover and Tomas 1991). EM-NBC-Ensemble To further improve performance, we use an ensemble (i.e., a linear combination) of EM-NBCs (EM-NBC-Ensemble), while the classifiers are constructed on the basis of the data in different contexts with different window sizes. More specifically, we calculate where s),1,(i, L=iD denotes the data in different contexts. 3.3 Translation Selection -- EM-TF-IDF. We view the translation selection problem as that of calculating similarities between context vectors and use as context vectors TF-IDF vectors constructed with the EM Algorithm. Figure 4 describes the algorithm in which we use the same notations as those in EM-NBC-Ensemble. The idf value of a Chinese word c is calculated in advance and as )/)(log()( Fcdfcidf ?= (6) where )cdf( denotes the document frequency of c and F the total document frequency. Input: e~ , C~ , contexts containing e~ , contexts containing all Cc ~~ ? , Cc),cidf( ? ; 1. create a frequency vector )),(,),(),(( 21 mefefef L ),,1(, miEei L=? using contexts containing e~ ; transforming the vector into 21 )),c(f,),c(f),c(f( nEEE L ),,1(, niCci L=? , using a translation dictionary and the EM algorithm; create a TF-IDF vector 11 )),cidf())c(f,),cidf()c(f( nnEE L=A ),,1(, niCci L=? 2. for each ( Cc ~~ ? ){ create a frequency vector )),(,),(),(( 21 ncfcfcf L ),,1(, niCci L=? using contexts containing c~ ; create a TF-IDF vector 11 ))cidf())c(f,),cidf()c(f( nnL=B ),,1(, niCc i L=? ; calculate ),cos()c~tfidf( BA= ; } 3. Sort Cc ~~ ? in descending order of )c~tfidf( ;. Output: the top sorted results Figure 4. Algorithm of EM-TF-IDF 3.4 Advantage of Using EM Algorithm. The uses of EM-NBC-Ensemble and EM-TF-IDF can be viewed as extensions of existing methods for word or phrase translation using non-parallel corpora. Particularly, the use of the EM Algorithm can help to accurately transform a frequency vector from one language to another. Suppose that we are to determine if ? is a translation of ?information age? (actually it is). The frequency vectors of context words for ?information age? and ? are given in A and D in Figure 5, respectively. If for each English word we only retain the link connecting to the Chinese translation with the largest frequency (a link represented as a solid line) to establish a many-to-one mapping and transform vector A from English to Chinese, we obtain vector B. It turns out, however, that vector B is quite different from vector D, although they should be similar to each other. We will refer to this method as ?Major Translation? hereafter. With EM, vector A in Figure 5 is transformed into vector C, which is much closer to vector D, as expected. Specifically, EM can split the frequency of a word in English and distribute them into its translations in Chinese in a theoretically sound way (cf., the distributed frequencies of ?internet?). Note that if we assume a many-to-one (or one-to-one) mapping ? = = s i icP s cP 1 )|~(1)|~( DD (5) relationship, then the use of EM turns out to be equivalent to that of Major Translation. 3.5 Combination. In order to further boost the performance of translation, we propose to also use the translation method proposed in Nagata et al Specifically, we combine our method with that of Nagata et alby using a back-off strategy. Figure 6 illustrates the process of collecting Chinese translation candidates for an English Base NP ?information asymmetry? with Nagata et al?s method. In the combination of the two methods, we first use Nagata et als method to perform translation; if we cannot find translations, we next use our method. We will denote this strategy ?Back-off?. We conducted experiments on translation of the Base NPs from English to Chinese. We extracted Base NPs (noun-noun pairs) from the Encarta 1 English corpus using the tool developed by Xun et al(2000). There were about 1 http://encarta.msn.com/Default.asp 3000 Base NPs extracted. In the experiments, we. used the HIT English-Chinese word translation dictionary2 . The dictionary contains about 76000 Chinese words, 60000 English words, and 118000 translation links. As a web search engine, we used Google (http://www.google.com). Five translation experts evaluated the translation results by judging whether or not they were acceptable. The evaluations reported below are all based on their judgements. 4.1 Basic Experiment. In the experiment, we randomly selected 1000 Base NPs from the 3000 Base NPs. We next used our method to perform translation on the 1000 phrases. In translation selection, we employed EM-NBC-Ensemble and EM-TF-IDF. Table 1. Best translation result for each method Accuracy (%) Top 1 Top 3 Coverage (%) EM-NBC-Ensemble 61.7 80.3 Prior 57.6 77.6 MT-NBC-Ensemble 59.9 78.1 EM-KL-Ensemble 45.9 72.3 EM-NBC 60.8 78.9 EM-TF-IDF 61.9 80.8 MT-TF-IDF 58.2 77.6 EM-TF 55.8 77.8 89.9 Table 1 shows the results in terms of coverage and top n accuracy. Here, coverage is defined as the percentage of phrases which have translations selected, while top n accuracy is defined as the percentage of phrases whose selected top n translations include correct translations. For EM-NBC-Ensemble, we set the ? !in (4) to be 5 on the basis of our preliminary experimental results. For EM-TF-IDF, we used the non-web data described in Section 4.4 to estimate idf values of words. We used contexts with window sizes of ?1, ?3, ?5, ?7, ?9, ?11. 2 The dictionary is created by the Harbin Institute of Technology.. A B C D  Figure 5. Example of frequency vector transformation 1. Input ?information asymmetry?;. 2. Search the English Base NP on web sites in Chinese. and obtain documents as follows (i.e., using partial parallel corpora):       !#$ %()*#+ information asymmetry , 3. Find the most frequently occurring Chinese phrases. immediately before the brackets containing the English Base NP, using a suffix tree; 4. Output the Chinese phrases and their document. frequencies: #+ 5 #-. 5 Figure 6. Nagata et als method  Figure 7. Translation results Figure 7 shows the results of EM-NBC-Ensemble and EM-TF-IDF, in which for EM-NBC-Ensemble ?window size? denotes that of the largest within an ensemble. Table 1 summarizes the best results for each of them. ?Prior? and ?MT-TF-IDF? are actually baseline methods relying on the existing technologies. In Prior, we select candidates whose prior probabilities are the largest, equivalently, document frequencies obtained in translation candidate collection are the largest. In MT-TF-IDF, we use TF-IDF vectors transformed with Major Translation. Our experimental results indicate that both EM-NBC-Ensemble and EM-TF-IDF significantly outperform Prior and MT-TF-IDF, when appropriate window sizes are chosen. The p-values of the sign tests are 0.00056 and 0.00133 for EM-NBC-Ensemble, 0.00002 and 0.00901 for EM-TF-IDF, respectively. We next removed each of the key components of EM-NBC-Ensemble and used the remaining components as a variant of it to perform translation selection. The key components are (1) distance calculation by KL divergence (2) EM, (3) prior probability, and (4) ensemble. The variants, thus, respectively make use of (1) the baseline method ?Prior?, (2) an ensemble of Na?ve Bayesian Classifiers based on Major Translation (MT-NBC-Ensemble), (3) an ensemble of EM-based KL divergence calculations (EM-KL-Ensemble), and (4) EM-NBC. Figure 7 and Table 1 show the results. We see that EM-NBC-Ensemble outperforms all of the variants, indicating that all the components within EM-NBC-Ensemble play positive roles. We removed each of the key components of EM-TF-IDF and used the remaining components as a variant of it to perform translation selection. The key components are (1) idf value and (2) EM. The variants, thus, respectively make use of (1) EM-based frequency vectors (EM-TF), (2) the baseline method MT-TF-IDF. Figure 7 and Table 1 show the results. We see that EM-TF-IDF outperforms both variants, indicating that all of the components within EM-TF-IDF are needed. Comparing the results between MT-NBC-Ensemble and EM-NBC-Ensemble and the results between MT-TF-IDF and EM-TF-IDF, we see that the uses of the EM Algorithm can indeed help to improve translation accuracies. Table 2. Sample of translation outputs Base NP Translation calcium ion  adventure tale      lung cancer aircraft carrier * adult literacy * * Table 2 shows translations of five Base NPs as output by EM-NBC-Ensemble, in which the translations marked with * were judged incorrect by human experts. We analyzed the reasons for incorrect translations and found that the incorrect translations were due to: (1) no existence of dictionary entry (19%), (2) non-compositional translation (13%), (3) ranking error (68%). 4.2 Our Method vs. Nagata et als Method. Table 3. Translation results Accuracy (%) Top 1 Top 3 Coverage (%) Our Method 61.7 80.3 89.9 Nagata et als 72.0 76.0 10.5 We next used Nagata et als method to perform translation. From Table 3, we can see that the accuracy of Nagata et als method is higher than that of our method, but the coverage of it is lower. The results indicate that our proposed Back-off strategy for translation is justifiable. 4.3 Combination. In the experiment, we tested the Back-off strategy, Table 4 shows the results. The Back-off strategy Table 4. Translation results Accuracy% Top 1 Top 3 Coverage % Back-off (Ensemble) 62.9 79.7 Back-off (TF-IDF) 62.2 79.8 91.4 helps to further improve the results whether EM-NBC-Ensemble or EM-TF-IDF is used. 4.4 Web Data vs. Non-web Data. To test the effectiveness of the use of web data, we conducted another experiment in which we performed translation by using non-web data. The data comprised of the Wall Street Journal corpus in English (1987-1992, 500MB) and the People?s Daily corpus in Chinese (1982-1998, 700MB). We followed the Back-off strategy as in Section 4.3 to translate the 1000 Base NPs. Table 5. Translation results Accuracy%Data Top 1 Top 3 Coverage % Web (EM-NBC-Ensemble) 62.9 79.7 91.4 Non-web (EM-NBC-Ensemble) 56.9 74.7 79.3 Web (EM-IF-IDF) 62.2 79.8 91.4 Non-web (EM-TF-IDF) 51.5 71.4 78.5 The results in Table 5 show that the use of web data can yield better results than non-use of it, although the sizes of the non-web data we used were considerably large in practice. For Nagata et al?s method, we found that it was almost impossible to find partial-parallel corpora in the non-web data.","We conducted experiments on translation of the Base NPs from English to Chinese. We extracted Base NPs (noun-noun pairs) from the Encarta 1 English corpus using the tool developed by Xun et al(2000). There were about 1 http://encarta.msn.com/Default.asp 3000 Base NPs extracted. In the experiments, we. used the HIT English-Chinese word translation dictionary2 . The dictionary contains about 76000 Chinese words, 60000 English words, and 118000 translation links. As a web search engine, we used Google (http://www.google.com). Five translation experts evaluated the translation results by judging whether or not they were acceptable. The evaluations reported below are all based on their judgements. 4.1 Basic Experiment. In the experiment, we randomly selected 1000 Base NPs from the 3000 Base NPs. We next used our method to perform translation on the 1000 phrases. In translation selection, we employed EM-NBC-Ensemble and EM-TF-IDF. Table 1. Best translation result for each method Accuracy (%) Top 1 Top 3 Coverage (%) EM-NBC-Ensemble 61.7 80.3 Prior 57.6 77.6 MT-NBC-Ensemble 59.9 78.1 EM-KL-Ensemble 45.9 72.3 EM-NBC 60.8 78.9 EM-TF-IDF 61.9 80.8 MT-TF-IDF 58.2 77.6 EM-TF 55.8 77.8 89.9 Table 1 shows the results in terms of coverage and top n accuracy. Here, coverage is defined as the percentage of phrases which have translations selected, while top n accuracy is defined as the percentage of phrases whose selected top n translations include correct translations. For EM-NBC-Ensemble, we set the ? !in (4) to be 5 on the basis of our preliminary experimental results. For EM-TF-IDF, we used the non-web data described in Section 4.4 to estimate idf values of words. We used contexts with window sizes of ?1, ?3, ?5, ?7, ?9, ?11. 2 The dictionary is created by the Harbin Institute of Technology.. A B C D  Figure 5. Example of frequency vector transformation 1. Input ?information asymmetry?;. 2. Search the English Base NP on web sites in Chinese. and obtain documents as follows (i.e., using partial parallel corpora):       !#$ %()*#+ information asymmetry , 3. Find the most frequently occurring Chinese phrases. immediately before the brackets containing the English Base NP, using a suffix tree; 4. Output the Chinese phrases and their document. frequencies: #+ 5 #-. 5 Figure 6. Nagata et als method  Figure 7. Translation results Figure 7 shows the results of EM-NBC-Ensemble and EM-TF-IDF, in which for EM-NBC-Ensemble ?window size? denotes that of the largest within an ensemble. Table 1 summarizes the best results for each of them. ?Prior? and ?MT-TF-IDF? are actually baseline methods relying on the existing technologies. In Prior, we select candidates whose prior probabilities are the largest, equivalently, document frequencies obtained in translation candidate collection are the largest. In MT-TF-IDF, we use TF-IDF vectors transformed with Major Translation. Our experimental results indicate that both EM-NBC-Ensemble and EM-TF-IDF significantly outperform Prior and MT-TF-IDF, when appropriate window sizes are chosen. The p-values of the sign tests are 0.00056 and 0.00133 for EM-NBC-Ensemble, 0.00002 and 0.00901 for EM-TF-IDF, respectively. We next removed each of the key components of EM-NBC-Ensemble and used the remaining components as a variant of it to perform translation selection. The key components are (1) distance calculation by KL divergence (2) EM, (3) prior probability, and (4) ensemble. The variants, thus, respectively make use of (1) the baseline method ?Prior?, (2) an ensemble of Na?ve Bayesian Classifiers based on Major Translation (MT-NBC-Ensemble), (3) an ensemble of EM-based KL divergence calculations (EM-KL-Ensemble), and (4) EM-NBC. Figure 7 and Table 1 show the results. We see that EM-NBC-Ensemble outperforms all of the variants, indicating that all the components within EM-NBC-Ensemble play positive roles. We removed each of the key components of EM-TF-IDF and used the remaining components as a variant of it to perform translation selection. The key components are (1) idf value and (2) EM. The variants, thus, respectively make use of (1) EM-based frequency vectors (EM-TF), (2) the baseline method MT-TF-IDF. Figure 7 and Table 1 show the results. We see that EM-TF-IDF outperforms both variants, indicating that all of the components within EM-TF-IDF are needed. Comparing the results between MT-NBC-Ensemble and EM-NBC-Ensemble and the results between MT-TF-IDF and EM-TF-IDF, we see that the uses of the EM Algorithm can indeed help to improve translation accuracies. Table 2. Sample of translation outputs Base NP Translation calcium ion  adventure tale      lung cancer aircraft carrier * adult literacy * * Table 2 shows translations of five Base NPs as output by EM-NBC-Ensemble, in which the translations marked with * were judged incorrect by human experts. We analyzed the reasons for incorrect translations and found that the incorrect translations were due to: (1) no existence of dictionary entry (19%), (2) non-compositional translation (13%), (3) ranking error (68%). 4.2 Our Method vs. Nagata et als Method. Table 3. Translation results Accuracy (%) Top 1 Top 3 Coverage (%) Our Method 61.7 80.3 89.9 Nagata et als 72.0 76.0 10.5 We next used Nagata et als method to perform translation. From Table 3, we can see that the accuracy of Nagata et als method is higher than that of our method, but the coverage of it is lower. The results indicate that our proposed Back-off strategy for translation is justifiable. 4.3 Combination. In the experiment, we tested the Back-off strategy, Table 4 shows the results. The Back-off strategy Table 4. Translation results Accuracy% Top 1 Top 3 Coverage % Back-off (Ensemble) 62.9 79.7 Back-off (TF-IDF) 62.2 79.8 91.4 helps to further improve the results whether EM-NBC-Ensemble or EM-TF-IDF is used. 4.4 Web Data vs. Non-web Data. To test the effectiveness of the use of web data, we conducted another experiment in which we performed translation by using non-web data. The data comprised of the Wall Street Journal corpus in English (1987-1992, 500MB) and the People?s Daily corpus in Chinese (1982-1998, 700MB). We followed the Back-off strategy as in Section 4.3 to translate the 1000 Base NPs. Table 5. Translation results Accuracy%Data Top 1 Top 3 Coverage % Web (EM-NBC-Ensemble) 62.9 79.7 91.4 Non-web (EM-NBC-Ensemble) 56.9 74.7 79.3 Web (EM-IF-IDF) 62.2 79.8 91.4 Non-web (EM-TF-IDF) 51.5 71.4 78.5 The results in Table 5 show that the use of web data can yield better results than non-use of it, although the sizes of the non-web data we used were considerably large in practice. For Nagata et al?s method, we found that it was almost impossible to find partial-parallel corpora in the non-web data."
64,"Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality.","Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality. Broad-coverage lexical resources such as WordNet are extremely useful in applications such as Word Sense Disambiguation (Leacock, Chodorow, Miller 1998) and Question Answering (Pasca and Harabagiu 2001). However, they often include many rare senses while missing domain-specific senses. For example, in WordNet, the words dog, computer and company all have a sense that is a hyponym of person. Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person. On the other hand, WordNet misses the user-interface object sense of the word dialog (as often used in software manuals). One way to deal with these problems is to use a clustering algorithm to automatically induce semantic classes (Lin and Pantel 2001). Many clustering algorithms represent a cluster by the centroid of all of its members (e.g., K means) (McQueen 1967) or by a representative element (e.g., K-medoids) (Kaufmann and Rousseeuw 1987). When averaging over all elements in a cluster, the centroid of a cluster may be unduly influenced by elements that only marginally belong to the cluster or by elements that also belong to other clusters. For example, when clustering words, we can use the contexts of the words as features and group together the words that tend to appear in similar contexts. For instance, U.S. state names can be clustered this way because they tend to appear in the following contexts: (List A) ___ appellate court campaign in ___ ___ capital governor of ___ ___ driver's license illegal in ___ ___ outlaws sth. primary in ___ ___'s sales tax senator for ___ If we create a centroid of all the state names, the centroid will also contain features such as: (List B) ___'s airport archbishop of ___ ___'s business district fly to ___ ___'s mayor mayor of ___ ___'s subway outskirts of ___ because some of the state names (like New York and Washington) are also names of cities. Using a single representative from a cluster may be problematic too because each individual element has its own idiosyncrasies that may not be shared by other members of the cluster. In this paper, we propose a clustering algo rithm, CBC (Clustering By Committee), in which the centroid of a cluster is constructed by averaging the feature vectors of a subset of the cluster members. The subset is viewed as a committee that determines which other elements belong to the cluster. By carefully choosing committee members, the features of the centroid tend to be the more typical features of the target class. For example, our system chose the following committee members to compute the centroid of the state cluster: Illinois, Michigan, Minnesota, Iowa, Wisconsin, Indiana, Nebraska and Vermont. As a result, the centroid contains only features like those in List A. Evaluating clustering results is a very difficult task. We introduce a new evaluation methodol ogy that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Clustering algorithms are generally categorized as hierarchical and partitional. In hierarchical agglomerative algorithms, clusters are constructed by iteratively merging the most similar clusters. These algorithms differ in how they compute cluster similarity. In single-link clustering, the similarity between two clusters is the similarity between their most similar members while complete-link clustering uses the similarity between their least similar members. Average-link clustering computes this similarity as the average similarity between all pairs of elements across clusters. The complexity of these algorithms is O(n2logn), where n is the number of elements to be clustered (Jain, Murty, Flynn 1999). Chameleon is a hierarchical algorithm that employs dynamic modeling to improve clustering quality (Karypis, Han, Kumar 1999). When merging two clusters, one might consider the sum of the similarities between pairs of elements across the clusters (e.g. average-link clustering). A drawback of this approach is that the existence of a single pair of very similar elements might unduly cause the merger of two clusters. An alternative considers the number of pairs of elements whose similarity exceeds a certain threshold (Guha, Rastogi, Kyuseok 1998). However, this may cause undesirable mergers when there are a large number of pairs whose similarities barely exceed the threshold. Chameleon clustering combines the two approaches. K-means clustering is often used on large data sets since its complexity is linear in n, the number of elements to be clustered. K-means is a family of partitional clustering algorithms that iteratively assigns each element to one of K clusters according to the centroid closest to it and recomputes the centroid of each cluster as the average of the cluster?s elements. K-means has complexity O(K?T?n) and is efficient for many clustering tasks. Because the initial centroids are randomly selected, the resulting clusters vary in quality. Some sets of initial centroids lead to poor convergence rates or poor cluster quality. Bisecting K-means (Steinbach, Karypis, Kumar 2000), a variation of K-means, begins with a set containing one large cluster consisting of every element and iteratively picks the largest cluster in the set, splits it into two clusters and replaces it by the split clusters. Splitting a cluster consists of applying the basic K-means algorithm ? times with K=2 and keeping the split that has the highest average element centroid similarity. Hybrid clustering algorithms combine hierarchical and partitional algorithms in an attempt to have the high quality of hierarchical algorithms with the efficiency of partitional algorithms. Buckshot (Cutting, Karger, Pedersen, Tukey 1992) addresses the problem of randomly selecting initial centroids in K-means by combining it with average-link clustering. Buckshot first applies average-link to a random sample of n elements to generate K clusters. It then uses the centroids of the clusters as the initial K centroids of K-means clustering. The sample size counterbalances the quadratic running time of average-link to make Buckshot efficient: O(K?T?n + nlogn). The parameters K and T are usually considered to be small numbers. Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, ?threaten with __? is a context. If the word handgun occurred in this context, the context is a feature of handgun. The value of the feature is the pointwise mutual information (Manning and Sch?tze 1999 p.178) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information between c and w is defined as: ( ) ( ) ( ) N jF N wF N wF cw j c i i c mi ??? =, where N = ( )?? i j i jF is the total frequency counts of all words and their contexts. A well known problem with mutual information is that it is biased towards infrequent words/features. We therefore multiplied miw,c with a discounting factor: ( ) ( ) ( ) ( ) ( ) ( ) 11 +??? i j ci i j ci c c jF,wFmin jF,wFmin wF wF We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: ( ) ?? = c cw c cw c cwcw ji ji ji mimi mimi w,wsim 22 CBC consists of three phases. In Phase I, we compute each element?s top-k similar elements. In our experiments, we used k = 20. In Phase II, we construct a collection of tight clusters, where the elements of each cluster form a committee. The algorithm tries to form as many committees as possible on the condition that each newly formed committee is not very similar to any existing committee. If the condition is violated, the committee is simply discarded. In the final phase of the algorithm, each element is assigned to its most similar cluster. 4.1. Phase I: Find top-similar elements. Computing the complete similarity matrix between pairs of elements is obviously quadratic. However, one can dramatically reduce the running time by taking advantage of the fact that the feature vector is sparse. By indexing the features, one can retrieve the set of elements that have a given feature. To compute the top similar words of a word w, we first sort w?s features according to their mutual information with w. We only compute pairwise similarities between w and the words that share a high mutual information feature with w. 4.2. Phase II: Find committees. The second phase of the clustering algorithm recursively finds tight clusters scattered in the similarity space. In each recursive step, the algorithm finds a set of tight clusters, called committees, and identifies residue elements that are not covered by any committee. We say a committee covers an element if the element?s similarity to the centroid of the committee exceeds some high similarity threshold. The algorithm then recursively attempts to find more committees among the residue elements. The output of the algorithm is the union of all committees found in each recursive step. The details of Phase II are presented in Figure 1. In Step 1, the score reflects a preference for bigger and tighter clusters. Step 2 gives preference to higher quality clusters in Step 3, where a cluster is only kept if its similarity to all previously kept clusters is below a fixed threshold. In our experiments, we set ?1 = 0.35. Input: A list of elements E to be clustered, a similarity database S from Phase I, thresh olds ?1 and ?2. Step 1: For each element e ? E Cluster the top similar elements of e from S using average-link clustering. For each cluster discovered c compute the following score: |c| ? avgsim(c), where |c| is the number of elements in c and avgsim(c) is the average pairwise simi larity between elements in c. Store the highest-scoring cluster in a list L. Step 2: Sort the clusters in L in descending order of their scores. Step 3: Let C be a list of committees, initially empty. For each cluster c ? L in sorted order Compute the centroid of c by averaging the frequency vectors of its elements and computing the mutual information vector of the centroid in the same way as we did for individual elements. If c?s similarity to the centroid of each committee previously added to C is be low a threshold ?1, add c to C. Step 4: If C is empty, we are done and return C. Step 5: For each element e ? E If e?s similarity to every committee in C is below threshold ?2, add e to a list of resi dues R. Step 6: If R is empty, we are done and return C. Otherwise, return the union of C and the output of a recursive call to Phase II us ing the same input except replacing E with R. Output: A list of committees. Figure 1. Phase II of CBC. Step 4 terminates the recursion if no committee is found in the previous step. The residue elements are identified in Step 5 and if no residues are found, the algorithm terminates; otherwise, we recursively apply the algorithm to the residue elements. Each committee that is discovered in this phase defines one of the final output clusters of the algorithm. 4.3. Phase III: Assign elements to clusters. In Phase III, every element is assigned to the cluster containing the committee to which it is most similar. This phase resembles K-means in that every element is assigned to its closest centroid. Unlike K-means, the number of clusters is not fixed and the centroids do not change (i.e. when an element is added to a cluster, it is not added to the committee of the cluster). Many cluster evaluation schemes have been proposed. They generally fall under two categories: ? comparing cluster outputs with manually generated answer keys (hereon referred to as classes); or ? embedding the clusters in an application and using its evaluation measure. An example of the first approach considers the average entropy of the clusters, which measures the purity of the clusters (Steinbach, Karypis, and Kumar 2000). However, maximum purity is trivially achieved when each element forms its own cluster. An example of the second approach evaluates the clusters by using them to smooth probability distributions (Lee and Pereira 1999). Like the entropy scheme, we assume that there is an answer key that defines how the elements are supposed to be clustered. Let C be a set of clusters and A be the answer key. We define the editing distance, dist(C, A), as the number of operations required to make C consistent with A. We say that C is consistent with A if there is a one to one mapping between clusters in C and the classes in A such that for each cluster c in C, all elements of c belong to the same class in A. We allow two editing operations: ? merge two clusters; and ? move an element from one cluster to another. Let B be the baseline clustering where each element is its own cluster. We define the quality of a set of clusters C as follows: ( ) ( )ABdist ACdist , ,1? Suppose the goal is to construct a clustering consistent with the answer key. This measure can be interpreted as the percentage of operations saved by starting from C versus starting from the baseline. We aim to construct a clustering consistent with A as opposed to a clustering identical to A because some senses in A may not exist in the corpus used to generate C. In our experiments, we extract answer classes from WordNet. The word dog belongs to both the Person and Animal classes. However, in the newspaper corpus, the Person sense of dog is at best extremely rare. There is no reason to expect a clustering algorithm to discover this sense of dog. The baseline distance dist(B, A) is exactly the number of elements to be clustered. We made the assumption that each element belongs to exactly one cluster. The transforma tion procedure is as follows: 1. Suppose there are m classes in the answer. key. We start with a list of m empty sets, each of which is labeled with a class in the answer key. 2. For each cluster, merge it with the set. whose class has the largest number of elements in the cluster (a tie is broken arbitrarily). 3. If an element is in a set whose class is not. the same as one of the element?s classes, move the element to a set where it be longs. dist(C, A) is the number of operations performed using the above transformation rules on C. a b e c d e a c d b e b a c d e a b c d e A) B) C) D) E) Figure 2. An example of applying the transformation rules to three clusters. A) The classes in the answer key; B) the clusters to be transformed; C) the sets used to reconstruct the classes (Rule 1); D) the sets after three merge operations (Step 2); E) the sets after one move operation (Step 3). Figure 2 shows an example. In D) the cluster containing e could have been merged with either set (we arbitrarily chose the second). The total number of operations is 4. We generated clusters from a news corpus using CBC and compared them with classes extracted from WordNet (Miller 1990). 6.1. Test Data. To extract classes from WordNet, we first estimate the probability of a random word belonging to a subhierarchy (a synset and its hyponyms). We use the frequency counts of synsets in the SemCor corpus (Landes, Leacock, Tengi 1998) to estimate the probability of a subhierarchy. Since SemCor is a fairly small corpus, the frequency counts of the synsets in the lower part of the WordNet hierarchy are very sparse. We smooth the probabilities by assuming that all siblings are equally likely given the parent. A class is then defined as the maximal subhierarchy with probability less than a threshold (we used e-2). We used Minipar 1 (Lin 1994), a broad coverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500 words/second on a PIII-750 with 512MB memory. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3. The test set is constructed by intersecting the words in WordNet with the nouns in the corpus whose total mutual information with all of its contexts exceeds a threshold m. Since WordNet has a low coverage of proper names, we removed all capitalized nouns. We constructed two test sets: S13403 consisting of 13403 words (m = 250) and S3566 consisting of 3566 words (m = 3500). We then removed from the answer classes the words that did not occur in the test sets. Table 1 summa rizes the test sets. The sizes of the WordNet classes vary a lot. For S13403 there are 99 classes that contain three words or less and the largest class contains 3246 words. For S3566, 78 classes have three or less words and the largest class contains 1181 words. 1Available at www.cs.ualberta.ca/~lindek/minipar.htm. 6.2. Cluster Evaluation. We clustered the test sets using CBC and the clustering algorithms of Section 2 and applied the evaluation methodology from the previous section. Table 2 shows the results. The columns are our editing distance based evaluation measure. Test set S3566 has a higher score for all algorithms because it has a higher number of average features per word than S13403. For the K-means and Buckshot algorithms, we set the number of clusters to 250 and the maximum number of iterations to 8. We used a sample size of 2000 for Buckshot. For the Bisecting K-means algorithm, we applied the basic K-means algorithm twice (? = 2 in Section 2) with a maximum of 8 iterations per split. Our implementation of Chameleon was unable to complete clustering S13403 in reasonable time due to its time complexity. Table 2 shows that K-means, Buckshot and Average-link have very similar performance. CBC outperforms all other algorithms on both data sets. 6.3. Manual Inspection. Let c be a cluster and wn(c) be the WordNet class that has the largest intersection with c. The precision of c is defined as: Table 1. A description of the test sets in our experiments. DATA SET TOTAL WORDS m Average # of Features TOTAL CLASSES S13403 13403 250 740.8 202 S3566 3566 3500 2218.3 150 DATA SET TOTAL WORDS M Avg. Features per Word 13403 250 740.8 3566 3500 2218.3 Table 2. Cluster quality (%) of several clustering algorithms on the test sets. ALGORITHM S13403 S3566 CBC 60.95 65.82 K-means (K=250) 56.70 62.48 Buckshot 56.26 63.15 Bisecting K-means 43.44 61.10 Chameleon n/a 60.82 Average-link 56.26 62.62 Complete-link 49.80 60.29 Single-link 20.00 31.74 ( ) ( ) c cwnc cprecision ?= CBC discovered 943 clusters. We sorted them according to their precision. Table 3 shows five of the clusters evenly distributed according to their precision ranking along with their Top-15 features with highest mutual-information. The words in the clusters are listed in descending order of their similarity to the cluster centroid. For each cluster c, we also include wn(c). The underlined words are in wn(c). The first cluster is clearly a cluster of firearms and the second one is of pests. In WordNet, the word pest is curiously only under the person hierarchy. The words stopwatch and houseplant do not belong to the clusters but they have low similarity to their cluster centroid. The third cluster represents some kind of control. In WordNet, the legal power sense of jurisdiction is not a hyponym of social control as are supervision, oversight and governance. The fourth cluster is about mixtures. The words blend and mix as the event of mixing are present in WordNet but not as the result of mixing. The last cluster is about consumers. Here is the consumer class in WordNet 1.5: addict, alcoholic, big spender, buyer, client, concert-goer, consumer, customer, cutter, diner, drinker, drug addict, drug user, drunk, eater, feeder, fungi, head, heroin addict, home buyer, junkie, junky, lush, nonsmoker, patron, policy holder, purchaser, reader, regular, shopper, smoker, spender, subscriber, sucker, taker, user, vegetarian, wearer In our cluster, only the word client belongs to WordNet?s consumer class. The cluster is ranked very low because WordNet failed to consider words like patient, tenant and renter as consumers. Table 3 shows that even the lowest ranking CBC clusters are fairly coherent. The features associated with each cluster can be used to classify previously unseen words into one or more existing clusters. Table 4 shows the clusters containing the word cell that are discovered by various clustering algorithms from S13403. The underlined words represent the words that belong to the cell class in WordNet. The CBC cluster corresponds almost exactly to WordNet?s cell class. K-means and Buckshot produced fairly coherent clusters. The cluster constructed by Bisecting K-means is obviously of inferior quality. This is consistent with the fact that Bisecting K-means has a much lower score on S13403 compared to CBC, K means and Buckshot. Table 3. Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutual information and the WordNet classes that have the largest intersection with each cluster. RANK MEMBERS TOP-15 FEATURES wn(c) 1 handgun, revolver, shotgun, pistol, rifle, machine gun, sawed-off shotgun, submachine gun, gun, automatic pistol, automatic rifle, firearm, carbine, ammunition, magnum, cartridge, automatic, stopwatch __ blast, barrel of __ , brandish __, fire __, point __, pull out __, __ discharge, __ fire, __ go off, arm with __, fire with __, kill with __, open fire with __, shoot with __, threaten with __ artifact / artifact 236 whitefly, pest, aphid, fruit fly, termite, mosquito, cockroach, flea, beetle, killer bee, maggot, predator, mite, houseplant, cricket __ control, __ infestation, __ larvae, __ population, infestation of __, specie of __, swarm of __ , attract __, breed __, eat __, eradicate __, feed on __, get rid of __, repel __, ward off __ animal / animate being / beast / brute / creature / fauna 471 supervision, discipline, oversight, control, governance, decision making, jurisdiction breakdown in __, lack of __ , loss of __, assume __, exercise __, exert __, maintain __, retain __, seize __, tighten __, bring under __, operate under __, place under __, put under __, remain under __ act / human action / human activity 706 blend, mix, mixture, combination, juxtaposition, combine, amalgam, sprinkle, synthesis, hybrid, melange dip in __, marinate in __, pour in __, stir in __, use in __, add to __, pour __, stir __, curious __, eclectic __, ethnic __, odd __, potent __, unique __, unusual __ group / grouping 941 employee, client, patient, applicant, tenant, individual, participant, renter, volunteer, recipient, caller, internee, enrollee, giver benefit for __, care for __, housing for __, benefit to __, service to __, filed by __, paid by __, use by __, provide for __, require for --, give to __, offer to __, provide to __, disgruntled __, indigent __ worker","We generated clusters from a news corpus using CBC and compared them with classes extracted from WordNet (Miller 1990). 6.1. Test Data. To extract classes from WordNet, we first estimate the probability of a random word belonging to a subhierarchy (a synset and its hyponyms). We use the frequency counts of synsets in the SemCor corpus (Landes, Leacock, Tengi 1998) to estimate the probability of a subhierarchy. Since SemCor is a fairly small corpus, the frequency counts of the synsets in the lower part of the WordNet hierarchy are very sparse. We smooth the probabilities by assuming that all siblings are equally likely given the parent. A class is then defined as the maximal subhierarchy with probability less than a threshold (we used e-2). We used Minipar 1 (Lin 1994), a broad coverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500 words/second on a PIII-750 with 512MB memory. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3. The test set is constructed by intersecting the words in WordNet with the nouns in the corpus whose total mutual information with all of its contexts exceeds a threshold m. Since WordNet has a low coverage of proper names, we removed all capitalized nouns. We constructed two test sets: S13403 consisting of 13403 words (m = 250) and S3566 consisting of 3566 words (m = 3500). We then removed from the answer classes the words that did not occur in the test sets. Table 1 summa rizes the test sets. The sizes of the WordNet classes vary a lot. For S13403 there are 99 classes that contain three words or less and the largest class contains 3246 words. For S3566, 78 classes have three or less words and the largest class contains 1181 words. 1Available at www.cs.ualberta.ca/~lindek/minipar.htm. 6.2. Cluster Evaluation. We clustered the test sets using CBC and the clustering algorithms of Section 2 and applied the evaluation methodology from the previous section. Table 2 shows the results. The columns are our editing distance based evaluation measure. Test set S3566 has a higher score for all algorithms because it has a higher number of average features per word than S13403. For the K-means and Buckshot algorithms, we set the number of clusters to 250 and the maximum number of iterations to 8. We used a sample size of 2000 for Buckshot. For the Bisecting K-means algorithm, we applied the basic K-means algorithm twice (? = 2 in Section 2) with a maximum of 8 iterations per split. Our implementation of Chameleon was unable to complete clustering S13403 in reasonable time due to its time complexity. Table 2 shows that K-means, Buckshot and Average-link have very similar performance. CBC outperforms all other algorithms on both data sets. 6.3. Manual Inspection. Let c be a cluster and wn(c) be the WordNet class that has the largest intersection with c. The precision of c is defined as: Table 1. A description of the test sets in our experiments. DATA SET TOTAL WORDS m Average # of Features TOTAL CLASSES S13403 13403 250 740.8 202 S3566 3566 3500 2218.3 150 DATA SET TOTAL WORDS M Avg. Features per Word 13403 250 740.8 3566 3500 2218.3 Table 2. Cluster quality (%) of several clustering algorithms on the test sets. ALGORITHM S13403 S3566 CBC 60.95 65.82 K-means (K=250) 56.70 62.48 Buckshot 56.26 63.15 Bisecting K-means 43.44 61.10 Chameleon n/a 60.82 Average-link 56.26 62.62 Complete-link 49.80 60.29 Single-link 20.00 31.74 ( ) ( ) c cwnc cprecision ?= CBC discovered 943 clusters. We sorted them according to their precision. Table 3 shows five of the clusters evenly distributed according to their precision ranking along with their Top-15 features with highest mutual-information. The words in the clusters are listed in descending order of their similarity to the cluster centroid. For each cluster c, we also include wn(c). The underlined words are in wn(c). The first cluster is clearly a cluster of firearms and the second one is of pests. In WordNet, the word pest is curiously only under the person hierarchy. The words stopwatch and houseplant do not belong to the clusters but they have low similarity to their cluster centroid. The third cluster represents some kind of control. In WordNet, the legal power sense of jurisdiction is not a hyponym of social control as are supervision, oversight and governance. The fourth cluster is about mixtures. The words blend and mix as the event of mixing are present in WordNet but not as the result of mixing. The last cluster is about consumers. Here is the consumer class in WordNet 1.5: addict, alcoholic, big spender, buyer, client, concert-goer, consumer, customer, cutter, diner, drinker, drug addict, drug user, drunk, eater, feeder, fungi, head, heroin addict, home buyer, junkie, junky, lush, nonsmoker, patron, policy holder, purchaser, reader, regular, shopper, smoker, spender, subscriber, sucker, taker, user, vegetarian, wearer In our cluster, only the word client belongs to WordNet?s consumer class. The cluster is ranked very low because WordNet failed to consider words like patient, tenant and renter as consumers. Table 3 shows that even the lowest ranking CBC clusters are fairly coherent. The features associated with each cluster can be used to classify previously unseen words into one or more existing clusters. Table 4 shows the clusters containing the word cell that are discovered by various clustering algorithms from S13403. The underlined words represent the words that belong to the cell class in WordNet. The CBC cluster corresponds almost exactly to WordNet?s cell class. K-means and Buckshot produced fairly coherent clusters. The cluster constructed by Bisecting K-means is obviously of inferior quality. This is consistent with the fact that Bisecting K-means has a much lower score on S13403 compared to CBC, K means and Buckshot. Table 3. Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutual information and the WordNet classes that have the largest intersection with each cluster. RANK MEMBERS TOP-15 FEATURES wn(c) 1 handgun, revolver, shotgun, pistol, rifle, machine gun, sawed-off shotgun, submachine gun, gun, automatic pistol, automatic rifle, firearm, carbine, ammunition, magnum, cartridge, automatic, stopwatch __ blast, barrel of __ , brandish __, fire __, point __, pull out __, __ discharge, __ fire, __ go off, arm with __, fire with __, kill with __, open fire with __, shoot with __, threaten with __ artifact / artifact 236 whitefly, pest, aphid, fruit fly, termite, mosquito, cockroach, flea, beetle, killer bee, maggot, predator, mite, houseplant, cricket __ control, __ infestation, __ larvae, __ population, infestation of __, specie of __, swarm of __ , attract __, breed __, eat __, eradicate __, feed on __, get rid of __, repel __, ward off __ animal / animate being / beast / brute / creature / fauna 471 supervision, discipline, oversight, control, governance, decision making, jurisdiction breakdown in __, lack of __ , loss of __, assume __, exercise __, exert __, maintain __, retain __, seize __, tighten __, bring under __, operate under __, place under __, put under __, remain under __ act / human action / human activity 706 blend, mix, mixture, combination, juxtaposition, combine, amalgam, sprinkle, synthesis, hybrid, melange dip in __, marinate in __, pour in __, stir in __, use in __, add to __, pour __, stir __, curious __, eclectic __, ethnic __, odd __, potent __, unique __, unusual __ group / grouping 941 employee, client, patient, applicant, tenant, individual, participant, renter, volunteer, recipient, caller, internee, enrollee, giver benefit for __, care for __, housing for __, benefit to __, service to __, filed by __, paid by __, use by __, provide for __, require for --, give to __, offer to __, provide to __, disgruntled __, indigent __ worker"
65,"We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated.","We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated. We are concerned with surface-syntactic parsing of running text. Our main goal is to describe syntactic analyses of sentences using dependency links that show the head-modifier relations between words. In addition, these links have labels that refer to the syntactic function of the modifying word. A simplified example is in Figure 1, where the link between I and see denotes that I is the modifier of see and its syntactic function is that of subject. Similarly, a modifies bird, and it is a determiner. First, in this paper, we explain some central concepts of the Constraint Grammar framework from which many of the ideas are derived. Then, we give some linguistic background to the notations we are using, with a brief comparison to other current dependency formalisms and systems. New formalism is described briefly, and it is utilised in a small toy grammar to illustrate how the formalism works. Finally, the real parsing system, with a grammar of some 2 500 rules, is evaluated. The parser corresponds to over three man-years of work, which does not include the lexical analyser and the morphological disambiguator, both parts of the existing English Constraint Grammar parser (Karlsson et al., 1995). The parsers can be tested via WWW'. Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990). A detailed description of the English Constraint Grammar (ENGCG) is in Karlsson et al. (1995). The basic rule types of the Constraint Grammar (Tapanainen, 1996)2 are REMOVE and SELECT for discarding and selecting an alternative reading of a word. Rules also have contextual tests that describe the condition according to which they may be applied. For example, the rule discards a verb (V) reading if the preceding word (-1) is unambiguously (C) a determiner (DET). More than one such test can be appended to a rule. The rule above represents a local rule: the test checks only neighbouring words in a foreknown position before or after the target word. The test may also refer to the positions somewhere in the sentence without specifying the exact location. For instance, means that a nominal head (NOM-HEAD is a set that contains part-of-speech tags that may represent a nominal head) may not appear anywhere to the left (NOT *-1). This &quot;anywhere&quot; to the left or right may be restricted by BARRIERs, which restrict the area of the test. Basically, the barrier can be used to limit the test only to the current clause (by using clause boundary markers and &quot;stop words&quot;) or to a constituent (by using &quot;stop categories&quot;) instead of the whole sentence. In addition, another test may be added relative to the unrestricted context position using keyword LINK. For example, the following rule discards the syntactic function' CI-OBJ (indirect object): The rule holds if the closest finite verb to the left is unambiguously (C) a finite verb (VFIN), and there is no ditransitive verb or participle (subcategorisation SV00) between the verb and the indirect object. If, in addition, the verb does not take indirect objects, i.e. there is no SVO0 in the same verb (LINK NOT 0 SV00), the CI-OBJ reading will be discarded. In essence, the same formalism is used in the syntactic analysis in Jarvinen (1994) and Anttila (1995). After the morphological disambiguation, all legitimate surface-syntactic labels are added to the set of morphological readings. Then, the syntactic rules discard contextually illegitimate alternatives or select legitimate ones. The syntactic tagset of the Constraint Grammar provides an underspecific dependency description. For example, labels for functional heads (such as CSUBJ, COBJ, CI-OBJ) mark the word which is a head of a noun phrase having that function in the clause, but the parent is not indicated. In addition, the representation is shallow, which means that, e.g., objects of infinitives and participles receive the same type of label as objects of finite verbs. On the other hand, the non-finite verb forms functioning as objects receive only verbal labels. When using the grammar formalism described above, a considerable amount of syntactic ambiguity can not be resolved reliably and is therefore left pending in the parse. As a consequence, the output is not optimal in many applications. For example, it is not possible to reliably pick head-modifier pairs from the parser output or collect arguments of verbs, which was one of the tasks we originally were interested in. To solve the problems, we developed a more powerful rule formalism which utilises an explicit dependency representation. The basic Constraint Gram'The convention in the Constraint Grammar is that the tags for syntactic functions begin with the 0-sign. mar idea of introducing the information in a piecemeal fashion is retained, but the integration of different pieces of information is more efficient in the new system. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesniere (1959) and later advocated by Igor Mel'auk (1987). In Tesniere's and Mel'auk's dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Ternperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. This, however, is a matter of output formatting, for which the system makes several options available. The verbs (as well as other elements) have a valency that describes the number and type of the modifiers they may have. In valency theory, usually, complements (obligatory) and adjuncts (optional) are distinguished. Our notation makes a difference between valency (rule-based) and subcategorisation (lexical): the valency tells which arguments are expected; the subcategorisation tells which combinations are legitimate. The valency merely provides a possibility to have an argument. Thus, a verb having three valency slots may have e.g. subcategorisation SVOO or SVOC. The former denotes: Subject, Verb, indirect Object and Object, and the latter: Subject, Verb, Object and Object Complement. The default is a nominal type of complement, but there might also be additional information concerning the range of possible complements, e.g., the verb say may have an object (SVO), which may also be realised as a to-infinitive clause, WH-clause, that-clause or quote structure. The adjuncts are not usually marked in the verbs because most of the verbs may have e.g. spatiotemporal arguments. Instead, adverbial complements and adjuncts that are typical of particular verbs are indicated. For instance, the verb decide has the tag <P/on> which means that the prepositional phrase on is typically attached to it. The distinction between the complements and the adjuncts is vague in the implementation; neither the complements nor the adjuncts are obligatory. Usually, both the dependent element and its head are implicitly (and ambiguously) present in the Constraint Grammar type of rule. Here, we make this dependency relation explicit. This is done by declaring the heads and the dependents (complement or modifier) in the context tests. For example, the subject label (OSOBJ) is chosen and marked as a dependent of the immediately following auxiliary (AUXMOD) in the following rule: SELECT (@SUBJ) IF (IC AUXMOD HEAD); To get the full benefit of the parser, it is also useful to name the valency slot in the rule. This has two effects: (1) the valency slot is unique, i.e. no more than one subject is linked to a finite verb', and (2) we can explicitly state in rules which kind of valency slots we expect to be filled. The rule thus is of the form: IF (IC AUXMOD HEAD = subject); The rule above works well in an unambiguous context but there is still need to specify more tolerant rules for ambiguous contexts. The rule differs from the previous rule in that it leaves the other readings of the noun intact and only adds a (possible) subject dependency, while both the previous rules disambiguated the noun reading also. But especially in the rule above, the contextual test is far from being sufficient to select the subject reading reliably. Instead, it leaves open a possibility to attach a dependency from another syntactic function, i.e. the dependency relations remain ambiguous. The grammar tries to be careful not to introduce false dependencies but for an obvious reason this is not always possible. If several syntactic functions of a word have dependency relations, they form a dependency forest. Therefore, when the syntactic function is not rashly disambiguated, the correct reading may survive even after illegitimate linking, as the global pruning (Section 5) later extracts dependency links that form consistent trees. Links formed between syntactic labels constitute partial trees, usually around verbal nuclei. But a new mechanism is needed to make full use of the structural information provided by multiple rules. Once a link is formed between labels, it can be used by the other rules. For example, when a head of an object phrase (OM) is found and indexed to a verb, the noun phrase to the right (if any) is probably an object complement (CPCOMPL-0). It should have the same head as the existing object if the verb has the proper subcategorisation tag (SVOC). The following rule establishes a dependency relation of a verb and its object complement, if the object already exists. The rule says that a dependency relation (o-compl) should be added but the syntactic functions should not be disambiguated (INDEX). The object complement OPCOMPL-0 is linked to the verb readings having the subcategorisation SVOC. The relation of the object complement and its head is such that the noun phrase to the left of the object complement is an object (CM) that has established a dependency relation (object) to the verb. Naturally, the dependency relations may also be followed downwards (DOWN). But it is also possible to declare the last item in a chain of the links (e.g. the verb chain would have been wanted) using the keywords TOP and BOTTOM. We pursue the following strategy for linking and disambiguation. in the new dependency grammar. In practice, these rules are most likely to cause errors, apart from their linguistic interpretation often being rather obscure. Moreover, there is no longer any need to remove these readings explicitly by rules, because the global pruning removes readings which have not obtained any &quot;extra evidence&quot;. Roughly, one could say that the REMOVE rules of the Constraint Grammar are replaced by the INDEX rules. The overall result is that the rules in the new framework are much more careful than those of ENGCG. As already noted, the dependency grammar has a big advantage over ENGCG in dealing with ambiguity. Because the dependencies are supposed to form a tree, we can heuristically prune readings that are not likely to appear in such a tree. We have the following hypotheses: (1) the dependency forest is quite sparse and a whole parse tree can not always be found; (2) pruning should favour large (sub)trees; (3) unlinked readings of a word can be removed when there is a linked reading present among the alternatives; (4) unambiguous subtrees are more likely to be correct than ambiguous ones; and (5) pruning need not force the words to be unambiguous. Instead, we can apply the rules iteratively, and usually some of the rules apply when the ambiguity is reduced. Pruning is then applied again, and so on. Furthermore, the pruning mechanism does not contain any language specific statistics, but works on a topological basis only. Some of the most heuristic rules may be applied only after pruning. This has two advantages: very heuristic links would confuse the pruning mechanism, and words that would not otherwise have a head, may still get one. In this section, we present a set of rules, and show how those rules can parse the sentence &quot;Joan said whatever John likes to decide suits her&quot;. The toy grammar containing 8 rules is presented in Figure 3. The rules are extracted from the real grammar, and they are then simplified; some tests are omitted and some tests are made simpler. The grammar is applied to the input sentence in Figure 4, where the tags are almost equivalent to those used by the English Constraint Grammar, and the final result equals Figure 2, where only the dependencies between the words and certain tags are printed. Some comments concerning the rules in the toy grammar (Figure 3) are in order: The rule states: the first noun phrase head label to the right is a subject (OSOBJ), link subj exists and is followed up to the finite verb (0+F) in a verb chain (v-ch), which is then followed up to the main verb. Then object or complement links are followed downwards (BOTTOM), to the last verbal reading (here decide). If then a verb with subcategorisation for objects is encountered, an object link from the WH-pronoun is formed. This kind of rule that starts from word A, follows links up to word B and then down to word C, introduces a non-projective dependency link if word B is between words A and C. Note that the conditions TOP and BOTTOM follow the chain of named link, if any, to the upper or lower end of a chain of a multiple (zero or more) links with the same name. Therefore TOP v-ch: @MA INV always ends with the main verb in the verb chain, whether this be a single finite verb like likes or a chain like would have been liked. 6. The WH-clause itself may function as a subject, object, etc. Therefore, there is a set of rules for each function. The &quot;WH-clause as subject&quot; rule looks for a finite verb to the right. No intervening subject labels and clause boundaries are allowed. * Rules 1-5 are applied in the first round. After that, the pruning operation disambiguates finite verbs, and rule 6 will apply. Pruning will be applied once again. The sentence is thus disambiguated both morphologically and morphosyntactically, and a syntactic phosyntactic alternatives, e.g. whatever is ambiguous in 10 ways. The subcategorisation/valency information is not printed here. reading from each word belongs to a subtree of which the root is said or suits. 7. The syntactic relationship between the verbs is established by a rule stating that the rightmost main verb is the (clause) object of a main verb to the left, which allows such objects. 8. Finally, there is a single main verb, which is indexed to the root (<s>) (in position 00). The evaluation was done using small excerpts of data, not used in the development of the system. All text samples were excerpted from three different genres in the Bank of English (Jarvinen, 1994) data: American National Public Radio (broadcast), British Books data (literature), and The Independent (newspaper). Figure 5 lists the samples, their sizes, and the average and maximum sentence lengths. The measure is in words excluding punctuation. size avg. max. total In addition, Figure 5 shows the total processing time required for the syntactic analysis of the samples. The syntactic analysis has been done in a normal PC with the Linux operating system. The PC has a Pentium 90 MHz processor and 16 MB of memory. The speed roughly corresponds to 200 words in second. The time does not include morphological analysis and disambiguation6. broadcast literature newspaper One obvious point of reference is the ENGCG syntax, which shares a level of similar representation with an almost identical tagset to the new system. In addition, both systems use the front parts of the ENGCG system for processing the input. These include the tokeniser, lexical analyser and morphological disambiguator. Figure 6 shows the results of the comparison of the ENGCG syntax and the morphosyntactic level of the dependency grammar. Because both systems leave some amount of the ambiguity pending, two figures are given: the success rate, which is the percentage of correct morphosyntactic labels present in the output, and the ambiguity rate, which is the percentage of words containing more than one label. The ENGCG results compare to those reported elsewhere (Jirvinen, 1994; Tapanainen and Jarvinen, 1994). The DG success rate is similar or maybe even slightly better than in ENGCG. More importantly, the ambiguity rate is only about a quarter of that in the ENGCG output. The overall result should be considered good in the sense that the output contains information about the syntactic functions (see Figure 4) not only part-of-speech tags. The major improvement over ENGCG is the level of explicit dependency representation, which makes it possible to excerpt modifiers of certain elements, such as arguments of verbs. This section evaluates the success of the level of dependencies. One of the crude measures to evaluate dependencies is to count how many times the correct head is found. The results are listed in Fig( received. correct links, ) ure 7. Precision is and rereceived links call ( ). received correct links, The difference between ‘ desired lnks precision and recalli is due to the fact that the parser does not force a head on every word. Trying out some very heuristic methods to assign heads would raise recall but lower precision. A similar measure is used in (Eisner, 1996) except that every word has a head, i.e. the precision equals recall, reported as 79.2%. We evaluated our parser against the selected dependencies in the test samples. The samples being rather small, only the most common dependencies are evaluated: subject, object and predicative. These dependencies are usually resolved more reliably than, say, appositions, prepositional attachments etc. The results of the test samples are listed in Figure 8. It seems the parser leaves some amount of the words unlinked (e.g. 10-15% of subjects) but what it has recognised is generally correct (precision 95-98% for subjects). Dekang Lin (1996) has earlier used this kind of evaluation, where precision and recall were for subjects 87% and 78%, and for complements (including objects) 84% and 72 %, respectively. The results are not strictly comparable because the syntactic description is somewhat different. In this paper, we have presented some main features of our new framework for dependency syntax. The most important result is that the new framework allows us to describe non-projective dependency grammars and apply them efficiently. This is a property that will be crucial when we will apply this framework to a language having free word-order. Basically, the parsing framework combines the Constraint Grammar framework (removing ambiguous readings) with a mechanism that adds dependencies between readings or tags. This means that while the parser disambiguates it also builds up a dependency forest that, in turn, is reduced by other disambiguation rules and a global pruning mechanism. This setup makes it possible to operate on several layers of information, and use and combine structural information more efficiently than in the original Constraint Grammar framework, without any further disadvantage in dealing with ambiguity. First preliminary evaluations are presented. Compared to the ENGCG syntactic analyser, the output not only contains more information but it is also more accurate and explicit. The ambiguity rate is reduced to a quarter without any compromise in correctness. We did not have access to other systems, and care must be taken when interpreting the results which are not strictly comparable. However, the comparison to other current systems suggests that our dependency parser is very promising both theoretically and practically.","In this paper, we have presented some main features of our new framework for dependency syntax. The most important result is that the new framework allows us to describe non-projective dependency grammars and apply them efficiently. This is a property that will be crucial when we will apply this framework to a language having free word-order. Basically, the parsing framework combines the Constraint Grammar framework (removing ambiguous readings) with a mechanism that adds dependencies between readings or tags. This means that while the parser disambiguates it also builds up a dependency forest that, in turn, is reduced by other disambiguation rules and a global pruning mechanism. This setup makes it possible to operate on several layers of information, and use and combine structural information more efficiently than in the original Constraint Grammar framework, without any further disadvantage in dealing with ambiguity. First preliminary evaluations are presented. Compared to the ENGCG syntactic analyser, the output not only contains more information but it is also more accurate and explicit. The ambiguity rate is reduced to a quarter without any compromise in correctness. We did not have access to other systems, and care must be taken when interpreting the results which are not strictly comparable. However, the comparison to other current systems suggests that our dependency parser is very promising both theoretically and practically."
66,"In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally. We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems.","In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally. We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. Sentence simplification transforms long and dif ficult sentences into shorter and more readable ones. This helps humans read texts more easilyand faster. Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facili tate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently,sentence simplification has also been shown help ful for summarization (Knight and Marcu, 2000), ? This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) underthe grant No. GU 798/3-1, and by the Volkswagen Founda tion as part of the Lichtenberg-Professorship Program under the grant No. I/82806.sentence fusion (Filippova and Strube, 2008b), se mantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sen tence simplification can therefore be classifiedinto two types: lexical simplification and syntac tic simplification (Carroll et al, 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations. The splitting operation splits a long sentence intoseveral shorter sentences to decrease the complex ity of the long sentence. The dropping operation further removes unimportant parts of a sentence to make it more concise. The reordering operationinterchanges the order of the split sentences (Sid dharthan, 2006) or parts in a sentence (Watanabeet al, 2009). Finally, the substitution operation re places difficult phrases or words with their simpler synonyms.In most cases, different simplification operations happen simultaneously. It is therefore nec essary to consider the simplification process as a combination of different operations and treatthem as a whole. However, most of the existing models only consider one of these operations. Siddharthan (2006) and Petersen and Ostendorf (2007) focus on sentence splitting, while sen tence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As faras lexical simplification is concerned, word substitution is usually done by selecting simpler syn onyms from Wordnet based on word frequency (Carroll et al, 1999).In this paper, we propose a sentence simplifica tion model by tree transformation which is based 1353 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada andKnight, 2002; Graehl et al, 2008). Our model in tegrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of ourmodel can be efficiently learned from complex simple parallel datasets. The transformation froma complex sentence to a simple sentence is con ducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingualword mapping which speeds up the training pro cess significantly. Finally, a decoder is designed to generate the simplified sentences using a greedy strategy and integrates language models.In order to train our model, we further com pile a large-scale complex-simple parallel dataset(PWKP) from Simple English Wikipedia1 and En glish Wikipedia2, as such datasets are rare.We organize the remainder of the paper as follows: Section 2 describes the PWKP dataset. Sec tion 3 presents our TSM model. Sections 4 and 5 are devoted to training and decoding, respectively. Section 6 details the evaluation. The conclusions follow in the final section. We collected a paired dataset from the English Wikipedia and Simple English Wikipedia. The targeted audience of Simple Wikipedia includes?children and adults who are learning English lan guage?. The authors are requested to ?use easy words and short sentences? to compose articles. We processed the dataset as follows: Article Pairing 65,133 articles from SimpleWikipedia3 and Wikipedia4 were paired by fol lowing the ?language link? using the dump filesin Wikimedia.5 Administration articles were fur ther removed. Plain Text Extraction We use JWPL (Zesch etal., 2008) to extract plain texts from Wikipedia ar ticles by removing specific Wiki tags. Pre-processing including sentence boundary detection and tokenization with the Stanford 1http://simple.wikipedia.org 2http://en.wikipedia.org 3As of Aug 17th, 2009 4As of Aug 22nd, 2009 5http://download.wikimedia.org Parser package (Klein and Manning, 2003), and lemmatization with the TreeTagger (Schmid, 1994). Monolingual Sentence Alignment As we need a parallel dataset algned at the sentence level,we further applied monolingual sentence align ment on the article pairs. In order to achieve the best sentence alignment on our dataset, we tested three similarity measures: (i) sentence-level TF*IDF (Nelken and Shieber, 2006), (ii) word overlap (Barzilay and Elhadad, 2003) and (iii)word-based maximum edit distance (MED) (Lev enshtein, 1966) with costs of insertion, deletionand substitution set to 1. To evaluate their perfor mance we manually annotated 120 sentence pairs from the article pairs. Tab. 1 reports the precision and recall of these three measures. We manually adjusted the similarity threshold to obtain a recallvalue as close as possible to 55.8% which was pre viously adopted by Nelken and Shieber (2006). Similarity Precision Recall TF*IDF 91.3% 55.4% Word Overlap 50.5% 55.1% MED 13.9% 54.7% Table 1: Monolingual Sentence Alignment The results in Tab. 1 show that sentence-levelTF*IDF clearly outperforms the other two mea sures, which is consistent with the results reported by Nelken and Shieber (2006). We henceforth chose sentence-level TF*IDF to align our dataset. As shown in Tab. 2, PWKP contains more than 108k sentence pairs. The sentences from Wikipedia and Simple Wikipedia are considered as ?complex? and ?simple? respectively. Both the average sentence length and average token length in Simple Wikipedia are shorter than those inWikipedia, which is in compliance with the pur pose of Simple Wikipedia. Avg. Sen. Len Avg. Tok. Len #Sen.Pairscomplex simple complex simple 25.01 20.87 5.06 4.89 108,016 Table 2: Statistics for the PWKP datasetIn order to account for sentence splitting, we al low 1 to n sentence alignment to map one complexsentence to several simple sentences. We first per form 1 to 1 mapping with sentence-level TF*IDF and then combine the pairs with the same complex sentence and adjacent simple sentences. We apply the following simplification operations to the parse tree of a complex sentence: splitting, 1354dropping, reordering and substitution. In this sec tion, we use a running example to illustrate thisprocess. c is the complex sentence to be simpli fied in our example. Fig. 1 shows the parse tree of c (we skip the POS level).c: August was the sixth month in the ancient Ro man calendar which started in 735BC. NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar whichthe Roman month Figure 1: Parse Tree of c 3.1 Splitting. The first operation is sentence splitting, which wefurther decompose into two subtasks: (i) segmen tation, which decides where and whether to split a sentence and (ii) completion, which makes the new split sentences complete. First, we decide where we can split a sentence. In our model, the splitting point is judged by the syntactic constituent of the split boundary word in the complex sentence. The decision whether a sentence should be split is based on the length of the complex sentence. The features used in the segmentation step are shown in Tab. 3. Word Constituent iLength isSplit Prob. ?which? SBAR 1 true 0.0016 ?which? SBAR 1 false 0.9984 ?which? SBAR 2 true 0.0835 ?which? SBAR 2 false 0.9165 Table 3: Segmentation Feature Table (SFT) Actually, we do not use the direct constituent of a word in the parse tree. In our example, the directconstituent of the word ?which? is ?WHNP?. In stead, we use Alg. 1 to calculate the constituentof a word. Alg. 1 returns ?SBAR? as the adjusted constituent for ?which?. Moreover, di rectly using the length of the complex sentenceis affected by the data sparseness problem. In stead, we use iLength as the feature which is calculated as iLength = ceiling( comLengthavgSimLength), where comLength is the length of the complex sentence and avgSimLength is the average length of simple sentences in the training dataset. The ?Prob.? column shows the probabilities obtained after training on our dataset. Algorithm 1 adjustConstituent(word, tree) constituent? word.father; father ? constituent.father; while father 6= NULL AND constituent is the most left child of father do constituent? father; father ? father.father; end while return constituent; In our model, one complex sentence can be split into two or more sentences. Since many splitting operations are possible, we need to select the mostlikely one. The probability of a segmentation op eration is calculated as: P (seg|c) = ? w:c SFT (w|c) (1) where w is a word in the complex sentence c and SFT (w|c) is the probability of the word w in the Segmentation Feature Table (SFT); Fig. 2 shows a possible segmentation result of our example. NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar which the Roman month Figure 2: Segmentation The second step is completion. In this step, we try to make the split sentences complete and grammatical. In our example, to make the second sentence ?which started in 735BC? complete and grammatical we should first drop the border word ?which? and then copy the dependent NP ?the ancient Roman calendar? to the left of ?started?to obtain the complete sentence ?the ancient Ro man calendar started in 735BC?. In our model, whether the border word should be dropped or retained depends on two features of the border word: the direct constituent of the word and the word itself, as shown in Tab. 4. Const. Word isDropped Prob. WHNP which True 1.0 WHNP which False Prob.Min Table 4: Border Drop Feature Table (BDFT) In order to copy the necessary parts to complete the new sentences, we must decide which parts should be copied and where to put these parts in the new sentences. In our model, this is judged by two features: the dependency relation and theconstituent. We use the Stanford Parser for parsing the dependencies. In our example, the de 1355pendency relation between ?calendar? in the com plex sentence and the verb ?started? in the secondsplit sentence is ?gov nsubj?.6 The direct constituent of ?started? is ?VP? and the word ?calen dar? should be put on the ?left? of ?started?, see Tab. 5. Dep. Const. isCopied Pos. Prob. gov nsubj VP(VBD) True left 0.9000 gov nsubj VP(VBD) True right 0.0994 gov nsubj VP(VBD) False - 0.0006 Table 5: Copy Feature Table (CFT) For dependent NPs, we copy the whole NP phrase rather than only the head noun.7 In ourexample, we copy the whole NP phrase ?the an cient Roman calendar? to the new position rather than only the word ?calendar?. The probability of a completion operation can be calculated as P (com|seg) = Y bw:s BDFT (bw|s) Y w:s Y dep:w CFT (dep). where s are the split sentences, bw is a border word in s, w is a word in s, dep is a dependency of w which is out of the scope of s. Fig. 3 shows the most likely result of the completion operation for our example. NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendarthe RomanNP ancient calendarthe Roman month Figure 3: Completion 3.2 Dropping and Reordering. We first apply dropping and then reordering to each non-terminal node in the parse tree from topto bottom. We use the same features for both drop ping and reordering: the node?s direct constituent and its children?s constituents pattern, see Tab. 6 and Tab. 7. Constituent Children Drop Prob. NP DT JJ NNP NN 1101 7.66E-4 NP DT JJ NNP NN 0001 1.26E-7 Table 6: Dropping Feature Table (DFT) 6With Stanford Parser, ?which? is a referent of ?calender? and the nsubj of ?started?. ?calender? thus can be considered to be the nsubj of ?started? with ?started? as the governor. 7The copied NP phrase can be further simplified in the following steps. Constituent Children Reorder Prob. NP DT JJ NN 012 0.8303 NP DT JJ NN 210 0.0039 Table 7: Reordering Feature Table (RFT)The bits ?1? and ?0? in the ?Drop? column indicate whether the corresponding constituent is re tained or dropped. The number in the ?Reorder? column represents the new order for the children. The probabilities of the dropping and reordering operations can be calculated as Equ. 2 and Equ. 3. P (dp|node) = DFT (node) (2) P (ro|node) = RFT (node) (3) In our example, one of the possible results is dropping the NNP ?Roman?, as shown in Fig. 4. NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendartheNP ancient calendarthe month Figure 4: Dropping & Reordering 3.3 Substitution. 3.3.1 Word SubstitutionWord substitution only happens on the termi nal nodes of the parse tree. In our model, the conditioning features include the original word and the substitution. The substitution for a word can be another word or a multi-word expression(see Tab. 8). The probability of a word substitu tion operation can be calculated as P (sub|w) = SubFT (Substitution|Origin). Origin Substitution Prob. ancient ancient 0.963 ancient old 0.0183 ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase SubstitutionPhrase substitution happens on the non terminal nodes and uses the same conditioningfeatures as word substitution. The ?Origin? con sists of the leaves of the subtree rooted at the node. When we apply phrase substitution on anon-terminal node, then any simplification operation (including dropping, reordering and substitu tion) cannot happen on its descendants any more 1356 because when a node has been replaced then its descendants are no longer existing. Therefore, for each non-terminal node we must decide whether a substitution should take place at this node or at itsdescendants. We perform substitution for a non terminal node if the following constraint is met: Max(SubFT (?|node)) ? Y ch:node Max(SubFT (?|ch)). where ch is a child of the node. canbe any substitution in the SubFT. The proba bility of the phrase substitution is calculated as P (sub|node) = SubFT (Substitution|Origin).Fig. 5 shows one of the possible substitution re sults for our example where ?ancient? is replaced by ?old?. NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC old calendartheNP old calendarthe month Figure 5: Substitution As a result of all the simplification operations, we obtain the following two sentences: s1 = Str(pt1)=?August was the sixth month in the old calendar.? and s2 = Str(pt2)=?The old calendar started in 735BC.? 3.4 The Probabilistic Model. Our model can be formalized as a direct translation model from complex to simple P (s|c) multi plied by a language model P (s) as shown in Equ. 4. s = argmax s P (s|c)P (s) (4) We combine the parts described in the previous sections to get the direct translation model: P (s|c) = ? ?:Str(?(c))=s (P (seg|c)P (com|seg) (5) ? node P (dp|node)P (ro|node)P (sub|node) ? w (sub|w)). where ? is a sequence of simplification operationsand Str(?(c)) corresponds to the leaves of a simplified tree. There can be many sequences of op erations that result in the same simplified sentence and we sum up all of their probabilities. In this section, we describe how we train the prob abilities in the tables. Following the work of Yamada and Knight (2001), we train our model by maximizing P (s|c) over the training corpuswith the EM algorithm described in Alg. 2, us ing a constructed graph structure. We develop the Training Tree (Fig. 6) to calculate P (s|c). P (s|c) is equal to the inside probability of the root in theTraining Tree. Alg. 3 and Alg. 4 are used to calculate the inside and outside probabilities. We re fer readers to Yamada and Knight (2001) for more details. Algorithm 2 EM Training (dataset)Initialize all probability tables using the uniform distribu tion; for several iterations do reset al cnt = 0; for each sentence pair < c, s > in dataset do tt = buildTrainingTree(< c, s >); calcInsideProb(tt); calcOutsideProb(tt); update cnt for each conditioning feature in each node of tt: cnt = cnt + node.insideProb ? node.outsideProb/root.insideProb; end for updateProbability(); end for root sp sp_res1 sp_res2 dp ro mp mp_res1 mp_res2 sub mp mp_res subsub dp ro mp_res root sp sp_res sp_res dp ro ro_res ro_res sub ro_res subsub dp ro ro_res sub_res sub_res sub_res Figure 6: Training Tree (Left) and Decoding Tree (Right) We illustrate the construction of the training tree with our running example. There are two kinds of nodes in the training tree: data nodes in rectangles and operation nodes in circles. Data nodes contain data and operation nodes execute operations. The training is a supervised learning 1357 process with the parse tree of c as input and the two strings s1 and s2 as the desired output. root stores the parse tree of c and also s1 and s2. sp, ro, mp and sub are splitting, reordering, mapping and substitution operations. sp res and mp res store the results of sp and mp. In our example, sp splits the parse tree into two parse trees pt1 and pt2 (Fig. 3). sp res1 contains pt1 and s1. sp res2 contains pt2 and s2. Then dp, ro and mp are iteratively applied to each non-terminal node at each level of pt1 and pt2 from top to down. This process continues until the terminal nodesare reached or is stopped by a sub node. The function of mp operation is similar to the word mapping operation in the string-based machine trans lation. It maps substrings in the complex sentence which are dominated by the children of the current node to proper substrings in the simple sentences. Speeding Up The example above is only oneof the possible paths. We try all of the promis ing paths in training. Promising paths are thepaths which are likely to succeed in transform ing the parse tree of c into s1 and s2. We select the promising candidates using monolingual word mapping as shown in Fig. 7. In this example,only the word ?which? can be a promising can didate for splitting. We can select the promisingcandidates for the dropping, reordering and map ping operations similarly. With this improvement, we can train on the PWKP dataset within 1 hour excluding the parsing time taken by the Stanford Parser. We initialize the probabilities with the uniform distribution. The binary features, such as SFT and BDFT, are assigned the initial value of 0.5. For DFT and RFT, the initial probability is 1N! , whereN is the number of the children. CFT is initial ized as 0.25. SubFT is initialized as 1.0 for anysubstitution at the first iteration. After each itera tion, the updateProbability function recalculatesthese probabilities based on the cnt for each fea ture. Algorithm 3 calcInsideProb (TrainingTree tt) for each node from level = N to root of tt do if node is a sub node then node.insideProb = P (sub|node); else if node is a mp OR sp node then node.insideProb =Qchild child.insideProb;else node.insideProb =Pchild child.insideProb;end if end for Algorithm 4 calcOutsideProb (TrainingTree tt) for each node from root to level = N of tt do if node is the root then node.outsideProb = 1.0; else if node is a sp res OR mp res node then {COMMENT: father are the fathers of the current node, sibling are the children of father excluding the current node} node.outsideProb = P father father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then node.outsideProb = father.outsideProb ? 1.0; else if node is a sp, ro, dp or sub node then node.outsideProb = father.outsideProb ? P (sp or ro or dp or sub|node); end if end for August was the sixth in the ancient Roman calendar statedwhich in 735BC August was the sixth in the old Roman calendar stated in 735BCThe old calendar. Complex sentence Simple sentences month month Figure 7: Monolingual Word Mapping For decoding, we construct the decoding tree(Fig. 6) similarly to the construction of the training tree. The decoding tree does not have mp op erations and there can be more than one sub nodes attached to a single ro res. The root contains the parse tree of the complex sentence. Due to space limitations, we cannot provide all the details of the decoder.We calculate the inside probability and out side probability for each node in the decoding tree. When we simplify a complex sentence, we start from the root and greedily select the branchwith the highest outside probability. For the sub stitution operation, we also integrate a trigram language model to make the generated sentences more fluent. We train the language model with SRILM (Stolcke, 2002). All the articles from the Simple Wikipedia are used as the training corpus, amounting to about 54 MB. Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from PWKP. They have not been used for training.Four baseline systems are compared in our eval uation. The first is Moses which is a state of the art SMT system widely used as a baseline in MT community. Obviously, the purpose of Mosesis cross-lingual translation rather than monolin 1358 gual simplification. The goal of our comparison is therefore to assess how well a standard SMT system may perform simplification when fed with a proper training dataset. We train Moses with the same part of PWKP as our model. The secondbaseline system is a sentence compression sys tem (Filippova and Strube, 2008a) whose demo system is available online.8 As the compressionsystem can only perform dropping, we further ex tend it to our third and fourth baseline systems, in order to make a reasonable comparison. In our third baseline system, we substitute the words in the output of the compression system with their simpler synonyms. This is done by looking up the synonyms in Wordnet and selecting the mostfrequent synonym for replacement. The word fre quency is counted using the articles from Simple Wikipedia. The fourth system performs sentence splitting on the output of the third system. This is simply done by splitting the sentences at ?and?,?or?, ?but?, ?which?, ?who? and ?that?, and dis carding the border words. In total, there are 5systems in our evaluation: Moses, the MT system; C, the compression system; CS, the compression+substitution system; CSS, the compres sion+substitution+split system; TSM, our model.We also provide evaluation measures for the sen tences in the evaluation dataset: CW: complexsentences from Normal Wikipedia and SW: par allel simple sentences from Simple Wikipedia. 6.1 Basic Statistics and Examples. The first three columns in Tab. 9 present the ba sic statistics for the evaluation sentences and theoutput of the five systems. tokenLen is the aver age length of tokens which may roughly reflect the lexical difficulty. TSM achieves an average token length which is the same as the Simple Wikipedia (SW). senLen is the average number of tokens inone sentence, which may roughly reflect the syn tactic complexity. Both TSM and CSS produce shorter sentences than SW. Moses is very close to CW. #sen gives the number of sentences. Moses, C and CS cannot split sentences and thus produce about the same number of sentences as available in CW. Here are two example results obtained with our TSM system.Example 1. CW: ?Genetic engineering has ex panded the genes available to breeders to utilize in creating desired germlines for new crops.? SW: 8http://212.126.215.106/compression/?New plants were created with genetic engineer ing.? TSM: ?Engineering has expanded the genes available to breeders to use in making germlines for new crops.? Example 2. CW: ?An umbrella term is a word thatprovides a superset or grouping of related con cepts, also called a hypernym.? SW: ?An umbrellaterm is a word that provides a superset or group ing of related concepts.? TSM: ?An umbrella term is a word. A word provides a superset of related concepts, called a hypernym.?In the first example, both substitution and dropping happen. TSM replaces ?utilize? and ?cre ating? with ?use? and ?making?. ?Genetic? isdropped. In the second example, the complex sen tence is split and ?also? is dropped. 6.2 Translation Assessment. In this part of the evaluation, we use traditional measures used for evaluating MT systems. Tab. 9 shows the BLEU and NIST scores. We use ?mteval-v11b.pl?9 as the evaluation tool. CWand SW are used respectively as source and ref erence sentences. TSM obtains a very high BLEU score (0.38) but not as high as Moses (0.55). However, the original complex sentences (CW) from Normal Wikipedia get a rather high BLEU (0.50), when compared to the simple sentences. We also find that most of the sentences generated by Moses are exactly the same as those in CW:this shows that Moses only performs few modi fications to the original complex sentences. This is confirmed by MT evaluation measures: if we set CW as both source and reference, the BLEU score obtained by Moses is 0.78. TSM gets 0.55 in the same setting which is significantly smaller than Moses and demonstrates that TSM is able to generate simplifications with a greater amount of variation from the original sentence. As shown inthe ?#Same? column of Tab. 9, 25 sentences generated by Moses are exactly identical to the com plex sentences, while the number for TSM is 2 which is closer to SW. It is however not clear how well BLEU and NIST discriminate simplification systems. As discussed in Jurafsky and Martin (2008), ?BLEU does poorly at comparing systems with radically different architectures and is most appropriate when evaluating incremental changes with similar architectures.? In our case, TSM andCSS can be considered as having similar architec tures as both of them can do splitting, dropping 9http://www.statmt.org/moses/ 1359 TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384 SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179 Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363 C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481 CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616 CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581 TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353 Table 9: Evaluation and substitution. But Moses mostly cannot split and drop. We may conclude that TSM and Moses have different architectures and BLEU or NIST isnot suitable for comparing them. Here is an exam ple to illustrate this: (CW): ?Almost as soon as heleaves, Annius and the guard Publius arrive to es cort Vitellia to Titus, who has now chosen her as his empress.? (SW): ?Almost as soon as he leaves,Annius and the guard Publius arrive to take Vitellia to Titus, who has now chosen her as his empress.? (Moses): The same as (SW). (TSM): ?An nius and the guard Publius arrive to take Vitellia to Titus. Titus has now chosen her as his empress.?In this example, Moses generates an exactly iden tical sentence to SW, thus the BLUE and NIST scores of Moses is the highest. TSM simplifies the complex sentence by dropping, splitting and substitution, which results in two sentences that are quite different from the SW sentence and thus gets lower BLUE and NIST scores. Nevertheless, the sentences generated by TSM seem better than Moses in terms of simplification. 6.3 Readability Assessment. Intuitively, readability scores should be suitable metrics for simplification systems. We use the Linux ?style? command to calculate the Fleschand Lix readability scores. The results are pre sented in Tab. 9. ?PE? in the Flesch column standsfor ?Plain English? and the ?Grade? in Lix repre sents the school year. TSM achieves significantly better scores than Moses which has the best BLEUscore. This implies that good monolingual trans lation is not necessarily good simplification. OOVis the percentage of words that are not in the Ba sic English BE850 list.10 TSM is ranked as the second best system for this criterion.The perplexity (PPL) is a score of text probability measured by a language model and normal ized by the number of words in the text (Equ. 6). 10http://simple.wikipedia.org/wiki/ Wikipedia:Basic_English_alphabetical_ wordlistPPL can be used to measure how tight the language model fits the text. Language models constitute an important feature for assessing readabil ity (Schwarm and Ostendorf, 2005). We train a trigram LM using the simple sentences in PWKP and calculate the PPL with SRILM. TSM gets the best PPL score. From this table, we can conclude that TSM achieves better overall readability than the baseline systems. PPL(text) = P (w1w2...wN )? 1 N (6)There are still some important issues to be con sidered in future. Based on our observations, the current model performs well for word substitution and segmentation. But the completion of the new sentences is still problematic. For example, we copy the dependent NP to the new sentences. This may break the coherence between sentences. Abetter solution would be to use a pronoun to replace the NP. Sometimes, excessive droppings oc cur, e.g., ?older? and ?twin? are dropped in ?She has an older brother and a twin brother...?. This results in a problematic sentence: ?She has anbrother and a brother...?. There are also some er rors which stem from the dependency parser. InExample 2, ?An umbrella term? should be a dependency of ?called?. But the parser returns ?su perset? as the dependency. In the future, we will investigate more sophisticated features and rules to enhance TSM.","Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from PWKP. They have not been used for training.Four baseline systems are compared in our eval uation. The first is Moses which is a state of the art SMT system widely used as a baseline in MT community. Obviously, the purpose of Mosesis cross-lingual translation rather than monolin 1358 gual simplification. The goal of our comparison is therefore to assess how well a standard SMT system may perform simplification when fed with a proper training dataset. We train Moses with the same part of PWKP as our model. The secondbaseline system is a sentence compression sys tem (Filippova and Strube, 2008a) whose demo system is available online.8 As the compressionsystem can only perform dropping, we further ex tend it to our third and fourth baseline systems, in order to make a reasonable comparison. In our third baseline system, we substitute the words in the output of the compression system with their simpler synonyms. This is done by looking up the synonyms in Wordnet and selecting the mostfrequent synonym for replacement. The word fre quency is counted using the articles from Simple Wikipedia. The fourth system performs sentence splitting on the output of the third system. This is simply done by splitting the sentences at ?and?,?or?, ?but?, ?which?, ?who? and ?that?, and dis carding the border words. In total, there are 5systems in our evaluation: Moses, the MT system; C, the compression system; CS, the compression+substitution system; CSS, the compres sion+substitution+split system; TSM, our model.We also provide evaluation measures for the sen tences in the evaluation dataset: CW: complexsentences from Normal Wikipedia and SW: par allel simple sentences from Simple Wikipedia. 6.1 Basic Statistics and Examples. The first three columns in Tab. 9 present the ba sic statistics for the evaluation sentences and theoutput of the five systems. tokenLen is the aver age length of tokens which may roughly reflect the lexical difficulty. TSM achieves an average token length which is the same as the Simple Wikipedia (SW). senLen is the average number of tokens inone sentence, which may roughly reflect the syn tactic complexity. Both TSM and CSS produce shorter sentences than SW. Moses is very close to CW. #sen gives the number of sentences. Moses, C and CS cannot split sentences and thus produce about the same number of sentences as available in CW. Here are two example results obtained with our TSM system.Example 1. CW: ?Genetic engineering has ex panded the genes available to breeders to utilize in creating desired germlines for new crops.? SW: 8http://212.126.215.106/compression/?New plants were created with genetic engineer ing.? TSM: ?Engineering has expanded the genes available to breeders to use in making germlines for new crops.? Example 2. CW: ?An umbrella term is a word thatprovides a superset or grouping of related con cepts, also called a hypernym.? SW: ?An umbrellaterm is a word that provides a superset or group ing of related concepts.? TSM: ?An umbrella term is a word. A word provides a superset of related concepts, called a hypernym.?In the first example, both substitution and dropping happen. TSM replaces ?utilize? and ?cre ating? with ?use? and ?making?. ?Genetic? isdropped. In the second example, the complex sen tence is split and ?also? is dropped. 6.2 Translation Assessment. In this part of the evaluation, we use traditional measures used for evaluating MT systems. Tab. 9 shows the BLEU and NIST scores. We use ?mteval-v11b.pl?9 as the evaluation tool. CWand SW are used respectively as source and ref erence sentences. TSM obtains a very high BLEU score (0.38) but not as high as Moses (0.55). However, the original complex sentences (CW) from Normal Wikipedia get a rather high BLEU (0.50), when compared to the simple sentences. We also find that most of the sentences generated by Moses are exactly the same as those in CW:this shows that Moses only performs few modi fications to the original complex sentences. This is confirmed by MT evaluation measures: if we set CW as both source and reference, the BLEU score obtained by Moses is 0.78. TSM gets 0.55 in the same setting which is significantly smaller than Moses and demonstrates that TSM is able to generate simplifications with a greater amount of variation from the original sentence. As shown inthe ?#Same? column of Tab. 9, 25 sentences generated by Moses are exactly identical to the com plex sentences, while the number for TSM is 2 which is closer to SW. It is however not clear how well BLEU and NIST discriminate simplification systems. As discussed in Jurafsky and Martin (2008), ?BLEU does poorly at comparing systems with radically different architectures and is most appropriate when evaluating incremental changes with similar architectures.? In our case, TSM andCSS can be considered as having similar architec tures as both of them can do splitting, dropping 9http://www.statmt.org/moses/ 1359 TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384 SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179 Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363 C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481 CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616 CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581 TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353 Table 9: Evaluation and substitution. But Moses mostly cannot split and drop. We may conclude that TSM and Moses have different architectures and BLEU or NIST isnot suitable for comparing them. Here is an exam ple to illustrate this: (CW): ?Almost as soon as heleaves, Annius and the guard Publius arrive to es cort Vitellia to Titus, who has now chosen her as his empress.? (SW): ?Almost as soon as he leaves,Annius and the guard Publius arrive to take Vitellia to Titus, who has now chosen her as his empress.? (Moses): The same as (SW). (TSM): ?An nius and the guard Publius arrive to take Vitellia to Titus. Titus has now chosen her as his empress.?In this example, Moses generates an exactly iden tical sentence to SW, thus the BLUE and NIST scores of Moses is the highest. TSM simplifies the complex sentence by dropping, splitting and substitution, which results in two sentences that are quite different from the SW sentence and thus gets lower BLUE and NIST scores. Nevertheless, the sentences generated by TSM seem better than Moses in terms of simplification. 6.3 Readability Assessment. Intuitively, readability scores should be suitable metrics for simplification systems. We use the Linux ?style? command to calculate the Fleschand Lix readability scores. The results are pre sented in Tab. 9. ?PE? in the Flesch column standsfor ?Plain English? and the ?Grade? in Lix repre sents the school year. TSM achieves significantly better scores than Moses which has the best BLEUscore. This implies that good monolingual trans lation is not necessarily good simplification. OOVis the percentage of words that are not in the Ba sic English BE850 list.10 TSM is ranked as the second best system for this criterion.The perplexity (PPL) is a score of text probability measured by a language model and normal ized by the number of words in the text (Equ. 6). 10http://simple.wikipedia.org/wiki/ Wikipedia:Basic_English_alphabetical_ wordlistPPL can be used to measure how tight the language model fits the text. Language models constitute an important feature for assessing readabil ity (Schwarm and Ostendorf, 2005). We train a trigram LM using the simple sentences in PWKP and calculate the PPL with SRILM. TSM gets the best PPL score. From this table, we can conclude that TSM achieves better overall readability than the baseline systems. PPL(text) = P (w1w2...wN )? 1 N (6)There are still some important issues to be con sidered in future. Based on our observations, the current model performs well for word substitution and segmentation. But the completion of the new sentences is still problematic. For example, we copy the dependent NP to the new sentences. This may break the coherence between sentences. Abetter solution would be to use a pronoun to replace the NP. Sometimes, excessive droppings oc cur, e.g., ?older? and ?twin? are dropped in ?She has an older brother and a twin brother...?. This results in a problematic sentence: ?She has anbrother and a brother...?. There are also some er rors which stem from the dependency parser. InExample 2, ?An umbrella term? should be a dependency of ?called?. But the parser returns ?su perset? as the dependency. In the future, we will investigate more sophisticated features and rules to enhance TSM."
67,"Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.","Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al 1979; Wallace 1975; Toulmin 2003). Recent computational work either focuses on sentence ?subjectivity? (Wiebe et al 2002; Riloff et al 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003). Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief. For example, the following opinions contain Claims but no Sentiments: ?I believe the world is flat? ?The Gap is likely to go bankrupt? ?Bin Laden is hiding in Pakistan? ?Water always flushes anti-clockwise in the southern hemisphere? Like Yu and Hatzivassiloglou (2003), we want to automatically identify Sentiments, which in this work we define as an explicit or implicit expression in text of the Holder?s positive, negative, or neutral regard toward the Claim about the Topic. (Other sentiments we plan to study later.) Sentiments always involve the Holder?s emotions or desires, and may be present explicitly or only implicitly: ?I think that attacking Iraq would put the US in a difficult position? (implicit) ?The US attack on Iraq is wrong? (explicit) ?I like Ike? (explicit) ?We should decrease our dependence on oil? (implicit) ?Reps. Tom Petri and William F. Goodling asserted that counting illegal aliens violates citizens? basic right to equal representation? (implicit) In this paper we address the following challenge problem. Given a Topic (e.g., ?Should abortion be banned??) and a set of texts about the topic, find the Sentiments expressed about (claims about) the Topic (but not its supporting subtopics) in each text, and identify the people who hold each sentiment. To avoid the problem of differentiating between shades of sentiments, we simplify the problem to: identify just expressions of positive, negative, or neutral sentiments, together with their holders. In addition, for sentences that do not express a sentiment but simply state that some sentiment(s) exist(s), return these sentences in a separate set. For example, given the topic ?What should be done with Medicare?? the sentence ?After years of empty promises, Congress has rolled out two Medicare prescription plans, one from House Republicans and the other from the Democratic Sentence POS Tagger verbs nounsAdjectives Adjective Senti ment classifier sentiment sentiment Sentence sentiment classifier Opinion region + polarity + holder Holder finder Named Entity Tagger Sentence Sentence texts + topic sentiment sentiment sentiment V rbs Verb Senti ment classifier Nouns Noun Senti ment classifier WordNet Sentence : Figure 1: System architecture. Sens. Bob Graham of Florida and Zell Miller of Georgia? should be returned in the separate set. We approach the problem in stages, starting with words and moving on to sentences. We take as unit sentiment carrier a single word, and first classify each adjective, verb, and noun by its sentiment. We experimented with several classifier models. But combining sentiments requires additional care, as Table 1 shows. California Supreme Court agreed that the state?s new term-limit law was constitutional. California Supreme Court disagreed that the state?s new term-limit law was constitutional. California Supreme Court agreed that the state?s new term-limit law was unconstitutional. California Supreme Court disagreed that the state?s new term-limit law was unconstitutional. Table 1: Combining sentiments. A sentence might even express opinions of different people. When combining word-level sentiments, we therefore first determine for each Holder a relevant region within the sentence and then experiment with various models for combining word sentiments. We describe our models and algorithm in Section 2, system experiments and discussion in Section 3, and conclude in Section 4. Given a topic and a set of texts, the system operates in four steps. First it selects sentences that contain both the topic phrase and holder candidates. Next, the holder-based regions of opinion are delimited. Then the sentence sentiment classifier calculates the polarity of all sentiment-bearing words individually. Finally, the system combines them to produce the holder?s sentiment for the whole sentence. Figure 1 shows the overall system architecture. Section 2.1 describes the word sentiment classifier and Section 2.2 describes the sentence sentiment classifier. 2.1 Word Sentiment Classifier. 2.1.1 Word Classification Models For word sentiment classification we developed two models. The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two lists?positive and negative?and then to grow this by adding words obtained from WordNet (Miller et al 1993; Fellbaum et al 1993). We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word ?good? has synonyms ?virtuous, honorable, righteous? and antonyms ?evil, disreputable, unrighteous?. Antonyms of negative words are added to the positive list, and synonyms to the negative one. To start the seed lists we selected verbs (23 positive and 21 negative) and adjectives (15 positive and 19 negative), adding nouns later. Since adjectives and verbs are structured differently in WordNet, we obtained from it synonyms and antonyms for adjectives but only synonyms for verbs. For each seed word, we extracted from WordNet its expansions and added them back into the appropriate seed lists. Using these expanded lists, we extracted an additional cycle of words from WordNet, to obtain finally 5880 positive adjectives, 6233 negative adjectives, 2840 positive verbs, and 3239 negative verbs. However, not all synonyms and antonyms could be used: some had opposite sentiment or were neutral. In addition, some common words such as ?great?, ?strong?, ?take?, and ?get? occurred many times in both positive and negative categories. This indicated the need to develop a measure of strength of sentiment polarity (the alternative was simply to discard such ambiguous words)?to determine how strongly a word is positive and also how strongly it is negative. This would enable us to discard sentiment-ambiguous words but retain those with strengths over some threshold. Armed with such a measure, we can also assign strength of sentiment polarity to as yet unseen words. Given a new word, we use WordNet again to obtain a synonym set of the unseen word to determine how it interacts with our sentiment seed lists. That is, we compute (1) ).....,|(maxarg )|(maxarg 21 n c c synsynsyncP wcP ? where c is a sentiment category (positive or negative), w is the unseen word, and synn are the WordNet synonyms of w. To compute Equation (1), we tried two different models: (2) )|()(maxarg )|()(maxarg )|()(maxarg)|(maxarg 1 ))(,( ...3 2 1 ? = = = = m k wsynsetfcount k c n c cc kcfPcP csynsynsynsynPcP cwPcPwcP where fk is the kth feature (list word) of sentiment class c which is also a member of the synonym set of w, and count(fk,synset(w)) is the total number of occurrences of fk in the synonym set of w. P(c) is the number of words in class c divided by the total number of words considered. This model derives from document classification. We used the synonym and antonym lists obtained from Wordnet instead of learning word sets from a corpus, since the former is simpler and does not require manually annotated data for training. Equation (3) shows the second model for a word sentiment classifier. (3) )( ),( )(maxarg )|()(maxarg)|(maxarg 1 ccount csyncount cP cwPcPwcP n i i c cc ? == = To compute the probability P(w|c) of word w given a sentiment class c, we count the occurrence of w?s synonyms in the list of c. The intuition is that the more synonyms occuring in c, the more likely the word belongs. We computed both positive and negative sentiment strengths for each word and compared their relative magnitudes. Table 2 shows several examples of the system output, computed with Equation (2), in which ?+? represents positive category strength and ?-? negative. The word ?amusing?, for example, was classified as carrying primarily positive sentiment, and ?blame? as primarily negative. The absolute value of each category represents the strength of its sentiment polarity. For instance, ?afraid? with strength -0.99 represents strong negavitity while ?abysmal? with strength -0.61 represents weaker negativity. abysmal : NEGATIVE [+ : 0.3811][- : 0.6188] adequate : POSITIVE [+ : 0.9999][- : 0.0484e-11] afraid : NEGATIVE [+ : 0.0212e-04][- : 0.9999] ailing : NEGATIVE [+ : 0.0467e-8][- : 0.9999] amusing : POSITIVE [+ : 0.9999][- : 0.0593e-07] answerable : POSITIVE [+ : 0.8655][- : 0.1344] apprehensible: POSITIVE [+ : 0.9999][- : 0.0227e-07] averse : NEGATIVE [+ : 0.0454e-05][- : 0.9999] blame : NEGATIVE [+ : 0.2530][- : 0.7469] Table 2: Sample output of word sentiment classifier. 2.2 Sentence Sentiment Classifier. As shows in Table 1, combining sentiments in a sentence can be tricky. We are interested in the sentiments of the Holder about the Claim. Manual analysis showed that such sentiments can be found most reliably close to the Holder; without either Holder or Topic/Claim nearby as anchor points, even humans sometimes have trouble reliably determining the source of a sentiment. We therefore included in the algorithm steps to identify the Topic (through direct matching, since we took it as given) and any likely opinion Holders (see Section 2.2.1). Near each Holder we then identified a region in which sentiments would be considered; any sentiments outside such a region we take to be of undetermined origin and ignore (Section 2.2.2). We then defined several models for combining the sentiments expressed within a region (Section 2.2.3). 2.2.1 Holder Identification We used BBN?s named entity tagger IdentiFinder to identify potential holders of an opinion. We considered PERSON and ORGANIZATION as the only possible opinion holders. For sentences with more than one Holder, we chose the one closest to the Topic phrase, for simplicity. This is a very crude step. A more sophisticated approach would employ a parser to identify syntactic relationships between each Holder and all dependent expressions of sentiment. 2.2.2 Sentiment Region Lacking a parse of the sentence, we were faced with a dilemma: How large should a region be? We therefore defined the sentiment region in various ways (see Table 3) and experimented with their effectiveness, as reported in Section 3. Window1: full sentence Window2: words between Holder and Topic Window3: window2 ? 2 words Window4: window2 to the end of sentence Table 3: Four variations of region size. 2.2.3 Classification Models We built three models to assign a sentiment category to a given sentence, each combining the individual sentiments of sentiment-bearing words, as described above, in a different way. Model 0 simply considers the polarities of the sentiments, not the strengths: Model 0: ? (signs in region) The intuition here is something like ?negatives cancel one another out?. Here the system assigns the same sentiment to both ?the California Supreme Court agreed that the state?s new term-limit law was constitutional? and ?the California Supreme Court disagreed that the state?s new term-limit law was unconstitutional?. For this model, we also included negation words such as not and never to reverse the sentiment polarity. Model 1 is the harmonic mean (average) of the sentiment strengths in the region: Model 1: cwcp wcp cn scP ij n i i = = ? = )|(argmax if ,)|( )( 1)|( j 1 Here n(c) is the number of words in the region whose sentiment category is c. If a region contains more and stronger positive than negative words, the sentiment will be positive. Model 2 is the geometric mean: Model 2: cwcpif wcpscP ij n i i cn = ?= ? = ? )|(argmax ,)|(10)|( j 1 1)( 2.2.4 Examples The following are two example outputs. Public officials throughout California have condemned a U.S. Senate vote Thursday to exclude illegal aliens from the 1990 census, saying the action will shortchange California in Congress and possibly deprive the state of millions of dollars of federal aid for medical emergency services and other programs for poor people. TOPIC : illegal alien HOLDER : U.S. Senate OPINION REGION: vote/NN Thursday/NNP to/TO exclude/VB illegal/JJ aliens/NNS from/IN the/DT 1990/CD census,/NN SENTIMENT_POLARITY: negative For that reason and others, the Constitutional Convention unanimously rejected term limits and the First Congress soundly defeated two subsequent term-limit proposals. TOPIC : term limit HOLDER : First Congress OPINION REGION: soundly/RB defeated/VBD two/CD subsequent/JJ term-limit/JJ proposals./NN SENTIMENT_POLARITY: negative The first experiment examines the two word sentiment classifier models and the second the three sentence sentiment classifier models. 3.1 Word Sentiment Classifier. For test material, we asked three humans to classify data. We started with a basic English word list for foreign students preparing for the TOEFL test and intersected it with an adjective list containing 19748 English adjectives and a verb list of 8011 verbs to obtain common adjectives and verbs. From this we randomly selected 462 adjectives and 502 verbs for human classification. Human1 and human2 each classified 462 adjectives, and human2 and human3 502 verbs. The classification task is defined as assigning each word to one of three categories: positive, negative, and neutral. 3.1.1 Human?Human Agreement Adjectives Verbs Human1 : Human2 Human1 : Human3 Strict 76.19% 62.35% Lenient 88.96% 85.06% Table 4: Inter-human classification agreement. Table 4 shows inter-human agreement. The strict measure is defined over all three categories, whereas the lenient measure is taken over only two categories, where positive and neutral have been merged, should we choose to focus only on differentiating words of negative sentiment. 3.1.2 Human?Machine Agreement Table 5 shows results, using Equation (2) of Section 2.1.1, compared against a baseline that randomly assigns a sentiment category to each word (averaged over 10 iterations). The system achieves lower agreement than humans but higher than the random process. Of the test data, the algorithm classified 93.07% of adjectives and 83.27% of verbs as either positive and negative. The remainder of adjectives and verbs failed to be classified, since they did not overlap with the synonym set of adjectives and verbs. In Table 5, the seed list included just a few manually selected seed words (23 positive and 21 negative verbs and 15 and 19 adjectives, repectively). We decided to investigate the effect of more seed words. After collecting the annotated data, we added half of it (231 adjectives and 251 verbs) to the training set, retaining the other half for the test. As Table 6 shows, agreement of both adjectives and verbs with humans improves. Recall is also improved. Adjective (Train: 231 Test : 231) Verb (Train: 251 Test : 251) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall 75.66% 77.88% 97.84% 81.20% 79.06% 93.23% Table 6: Results including manual data. 3.2 Sentence Sentiment Classifier. 3.2.1 Data 100 sentences were selected from the DUC 2001 corpus with the topics ?illegal alien?, ?term limits?, ?gun control?, and ?NAFTA?. Two humans annotated the 100 sentences with three categories (positive, negative, and N/A). To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988). The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable. 3.2.2 Test on Human Annotated Data We experimented on Section 2.2.3?s 3 models of sentiment classifiers, using the 4 different window definitions and 4 variations of word-level classifiers (the two word sentiment equations introduced in Section 2.1.1, first with and then without normalization, to compare performance). Since Model 0 considers not probabilities of words but only their polarities, the two word- level classifier equations yield the same results. Consequently, Model 0 has 8 combinations and Models 1 and 2 have 16 each. To test the identification of opinion Holder, we first ran models with holders that were annotated by humans then ran the same models with the automatic holder finding strategies. The results appear in Figures 2 and 3. The models are numbered as follows: m0 through m4 represent 4 sentence classifier models, Table 5. Agreement between humans and system. Adjective (test: 231 adjectives) Verb (test : 251 verbs) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall Random selection (average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% p1/p2 and p3/p4 represent the word classifier models in Equation (2) and Equation (3) with normalization and without normalization respectively. 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu ra cy Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 2: Results with manually annotated Holder. 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 3: Results with automatic Holder detection. Correctness of an opinion is determined when the system finds both a correct holder and the appropriate sentiment within the sentence. Since human1 classified 33 sentences positive and 33 negative, random classification gives 33 out of 66 sentences. Similarly, since human2 classified 29 positive and 34 negative, random classification gives 34 out of 63 when the system blindly marks all sentences as negative and 29 out of 63 when it marks all as positive. The system?s best model performed at 81% accuracy with the manually provided holder and at 67% accuracy with automatic holder detection. 3.3 Problems. 3.3.1 Word Sentiment Classification As mentioned, some words have both strong positive and negative sentiment. For these words, it is difficult to pick one sentiment category without considering context. Second, a unigram model is not sufficient: common words without much sentiment alone can combine to produce reliable sentiment. For example, in ??Term limits really hit at democracy,? says Prof. Fenno?, the common and multi-meaning word ?hit? was used to express a negative point of view about term limits. If such combinations occur adjacently, we can use bigrams or trigrams in the seed word list. When they occur at a distance, however, it is more difficult to identify the sentiment correctly, especially if one of the words falls outside the sentiment region. 3.3.2 Sentence Sentiment Classification Even in a single sentence, a holder might express two different opinions. Our system only detects the closest one. Another difficult problem is that the models cannot infer sentiments from facts in a sentence. ?She thinks term limits will give women more opportunities in politics? expresses a positive opinion about term limits but the absence of adjective, verb, and noun sentiment-words prevents a classification. Although relatively easy task for people, detecting an opinion holder is not simple either. As a result, our system sometimes picks a wrong holder when there are multiple plausible opinion holder candidates present. Employing a parser to delimit opinion regions and more accurately associate them with potential holders should help. 3.4 Discussion. Which combination of models is best? The best overall performance is provided by Model 0. Apparently, the mere presence of negative words is more important than sentiment strength. For manually tagged holder and topic, Model 0 has the highest single performance, though Model 1 averages best. Which is better, a sentence or a region? With manually identified topic and holder, the region window4 (from Holder to sentence end) performs better than other regions. How do scores differ from manual to automatic holder identification? Table 7 compares the average results with automatic holder identification to manually annotated holders in 40 different models. Around 7 more sentences (around 11%) were misclassified by the automatic detection method. positive negative total Human1 5.394 1.667 7.060 Human2 4.984 1.714 6.698 Table 7: Average difference between manual and automatic holder detection. How does adding the neutral sentiment as a separate category affect the score? It is very confusing even for humans to distinguish between a neutral opinion and non opinion bearing sentences. In previous research, we built a sentence subjectivity classifier. Unfortunately, in most cases it classifies neutral and weak sentiment sentences as non-opinion bearing sentences.","The first experiment examines the two word sentiment classifier models and the second the three sentence sentiment classifier models. 3.1 Word Sentiment Classifier. For test material, we asked three humans to classify data. We started with a basic English word list for foreign students preparing for the TOEFL test and intersected it with an adjective list containing 19748 English adjectives and a verb list of 8011 verbs to obtain common adjectives and verbs. From this we randomly selected 462 adjectives and 502 verbs for human classification. Human1 and human2 each classified 462 adjectives, and human2 and human3 502 verbs. The classification task is defined as assigning each word to one of three categories: positive, negative, and neutral. 3.1.1 Human?Human Agreement Adjectives Verbs Human1 : Human2 Human1 : Human3 Strict 76.19% 62.35% Lenient 88.96% 85.06% Table 4: Inter-human classification agreement. Table 4 shows inter-human agreement. The strict measure is defined over all three categories, whereas the lenient measure is taken over only two categories, where positive and neutral have been merged, should we choose to focus only on differentiating words of negative sentiment. 3.1.2 Human?Machine Agreement Table 5 shows results, using Equation (2) of Section 2.1.1, compared against a baseline that randomly assigns a sentiment category to each word (averaged over 10 iterations). The system achieves lower agreement than humans but higher than the random process. Of the test data, the algorithm classified 93.07% of adjectives and 83.27% of verbs as either positive and negative. The remainder of adjectives and verbs failed to be classified, since they did not overlap with the synonym set of adjectives and verbs. In Table 5, the seed list included just a few manually selected seed words (23 positive and 21 negative verbs and 15 and 19 adjectives, repectively). We decided to investigate the effect of more seed words. After collecting the annotated data, we added half of it (231 adjectives and 251 verbs) to the training set, retaining the other half for the test. As Table 6 shows, agreement of both adjectives and verbs with humans improves. Recall is also improved. Adjective (Train: 231 Test : 231) Verb (Train: 251 Test : 251) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall 75.66% 77.88% 97.84% 81.20% 79.06% 93.23% Table 6: Results including manual data. 3.2 Sentence Sentiment Classifier. 3.2.1 Data 100 sentences were selected from the DUC 2001 corpus with the topics ?illegal alien?, ?term limits?, ?gun control?, and ?NAFTA?. Two humans annotated the 100 sentences with three categories (positive, negative, and N/A). To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988). The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable. 3.2.2 Test on Human Annotated Data We experimented on Section 2.2.3?s 3 models of sentiment classifiers, using the 4 different window definitions and 4 variations of word-level classifiers (the two word sentiment equations introduced in Section 2.1.1, first with and then without normalization, to compare performance). Since Model 0 considers not probabilities of words but only their polarities, the two word- level classifier equations yield the same results. Consequently, Model 0 has 8 combinations and Models 1 and 2 have 16 each. To test the identification of opinion Holder, we first ran models with holders that were annotated by humans then ran the same models with the automatic holder finding strategies. The results appear in Figures 2 and 3. The models are numbered as follows: m0 through m4 represent 4 sentence classifier models, Table 5. Agreement between humans and system. Adjective (test: 231 adjectives) Verb (test : 251 verbs) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall Random selection (average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% p1/p2 and p3/p4 represent the word classifier models in Equation (2) and Equation (3) with normalization and without normalization respectively. 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu ra cy Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 2: Results with manually annotated Holder. 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 3: Results with automatic Holder detection. Correctness of an opinion is determined when the system finds both a correct holder and the appropriate sentiment within the sentence. Since human1 classified 33 sentences positive and 33 negative, random classification gives 33 out of 66 sentences. Similarly, since human2 classified 29 positive and 34 negative, random classification gives 34 out of 63 when the system blindly marks all sentences as negative and 29 out of 63 when it marks all as positive. The system?s best model performed at 81% accuracy with the manually provided holder and at 67% accuracy with automatic holder detection. 3.3 Problems. 3.3.1 Word Sentiment Classification As mentioned, some words have both strong positive and negative sentiment. For these words, it is difficult to pick one sentiment category without considering context. Second, a unigram model is not sufficient: common words without much sentiment alone can combine to produce reliable sentiment. For example, in ??Term limits really hit at democracy,? says Prof. Fenno?, the common and multi-meaning word ?hit? was used to express a negative point of view about term limits. If such combinations occur adjacently, we can use bigrams or trigrams in the seed word list. When they occur at a distance, however, it is more difficult to identify the sentiment correctly, especially if one of the words falls outside the sentiment region. 3.3.2 Sentence Sentiment Classification Even in a single sentence, a holder might express two different opinions. Our system only detects the closest one. Another difficult problem is that the models cannot infer sentiments from facts in a sentence. ?She thinks term limits will give women more opportunities in politics? expresses a positive opinion about term limits but the absence of adjective, verb, and noun sentiment-words prevents a classification. Although relatively easy task for people, detecting an opinion holder is not simple either. As a result, our system sometimes picks a wrong holder when there are multiple plausible opinion holder candidates present. Employing a parser to delimit opinion regions and more accurately associate them with potential holders should help. 3.4 Discussion. Which combination of models is best? The best overall performance is provided by Model 0. Apparently, the mere presence of negative words is more important than sentiment strength. For manually tagged holder and topic, Model 0 has the highest single performance, though Model 1 averages best. Which is better, a sentence or a region? With manually identified topic and holder, the region window4 (from Holder to sentence end) performs better than other regions. How do scores differ from manual to automatic holder identification? Table 7 compares the average results with automatic holder identification to manually annotated holders in 40 different models. Around 7 more sentences (around 11%) were misclassified by the automatic detection method. positive negative total Human1 5.394 1.667 7.060 Human2 4.984 1.714 6.698 Table 7: Average difference between manual and automatic holder detection. How does adding the neutral sentiment as a separate category affect the score? It is very confusing even for humans to distinguish between a neutral opinion and non opinion bearing sentences. In previous research, we built a sentence subjectivity classifier. Unfortunately, in most cases it classifies neutral and weak sentiment sentences as non-opinion bearing sentences."
68,"Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.","Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE. To automatically evaluate machine translations, the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al 2001). A similar metric, NIST, used by NIST (NIST 2002) in a couple of machine translation evaluations in the past two years is based on BLEU. The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric. Although the idea of using objective functions to automatically evaluate machine translation quality is not new (Su et al 1992), the success of BLEU prompts a lot of interests in developing better automatic evaluation metrics. For example, Akiba et al (2001) proposed a metric called RED based on edit distances over a set of multiple references. Nie?en et al (2000) calculated the length normalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al (2003) proposed a related measure called position independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Turian et al (2003) introduced General Text Matcher (GTM) based on accuracy measures such as recall, precision, and F-measure. With so many different automatic metrics available, it is necessary to have a common and objective way to evaluate these metrics. Comparison of automatic evaluation metrics are usually conducted on corpus level using correlation analysis between human scores and automatic scores such as BLEU, NIST, WER, and PER. However, the performance of automatic metrics in terms of human vs. system correlation analysis is not stable across different evaluation settings. For example, Table 1 shows the Pearson?s linear correlation coefficient analysis of 8 machine translation systems from 2003 NIST Chinese English machine translation evaluation. The Pearson? correlation coefficients are computed according to different automatic evaluation methods vs. human assigned adequacy and fluency. BLEU1, 4, and 12 are BLEU with maximum n-gram lengths of 1, 4, and 12 respectively. GTM10, 20, and 30 are GTM with exponents of 1.0, 2.0, and 3.0 respectively. 95% confidence intervals are estimated using bootstrap resampling (Davison and Hinkley 1997). From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation. GTM with smaller exponent has better adequacy correlation and GTM with larger exponent has better fluency correlation. NIST is very good in adequacy correlation but not as good as GTM30 in fluency correlation. Based on these observations, we are not able to conclude which metric is the best because it depends on the manual evaluation criteria. This results also indicate that high correlation between human and automatic scores in both adequacy and fluency cannot always been achieved at the same time. The best performing metrics in fluency according to Table 1 are BLEU12 and GTM30 (dark/green cells). However, many metrics are statistically equivalent (gray cells) to them when we factor in the 95% confidence intervals. For example, even PER is as good as BLEU12 in adequacy. One reason for this might be due to data sparseness since only 8 systems are available. The other potential problem for correlation analysis of human vs. automatic framework is that high corpus-level correlation might not translate to high sentence-level correlation. However, high sentence-level correlation is often an important property that machine translation researchers look for. For example, candidate translations shorter than 12 words would have zero BLEU12 score but BLEU12 has the best correlation with human judgment in fluency as shown in Table 1. In order to evaluate the ever increasing number of automatic evaluation metrics for machine translation objectively, efficiently, and reliably, we introduce a new evaluation method: ORANGE. We describe ORANGE in details in Section 2 and briefly introduce three new automatic metrics that will be used in comparisons in Section 3. The results of comparing several existing automatic metrics and the three new automatic metrics using ORANGE will be presented in Section 4. We conclude this paper and discuss future directions in Section 5. Intuitively a good evaluation metric should give higher score to a good translation than a bad one. Therefore, a good translation should be ranked higher than a bad translation based their scores. One basic assumption of all automatic evaluation metrics for machine translation is that reference translations are good translations and the more a machine translation is similar to its reference translations the better. We adopt this assumption and add one more assumption that automatic translations are usually worst than their reference translations. Therefore, reference translations should be ranked higher than machine translations on average if a good automatic evaluation metric is used. Based on these assumptions, we propose a new automatic evaluation method for evaluation of automatic machine translation metrics as follows: Given a source sentence, its machine translations, and its reference translations, we compute the average rank of the reference translations within the combined machine and reference translation list. For example, a statistical machine translation system such as ISI?s AlTemp SMT system (Och 2003) can generate a list of n-best alternative translations given a source sentence. We compute the automatic scores for the n-best translations and their reference translations. We then rank these translations, calculate the average rank of the references in the n-best list, and compute the ratio of the average reference rank to the length of the n-best list. We call this ratio ?ORANGE? (Oracle1 Ranking for Gisting Evaluation) and the smaller the ratio is, the better the automatic metric is. There are several advantages of the proposed ORANGE evaluation method: ? No extra human involvement ? ORANGE uses the existing human references but not human evaluations. Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided. This is a feature that many machine translation researchers look for. Many existing data points ? Every sentence is a data point instead of every system (corpus-level). For example, there are 919 sentences vs. 8 systems in the 2003 NIST Chinese-English machine translation evaluation. Only one objective function to optimize ? Minimize a single ORANGE score instead of maximize Pearson?s correlation coefficients between automatic scores and human judgments in adequacy, fluency, or other quality metrics. A natural fit to the existing statistical machine translation framework ? A metric that ranks a good translation high in an n best list could be easily integrated in a minimal error rate statistical machine translation training framework (Och 2003). The overall system performance in terms of 1 Oracles refer to the reference translations used in. the evaluation procedure. Method Pearson 95%L 95%U Pearson 95%L 95%U BLEU1 0.86 0.83 0.89 0.81 0.75 0.86 BLEU4 0.77 0.72 0.81 0.86 0.81 0.90 BLEU12 0.66 0.60 0.72 0.87 0.76 0.93 NIST 0.89 0.86 0.92 0.81 0.75 0.87 WER 0.47 0.41 0.53 0.69 0.62 0.75 PER 0.67 0.62 0.72 0.79 0.74 0.85 GTM10 0.82 0.79 0.85 0.73 0.66 0.79 GTM20 0.77 0.73 0.81 0.86 0.81 0.90 GTM30 0.74 0.70 0.78 0.87 0.81 0.91 Adequacy Fluency Table 1. Pearson's correlation analysis of 8 machine translation systems in 2003 NIST Chinese-English machine translation evaluation. generating more human like translations should also be improved. Before we demonstrate how to use ORANGE to evaluate automatic metrics, we briefly introduce three new metrics in the next section. ROUGE-L and ROUGE-S are described in details in Lin and Och (2004). Since these two metrics are relatively new, we provide short summaries of them in Section 3.1 and Section 3.3 respectively. ROUGE-W, an extension of ROUGE-L, is new and is explained in details in Section 3.2. 3.1 ROUGE-L: Longest Common Sub-. sequence Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length (Cormen et al 1989). To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition is that the longer the LCS of two translations is, the more similar the two translations are. We propose using LCS-based F-measure to estimate the similarity between two translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, as follows: Rlcs m YXLCS ),( = (1) Plcs n YXLCS ),( = (2) Flcs lcslcs lcslcs PR PR 2 2 )1( ? ? + + = (3) Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and ? = Plcs/Rlcs when ?Flcs/?Rlcs_=_?Flcs/?Plcs. We call the LCS based F-measure, i.e. Equation 3, ROUGE-L. Notice that ROUGE-L is 1 when X = Y since LCS(X,Y) = m or n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as n grams. The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary. By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way. Consider the following example: S1. police killed the gunman S2. police kill the gunman S3. the gunman kill police Using S1 as the reference translation, S2 has a ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE L score of 2/4 = 0.5, with ? = 1. Therefore S2 is better than S3 according to ROUGE-L. This example illustrated that ROUGE-L can work reliably at sentence level. However, LCS suffers one disadvantage: it only counts the main in sequence words; therefore, other alternative LCSes and shorter sequences are not reflected in the final score. In the next section, we introduce ROUGE-W. 3.2 ROUGE-W: Weighted Longest Common. Subsequence LCS has many nice properties as we have described in the previous sections. Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences. For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: X: [A B C D E F G] Y1: [A B C D H I K] Y2: [A H B K C I D] Y1 and Y2 have the same ROUGE-L score. However, in this case, Y1 should be the better choice than Y2 because Y1 has consecutive matches. To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj. Given two sentences X and Y, the recurrent relations can be written as follows: (1) If xi = yj Then // the length of consecutive matches at // position i-1 and j-1 k = w(i-1,j-1) c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) // remember the length of consecutive // matches at position i, j w(i,j) = k+1 (2) Otherwise If c(i-1,j) > c(i,j-1) Then c(i,j) = c(i-1,j) w(i,j) = 0 // no match at i, j Else c(i,j) = c(i,j-1) w(i,j) = 0 // no match at i, j (3) WLCS(X,Y) = c(m,n) Where c is the dynamic programming table, 0 <= i <= m, 0 <= j <= n, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table position, c(i,j). Notice that by providing different weighting function f, we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches. The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y. In other words, consecutive matches are awarded more scores than non-consecutive matches. For example, f(k)-=-?k ? ? when k >= 0, and ?, ? > 0. This function charges a gap penalty of ?? for each non-consecutive n-gram sequences. Another possible function family is the polynomial family of the form k? where -? > 1. However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form inverse function. For example, f(k)-=-k2 has a close form inverse function f -1(k)-=-k1/2. F-measure based on WLCS can be computed as follows, given two sequences X of length m and Y of length n: Rwlcs ??? = ? )( ),(1 mf YXWLCSf (4) Pwlcs ??? = ? )( ),(1 nf YXWLCSf (5) Fwlcs wlcswlcs wlcswlcs PR PR 2 2 )1( ? ? + + = (6) f -1 is the inverse function of f. We call the WLCS-based F-measure, i.e. Equation 6, ROUGE W. Using Equation 6 and f(k)-=-k2 as the weighting function, the ROUGE-W scores for sequences Y1 and Y2 are 0.571 and 0.286 respectively. Therefore, Y1 would be ranked higher than Y2 using WLCS. We use the polynomial function of the form k? in the experiments described in Section 4 with the weighting factor ? varying from 1.1 to 2.0 with 0.1 increment. ROUGE-W is the same as ROUGE-L when ? is set to 1. In the next section, we introduce the skip-bigram co-occurrence statistics. 3.3 ROUGE-S: Skip-Bigram Co-Occurrence. Statistics Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. Skip-bigram cooccurrence statistics measure the overlap of skip bigrams between a candidate translation and a set of reference translations. Using the example given in Section 3.1: S1. police killed the gunman S2. police kill the gunman S3. the gunman kill police S4. the gunman police killed each sentence has C(4,2)2 = 6 skip-bigrams. For example, S1 has the following skip-bigrams: (?police killed?, ?police the?, ?police gunman?, ?killed the?, ?killed gunman?, ?the gunman?) Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram based F-measure as follows: Rskip2 )2,( ),(2 mC YXSKIP = (7) Pskip2 )2,( ),(2 nC YXSKIP = (8) Fskip2 2 2 2 22 2 )1( skipskip skipskip PR PR ? ? + + = (9) Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y, ? = Pskip2/Rskip2 when ?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and C is the combination function. We call the skip-bigram based F-measure, i.e. Equation 9, ROUGE-S. Using Equation 9 with ? = 1 and S1 as the reference, S2?s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333. Therefore, S2 is better than S3 and S4, and S4 is better than S3. One advantage of skip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order. Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence. We can limit the maximum skip distance, between two in-order words to control the admission of a skip-bigram. We use skip distances of 1 to 9 with increment of 1 (ROUGE-S1 to 9) and without any skip distance constraint (ROUGE-S*). In the next section, we present the evaluations of BLEU, NIST, PER, WER, ROUGE-L, ROUGE-W, and ROUGE-S using the ORANGE evaluation method described in Section 2. 2 Combinations: C(4,2) = 4!/(2!*2!) = 6.. Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward. To simulate real world scenario, we use n-best lists from ISI?s state-of-the-art statistical machine translation system, AlTemp (Och 2003), and the 2002 NIST Chinese-English evaluation corpus as. the test corpus. There are 878 source sentences in Chinese and 4 sets of reference translations provided by LDC3. For exploration study, we generate 1024-best list using AlTemp for 872 source sentences. AlTemp generates less than 1024 alternative translations for 6 out of the 878 source 3 Linguistic Data Consortium prepared these manual. translations as part of the DARPA?s TIDES project. sentences. These 6 source sentences are excluded from the 1024-best set. In order to compute BLEU at sentence level, we apply the following smoothing technique: Add one count to the n-gram hit and total n gram count for n > 1. Therefore, for candidate translations with less than n words, they can still get a positive smoothed BLEU score from shorter n-gram matches; however if nothing matches then they will get zero scores. We call the smoothed BLEU: BLEUS. For each candidate translation in the 1024-best list and each reference, we compute the following scores: 1. BLEUS1 to 9. 2. NIST, PER, and WER. 3. ROUGE-L. 4. ROUGE-W with weight ranging from 1.1. to 2.0 with increment of 0.1","Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward. To simulate real world scenario, we use n-best lists from ISI?s state-of-the-art statistical machine translation system, AlTemp (Och 2003), and the 2002 NIST Chinese-English evaluation corpus as. the test corpus. There are 878 source sentences in Chinese and 4 sets of reference translations provided by LDC3. For exploration study, we generate 1024-best list using AlTemp for 872 source sentences. AlTemp generates less than 1024 alternative translations for 6 out of the 878 source 3 Linguistic Data Consortium prepared these manual. translations as part of the DARPA?s TIDES project. sentences. These 6 source sentences are excluded from the 1024-best set. In order to compute BLEU at sentence level, we apply the following smoothing technique: Add one count to the n-gram hit and total n gram count for n > 1. Therefore, for candidate translations with less than n words, they can still get a positive smoothed BLEU score from shorter n-gram matches; however if nothing matches then they will get zero scores. We call the smoothed BLEU: BLEUS. For each candidate translation in the 1024-best list and each reference, we compute the following scores: 1. BLEUS1 to 9. 2. NIST, PER, and WER. 3. ROUGE-L. 4. ROUGE-W with weight ranging from 1.1. to 2.0 with increment of 0.1"
69,"In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches. The model incorporates a novelgrammar extraction method, uses a lan guage model for coherent output, and canbe easily tuned to a wide range of compres sion specific loss functions.","In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches. The model incorporates a novelgrammar extraction method, uses a lan guage model for coherent output, and canbe easily tuned to a wide range of compres sion specific loss functions. Automatic sentence compression can be broadly described as the task of creating a grammaticalsummary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of sub titles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w 1 , w 2 . . . w n , a compression is formed by dropping any subset of these words (Knight c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight andMarcu, 2002; Turner and Charniak, 2005; McDon ald, 2006). Furthermore, constraining the problemto word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007;Turner and Charniak, 2005), to large-margin learn ing (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the tasksomewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and inser tion. In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). Therefore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts. In this framework, the goal is to find a summary of theoriginal sentence which is grammatical and conveys the most important information without necessarily using the same words in the same or der. Our task is related to, but different from, paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them in order to generate new text, while attempting to produce a shorter resulting string. Quirk et al (2004) present an end-to-end paraphrasing system inspired byphrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. However, their model is limited to lexical substitution ? no reordering takes place ? and is 137 lacking the compression objective.Once we move away from extractive compres sion we are faced with two problems. First, wemust find an appropriate training set for our abstractive task. Compression corpora are not natu rally available and existing paraphrase corpora do not normally contain compressions. Our second problem concerns the modeling task itself. Ideally, our learning framework should handle structural mismatches and complex rewriting operations.In what follows, we first present a new cor pus for abstractive compression which we created by having annotators compress sentences while rewriting them. Besides obtaining useful data formodeling purposes, we also demonstrate that ab stractive compression is a meaningful task. We then present a tree-to-tree transducer capable of transforming an input parse tree into a compressed parse tree. Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)),a formalism that can account for structural mismatches, and is trained discriminatively. Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task. We present a noveltree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature. We also develop a number of loss functions suited to the abstractive compression task. We hope that some of the work described here might be of relevance to other gen eration tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). A stumbling block to studying abstractive sentence compression is the lack of widely available corpora for training and testing. Previous work has beenconducted almost exclusively on Ziff-Davis, a cor pus derived automatically from document abstractpairs (Knight and Marcu, 2002), or on human authored corpora (Clarke, 2008). Unfortunately,none of these data sources are suited to our problem since they have been produced with a single rewriting operation, namely word deletion. Al though there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and theMicrosoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004), they are also not ideal, since they have not been created 1 Available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. with compression in mind. They contain amplerewriting operations, however they do not explic itly target information loss. For the reasons just described, we created our own corpus. We collected 30 newspaper articles (575 sentences) from the British National Corpus (BNC) and the American News Text corpus, forwhich we obtained manual compressions. In or der to confirm that the task was feasible, five of these documents were initially compressed by two annotators (not the authors). The annotators weregiven instructions that explained the task and defined sentence compression with the aid of examples. They were asked to paraphrase while preserv ing the most important information and ensuring the compressed sentences remained grammatical.They were encouraged to use any rewriting opera tions that seemed appropriate, e.g., to delete words, add new words, substitute them or reorder them.Assessing inter-annotator agreement is notori ously difficult for paraphrasing tasks (Barzilay, 2003) since there can be many valid outputs for a given input. Also our task is doubly subjective in deciding which information to remove from the sentence and how to rewrite it. In default of an agreement measure that is well suited to the task and takes both decisions into account, we assessedthem separately. We first examined whether the annotators compressed at a similar level. The com pression rate was 56% for one annotator and 54% for the other. 2 We also assessed whether theyagreed in their rewrites by measuring BLEU (Pap ineni et al, 2002). The inter-annotator BLEU score was 23.79%, compared with the source agreement BLEU of only 13.22%. Both the compression rateand BLEU score indicate that the task is welldefined and the compressions valid. The remaining 25 documents were compressed by a single an notator to ensure consistency. All our experiments used the data from this annotator. 3Table 1 illustrates some examples from our corpus. As can be seen, some sentences contain a single rewrite operation. For instance, a PP is para phrased with a genitive (see (1)), a subordinate clause with a present participle (see (2)), a passive sentence with an active one (see (3)). However, in most cases many rewrite decisions take place allat once. Consider sentence (4). Here, the conjunc tion high winds and snowfalls is abbreviated to 2 The term ?compression rate? refers to the percentage of words retained in the compression. 3 Available from http://homepages.inf.ed.ac.uk/ tcohn/paraphrase. 138 1a. The future of the nation is in your hands. 1b. The nation?s future is in your hands. 2a. As he entered a polling booth in Katutura, he said. 2b. Entering a polling booth in Katutura, he said. 3a. Mr Usta was examined by Dr Raymond Crockett, a Harley Street physician specialising in kidney disease.3b. Dr Raymond Crockett, a Harley Street physician, ex amined Mr Usta. 4a. High winds and snowfalls have, however, grounded at a lower level the powerful US Navy Sea Stallion helicopters used to transport the slabs. 4b. Bad weather, however, has grounded the helicopters transporting the slabs. 5a. To experts in international law and relations, the USaction demonstrates a breach by a major power of in ternational conventions.5b. Experts say the US are in breach of international con ventions.Table 1: Compression examples from our corpus; (a) sen tences are the source, (b) sentences the target. bad weather and the infinitive clause to transport to the present participle transporting. Note that the prenominal modifiers US Navy Sea Stallion and the verb used have been removed. In sentence (5), the verb say is added and the NP a breach by amajor power of international conventions is para phrased by the sentence the US are in breach of international conventions. Our work builds on the model developed by Cohnand Lapata (2007). They formulate sentence compression as a tree-to-tree rewriting task. A syn chronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. Each grammar rule is assigned a weight, and these weights are learnt in discriminative training. For prediction, a specialised generation algorithmfinds the best scoring compression using the grammar rules. Cohn and Lapata apply this model to ex tractive compression with state-of-the-art results. This model is appealing for our task for severalreasons. Firstly, the synchronous grammar provides expressive power to model consistent syntactic effects such as reordering, changes in nonterminal categories and lexical substitution. Sec ondly, it is discriminatively trained, which allowsfor the incorporation of all manner of powerful features. Thirdly, the learning framework can be tai lored to the task by choosing an appropriate loss function. In the following we describe their model in more detail with emphasis on the synchronous grammar, the model structure, and the predictionand training algorithms. Section 4 presents our ex tensions and modifications. Grammar The grammar defines a space oftree pairs over uncompressed and compressed sen Grammar rules: ?S, S? ?NP 1 VBD 2 NP 3 , NP 1 VBD 2 NP 3 ? ?S, S? ?NP 1 VBD 2 NP 3 , NP 3 was VBN 2 by NP 1 ? ?NP, NP? ?he, him? ?NP, NP? ?he, he? ?NP, NP? ?he, Peter? ?VBD, VBN? ?sang, sung? ?NP, NP? ?a song, a song? Input tree: [S [NP He NP [VP sang VBD [NP a DT song NN ]]] Output trees: [S [NP He] [VP sang [NP a song]]] [S [NP Him] [VP sang [NP a song]]] [S [NP Peter] [VP sang [NP a song]]] [S [NP A song] [VP was [VP sung [PP by he]]]] [S [NP A song] [VP was [VP sung [PP by him]]]] [S [NP A song] [VP was [VP sung [PP by Peter]]]] Figure 1: Example grammar and the output trees it licences for an input tree. The numbered boxes in the rules denote linked variables. Pre-terminal categories are not shown for the output trees for the sake of brevity. tences, which we refer to henceforth as the source and target. We use the grammar to find the set of sister target sentences for a given source sentence.Figure 1 shows a toy grammar and the set of possi ble target (output) trees for the given source (input)tree. Each output tree is created by applying a se ries of grammar rules, where each rule matches a fragment of the source and creates a fragment of the target tree. A rule in the grammar consists of a pair of elementary trees and a mapping between the variables (frontier non-terminals) in both trees. A derivation is a sequence of rules yielding a target tree with no remaining variables. Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and target sentences. Specifically, they extract the mini mal set of synchronous rules which can describe each tree pair. These rules are minimal in the sensethat they cannot be made smaller (e.g., by replac ing a subtree with a variable) while still honouring the word-alignment. Decoding The grammar allows us to search for all sister trees for a given tree. The decoder maximises over this space: y ? =argmax y:S(y)=x ?(y) (1) where ?(y) = ? r?y ??(r, S(y)), ?? (2) Here x is the source (uncompressed) tree, y is a derivation which produces the source tree, S(y) = x, and a target tree, T (y), 4and r is a gram mar rule. The ? function scores the derivation and 4 Equation 1 optimises over derivations rather than target trees to allow tractable inference. 139 is defined in (2) as a linear function over the rules used. Each rule?s score is an inner product between its feature vector, ?(r,y S), and the model parame ters, ?. The feature functions are set by hand, while the model parameters are learned in training. The maximisation problem in (1) can be solved efficiently using a dynamic program. Derivations will have common sub-structures whenever they transduce the same source sub-tree into a target sub-tree. This is captured in a chart, leading to an efficient bottom-up algorithm. The asymptotic time complexity of this search is O(SR) where S is the number of source nodes andR is the number of rules matching a given node. Training The model is trained using SVM struct , a large margin method for structured output problems (Joachims, 2005; Tsochantaridis et al, 2005). This training method allows the use of a configurable loss function, ?(y ? ,y), whichmeasures the extent to which the model?s predic tion, y, differs from the reference, y ? . Central. to training is the search for a derivation which is both high scoring and has high loss compared to the gold standard. 5 This requires finding the maximiser of H(y) in one of: H s = (1? ??(y ? )??(y), ??)?(y ? ,y) H m = ?(y ? ,y)? ??(y ? )??(y), ?? (3) where the subscripts s and m denote slack and margin rescaling, which are different formulations of the training problem (see Tsochantaridis et al (2005) and Taskar et al (2003) for details). The search for the maximiser of H(y) in (3) requires the tracking of the loss value. This can be achieved by extending the decoding algorithmsuch that the chart cells also store the loss param eters (e.g., for precision, the number of true and false positives (Joachims, 2005)). Consequently, this extension leads to a considerably higher time and space complexity compared to decoding. For example, with precision loss the time complexity is O(S 3 R) as each step must consider O(S 2) pos sible loss parameter values. In this section we present our extensions of Cohnand Lapata?s (2007) model. The latter was de signed with the simpler extractive compression in mind and cannot be readily applied to our task. 5 Spurious ambiguity in the grammar means that there areoften many derivations linking the source and target. We fol low Cohn and Lapata (2007) by choosing the derivation with the most rules, which should provide good generalisation. Grammar It is relatively straightforward to extract a grammar from our corpus. This grammar will contain many rules encoding deletions and structural transformations but there will be many unobserved paraphrases, no matter how good the extraction method (recall that our corpus consistssolely of 565 sentences). For this reason, we ex tract a grammar from our abstractive corpus in the manner of Cohn and Lapata (2007) (see Section 5for details) and augment it with a larger grammar obtained from a parallel bilingual corpus. Crucially, our second grammar will not contain com pression rules, just paraphrasing ones. We leave itto the model to learn which rules serve the com pression objective. Our paraphrase grammar extraction method uses bilingual pivoting to learn paraphrases over syntax tree fragments, i.e., STSG rules. Pivoting treats the paraphrasing problem as a two-stage translation process. Some English text is translated to a foreign language, and then translated back into English (Bannard and Callison-Burch, 2005): p(e ? |e) = ? f p(e ? |f)p(f |e) (4) where p(f |e) is the probability of translating an English string e into a foreign string f and p(e ?|f) the probability of translating the same for eign string into some other English string e ? . We. thus obtain English-English translation probabili ties p(e ? |e) by marginalizing out the foreign text.Instead of using strings (Bannard and CallisonBurch, 2005), we use elementary trees on the En glish side, resulting in a monolingual STSG. Weobtain the elementary trees and foreign strings us ing the GKHM algorithm (Galley et al, 2004). This takes as input a bilingual word-aligned corpus with trees on one side, and finds the minimal set of tree fragments and their corresponding strings which is consistent with the word alignment. This process is illustrated in Figure 2 where the aligned pair on the left gives rise to the rules shown onthe right. Note that the English rules and for eign strings shown include variable indices where they have been generalised. We estimate p(f |e) and p(e ? |f) from the set of tree-to-string rules and then then pivot each tree fragment to produce STSG rules. Figure 3 illustrates the process for the [VP does not VP] fragment. Modeling and Decoding Our grammar is much larger and noisier than a grammar extractedsolely for deletion-based compression. So, in order to encourage coherence and inform lexical se 140 SNP VP VBZ does RB goHe not ne pasIl va PRP VP NP He Il PRP go vaVP VP VBZ does RB not ne pas VP S NP VP 1 2 1 1 1 2 Figure 2: Tree-to-string grammar extraction using the GHKM algorithm, showing the aligned sentence pair and the resulting rules as tree fragments and their matching strings. The boxed numbers denote variables. VP VBZ does RB not ne pas VP n ' ne ne peut ... VP MD will RB not VB VP VBP do RB not VB 1 1 1 1 1 1 1 Figure 3: Pivoting the [VP does not VP] fragment. lection we incorporate a ngram language model(LM) as a feature. This requires adapting the scor ing function, ?, in (2) to allow features over target ngrams: ?(y) = ? r?y ??(r, S(y)), ??+ ? m?T (y) ??(m,S(y)), ?? (5)where m are the ngrams and ? is a new fea ture function over these ngrams (we use only one ngram feature: the trigram log-probability). Sadly, the scoring function in (5) renders the chart-based search used for training and decoding intractable.In order to provide sufficient context to the chart based algorithm, we must also store in each chart cell the n ? 1 target tokens at the left and right edges of its yield. This is equivalent to using as our grammar the intersection between the original grammar and the ngram LM (Chiang, 2007), and increases the decoding complexity to an infeasible O(SRL 2(n?1)V )whereL is the size of the lexicon. We adopt a popular approach in syntax-inspiredmachine translation to address this problem (Chi ang, 2007). The idea is to use a beam-search overthe intersection grammar coupled with the cube pruning heuristic. The beam limits the number ofitems in a given chart cell to a fixed constant, re gardless of the number of possible LM contexts and non-terminal categories. Cube-pruning furtherlimits the number of items considered for inclu sion in the beam, reducing the time complexity to a more manageable O(SRBV ) where B is the beam size. We refer the interested reader to Chiang (2007) for details. Training The extensions to the model in (5)also necessitate changes in the training proce dure. Recall that training the basic model of Cohn and Lapata (2007) requires finding the maximiserof H(y) in (3). Their model uses a chart-based al gorithm for this purpose. As in decoding we also use a beam search for training, thereby avoiding the exponential time complexity of exact search.The beam search requires an estimate of the qual ity for incomplete derivations. We use the margin rescaling objective, H m in (3), and approximatethe loss using the current (incomplete) loss param eter values in each chart cell. We use a wide beam of 200 unique items or 500 items in total to reduce the impact of the approximation. Our loss functions are tailored to the task anddraw inspiration from metrics developed for ex tractive compression but also for summarisation and machine translation. They are based on the Hamming distance over unordered bags of items. This measures the number of predicted items that did not appear in the reference, along with a penalty for short output: ? hamming (y ? ,y) = f+max (l ? (t+ f), 0) (6) where t and f are the number of true and falsepositives, respectively, when comparing the pre dicted target, y, with the reference, y ? , and l isthe length of the reference. The second term pe nalises short output, as predicting very little or nothing would otherwise be unpenalised. We have three Hamming loss functions over: 1) tokens, 2) ngrams (n ? 3), or 3) CFG productions. Theselosses all operate on unordered bags and therefore might reward erroneous predictions. For ex ample, a permutation of the reference tokens has zero token-loss. The CFG and ngram losses have overlapping items which encode a partial order, and therefore are less affected.In addition, we developed a fourth loss func tion to measure the edit distance between themodel?s prediction and the reference, both as bags of-tokens. This measures the number of insertionsand deletions. In contrast to the previous loss func tions, this requires the true positive counts to be clipped to the number of occurrences of each type in the reference. The edit distance is given by: ? edit (y ? ,y) = p+ q ? 2 ? i min(p i , q i ) (7) where p and q denote the number of target tokensin the predicted and reference derivation, respec tively, and p i and q i are the counts for type i. 141 ?ADJP,NP? ?subject [PP to NP 1 ], part [PP of NP 1 ]? (T) ?ADVP,RB? ?as well, also? (T) ?ADJP,JJ? ?too little, insufficient? (P) ?S,S? ? ?S 1 and S 2 , S 2 and S 1 ? (P) ?NP,NP? ?DT 1 NN 2 , DT 1 NN 2 ? (S) ?NP,NP? ?DT 1 NN 2 , NN 2 ? (S) Table 2: Sample grammar rules extracted from the training set (T), pivoted set (P) or generated from the source (S). In this section we present our experimental set up for assessing the performance of our model. We give details on the corpora and grammars we used, model parameters and features, 6 the baselineused for comparison with our approach, and ex plain how our system output was evaluated. Grammar Extraction Our grammar usedrules extracted directly from our compression cor pus (the training partition, 480 sentences) and a bilingual corpus (see Table 2 for examples). Theformer corpus was word-aligned using the Berke ley aligner (Liang et al, 2006) initialised with a lexicon of word identity mappings, and parsed with Bikel?s (2002) parser. From this we extracted grammar rules following the technique described in Cohn and Lapata (2007). For the pivot grammarwe use the French-English Europarl v2 which con tains approximately 688K sentences. Again, the corpus was aligned using the Berkeley aligner and the English side was parsed with Bikel?s parser. Weextracted tree-to-string rules using our implementation of the GHKM method. To ameliorate the effects of poor alignments on the grammar, we re moved singleton rules before pivoting. In addition to the two grammars described, wescanned the source trees in the compression cor pus and included STSG rules to copy each CFG production or delete up to two of its children. This is illustrated in Table 2 where the last two rules are derived from the CFG production NP?DT NN inthe source tree. All trees are rooted with a distinguished TOP non-terminal which allows the ex plicit modelling of sentence spanning sub-trees. These grammars each had 44,199 (pivot), 7,813 (train) and 22,555 (copy) rules. We took their union, resulting in 58,281 unique rules and 13,619 unique source elementary trees. Model Parameters Our model was trainedon 480 sentences, 36 sentences were used for de velopment and 59 for testing. We used a variety of syntax-based, lexical and compression-specific 6 The software and corpus can be downloaded from http://homepages.inf.ed.ac.uk/tcohn/paraphrase. For every rule: origin of rule for each origin, o: log p o (s, t), log p o (s|t), log p o (t|s) s R , t R , s R ? t R s, t, s ? t, s = t both s and t are pre-terminals and s = t or s 6= t number of terminals/variables/dropped variables ordering of variables as numbers/non-terminals non-terminal sequence of vars identical after reordering pre-terminal or terminal sequences are identical number/identity of common/inserted/dropped terminals source is shorter/longer than target target is a compression of the source using deletes For every ngram : log p(w i |w i?1 i?(n?1) ) Table 3: The feature set. Rules were drawn from the training set, bilingual pivoting and directly from the source trees. s andt are the source and target elementary trees in a rule, the sub script R references the root non-terminal, w are the terminals in the target tree. features (196,419 in total). These are summarised in Table 3. We also use a trigram language model trained on the BNC (100 million words) using the SRI Language Modeling toolkit (Stolcke, 2002), with modified Kneser-Ney smoothing.An important parameter in our modeling frame work is the choice of loss function. We evaluatedthe loss functions presented in Section 4 on the de velopment set. We ran our system for each of the four loss functions and asked two human judgesto rate the output on a scale of 1 to 5. The Ham ming loss over tokens performed best with a meanrating of 3.18, closely followed by the edit dis tance (3.17). We chose the former over the latter as it is less coarsely approximated during search. Baseline There are no existing models thatcan be readily trained on our abstractive com pression data. Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline. The latter was trained on an extractive compression corpus drawn from the BNC (Clarke, 2008) and tunedto provide a similar compression rate to our sys tem. Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora.Evaluation Methodology Sentence compres sion output is commonly evaluated by eliciting human judgments. Following Knight and Marcu(2002), we asked participants to rate the grammati cality of the target compressions and howwell they preserved the most important information from the source. In both cases they used a five pointrating scale where a high number indicates better performance. We randomly selected 30 sen tences from the test portion of our corpus. These 142 Models Grammaticality Importance CompR Extract 3.10 ? 2.43 ? 82.5 Abstract 3.38 ? 2.85 ? ? 79.2 Gold 4.51 4.02 58.4Table 4: Mean ratings on compression output elicited by hu mans; ? : significantly different from the gold standard; ?: sig nificantly different from the baseline. sentences were compressed automatically by our model and the baseline. We also included goldstandard compressions. Our materials thus con sisted of 90 (30 ? 3) source-target sentences. We collected ratings from 22 unpaid volunteers, all self reported native English speakers. Both studies were conducted over the Internet using a custom built web interface.","In this section we present our experimental set up for assessing the performance of our model. We give details on the corpora and grammars we used, model parameters and features, 6 the baselineused for comparison with our approach, and ex plain how our system output was evaluated. Grammar Extraction Our grammar usedrules extracted directly from our compression cor pus (the training partition, 480 sentences) and a bilingual corpus (see Table 2 for examples). Theformer corpus was word-aligned using the Berke ley aligner (Liang et al, 2006) initialised with a lexicon of word identity mappings, and parsed with Bikel?s (2002) parser. From this we extracted grammar rules following the technique described in Cohn and Lapata (2007). For the pivot grammarwe use the French-English Europarl v2 which con tains approximately 688K sentences. Again, the corpus was aligned using the Berkeley aligner and the English side was parsed with Bikel?s parser. Weextracted tree-to-string rules using our implementation of the GHKM method. To ameliorate the effects of poor alignments on the grammar, we re moved singleton rules before pivoting. In addition to the two grammars described, wescanned the source trees in the compression cor pus and included STSG rules to copy each CFG production or delete up to two of its children. This is illustrated in Table 2 where the last two rules are derived from the CFG production NP?DT NN inthe source tree. All trees are rooted with a distinguished TOP non-terminal which allows the ex plicit modelling of sentence spanning sub-trees. These grammars each had 44,199 (pivot), 7,813 (train) and 22,555 (copy) rules. We took their union, resulting in 58,281 unique rules and 13,619 unique source elementary trees. Model Parameters Our model was trainedon 480 sentences, 36 sentences were used for de velopment and 59 for testing. We used a variety of syntax-based, lexical and compression-specific 6 The software and corpus can be downloaded from http://homepages.inf.ed.ac.uk/tcohn/paraphrase. For every rule: origin of rule for each origin, o: log p o (s, t), log p o (s|t), log p o (t|s) s R , t R , s R ? t R s, t, s ? t, s = t both s and t are pre-terminals and s = t or s 6= t number of terminals/variables/dropped variables ordering of variables as numbers/non-terminals non-terminal sequence of vars identical after reordering pre-terminal or terminal sequences are identical number/identity of common/inserted/dropped terminals source is shorter/longer than target target is a compression of the source using deletes For every ngram : log p(w i |w i?1 i?(n?1) ) Table 3: The feature set. Rules were drawn from the training set, bilingual pivoting and directly from the source trees. s andt are the source and target elementary trees in a rule, the sub script R references the root non-terminal, w are the terminals in the target tree. features (196,419 in total). These are summarised in Table 3. We also use a trigram language model trained on the BNC (100 million words) using the SRI Language Modeling toolkit (Stolcke, 2002), with modified Kneser-Ney smoothing.An important parameter in our modeling frame work is the choice of loss function. We evaluatedthe loss functions presented in Section 4 on the de velopment set. We ran our system for each of the four loss functions and asked two human judgesto rate the output on a scale of 1 to 5. The Ham ming loss over tokens performed best with a meanrating of 3.18, closely followed by the edit dis tance (3.17). We chose the former over the latter as it is less coarsely approximated during search. Baseline There are no existing models thatcan be readily trained on our abstractive com pression data. Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline. The latter was trained on an extractive compression corpus drawn from the BNC (Clarke, 2008) and tunedto provide a similar compression rate to our sys tem. Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora.Evaluation Methodology Sentence compres sion output is commonly evaluated by eliciting human judgments. Following Knight and Marcu(2002), we asked participants to rate the grammati cality of the target compressions and howwell they preserved the most important information from the source. In both cases they used a five pointrating scale where a high number indicates better performance. We randomly selected 30 sen tences from the test portion of our corpus. These 142 Models Grammaticality Importance CompR Extract 3.10 ? 2.43 ? 82.5 Abstract 3.38 ? 2.85 ? ? 79.2 Gold 4.51 4.02 58.4Table 4: Mean ratings on compression output elicited by hu mans; ? : significantly different from the gold standard; ?: sig nificantly different from the baseline. sentences were compressed automatically by our model and the baseline. We also included goldstandard compressions. Our materials thus con sisted of 90 (30 ? 3) source-target sentences. We collected ratings from 22 unpaid volunteers, all self reported native English speakers. Both studies were conducted over the Internet using a custom built web interface."
70,"In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers. Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays. In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.","In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers. Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays. In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems. The long-term goal of our work is to develop asystem which detects errors in grammar and us age so that appropriate feedback can be given to non-native English writers, a large and growing segment of the world?s population. Estimates arethat in China alone as many as 300 million people are currently studying English as a second lan guage (ESL). Usage errors involving prepositions are among the most common types seen in thewriting of non-native English speakers. For ex ample, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% ina Japanese learner corpus. Errors can involve incorrect selection (?we arrived to the station?), ex traneous use (?he went to outside?), and omission (?we are fond null beer?). What is responsiblefor making preposition usage so difficult for non native speakers? c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. At least part of the difficulty seems to be due tothe great variety of linguistic functions that prepositions serve. When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, preposition selection is con strained by the argument role that it marks, thenoun which fills that role, and the particular predi cate. Many English verbs also display alternations (Levin, 1993) in which an argument is sometimes marked by a preposition and sometimes not (e.g., ?They loaded the wagon with hay? / ?They loaded hay on the wagon?). When prepositions introduceadjuncts, such as those of time or manner, selec tion is constrained by the object of the preposition (?at length?, ?in time?, ?with haste?). Finally, the selection of a preposition for a given context also depends upon the intended meaning of the writer (?we sat at the beach?, ?on the beach?, ?near the beach?, ?by the beach?). With so many sources of variation in Englishpreposition usage, we wondered if the task of se lecting a preposition for a given context might prove challenging even for native speakers. To investigate this possibility, we randomly selected200 sentences from Microsoft?s Encarta Encyclopedia, and, in each sentence, we replaced a ran domly selected preposition with a blank line. We then asked two native English speakers to perform a cloze task by filling in the blank with the best preposition, given the context provided by the rest of the sentence. Our results showed only about75% agreement between the two raters, and be tween each of our raters and Encarta.The presence of so much variability in prepo sition function and usage makes the task of thelearner a daunting one. It also poses special chal lenges for developing and evaluating an NLP error detection system. This paper addresses both the 865 development and evaluation of such a system. First, we describe a machine learning system that detects preposition errors in essays of ESL writers. To date there have been relatively few attempts to address preposition error detection,though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading sys tems. We extend our previous work (Chodorow etal., 2007) by experimenting with combination fea tures, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994).Second, we discuss drawbacks in current meth ods of annotating ESL data and evaluating errordetection systems, which are not limited to prepo sition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one raterto either create an annotated corpus of learner errors, or to check the system?s output. Some grammatical errors, such as number disagreement be tween subject and verb, no doubt show very highreliability, but others, such as usage errors involv ing prepositions or determiners are likely to be much less reliable. Our results show that relyingon one rater for system evaluation can be problem atic, and we provide a sampling approach which can facilitate using multiple raters for this task. In the next section, we describe a system that automatically detects errors involving incorrect preposition selection (?We arrived to the station?) and extraneous preposition usage (?He went to outside?). In sections 3 and 4, we discuss theproblem of relying on only one rater for exhaus tive annotation and show how multiple raters can be used more efficiently with a sampling approach.Finally, in section 5 we present an analysis of com mon preposition errors that non-native speakers make. 2.1 Model. We have used a Maximum Entropy (ME) classi fier (Ratnaparkhi, 1998) to build a model of correctpreposition usage for 34 common English prepo sitions. The classifier was trained on 7 million preposition contexts extracted from parts of the MetaMetrics Lexile corpus that contain textbooks and other materials for high school students. Each context was represented by 25 features consisting of the words and part-of-speech (POS) tags found in a local window of +/- two positions around the preposition, plus the head verb of the preceding verb phrase (PV), the head noun of the precedingnoun phrase (PN), and the head noun of the following noun phrase (FH), among others. In analyzing the contexts, we used only tagging and heuris tic phrase-chunking, rather than parsing, so as to avoid problems that a parser might encounter with ill-formed non-native text 1 . In test mode, the clas-. sifier was given the context in which a preposition occurred, and it returned a probability for each of the 34 prepositions. 2.2 Other Components. While the ME classifier constitutes the core of thesystem, it is only one of several processing com ponents that refines or blocks the system?s output. Since the goal of an error detection system is to provide diagnostic feedback to a student, typically a system?s output is heavily constrained so that it minimizes false positives (i.e., the system tries toavoid saying a writer?s preposition is used incor rectly when it is actually right), and thus does not mislead the writer.Pre-Processing Filter: A pre-processing pro gram skips over preposition contexts that contain spelling errors. Classifier performance is poor in such cases because the classifier was trained on well-edited text, i.e., without misspelled words. Inthe context of a diagnostic feedback and assess ment tool for writers, a spell checker would first highlight the spelling errors and ask the writer tocorrect them before the system analyzed the prepo sitions.Post-Processing Filter: After the ME clas sifier has output a probability for each of the 34prepositions but before the system has made its fi nal decision, a series of rule-based post-processingfilters block what would otherwise be false posi tives that occur in specific contexts. The first filter prevents the classifier from marking as an error acase where the classifier?s most probable preposi tion is an antonym of what the writer wrote, such as ?with/without? and ?from/to?. In these cases, resolution is dependent on the intent of the writerand thus is outside the scope of information cap 1 For an example of a common ungrammatical sentence from our corpus, consider: ?In consion, for some reasons,museums, particuraly known travel place, get on many peo ple.? 866 tured by the current feature set. Another problem for the classifier involves differentiating between certain adjuncts and arguments. For example, in the sentence ?They described a part for a kid?, thesystem?s top choices were of and to. The benefac tive adjunct introduced by for is difficult for theclassifier to learn, perhaps because it so freely occurs in many locations within a sentence. A post processing filter prevents the system from marking as an error a prepositional phrase that begins with for and has an object headed by a human noun (a WordNet hyponym of person or group). Extraneous Use Filter: To cover extraneous use errors, we developed two rule-based filters: 1) Plural Quantifier Constructions, to handle casessuch as ?some of people? and 2) Repeated Prepo sitions, where the writer accidentally repeated the same preposition two or more times, such as ?canfind friends with with?. We found that extrane ous use errors usually constituted up to 18% of all preposition errors, and our extraneous use filters handle a quarter of that 18%.Thresholding: The final step for the preposi tion error detection system is a set of thresholds that allows the system to skip cases that are likely to result in false positives. One of these is wherethe top-ranked preposition and the writer?s prepo sition differ by less than a pre-specified amount. This was also meant to avoid flagging cases where the system?s preposition has a score only slightly higher than the writer?s preposition score, such as: ?My sister usually gets home around 3:00? (writer: around = 0.49, system: by = 0.51). In these cases, the system?s and the writer?s prepositions both fit the context, and it would be inappropriate to claimthe writer?s preposition was used incorrectly. Another system threshold requires that the probability of the writer?s preposition be lower than a pre specified value in order for it to be flagged as anerror. The thresholds were set so as to strongly fa vor precision over recall due to the high number offalse positives that may arise if there is no thresh olding. This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006). Both thresholds were empirically set on a development corpus. 2.3 Combination Features. ME is an attractive choice of machine learning al gorithm for a problem as complex as preposition error detection, in no small part because of theavailability of ME implementations that can han dle many millions of training events and features. However, one disadvantage of ME is that it does not automatically model the interactions amongfeatures as some other approaches do, such as sup port vector machines (Jurafsky and Martin, 2008).To overcome this, we have experimented with aug menting our original feature set with ?combinationfeatures? which represent richer contextual struc ture in the form of syntactic patterns.Table 1 (first column) illustrates the four com bination features used for the example context ?take our place in the line?. The p denotes a preposition, so N-p-N denotes a syntactic context where the preposition is preceded and followed by a noun phrase. We use the preceding noun phrase (PN) and following head (FH) from the original feature set for the N-p-N feature. Column 3 shows one instantiation of combination features:Combo:word. For the N-p-N feature, the corresponding Combo:word instantiation is ?place line? since ?place? is the PN and ?line? is theFH. We also experimented with using combinations of POS tags (Combo:tag) and word+tag com binations (Combo:word+tag). So for the example, the Combo:tag N-p-N feature would be ?NN-NN?, and the Combo:word+tag N-p-N feature would beplace NN+line NN (see the fourth column of Ta ble 1). The intuition with the Combo:tag features is that the Combo:word features have the potentialto be sparse, and these capture more general pat terns of usage. We also experimented with other features such as augmenting the model with verb-preposition preferences derived from Comlex (Grishman et al, 1994), and querying the Google Terabyte N-gramcorpus with the same patterns used in the combina tion features. The Comlex-based features did not improve the model, and though the Google N-gram corpus represents much more information than our7 million event model, its inclusion improved per formance only marginally. 2.4 Evaluation. In our initial evaluation of the system we col lected a corpus of 8,269 preposition contexts,error-annotated by two raters using the scheme de scribed in Section 3 to serve as a gold standard. In this study, we focus on two of the three types of preposition errors: using the incorrect preposition and using an extraneous preposition. We compared 867 Class Components Combo:word Features Combo:tag Features p-N FH line NN N-p-N PN-FH place-line NN-NN V-p-N PV-PN take-line VB-NN V-N-p-N PV-PN-FH take-place-line VB-NN-NN Table 1: Feature Examples for take our place in the line different models: the baseline model of 25 features and baseline with combination features added. Theprecision and recall for the top performing models are shown in Table 2. These results do not in clude the extraneous use filter; this filter generally increased precision by as much as 2% and recall by as much as 5%. Evaluation Metrics In the tasks of determiner and preposition selection in well-formed, nativetexts (such as (Knight and Chander, 1994), (Min nen et al, 2000), (Turner and Charniak, 2007) and (Gamon et al, 2008)), the evaluation metric most commonly used is accuracy. In these tasks, one compares the system?s output on a determiner or preposition to the gold standard of what the writeroriginally wrote. However, in the tasks of deter miner and preposition error detection, precision and recall are better metrics to use because oneis only concerned with a subset of the preposi tions (or determiners), those used incorrectly, as opposed to all of them in the selection task. In essence, accuracy has the problem of distorting system performance. Results The baseline system (described in(Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. Next we tested the differ ent combination models: word, tag, word+tag, andall three. Surprisingly, three of the four combina tion models: tag, word+tag, all, did not improve performance of the system when added to the model, but using just the +Combo:word features improved recall by 1%. We use the +Combo:word model to test our sampling approach in section 4. As a final test, we tuned our training corpus of 7 million events by removing any contexts with unknown or misspelled words, and then retrained the model. This ?purge? resulted in a removal of nearly 200,000 training events. With this new training corpus, the +Combo:tag feature showed the biggest improvement over the baseline, withan improvement in both precision (+2.3%) and re call (+2.4%) to 82.1% and 14.1% respectively (last line of Table 2. While this improvement may seemsmall, it is in part due to the difficulty of the prob lem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). It should be noted that with the inclusion of the extraneous use filter, performance of the +Combo:tag rose to 84% precision and close to 19% recall. Model Precision Recall Baseline 79.8% 11.7% +Combo:word 79.8% 12.8% +Combo:tag (with purge) 82.1% 14.1%Table 2: Best System Results on Incorrect Selec tion Task 2.5 Related Work. Currently there are only a handful of approachesthat tackle the problem of preposition error detec tion in English learner texts. (Gamon et al, 2008)used a language model and decision trees to de tect preposition and determiner errors in the CLEC corpus of learner essays. Their system performs at 79% precision (which is on par with our system),however recall figures are not presented thus making comparison difficult. In addition, their eval uation differs from ours in that they also include errors of omission, and their work focuses on the top twelve most frequent prepositions, while ours has greater coverage with the top 34. (Izumi etal., 2003) and (Izumi et al, 2004) used an ME ap proach to classify different grammatical errors in transcripts of Japanese interviews. They do not present performance of prepositions specifically, but overall performance for the 13 error types they target reached 25% precision and 7% recall.(Eeg-Olofsson and Knuttson, 2003) created a rule based approach to detecting preposition errors in Swedish language learners (unlike the approaches presented here, which focus on English languagelearners), and their system performed at 25% ac curacy. (Lee and Seneff, 2006) used a language model to tackle the novel problem of prepositionselection in a dialogue corpus. While their perfor mance results are quite high, 88% precision and 868 78% recall, it should be noted that their evaluation was on a small corpus with a highly constraineddomain, and focused on a limited number of prepo sitions, thus making direct comparison with our approach difficult.Although our recall figures may seem low, es pecially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is. For example, in the problem of preposition selection in native text, a baseline using the most frequent preposition(of) results in precision and recall of 26%. In addi tion, the cloze tests presented earlier indicate thateven in well-formed text, agreement between na tive speakers on preposition selection is only 75%.In texts written by non-native speakers, rater dis agreement increases, as will be shown in the next section. While developing an error detection system forprepositions is certainly challenging, given the re sults from our work and others, evaluation also poses a major challenge. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output. The drawbacks of this approach are: 1. every time the system is changed, a rater is needed to re-check the output, and 2. it is very hard to estimate recall. What these two evaluation methods have in common is that they side-step the issue of annotator reliability. In this section, we show how relying on only onerater can be problematic for difficult error detec tion tasks, and in section 4, we propose a method(?the sampling approach?) for efficiently evaluat ing a system that does not require the amount ofeffort needed in the standard approach to annota tion. 3.1 Annotation. To create a gold-standard corpus of error annotations for system evaluation, and also to deter mine whether multiple raters are better than one, 2(Eeg-Olofsson and Knuttson, 2003) had a small evaluation on 40 preposition contexts and it is unclear whether mul tiple annotators were used. we trained two native English speakers with prior NLP annotation experience to annotate prepositionerrors in ESL text. The training was very extensive: both raters were trained on 2000 preposition contexts and the annotation manual was it eratively refined as necessary. To summarize the procedure, the two raters were shown sentences randomly selected from student essays with each preposition highlighted in the sentence. They marked each context (?2-word window around thepreposition, plus the commanding verb) for gram mar and spelling errors, and then judged whether the writer used an incorrect preposition, a correct preposition, or an extraneous preposition. Finally, the raters suggested prepositions that would best fit the context, even if there were no error (some contexts can license multiple prepositions). 3.2 Reliability. Each rater judged approximately 18,000 prepo sitions contexts, with 18 sets of 100 contextsjudged by both raters for purposes of comput ing kappa. Despite the rigorous training regimen, kappa ranged from 0.411 to 0.786, with an overall combined value of 0.630. Of the prepositions that Rater 1 judged to be errors, Rater 2 judged 30.2% to be acceptable. Conversely, of the prepositions Rater 2 judged to be erroneous, Rater 1 found 38.1% acceptable. The kappa of 0.630 shows the difficulty of this task and also shows how two highly trained raters can produce very different judgments. Details on our annotation and human judgment experiments can be found in (Tetreault and Chodorow, 2008). Variability in raters? judgments translates to variability of system evaluation. For instance, in our previous work (Chodorow et al, 2007), wefound that when our system?s output was com pared to judgments of two different raters, therewas a 10% difference in precision and a 5% differ ence in recall. These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance. The results from the previous section motivate theneed for a more refined evaluation. They sug gest that for certain error annotation tasks, such as preposition usage, it may not be appropriate to use only one rater and that if one uses multiple raters 869for error annotation, there is the possibility of cre ating an adjudicated set, or at least calculating the variability of the system?s performance. However,annotation with multiple raters has its own disadvantages as it is much more expensive and time consuming. Even using one rater to produce a sizeable evaluation corpus of preposition errors is extremely costly. For example, if we assume that500 prepositions can be annotated in 4 hours us ing our annotation scheme, and that the base rate for preposition errors is 10%, then it would take atleast 80 hours for a rater to find and mark 1000 er rors. In this section, we propose a more efficient annotation approach to circumvent this problem. 4.1 Methodology. Figure 1: Sampling Approach ExampleThe sampling procedure outlined here is inspired by the one described in (Chodorow and Lea cock, 2000) for the task of evaluating the usage of nouns, verbs and adjectives. The central idea is to skew the annotation corpus so that it contains a greater proportion of errors. Here are the steps in the procedure: 1. Process a test corpus of sentences so that each. preposition in the corpus is labeled ?OK? or ?Error? by the system. 2. Divide the processed corpus into two sub-. corpora, one consisting of the system?s ?OK? prepositions and the other of the system?s ?Error? prepositions. For the hypotheticaldata in Figure 1, the ?OK? sub-corpus con tains 90% of the prepositions, and the ?Error? sub-corpus contains the remaining 10%. 3. Randomly sample cases from each sub-. corpus and combine the samples into an an notation set that is given to a ?blind? human rater. We generally use a higher sampling rate for the ?Error? sub-corpus because we want to ?enrich? the annotation set with a larger proportion of errors than is found in the test corpus as a whole. In Figure 1, 75% of the ?Error? sub-corpus is sampled while only 16% of the ?OK? sub-corpus is sampled. 4. For each case that the human rater judges to. be an error, check to see which sub-corpus itcame from. If it came from the ?OK? sub corpus, then the case is a Miss (an error that the system failed to detect). If it came from the ?Error? sub-corpus, then the case is a Hit (an error that the system detected). If the rater judges a case to be a correct usage and it came from the ?Error? sub-corpus, then it is a False Positive (FP). the sample from the ?Error? sub-corpus. Forthe hypothetical data in Figure 1, these val ues are 600/750 = 0.80 for Hits, and 150/750 = 0.20 for FPs. Calculate the proportion ofMisses in the sample from the ?OK? sub corpus. For the hypothetical data, this is 450/1500 = 0.30 for Misses. 6. The values computed in step 5 are conditional. proportions based on the sub-corpora. To calculate the overall proportions in the test cor pus, it is necessary to multiply each value by the relative size of its sub-corpus. This is shown in Table 3, where the proportion ofHits in the ?Error? sub-corpus (0.80) is multiplied by the relative size of the ?Error? sub corpus (0.10) to produce an overall Hit rate (0.08). Overall rates for FPs and Misses are calculated in a similar manner. 7. Using the values from step 6, calculate Preci-. sion (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)). These are shown in the last two rows of Table 3. Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion Hits 0.80 * 0.10 = 0.08 FP 0.20 * 0.10 = 0.02 Misses 0.30 * 0.90 = 0.27 Precision 0.08/(0.08 + 0.02) = 0.80 Recall 0.08/(0.08 + 0.27) = 0.23 Table 3: Sampling Calculations (Hypothetical) 870 This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs fromactive learning applications in that there are no it erative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 4.2 Application. Next, we tested whether our proposed sampling approach provides good estimates of a sys tem?s performance. For this task, we used the +Combo:word model to separate a large corpusof student essays into the ?Error? and ?OK? sub corpora. The original corpus totaled over 22,000 prepositions which would normally take several weeks for two raters to double annotate and thenadjudicate. After the two sub-corpora were propor tionally sampled, this resulted in an annotation set of 752 preposition contexts (requiring roughly 6 hours for annotation), which is substantially more manageable than the full corpus. We had both raters work together to make judgments for each preposition. It is important to note that while these are notthe exact same essays used in the previous evalua tion of 8,269 preposition contexts, they come from the same pool of student essays and were on the same topics. Given these strong similarities, we feel that one can compare scores between the two approaches. The precision and recall scores forboth approaches are shown in Table 4 and are ex tremely similar, thus suggesting that the samplingapproach can be used as an alternative to exhaus tive annotation. Precision Recall Standard Approach 80% 12% Sampling Approach 79% 14% Table 4: Sampling Results It is important with the sampling approach to use appropriate sample sizes when drawing from the sub-corpora, because the accuracy of the estimatesof hits and misses will depend upon the propor tion of errors in each sub-corpus as well as on the sample sizes. The OK sub-corpus is expected to have even fewer errors than the overall base rate, so it is especially important to have a relativelylarge sample from this sub-corpus. The compari son study described above used an OK sub-corpussample that was twice as large as the Error subcorpus sample (about 500 contexts vs. 250 con texts). In short, the sampling approach is intended to alleviate the burden on annotators when faced with the task of having to rate several thousand errors of a particular type in order to produce a sizeable error corpus. On the other hand, one advantage that exhaustive annotation has over the sampling method is that it makes possible the comparison of multiple systems. With the sampling approach, one would have to resample and annotate for each system, thus multiplying the work needed. One aspect of automatic error detection that usu ally is under-reported is an analysis of the errors that learners typically make. The obvious benefit of this analysis is that it can focus development of the system. From our annotated set of preposition errors, we found that the most common prepositions that learners used incorrectly were in (21.4%), to (20.8%) and of (16.6%). The top ten prepositions accounted for 93.8% of all preposition errors in our learner corpus.Next, we ranked the common preposition ?con fusions?, the common mistakes made for each preposition. The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition). The most common of fenses were actually extraneous errors (see Table5): using to and of when no preposition was li censed accounted for 16.8% of all errors. It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text. This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner.","One aspect of automatic error detection that usu ally is under-reported is an analysis of the errors that learners typically make. The obvious benefit of this analysis is that it can focus development of the system. From our annotated set of preposition errors, we found that the most common prepositions that learners used incorrectly were in (21.4%), to (20.8%) and of (16.6%). The top ten prepositions accounted for 93.8% of all preposition errors in our learner corpus.Next, we ranked the common preposition ?con fusions?, the common mistakes made for each preposition. The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition). The most common of fenses were actually extraneous errors (see Table5): using to and of when no preposition was li censed accounted for 16.8% of all errors. It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text. This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner."
71,"Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.","Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure. In many NLP applications, such as Question An swering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text. For example, a QA system has to deduce that ?SCO sued IBM? is inferred from ?SCO won a lawsuit against IBM? to answer ?Whom did SCO sue??. This type of reasoning has been identified as a core semanticinference paradigm by the generic Textual Entail ment framework (Giampiccolo et al, 2007). An important type of knowledge needed for such inference is entailment rules. An entailmentrule specifies a directional inference relation be tween two templates, text patterns with variables, such as ?X win lawsuit against Y ? X sue Y ?. Applying this rule by matching ?X win lawsuit against Y ? in the above text allows a QA system to c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.infer ?X sue Y ? and identify ?IBM?, Y ?s instantiation, as the answer for the above question. Entail ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g. (Romano et al, 2006). One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet). Supervised learning of broad coverage rule-sets is an arduous task. This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005). Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable). However, a predicate quite of ten appears in the text with just a single variable(e.g. intransitive verbs or passives), where infer ence requires unary rules, e.g. ?X take a nap?X sleep? (further motivations in Section 3.1).In this paper we focus on unsupervised learning of unary entailment rules. Two learning ap proaches are proposed. In our main approach, rules are learned by measuring how similar the variable instantiations of two templates in a corpusare. In addition to adapting state-of-the-art similar ity measures for unary rule learning, we propose a new measure, termed Balanced-Inclusion, which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity. In a second approach, unary rules arederived from binary rules learned by state-of-the art binary rule learning methods. We tested the various unsupervised unary rule 849learning methods, as well as a binary rule learn ing method, on a test set derived from a standard IE benchmark. This provides the first comparisonbetween the performance of unary and binary rule sets. Several results rise from our evaluation: (a) while most work on unsupervised learning ignored unary rules, all tested unary methods outperformed the binary method; (b) it is better to learn unary rules directly than to derive them from a binary rule-base; (c) our proposed Balanced-Inclusion measure outperformed all other tested methods interms of F1 measure. Moreover, only BalancedInclusion improved F1 score over a baseline infer ence that does not use entailment rules at all . This section reviews relevant distributional simi larity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning. Distributional similarity measures follow the Distributional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954). Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f |q) Pr(f) ]. Weeds and Weir (2003) proposed to measure thesymmetric similarity between two words by av eraging two directional (asymmetric) scores: the coverage of each word?s features by the other. The coverage of u by v is measured by: Cover(u, v) = ? f?F u ?F v w u (f) ? f?F u w u (f) The average can be arithmetic or harmonic: WeedsA(u, v) = 1 2 [Cover(u, v) + Cover(v, u)] WeedsH(u, v) = 2 ? Cover(u, v) ? Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al also used pmi for feature weights. Binary rule learning algorithms adopted suchlexical similarity approaches for learning rules between templates, where the features of each tem plate are its variable instantiations in a corpus, such as {X=?SCO?, Y =?IBM?} for the example in Section 1. Some works focused on learningrules from comparable corpora, containing com parable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al, 2003). Such corpora are highly informative for identifying variations of the same meaning, since, typically, when variableinstantiations are shared across comparable docu ments the same predicates are described. However,it is hard to collect broad-scale comparable cor pora, as the majority of texts are non-comparable. A complementary approach is learning from the abundant regular, non-comparable, corpora. Yet,in such corpora it is harder to recognize varia tions of the same predicate. The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ). Some workstake the combination of the two variable instantiations in each template occurrence as a single complex feature, e.g. {X-Y =?SCO-IBM?}, and com pare between these complex features of t and t ? (Ravichandran and Hovy, 2002; Szpektor et al, 2004; Sekine, 2005).Directional Measures Most rule learning meth ods apply a symmetric similarity measure between two templates, viewing them as paraphrasing eachother. However, entailment is in general a direc tional relation. For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?. (Weeds and Weir, 2003) propose a directional measure for learning hyponymy between twowords, ?l? r?, by giving more weight to the cov erage of the features of l by r (with ? > 1 2 ): WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l) When ?=1, this measure degenerates into Cover(l, r), termed Precision(l, r). With 850 Precision(l, r) we obtain a ?soft? version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the ?important? features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al, 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al, 2002)and (Pekar, 2006) learn rules between unary tem plates. However, (Shinyama et al, 2002) relies on comparable corpora for identifying paraphrasesand simply takes any two templates from comparable sentences that share a named entity instan tiation to be paraphrases. Such approach is notfeasible for non-comparable corpora where statis tical measurement is required. (Pekar, 2006) learnsrules only between templates related by local dis course (information from different documents is ignored). In addition, their template structure islimited to only verbs and their direct syntactic ar guments, which may yield incorrect rules, e.g. forlight verbs (see Section 5.2). To overcome this limitation, we use a more expressive template struc ture. 3.1 Motivations. Most unsupervised rule learning algorithms focused on learning binary entailment rules. How ever, using binary rules for inference is not enough. First, a predicate that can have multiple arguments may still occur with only one of its arguments.For example, in ?The acquisition of TCA was successful?, ?TCA? is the only argument of ?acqui sition?. Second, some predicate expressions are unary by nature. For example, modifiers, such as ?the elected X?, or intransitive verbs. In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs.For these reasons, it seems that unary rule learn ing should be addressed in addition to binary rule learning. We are further motivated by the fact thatsome (mostly supervised) works in IE found learn ing unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al, 2003; Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail ment rules.This paper investigates acquisition of unary entailment rules from regular non-comparable cor pora. We first describe the structure of unarytemplates and then explore two conceivable approaches for learning unary rules. The first ap proach directly assesses the relation between twogiven templates based on the similarity of their in stantiations in the corpus. The second approach,which was also mentioned in (Iftene and Balahur Dobrescu, 2007), derives unary rules from learned binary rules. 3.2 Unary Template Structure. To learn unary rules we first need to define theirstructure. In this paper we work at the syntac tic representation level. Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees. Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodesas variables (Sudo et al, 2003). However, the num ber of possible templates is exponential in the sizeof the sentence. In the binary rule learning litera ture, the main solution for exhaustively learning allrules between any pair of templates in a given corpus is to restrict the structure of templates. Typi cally, a template is restricted to be a path in a parse tree between two variable nodes (Lin and Pantel, 2001; Ibrahim et al, 2003). Following this approach, we chose the structure of unary templates to be paths as well, where oneend of the path is the template?s variable. How ever, paths with one variable have more expressive power than paths between two variables, since the combination of two unary paths may generate a binary template that is not a path. For example, the combination of ?X call indictable? and ?call Y indictable? is the template ?X call Y indictable?, which is not a path between X and Y . For every noun node v in a parsed sentence, we generate templates with v as a variable as follows: 1. Traverse the path from v towards the root of. the parse tree. Whenever a candidate pred icate is encountered (any noun, adjective or verb) the path from that node to v is taken as a template. We stop when the first verb orclause boundary (e.g. a relative clause) is encountered, which typically represent the syn tactic boundary of a specific predicate. 851 2. To enable templates with control verbs and. light verbs, e.g. ?X help preventing?, ?Xmake noise?, whenever a verb is encountered we generate templates that are paths between v and the verb?s modifiers, either ob jects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated). 3. To capture noun modifiers that act as predi-. cates, e.g. ?the losingX?, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb. We use the Catvar database to identify verb derivations (Habash and Dorr, 2003). As an example for the procedure, the templates extracted from the sentence ?The losing party played it safe? with ?party? as the variable are: ?losing X?, ?X play? and ?X play safe?. 3.3 Direct Learning of Unary Rules. We applied the lexical similarity measures pre sented in Section 2 for unary rule learning. Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature?s weight. We first adaptedDIRT for unary templates (unary-DIRT, apply ing Lin-similarity to the single feature vector), as well as its output filtering by LEDIR. The various Weeds measures were also applied 1 : symmetric arithmetic average, symmetric harmonic average, weighted arithmetic average and Precision. After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations ?l, r? in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ?kill X ? injure X?. On the other hand, directional measures such as Weeds Precision tend to prefer directional rules inwhich the entailing template is infrequent. If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates. This behav ior generates high-score incorrect rules. Based on this analysis, we propose a new measure that balances the two behaviors, termed 1We applied the best performing parameter values pre sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).Balanced-Inclusion (BInc). BInc identifies entail ing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BInc(l, r) = ? Lin(l, r) ? Precision(l, r) 3.4 Deriving Unary Rules From Binary Rules. An alternative way to learn unary rules is to first learn binary entailment rules and then derive unary rules from them. We derive unary rules from a given binary rule-base in two steps. First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2). For example, from ?X find solu tion to Y ? X solve Y ? we generate the unary rules ?X find? X solve?, ?X find solution? Xsolve?, ?solution to Y ? solve Y ? and ?find solu tion to Y ? solve Y ?. The score of each generated rule is set to be the score of the original binary rule.The same unary rule can be derived from different binary rules. For example, ?hire Y ? employ Y ? is derived both from ?X hire Y ? X em ploy Y ? and ?hire Y for Z ? employ Y for Z?, having a different score from each original binary rule. The second step of the algorithm aggregates the different scores yielded for each derived rule to produce the final rule score. Three aggregationfunctions were tested: sum (Derived-Sum), aver age (Derived-Avg) and maximum (Derived-Max). We want to evaluate learned unary and binary rule bases by their utility for NLP applications throughassessing the validity of inferences that are per formed in practice using the rule base.To perform such experiments, we need a test set of seed templates, which correspond to a set of target predicates, and a corpus annotated with allargument mentions of each predicate. The evaluation assesses the correctness of all argument ex tractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application). Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose. This standard IE dataset includes 33 types of event predicates such as Injure, Sue and Divorce. 2 http://projects.ldc.upenn.edu/ace/ 852All event mentions are annotated in the corpus, in cluding the instantiated arguments of the predicate. ACE guidelines specify for each event its possible arguments, each associated with a semantic role. For instance, some of the Injure event arguments are Agent, Victim and Time.To utilize the ACE dataset for evaluating entail ment rule applications, we manually represented each ACE event predicate by unary seed templates. For example, the seed templates for Injure are ?A injure?, ?injure V ? and ?injure in T ?. We mapped each event role annotation to the corresponding seed template variable, e.g. ?Agent? to A and ?Victim? to V in the above example. Templatesare matched using a syntactic matcher that han dles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007). A rule application is considered correct if the matched argument is annotated by the corresponding ACE role. For testing binary rule-bases, we automatically generated binary seed templates from any twounary seeds that share the same predicate. For ex ample, for Injure the binary seeds ?A injure V ?, ?A injure in T ? and ?injure V in T ? were automatically generated from the above unary seeds. We performed two adaptations to the ACE dataset to fit it better to our evaluation needs. First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules. Thus, four events that correspond ambiguously tomultiple distinct predicates were ignored. For instance, the Transfer-Money event refers to both do nating and lending money, and thus annotations ofthis event cannot be mapped to a specific seed tem plate. We also omitted 3 events with less than 10mentions, and were left with 26 events (6380 argu ment mentions). Additionally, we regard all entailing mentions under the textual entailment definition as correct. However, event mentions are annotated as correct in ACE only if they explicitly describe the targetevent. For instance, a Divorce mention does entail a preceding marriage event but it does not ex plicitly describe it, and thus it is not annotated as a Marry event. To better utilize the ACE dataset, we considered for a target event the annotations of other events that entail it as being correct as well.We note that each argument was considered sep arately. For example, we marked a mention of a divorced person as entailing the marriage of that person, but did not consider the place and time of the divorce act to be those of the marriage . We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001). We executed each method over the Reuters RCV1 corpus 3 , learning for each template r in the corpus the top100 rules in which r is entailed by another tem plate l, ?l? r?. All rules were learned in canonical form (Szpektor and Dagan, 2007). The rule-base learned by binary DIRT was taken as the input for deriving unary rules from binary rules. The performance of each acquired rule-base was measured for each ACE event. We measured the percentage of correct argument mentions extracted out of all correct argument mentions annotated for the event (recall) and out of all argument mentionsextracted for the event (precision). We also mea sured F1, their harmonic average, and report macro average Recall, Precision and F1 over the 26 event types. No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges. Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex tracted only by direct matching of seed templates. Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching). It is thus useful for comparing different rule-bases,though the absolute extraction figures do not re flect the full potential of the rules. In Secion 5.2 we analyze the full-system?s errors to isolate the rules? contribution to overall system performance. 5.1 Results. In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg. We omitted the results of methods that were clearly inferior to others: (a) WeedsA, WeedsD and Weeds-Precision did not increase 3 http://about.reuters.com/researchandstandards/corpus/ 853Recall over not using rules because rules with in frequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT. Figure 1 presents Recall, Precision and F1 of themethods for different cutoff points. First, we observe that even when matching only the seed tem plates (K=0), unary seeds outperform the binary seeds in terms of both Precision and Recall. This surprising behavior is consistent through all rulecutoff points: all unary learning algorithms per form better than binary DIRT in all parameters. The inferior behavior of binary DIRT is analyzed in Section 5.2.The graphs show that symmetric unary approaches substantially increase recall, but dramati cally decrease precision already at the top 10 rules. As a result, F1 only decreases for these methods. Lin similarity (DIRT) and Weeds-Harmonic show similar behaviors. They consistently outperform Derived-Avg. One reason for this is that incorrectunary rules may be derived even from correct bi nary rules. For example, from ?X gain seat on Y ? elect X to Y ? the incorrect unary rule ?X gain? electX? is also generated. This problem is less frequent when unary rules are directly scored based on their corpus statistics. The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to theother algorithms. As a result, it is the only algo rithm that improves over the F1 baseline of K=0,with the best cutoff point at K=20. BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10). We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2). This is reflected in the very low precision of the other methods. On the other hand, some correct rules were only learned by BInc, e.g. ?countersuit againstX?X sue? and ?X take wife ? X marry?.When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated arFigure 1: Average Precision, Recall and F1 at dif ferent top K rule cutoff points. guments), binary rules either miss that mention, orextract both the correct argument and another in correct one. To neutralize this bias, we also testedthe various methods only on event mentions an notated with two or more arguments and obtained similar results to those presented for all mentions. This further emphasizes the general advantage of using unary rules over binary rules. 854 5.2 Analysis. Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions. We manually classified each rule ?l ? r? as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam ple, learning ?X decide? X meet? instead of ?X decide to meet ? X meet?; (e) Incorrect - other incorrect rules, e.g. ?charge X ? convict X?.Table 1 summarizes the analysis and demonstrates two problems of binary-DIRT. First, rela tive to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extractedmany more incorrect mentions for the same num ber of incorrect rules. Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side. Such rules are leaned because many binary templates have a more complex structure than paths between arguments. As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules. For example, BInc learned?take Y into custody ? arrest Y ? while binary DIRT learned ?X take Y ? X arrest Y ?. System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac tions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K=20), by sampling 200 extractions of each type. From the false positives analysis (Table 2) we see that 39% of the errors are due to incorrect rules. The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates. All learning methods suffer from these issues. As wasshown by our results, BInc provides a first step to wards reducing these problems. Yet, these issues require further research. Apart from incorrectly learned rules, incorrect template matching (e.g. due to parse errors) and context mismatch contribute together 46% of theerrors. Context mismatches occur when the entail ing template is matched in inappropriate contexts. For example, ?slam X ? attack X? should not be applied when X is a ball, only when it is a person. The rule-set net effect on system precision is better estimated by removing these errors and fixing the annotation errors, which yields 72% precision. Binary DIRT Balanced Inclusion Correct 16 (70) 38 (91) Partial Template 27 (2665) 6 (81) Incorrect 157 (2584) 156 (787) Total 200 (5319) 200 (959) Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions. The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses. Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20. Reason % mentions Rule not learned 61.5 Match error 25.0 Discourse analysis needed 12.0 Argument is predicative 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20. Table 3 presents the analysis of false negatives. First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary discourse analysis. Thus, a recall upper bound for en tailment rules is 88%. Many missed extractions aredue to rules that were not learned (61.5%). How ever, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules. By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall.","We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001). We executed each method over the Reuters RCV1 corpus 3 , learning for each template r in the corpus the top100 rules in which r is entailed by another tem plate l, ?l? r?. All rules were learned in canonical form (Szpektor and Dagan, 2007). The rule-base learned by binary DIRT was taken as the input for deriving unary rules from binary rules. The performance of each acquired rule-base was measured for each ACE event. We measured the percentage of correct argument mentions extracted out of all correct argument mentions annotated for the event (recall) and out of all argument mentionsextracted for the event (precision). We also mea sured F1, their harmonic average, and report macro average Recall, Precision and F1 over the 26 event types. No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges. Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex tracted only by direct matching of seed templates. Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching). It is thus useful for comparing different rule-bases,though the absolute extraction figures do not re flect the full potential of the rules. In Secion 5.2 we analyze the full-system?s errors to isolate the rules? contribution to overall system performance. 5.1 Results. In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg. We omitted the results of methods that were clearly inferior to others: (a) WeedsA, WeedsD and Weeds-Precision did not increase 3 http://about.reuters.com/researchandstandards/corpus/ 853Recall over not using rules because rules with in frequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT. Figure 1 presents Recall, Precision and F1 of themethods for different cutoff points. First, we observe that even when matching only the seed tem plates (K=0), unary seeds outperform the binary seeds in terms of both Precision and Recall. This surprising behavior is consistent through all rulecutoff points: all unary learning algorithms per form better than binary DIRT in all parameters. The inferior behavior of binary DIRT is analyzed in Section 5.2.The graphs show that symmetric unary approaches substantially increase recall, but dramati cally decrease precision already at the top 10 rules. As a result, F1 only decreases for these methods. Lin similarity (DIRT) and Weeds-Harmonic show similar behaviors. They consistently outperform Derived-Avg. One reason for this is that incorrectunary rules may be derived even from correct bi nary rules. For example, from ?X gain seat on Y ? elect X to Y ? the incorrect unary rule ?X gain? electX? is also generated. This problem is less frequent when unary rules are directly scored based on their corpus statistics. The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to theother algorithms. As a result, it is the only algo rithm that improves over the F1 baseline of K=0,with the best cutoff point at K=20. BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10). We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2). This is reflected in the very low precision of the other methods. On the other hand, some correct rules were only learned by BInc, e.g. ?countersuit againstX?X sue? and ?X take wife ? X marry?.When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated arFigure 1: Average Precision, Recall and F1 at dif ferent top K rule cutoff points. guments), binary rules either miss that mention, orextract both the correct argument and another in correct one. To neutralize this bias, we also testedthe various methods only on event mentions an notated with two or more arguments and obtained similar results to those presented for all mentions. This further emphasizes the general advantage of using unary rules over binary rules. 854 5.2 Analysis. Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions. We manually classified each rule ?l ? r? as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam ple, learning ?X decide? X meet? instead of ?X decide to meet ? X meet?; (e) Incorrect - other incorrect rules, e.g. ?charge X ? convict X?.Table 1 summarizes the analysis and demonstrates two problems of binary-DIRT. First, rela tive to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extractedmany more incorrect mentions for the same num ber of incorrect rules. Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side. Such rules are leaned because many binary templates have a more complex structure than paths between arguments. As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules. For example, BInc learned?take Y into custody ? arrest Y ? while binary DIRT learned ?X take Y ? X arrest Y ?. System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac tions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K=20), by sampling 200 extractions of each type. From the false positives analysis (Table 2) we see that 39% of the errors are due to incorrect rules. The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates. All learning methods suffer from these issues. As wasshown by our results, BInc provides a first step to wards reducing these problems. Yet, these issues require further research. Apart from incorrectly learned rules, incorrect template matching (e.g. due to parse errors) and context mismatch contribute together 46% of theerrors. Context mismatches occur when the entail ing template is matched in inappropriate contexts. For example, ?slam X ? attack X? should not be applied when X is a ball, only when it is a person. The rule-set net effect on system precision is better estimated by removing these errors and fixing the annotation errors, which yields 72% precision. Binary DIRT Balanced Inclusion Correct 16 (70) 38 (91) Partial Template 27 (2665) 6 (81) Incorrect 157 (2584) 156 (787) Total 200 (5319) 200 (959) Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions. The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses. Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20. Reason % mentions Rule not learned 61.5 Match error 25.0 Discourse analysis needed 12.0 Argument is predicative 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20. Table 3 presents the analysis of false negatives. First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary discourse analysis. Thus, a recall upper bound for en tailment rules is 88%. Many missed extractions aredue to rules that were not learned (61.5%). How ever, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules. By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall."
72,"Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a func tion of processing time and corpus size.","Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a func tion of processing time and corpus size. The Natural Language Processing (NLP) com munity has recently seen a growth in corpus-based methods. Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as ma chine translation (Och and Ney 2002), information extraction (Etzioni et al 2004), and question an swering (Brill et al 2001). In the last decade, we have seen an explosion in the amount of available digital text resources. It is estimated that the Internet contains hundreds of terabytes of text data, most of which is in an unstructured format. Yet, many NLP algorithms tap into only megabytes or gigabytes of this information. In this paper, we make a step towards acquiring semantic knowledge from terabytes of data. We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). We show that by simply utilizing more data on this task, we can achieve similar performance to a linguisticallyrich approach. The current state of the art co occurrence model requires an estimated 10 years just to parse a 1TB corpus (see Table 1). Instead of using a syntactically motivated co-occurrence ap proach as above, our system uses lexico-syntactic rules. In particular, it finds lexico-POS patterns by making modifications to the basic edit distance algorithm. Once these patterns have been learnt, the algorithm for finding new is-a relations runs in O(n), where n is the number of sentences. In semantic hierarchies such as WordNet (Miller 1990), an is-a relation between two words x and y represents a subordinate relationship (i.e. x is more specific than y). Many algorithms have recently been proposed to automatically mine is-a (hypo nym/hypernym) relations between words. Here, we focus on is-a relations that are characterized by the questions ?What/Who is X?? For example, Table 2 shows a sample of 10 is-a relations discovered by the algorithms presented in this paper. In this table, we call azalea, tiramisu, and Winona Ryder in stances of the respective concepts flower, dessert and actress. These kinds of is-a relations would be useful for various purposes such as ontology con struction, semantic information retrieval, question answering, etc. The main contribution of this paper is a comparison of the quality of our pattern-based and co occurrence models as a function of processing time and corpus size. Also, the paper lays a foundation for terascale acquisition of knowledge. We will show that, for very small or very large corpora or for situations where recall is valued over precision, the pattern-based approach is best. Previous approaches to extracting is-a relations fall under two categories: pattern-based and co occurrence-based approaches. 2.1 Pattern-based approaches. Marti Hearst (1992) was the first to use a pat tern-based approach to extract hyponym relations from a raw corpus. She used an iterative process to semi-automatically learn patterns. However, a corpus of 20MB words yielded only 400 examples. Our pattern-based algorithm is very similar to the one used by Hearst. She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns. 771Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. Berland and Charniak (1999) used similar pattern-based tech niques and other heuristics to extract meronymy (part-whole) relations. They reported an accuracy of about 55% precision on a corpus of 100,000 words. Girju et al (2003) improved upon Berland and Charniak's work using a machine learning filter. Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. Our pattern-based algorithm differs from these approaches in two ways. We learn lexico-POS patterns in an automatic way. Also, the patterns are learned with the specific goal of scaling to the terascale (see Table 2). 2.2 Co-occurrence-based approaches. The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algo rithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar gram matical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Commit tee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. Re cently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic de pendency features for each noun. Much of the research discussed above takes a similar approach of searching text for simple sur face or lexico-syntactic patterns in a bottom-up approach. Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. Hyponyms are gen erated in a top-down approach by naming each group of words and assigning that name as a hypo nym of each word in the group (i.e., one hyponym per instance/group label pair). The input to the extraction algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source. For example, following are two semantic classes discov ered by CBC: (A) peach, pear, pineapple, apricot, mango, raspberry, lemon, cherry, strawberry, melon, blueberry, fig, apple, plum, nectarine, avocado, grapefruit, papaya, banana, cantaloupe, cranberry, blackberry, lime, orange, tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O'Brien, Rosie O'Donnell, Jenny Jones, Sally Jessy Raph ael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... The extraction algorithm first labels concepts (A) and (B) with fruit and host respectively. Then, is-a relationships are extracted, such as: apple is a fruit, pear is a fruit, and David Letterman is a host. An instance such as pear is assigned a hypernym fruit not because it necessarily occurs in any par ticular syntactic relationship with the word fruit, but because it belongs to the class of instances that does. The labeling of semantic classes is performed in three phases, as outlined below. 3.1 Phase I. In the first phase of the algorithm, feature vec tors are extracted for each word that occurs in a semantic class. Each feature corresponds to a grammatical context in which the word occurs. For example, ?catch __? is a verb-object context. If the word wave occurred in this context, then the con text is a feature of wave. We then construct a mutual information vector MI(e) = (mie1, mie2, ?, miem) for each word e, where mief is the pointwise mutual information between word e and context f, which is defined as: N c N c N c ef m j ej n i if ef mi ?? == ? = 11 log Table 2. Sample of 10 is-a relationships discovered by our co-occurrence and pattern-based systems. CO-OCCURRENCE SYSTEM PATTERN-BASED SYSTEM Word Hypernym Word Hypernym azalea flower American airline bipolar disorder disease Bobby Bonds coach Bordeaux wine radiation therapy cancer treatment Flintstones television show tiramisu dessert salmon fish Winona Ryder actress Table 1. Approximate processing time on a single Pentium-4 2.5 GHz machine. TOOL 15 GB ORPUS 1 TB CORPUS POS Tagger 2 days 125 days NP Chunker 3 days 214 days Dependency Parser 56 days 10.2 years Syntactic Parser 5.8 years 388.4 years 772 where n is the number of elements to be clustered, cef is the frequency count of word e in grammatical context f, and N is the total frequency count of all features of all words. 3.2 Phase II. Following (Pantel and Lin 2002), a committee for each semantic class is constructed. A committee is a set of representative elements that unambi guously describe the members of a possible class. For example, in one of our experiments, the committees for semantic classes (A) and (B) from Sec tion 3 were: A) peach, pear, pineapple, apricot, mango, raspberry, lemon, blueberry B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman 3.3 Phase III. By averaging the feature vectors of the commit tee members of a particular semantic class, we obtain a grammatical template, or signature, for that class. For example, Figure 1 shows an excerpt of the grammatical signature for semantic class (B). The vector is obtained by averaging the fea ture vectors of the words in the committee of this class. The ?V:subj:N:joke? feature indicates a sub ject-verb relationship between the class and the verb joke while ?N:appo:N:host? indicates an ap position relationship between the class and the noun host. The two columns of numbers indicate the frequency and mutual information scores. To name a class, we search its signature for cer tain relationships known to identify class labels. These relationships, automatically learned in (Pantel and Ravichandran 2004), include apposi tions, nominal subjects, such as relationships, and like relationships. We sum up the mutual information scores for each term that occurs in these rela tionships with a committee of a class. The highest scoring term is the name of the class. The syntactical co-occurrence approach has worst-case time complexity O(n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). Just to parse a 1 TB corpus, this approach requires ap proximately 10.2 years (see Table 2). We propose an algorithm for learning highly scalable lexico-POS patterns. Given two sentences with their surface form and part of speech tags, the algorithm finds the optimal lexico-POS alignment. For example, consider the following 2 sentences: 1) Platinum is a precious metal. 2) Molybdenum is a metal. Applying a POS tagger (Brill 1995) gives the following output: Surface Platinum is a precious metal . POS NNP VBZ DT JJ NN . Surface Molybdenum is a metal . POS NNP VBZ DT NN . A very good pattern to generalize from the alignment of these two strings would be Surface is a metal . POS NNP . We use the following notation to denote this alignment: ?_NNP is a (*s*) metal.?, where ?_NNP represents the POS tag NNP?. To perform such alignments we introduce two wildcard operators, skip (*s*) and wildcard (*g*). The skip operator represents 0 or 1 instance of any word (similar to the \w* pattern in Perl), while the wildcard operator represents exactly 1 instance of any word (similar to the \w+ pattern in Perl). 4.1 Algorithm. We present an algorithm for learning patterns at multiple levels. Multilevel representation is de fined as the different levels of a sentence such as the lexical level and POS level. Consider two strings a(1, n) and b(1, m) of lengths n and m re spectively. Let a1(1, n) and a2(1, n) be the level 1 (lexical level) and level 2 (POS level) representa tions for the string a(1, n). Similarly, let b1(1, m) and b2(1, m) be the level 1 and level 2 representa tions for the string b(1, m). The algorithm consists of two parts: calculation of the minimal edit dis tance and retrieval of an optimal pattern. The minimal edit distance algorithm calculates the number of edit operations (insertions, deletions and replacements) required to change one string to another string. The optimal pattern is retrieved by {Phil Donahue,Pat Sajak,Arsenio Hall} N:gen:N talk show 93 11.77 television show 24 11.30 TV show 25 10.45 show 255 9.98 audience 23 7.80 joke 5 7.37 V:subj:N joke 39 7.11 tape 10 7.09 poke 15 6.87 host 40 6.47 co-host 4 6.14 banter 3 6.00 interview 20 5.89 N:appo:N host 127 12.46 comedian 12 11.02 King 13 9.49 star 6 7.47 Figure 1. Excerpt of the grammatical signature for the television host class. 773 keeping track of the edit operations (which is the second part of the algorithm). Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering. The above algorithm takes O(y2) time for every pair of strings of length at most y. Hence, if there are x strings in the collection, each string having at most length y, the algorithm has time complexity O(x2y2) to extract all the patterns in the collection. Applying the above algorithm on a corpus of 3GB with 50 is-a relationship seeds, we obtain a set of 600 lexico-POS. Following are two of them: 1) X_JJ#NN|JJ#NN#NN|NN _CC Y_JJ#JJ#NN|JJ |NNS|NN|JJ#NNS|NN#NN|JJ#NN|JJ#NN#NN e.g. ?caldera or lava lake? 2) X_NNP#NNP|NNP#NNP#NNP#NNP#NNP#CC#NNP |NNP|VBN|NN#NN|VBG#NN|NN ,_, _DT Y_NN#IN#NN|JJ#JJ#NN|JJ|NN|NN#IN#NNP |NNP#NNP|NN#NN|JJ#NN|JJ#NN#NN e.g. ?leukemia, the cancer of ... Note that we store different POS variations of the anchors X and Y. As shown in example 1, the POS variations of the anchor X are (JJ NN, JJ NN NN, NN). The variations for anchor Y are (JJ JJ NN, JJ, etc.). The reason is quite straightforward: we need to determine the boundary of the anchors X and Y and a reasonable way to delimit them would be to use POS information. All the patterns produced by the multi-level pattern learning algo rithm were generated from positive examples. From amongst these patterns, we need to find the most important ones. This is a critical step because frequently occurring patterns have low precision whereas rarely occurring patterns have high preci sion. From the Information Extraction point of view neither of these patterns is very useful. We need to find patterns with relatively high occurrence and high precision. We apply the log likeli hood principle (Dunning 1993) to compute this score. The top 15 patterns according to this metric are listed in Table 3 (we omit the POS variations for visibility). Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). 4.3 Time complexity. To extract hyponym relations, we use a fixed number of patterns across a corpus. Since we treat each sentences independently from others, the algorithm runs in linear time O(n) over the corpus size, where n is number of sentences in the corpus. In this section, we empirically compare the pattern-based and co-occurrence-based models pre sented in Section 3 and Section 4. The focus is on the precision and recall of the systems as a func tion of the corpus size. 5.1 Experimental Setup. We use a 15GB newspaper corpus consisting of TREC9, TREC 2002, Yahoo! News ~0.5GB, AP newswire ~2GB, New York Times ~2GB, Reuters ~0.8GB, Wall Street Journal ~1.2GB, and various online news website ~1.5GB. For our experiments, we extract from this corpus six data sets of differ ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB and 15GB. For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1. For the pattern-based approach, we use Brill?s. POS tagger (1995) to tag each data set. 5.2 Precision. We performed a manual evaluation to estimate the precision of both systems on each dataset. For each dataset, both systems extracted a set of is-a Table 3. Top 15 lexico-syntactic patterns discovered by our system. X, or Y X, _DT Y _(WDT|IN) Y like X and X, (a|an) Y X, _RB known as Y _NN, X and other Y X, Y X ( Y ) Y, including X, Y, or X Y such as X Y, such as X X is a Y X, _RB called Y Y, especially X 774relationships. Six sets were extracted for the pattern-based approach and five sets for the co occurrence approach (the 15GB corpus was too large to process using the co-occurrence model ? see dependency parsing time estimates in Table 2). From each resulting set, we then randomly se lected 50 words along with their top 3 highest ranking is-a relationships. For example, Table 4 shows three randomly selected names for the pat tern-based system on the 15GB dataset. For each word, we added to the list of hypernyms a human generated hypernym (obtained from an annotator looking at the word without any system or Word Net hyponym). We also appended the WordNet hypernyms for each word (only for the top 3 senses). Each of the 11 random samples contained a maximum of 350 is-a relationships to manually evaluate (50 random words with top 3 system, top 3 WordNet, and human generated relationship).. We presented each of the 11 random samples to two human judges. The 50 randomly selected words, together with the system, human, and WordNet generated is-a relationships, were ran domly ordered. That way, there was no way for a judge to know the source of a relationship nor each system?s ranking of the relationships. For each relationship, we asked the judges to assign a score of correct, partially correct, or incorrect. We then computed the average precision of the system, human, and WordNet on each dataset. We also computed the percentage of times a correct rela tionship was found in the top 3 is-a relationships of a word and the mean reciprocal rank (MRR). For each word, a system receives an MRR score of 1 / M, where M is the rank of the first name judged correct. Table 5 shows the results comparing the two automatic systems. Table 6 shows similar results for a more lenient evaluation where both correct and partially correct are judged correct. For small datasets (below 150MB), the pattern based method achieves higher precision since the co-occurrence method requires a certain critical mass of statistics before it can extract useful class signatures (see Section 3). On the other hand, the pattern-based approach has relatively constant precision since most of the is-a relationships se lected by it are fired by a single pattern. Once the co-occurrence system reaches its critical mass (at 150MB), it generates much more precise hypo nyms. The Kappa statistics for our experiments were all in the range 0.78 ? 0.85. Table 7 and Table 8 compare the precision of the pattern-based and co-occurrence-based methods with the human and WordNet hyponyms. The variation between the human and WordNet scores across both systems is mostly due to the relative cleanliness of the tokens in the co-occurrencebased system (due to the parser used in the ap proach). WordNet consistently generated higher precision relationships although both algorithms approach WordNet quality on 6GB (the pattern based algorithm even surpasses WordNet precision on 15GB). Furthermore, WordNet only generated a hyponym 40% of the time. This is mostly due to the lack of proper noun coverage in WordNet. On the 6 GB corpus, the co-occurrence approach took approximately 47 single Pentium-4 2.5 GHz processor days to complete, whereas it took the pattern-based approach only four days to complete on 6 GB and 10 days on 15 GB. 5.3 Recall. The co-occurrence model has higher precision than the pattern-based algorithm on most datasets. Table 4. Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset). RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED) Sanwa Bank bank none subsidiary / lender / bank MCI Worldcom Inc. telecommunications company none phone company / competitor / company cappuccino beverage none item / food / beverage Table 5. Average precision, top-3 precision, and MRR for both systems on each dataset. PATTERN SYSTEM CO-OCCURRENCE SYSTEM Prec Top-3 MRR Prec Top-3 MRR 1.5MB 38.7% 41.0% 41.0% 4.3% 8.0% 7.3% 15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3% 150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0% 1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7% 6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8% 15GB 55.9% 54.0% 52.0% Too large to process Table 6. Lenient average precision, top-3 precision, and MRR for both systems on each dataset. PATTERN SYSTEM CO-OCCURRENCE SYSTEM Prec Top-3 MRR Prec Top-3 MRR 1.5MB 56.6% 60.0% 60.0% 12.4% 20.0% 15.2% 15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3% 150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2% 1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8% 6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2% 15GB 67.8% 67.0% 65.0% Too large to process 775 However, Figure 2 shows that the pattern-based approach extracts many more relationships. Semantic extraction tasks are notoriously diffi cult to evaluate for recall. To approximate recall, we defined a relative recall measure and conducted a question answering (QA) task of answering defi nition questions. 5.3.1 Relative recall Although it is impossible to know the number of is-a relationships in any non-trivial corpus, it is possible to compute the recall of a system relative to another system?s recall. The recall of a system A, RA, is given by the following formula: C C R AA = where CA is the number of correct is-a relation ships extracted by A and C is the total number of correct is-a relationships in the corpus. We define relative recall of system A given system B, RA,B, as: B A B A BA C C R R R == ,Using the precision estimates, PA, from the pre vious section, we can estimate CA ? PA ? |A|, where A is the total number of is-a relationships discov ered by system A. Hence, BP AP R B A BA ? ? = ,Figure 3 shows the relative recall of A = pattern based approach relative to B = co-occurrence model. Because of sparse data, the pattern-based approach has much higher precision and recall (six times) than the co-occurrence approach on the small 15MB dataset. In fact, only on the 150MB dataset did the co-occurrence system have higher recall. With datasets larger than 150MB, the co occurrence algorithm reduces its running time by filtering out grammatical relationships for words that occurred fewer than k = 40 times and hence recall is affected (in contrast, the pattern-based approach may generate a hyponym for a word that it only sees once). 5.3.2 Definition questions Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. These questions are of the form ?Who is X?? and ?What is X?? For each question (e.g., ?Who is Niels Bohr??, ?What is feng shui??) we extract its respective instance (e.g., ?Neils Bohr? and ?feng shui?), look up their corresponding hyponyms from our is-a table, and present the corresponding hyponym as the answer. We compare the results of both our systems with WordNet. We extract at most the top 5 hyponyms provided by each system. We manually evaluate the three systems and assign 3 classes ?Correct (C)?, ?Partially Correct (P)? or ?Incorrect (I)? to each answer. This evaluation is different from the evaluation performed by the TREC organizers for definition questions. However, by being consistent across all Total Number of Is-A Relationships vs. Dataset 0 200000 400000 600000 800000 1000000 1200000 1400000 1.5MB 15MB 150MB 1.5GB 6GB 15GB Datasets To ta l Is A Re la tio n s hi ps s Pattern-based System Co-occurrence-based System Figure 2. Number of is-a relationships extracted by the pattern-based and co-occurrence-based approaches. Table 7. Average precision of the pattern-based sys tem vs. WordNet and human hyponyms. PRECISION MRR Pat. WNet Human Pat. WNet Human 1.5MB 38.7% 45.8% 83.0% 41.0% 84.4% 83.0% 15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0% 150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0% 1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0% 6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0% 15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0% Table 8. Average precision of the co-occurrence based system vs. WordNet and human hyponyms. PRECISION MRR Co-occ WNet Human Co-occ WNet Human 1.5MB 4.3% 42.7% 52.7% 7.3% 87.7% 95.0% 15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0% 150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0% 1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0% 6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0% Relative Recall (Pattern-based vs. Co-occurrence-based) 0.00 1.00 2.00 3.00 4.00 5.00 6.00 7.00 1.5MB 15MB 150MB 1.5GB 6GB 15GB (projected) Datesets Re la tiv e Re ca ll Figure 3. Relative recall of the pattern-based approach relative to the co-occurrence approach. 776 systems during the process, these evaluations give an indication of the recall of the knowledge base. We measure the performance on the top 1 and the top 5 answers returned by each system. Table 9 and Table 10 show the results. The corresponding scores for WordNet are 38% accuracy in both the top-1 and top-5 categories (for both strict and lenient). As seen in this experiment, the results for both the pattern-based and cooccurrence-based systems report very poor per formance for data sets up to 150 MB. However, there is an increase in performance for both systems on the 1.5 GB and larger datasets. The per formance of the system in the top 5 category is much better than that of WordNet (38%). There is promise for increasing our system accuracy by re ranking the outputs of the top-5 hypernyms.","In this section, we empirically compare the pattern-based and co-occurrence-based models pre sented in Section 3 and Section 4. The focus is on the precision and recall of the systems as a func tion of the corpus size. 5.1 Experimental Setup. We use a 15GB newspaper corpus consisting of TREC9, TREC 2002, Yahoo! News ~0.5GB, AP newswire ~2GB, New York Times ~2GB, Reuters ~0.8GB, Wall Street Journal ~1.2GB, and various online news website ~1.5GB. For our experiments, we extract from this corpus six data sets of differ ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB and 15GB. For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1. For the pattern-based approach, we use Brill?s. POS tagger (1995) to tag each data set. 5.2 Precision. We performed a manual evaluation to estimate the precision of both systems on each dataset. For each dataset, both systems extracted a set of is-a Table 3. Top 15 lexico-syntactic patterns discovered by our system. X, or Y X, _DT Y _(WDT|IN) Y like X and X, (a|an) Y X, _RB known as Y _NN, X and other Y X, Y X ( Y ) Y, including X, Y, or X Y such as X Y, such as X X is a Y X, _RB called Y Y, especially X 774relationships. Six sets were extracted for the pattern-based approach and five sets for the co occurrence approach (the 15GB corpus was too large to process using the co-occurrence model ? see dependency parsing time estimates in Table 2). From each resulting set, we then randomly se lected 50 words along with their top 3 highest ranking is-a relationships. For example, Table 4 shows three randomly selected names for the pat tern-based system on the 15GB dataset. For each word, we added to the list of hypernyms a human generated hypernym (obtained from an annotator looking at the word without any system or Word Net hyponym). We also appended the WordNet hypernyms for each word (only for the top 3 senses). Each of the 11 random samples contained a maximum of 350 is-a relationships to manually evaluate (50 random words with top 3 system, top 3 WordNet, and human generated relationship).. We presented each of the 11 random samples to two human judges. The 50 randomly selected words, together with the system, human, and WordNet generated is-a relationships, were ran domly ordered. That way, there was no way for a judge to know the source of a relationship nor each system?s ranking of the relationships. For each relationship, we asked the judges to assign a score of correct, partially correct, or incorrect. We then computed the average precision of the system, human, and WordNet on each dataset. We also computed the percentage of times a correct rela tionship was found in the top 3 is-a relationships of a word and the mean reciprocal rank (MRR). For each word, a system receives an MRR score of 1 / M, where M is the rank of the first name judged correct. Table 5 shows the results comparing the two automatic systems. Table 6 shows similar results for a more lenient evaluation where both correct and partially correct are judged correct. For small datasets (below 150MB), the pattern based method achieves higher precision since the co-occurrence method requires a certain critical mass of statistics before it can extract useful class signatures (see Section 3). On the other hand, the pattern-based approach has relatively constant precision since most of the is-a relationships se lected by it are fired by a single pattern. Once the co-occurrence system reaches its critical mass (at 150MB), it generates much more precise hypo nyms. The Kappa statistics for our experiments were all in the range 0.78 ? 0.85. Table 7 and Table 8 compare the precision of the pattern-based and co-occurrence-based methods with the human and WordNet hyponyms. The variation between the human and WordNet scores across both systems is mostly due to the relative cleanliness of the tokens in the co-occurrencebased system (due to the parser used in the ap proach). WordNet consistently generated higher precision relationships although both algorithms approach WordNet quality on 6GB (the pattern based algorithm even surpasses WordNet precision on 15GB). Furthermore, WordNet only generated a hyponym 40% of the time. This is mostly due to the lack of proper noun coverage in WordNet. On the 6 GB corpus, the co-occurrence approach took approximately 47 single Pentium-4 2.5 GHz processor days to complete, whereas it took the pattern-based approach only four days to complete on 6 GB and 10 days on 15 GB. 5.3 Recall. The co-occurrence model has higher precision than the pattern-based algorithm on most datasets. Table 4. Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset). RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED) Sanwa Bank bank none subsidiary / lender / bank MCI Worldcom Inc. telecommunications company none phone company / competitor / company cappuccino beverage none item / food / beverage Table 5. Average precision, top-3 precision, and MRR for both systems on each dataset. PATTERN SYSTEM CO-OCCURRENCE SYSTEM Prec Top-3 MRR Prec Top-3 MRR 1.5MB 38.7% 41.0% 41.0% 4.3% 8.0% 7.3% 15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3% 150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0% 1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7% 6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8% 15GB 55.9% 54.0% 52.0% Too large to process Table 6. Lenient average precision, top-3 precision, and MRR for both systems on each dataset. PATTERN SYSTEM CO-OCCURRENCE SYSTEM Prec Top-3 MRR Prec Top-3 MRR 1.5MB 56.6% 60.0% 60.0% 12.4% 20.0% 15.2% 15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3% 150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2% 1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8% 6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2% 15GB 67.8% 67.0% 65.0% Too large to process 775 However, Figure 2 shows that the pattern-based approach extracts many more relationships. Semantic extraction tasks are notoriously diffi cult to evaluate for recall. To approximate recall, we defined a relative recall measure and conducted a question answering (QA) task of answering defi nition questions. 5.3.1 Relative recall Although it is impossible to know the number of is-a relationships in any non-trivial corpus, it is possible to compute the recall of a system relative to another system?s recall. The recall of a system A, RA, is given by the following formula: C C R AA = where CA is the number of correct is-a relation ships extracted by A and C is the total number of correct is-a relationships in the corpus. We define relative recall of system A given system B, RA,B, as: B A B A BA C C R R R == ,Using the precision estimates, PA, from the pre vious section, we can estimate CA ? PA ? |A|, where A is the total number of is-a relationships discov ered by system A. Hence, BP AP R B A BA ? ? = ,Figure 3 shows the relative recall of A = pattern based approach relative to B = co-occurrence model. Because of sparse data, the pattern-based approach has much higher precision and recall (six times) than the co-occurrence approach on the small 15MB dataset. In fact, only on the 150MB dataset did the co-occurrence system have higher recall. With datasets larger than 150MB, the co occurrence algorithm reduces its running time by filtering out grammatical relationships for words that occurred fewer than k = 40 times and hence recall is affected (in contrast, the pattern-based approach may generate a hyponym for a word that it only sees once). 5.3.2 Definition questions Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. These questions are of the form ?Who is X?? and ?What is X?? For each question (e.g., ?Who is Niels Bohr??, ?What is feng shui??) we extract its respective instance (e.g., ?Neils Bohr? and ?feng shui?), look up their corresponding hyponyms from our is-a table, and present the corresponding hyponym as the answer. We compare the results of both our systems with WordNet. We extract at most the top 5 hyponyms provided by each system. We manually evaluate the three systems and assign 3 classes ?Correct (C)?, ?Partially Correct (P)? or ?Incorrect (I)? to each answer. This evaluation is different from the evaluation performed by the TREC organizers for definition questions. However, by being consistent across all Total Number of Is-A Relationships vs. Dataset 0 200000 400000 600000 800000 1000000 1200000 1400000 1.5MB 15MB 150MB 1.5GB 6GB 15GB Datasets To ta l Is A Re la tio n s hi ps s Pattern-based System Co-occurrence-based System Figure 2. Number of is-a relationships extracted by the pattern-based and co-occurrence-based approaches. Table 7. Average precision of the pattern-based sys tem vs. WordNet and human hyponyms. PRECISION MRR Pat. WNet Human Pat. WNet Human 1.5MB 38.7% 45.8% 83.0% 41.0% 84.4% 83.0% 15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0% 150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0% 1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0% 6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0% 15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0% Table 8. Average precision of the co-occurrence based system vs. WordNet and human hyponyms. PRECISION MRR Co-occ WNet Human Co-occ WNet Human 1.5MB 4.3% 42.7% 52.7% 7.3% 87.7% 95.0% 15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0% 150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0% 1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0% 6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0% Relative Recall (Pattern-based vs. Co-occurrence-based) 0.00 1.00 2.00 3.00 4.00 5.00 6.00 7.00 1.5MB 15MB 150MB 1.5GB 6GB 15GB (projected) Datesets Re la tiv e Re ca ll Figure 3. Relative recall of the pattern-based approach relative to the co-occurrence approach. 776 systems during the process, these evaluations give an indication of the recall of the knowledge base. We measure the performance on the top 1 and the top 5 answers returned by each system. Table 9 and Table 10 show the results. The corresponding scores for WordNet are 38% accuracy in both the top-1 and top-5 categories (for both strict and lenient). As seen in this experiment, the results for both the pattern-based and cooccurrence-based systems report very poor per formance for data sets up to 150 MB. However, there is an increase in performance for both systems on the 1.5 GB and larger datasets. The per formance of the system in the top 5 category is much better than that of WordNet (38%). There is promise for increasing our system accuracy by re ranking the outputs of the top-5 hypernyms."
73,"We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attributevectors and decomposition of the contex tual POS probabilities of the HMM into aproduct of attribute probabilities, (2) esti mation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German andCzech data, our tagger outperformed state of-the-art POS taggers.","We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attributevectors and decomposition of the contex tual POS probabilities of the HMM into aproduct of attribute probabilities, (2) esti mation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German andCzech data, our tagger outperformed state of-the-art POS taggers. A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence ? t N 1 = ? t 1 , ..., ? t N for a given word sequence w N 1 . ? t N 1 = argmax t N 1 p(t N 1 , w N 1 )The joint probability of the two sequences is de fined as the product of context probabilities and lexical probabilities over all POS tags: p(t N 1 , w N 1 ) = N ? i=1 p(t i |t i?1 i?k ) ? ?? context prob. p(w i |t i ) ? ?? lexical prob. (1)HMM taggers are fast and were successfully applied to a wide range of languages and training cor pora. c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information aboutnumber, gender, case and similar morphosyntactic features. For languages with a rich morphol ogy such as German or Czech, more fine-grained tagsets are often considered more appropriate. Theadditional information may also help to disam biguate the (base) part of speech. Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realit?at? (Is that reality?). The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun Realit?at (feminine), the article reading must be wrong. The German Tiger treebank (Brants et al, 2002) is an example of a corpus with a more fine-grainedtagset (over 700 tags overall). Large tagsets aggra vate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (?The to be taxed income decreases?; The taxable income decreases). This sentence should be tagged as shown in table 1. Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut sinkt VFIN.Full.3.Sg.Pres.Ind . SYM.Pun.Sent. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occurin the Tiger corpus. (Neither does the pair con sisting of the first two tags.) The unsmoothed 777context probability of the third POS tag is there fore 0. If the probability is smoothed with the backoff distribution p(?|PART.Zu), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather thanADJA.Pos.Nom.Sg.Neut. Thus, the agreement be tween the article and the adjective is not checked anymore. A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: ? All words appearing after an article (ART)and the infinitive particle zu (PART.zu) are at tributive adjectives (ADJA) (10 of 10 cases). All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases). All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases).? All adjectives appearing after a singular arti cle and a particle are singular (32 of 32 cases). All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases). By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) ? p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) ? p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) ? p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) ? p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the following expression for the context probability: p(ADJA | ART, PART.Zu) ? p(Pos | 2:ART, 1:PART, 0:ADJA) ? p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) ? p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) ? p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1. Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context. These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees toestimate the probability of each attribute. Decision trees are ideal for this task because the iden tification of relevant attribute combinations is at the heart of this method. The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here. Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature. Decision trees (Breiman et al, 1984; Quinlan,1993) are normally used as classifiers, i.e. they assign classes to objects which are represented as at tribute vectors. The non-terminal nodes are labeledwith attribute tests, the edges with the possible out comes of a test, and the terminal nodes are labeled with classes. An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating thetest of the daughter node, and so on until a termi nal node is reached whose class is assigned to the object.Decision Trees are turned into probability estimation trees by storing a probability for each pos sible class at the terminal nodes instead of a singleresult class. Figure 1 shows a probability estima tion tree for the prediction of the probability of the nominative attribute of nouns. 2.1 Induction of Decision Trees. Decision trees are incrementally built by first selecting the test which splits the manually anno tated training sample into the most homogeneous subsets with respect to the class. This test, which maximizes the information gain 1 wrt. the class, is 1The information gain measures how much the test de creases the uncertainty about the class. It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy 778 2:N.Reg p=0.571 p=0.938 p=0.999 0:N.Name 1:ART.Nom 0:N.Name 0:N.Name p=0.948 p=0.998 .... 1:ADJA.Nom yes yes no noyes no yes no no yesFigure 1: Probability estimation tree for the nomi native case of nouns. The test 1:ART.Nom checks if the preceding word is a nominative article. assigned to the top node. The tree is recursivelyexpanded by selecting the best test for each sub set and so on, until all objects of the current subsetbelong to the same class. In a second step, the decision tree may be pruned in order to avoid overfit ting to the training data. Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. The motivation was that a tree which predicts a single value (say verb) does notfragment the data with tests which are only relevant for the distinction of two other values (e.g. ar ticle and possessive pronoun). 2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.)The tree induction algorithm only considers bi nary tests, which check whether some particular attribute is present or not. The best test for each node is selected with the standard information gaincriterion. The recursive tree building process ter minates if the information gain is 0. The decision tree is pruned with the pruning criterion described below. Since the tagger creates a separate tree for eachattribute, the probabilities of a set of competing at tributes such as masculine, feminine, and neuter will not exactly sum up to 1. To understand why,assume that there are three trees for the gender attributes. Two of them (say the trees for mascu line and feminine) consist of a single terminal node in the two subsets. The weight of each subset is proportional to its size. 2We did not directly compare the two alternatives (two valued vs. multi-valued tests), because the implementational effort required would have been too large. which returns a probability of 0.3. The third tree for neuter has one non-terminal and two terminalnodes returning a probability of 0.3 and 0.5, re spectively. The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1. This problem is solved by renormalizing the probabilities. The probability of an attribute (such as ?Nom?) is always conditioned on the respective base POS (such as ?N?) (unless the predicted attribute is thebase POS) in order to make sure that the probabil ity of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is ?1:ART.Nom? which states that the preceding tag is an article with the attribute ?Nom?. ?1:ART? is also a valid attribute specification, but ?1:Nom? is not. The tagger further restricts the set of possible test attributes by requiring that some attribute ofthe POS tag at position i-k (i=position of the predicted POS tag, k ? 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined. This restriction improved the tagging accuracy for large contexts. 2.2 Pruning Criterion. The tagger applies 3the critical-value pruning strat egy proposed by (Mingers, 1989). A node ispruned if the information gain of the best test mul tiplied by the size of the data subsample is below a given threshold. To illustrate the pruning, assume that D is the data of the current node with 50 positive and 25 negative elements, and that D 1 (with 20 positive and 20 negative elements) and D 2(with 30 posi tive and 5 negative elements) are the two subsets induced by the best test. The entropy of D is ?2/3 log 2 2/3 ? 1/3 log 2 1/3 = 0.92, the entropy ofD 1 is?1/2 log 2 1/2?1/2 log 2 1/2 = 1, and the entropy of D 2 is ?6/7 log 2 6/7 ? 1/7 log 2 1/7 = 0.59. The information gain is therefore 0.92 ? (8/15 ? 1 ? 7/15 ? 0.59) = 0.11. The resulting score is 75 ? 0.11 = 8.25. Given a threshold of 6, the node is therefore not pruned. We experimented with pre-pruning (where a node is always pruned if the gain is below the 3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was neverlarger than 0.1% for any context size. Thus, the simpler prun ing strategy presented here was chosen. 779 threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes). The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold. A threshold of 6 consistently produced optimal or near optimal results for pre-pruning. Thus, pre-pruning with a threshold of 6 was used in the experiments. The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category. The number of additional attributes is fixed for each main category. The additionalattributes are category-specific. The singular at tribute of a noun and an adjective POS tag are therefore two different attributes. 4Each position in the POS tags of a given category corresponds to a feature. The attributes oc curring at a certain position constitute the value set of the feature. Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities. The probability of an attribute given the attributes of the preceding POS tags as well as the preceding attributes of the predicted POS tagis estimated with a decision tree as described be fore. The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities (which themselves were smoothed in the same way). The smoothing is implemented by adding the weighted class probabilities p p (c) of theparent node to the frequencies f(c) before normal izing them to probabilities: p(c) = f(c) + ?p p (c) ? + ? c f(c)The weight ? was fixed to 1 after a few experiments on development data. This smoothing strat egy is closely related to Witten-Bell smoothing. The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1). The best tag sequence is computed with theViterbi algorithm. The main differences of our tag ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom. and that the context probability p(t i |t i?1 i?k) is internally computed as a product of attribute probabili ties. In order to increase the speed, the tagger also applies a beam-search strategy which prunes allsearch paths whose probability is below the prob ability of the best path times a threshold. With a threshold of 10 ?3or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which sup plies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If anexternal lexicon is provided, the lexical probabili ties are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bellmethod is then applied to smooth the lexical prob abilities with the average probabilities. If the word w was observed with N different tags, and f(w, t) is the joint frequency of w and POS tag t, and p(t|[w]) is the average probability of t among words with the same set of possible tags as w, then the smoothed probability of t given w is defined as follows: p(t|w) = f(w, t) + Np(t|[w]) f(w) + NThe smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag). 5 4.2 Unknown Words. The lexical probabilities of unknown words areobtained as follows: The unknown words are di vided into four disjoint classes 6with numeric ex pressions, words starting with an upper-case letter, words starting with a lower-case letter, and a fourthclass for the other words. The tagger builds a suf fix trie for each class of unknown words using the known word types from that class. The maximal length of the suffixes is 7. The suffix tries are pruned until (i) all suffixeshave a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous. Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences. 6In earlier experiments, we had used a much larger num ber of word classes. Decreasing their number to 4 turned out to be better. 780 vided by the number of different POS tags is above a threshold of 1. More precisely, if T ? is the set of POS tags that occurred with suffix ?, |T | is the size of the set T , f ? is the frequency of suffix ?, and p ? (t) is the probability of POS tag t among the words with suffix ?, then the following condition must hold: f a? |T a? | ? t?T a? p a? (t) log p a? (t) p ? (t) < 1 The POS probabilities are recursively smoothedwith the POS probabilities of shorter suffixes us ing Witten-Bell smoothing. Our tagger was first evaluated on data from theGerman Tiger treebank. The results were com pared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gim?enez andM`arquez, 2004), which is based on support vec tor machines. 7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We tookstandard features from a 5 word window and M4LRL training without optimization of the regular ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0(Hladk?a et al, 2007) and compared to the TnT tag ger. 5.1 Tiger Corpus. The German Tiger treebank (Brants et al, 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagsetand with additional features encoding informa tion about number, gender, case, person, degree,tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable er rors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data. Some of the 54 STTS labels were mapped to new labels with dots, which reduced the numberof main categories to 23. Examples are the nom inal POS tags NN and NE which were mapped toN.Reg and N.Name. Some lexically decidable dis tinctions missing in the Tiger corpus have been 7 It was planned to include also the Stanford tagger (Toutanova et al, 2003) in this comparison, but it was not possible to train it on the Tiger data.automatically added. Examples are the distinc tion between definite and indefinite articles, and the distinction between hyphens, slashes, left andright parentheses, quotation marks, and other sym bols which the Tiger treebank annotates with ?$(?.A supplementary lexicon was created by analyz ing a word list which included all words from the training, development, and test data with a Germancomputational morphology. The analyses gener ated by the morphology were mapped to the Tiger tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create asupplementary lexicon for the corpus to be pro cessed.In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tagpair was unknown, and with a frequency propor tional to the prior probability of the tag if the wordwas unknown. This strategy returned the best results on the development data. In case of the SVM Tool, we were not able to successfully integrate the supplementary lexicon. 5.1.1 Refined Tagset Prepositions are not annotated with case in theTiger treebank, although this information is impor tant for the disambiguation of the case of the next noun phrase. In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with thiscase (APPR.Acc). Prepositions which select gen itive case, but also occur with dative case 8 , were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), werelexicalized (APPR.in). The refined tagset alo dis tinguished between the auxiliaries sein, haben, andwerden, and used lexicalized tags for the coor dinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from thedistribution of prototypical coordinating conjunc tions such as und (and) or oder (or). For evaluation purposes, the refined tags are mapped back to the original tags. This mapping is unambiguous. 8 In German, the genitive case of arguments is more and more replaced by the dative. 781 tagger default refined ref.+lexicon baseline 67.3 67.3 69.4 TnT 86.3 86.9 90.4 SVMTool 86.6 86.6 ? 2 tags 87.0 87.9 91.5 10 tags 87.6 88.5 92.2 Table 2: Tagging accuracies on development data in percent. Results for 2 and for 10 preceding POS tags as context are reported for our tagger. 5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data. The accuracy of a baseline tagger which chooses the most probable tag 9ignoring the context is 67.3% without and 69.4% with the supple mentary lexicon. The TnT tagger achieves 86.3% accuracy on the default tagset. A tag is considered correct if allattributes are correct. The tagset refinement increases the accuracy by about 0.6%, and the ex ternal lexicon by another 3.5%. The SVMTool is slightly better than the TnTtagger on the default tagset, but shows little im provement from the tagset refinement. Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement.With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outper forms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon. A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively. default refined ref.+lexicon TnT STTS 97.28 TnT Tiger 97.17 97.26 97.51 10 tags 97.39 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9Unknown words are tagged by choosing the most fre quent tag of words with the same capitalization. be expected, however, because the STTS tagset ismuch smaller. Table 3 shows the results of an eval uation based on the plain STTS tagset. The first result was obtained with TnT trained on Tiger data which was mapped to STTS before. The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output ismapped to STTS. The third row gives the corre sponding figures for our tagger. 91.491.5 91.691.7 91.891.9 9292.1 92.292.3 2 3 4 5 6 7 8 9 10 Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words? A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order. Since German is a verb-final language, these tests clearly make sense. Table 4 shows the performance on the test data. Our tagger was used with a context size of 10. The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon. These values were optimal on the development data. Theaccuracy of our tagger is lower than on the devel opment data. This could be due to the higher rate of unknown words (10.0% vs. 7.7%). Relative tothe TnT tagger, however, the accuracy is quite sim ilar for test and development data. The differences between the two taggers are significant. 10 tagger default refined ref.+lexicon TnT 83.45 84.11 89.14 our tagger 85.00 85.92 91.07 Table 4: Tagging accuracies on test data. By far the most frequent tagging error was the confusion of nominative and accusative case. If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger. The resulting score of a binomial test is below 0.001. 782 this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%. Our tagger is quite fast, although not as fast asthe TnT tagger. With a context size of 3 (10), it annotates 7000 (2000) tokens per second on a com puter with an Athlon X2 4600 CPU. The training with a context size of 10 took about 4 minutes. 5.2 Czech Academic Corpus. We also evaluated our tagger on the Czech Aca demic corpus (Hladk?a et al, 2007) which contains 652.131 tokens and about 1200 different POS tags. The data was divided into 80% training data, 10% development data and 10% test data. 88.5 88.6 88.7 88.8 88.9 89 2 3 4 5 6 7 8 9 10 ?context-data2?Figure 3: Accuracy on development data depend ing on context sizeThe best accuracy of our tagger on the develop ment set was 88.9% obtained with a context of 4 preceding POS tags. The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5. The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag ger. The difference is significant. Our tagger combines two ideas, the decompositionof the probability of complex POS tags into a prod uct of feature probabilities, and the estimation of the conditional probabilities with decision trees. A similar idea was previously presented in Kempe (1994), but apparently never applied again. The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method out-performed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.Schmid (1994) and M`arquez (1999) used decision trees for the estimation of contextual tag prob abilities, but without a decomposition of the tagprobability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a genera tive model.Provost & Domingos (2003) noted that well known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al,1984) fail to produce accurate probability esti mates. They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction). Ferri et al (2003) describe a more complex backoffsmoothing method. Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0). Another difference is that we used N twoclass trees with normalization to predict the prob abilities of N classes. These two-class trees can be pruned with a fixed pruning threshold. Hence there is no need to put aside training data for parameter tuning. An open question is whether the SVMTool (orother discriminatively trained taggers) could outperform the presented tagger if the same decompo sition of POS tags and the same context size was used. We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train theSVMTool (or other discriminatively trained taggers) without such a restriction given the difficul ties to train it with the standard context size.Czech POS tagging has been extensively stud ied in the past (Haji?c and Vidov?a-Hladk?a, 1998; Haji?c et al, 2001; Votrubec, 2006). Spoustov etal. (2007) compared several POS taggers includ ing an n-gram tagger and a discriminatively trained tagger (Mor?ce), and evaluated them on the PragueDependency Treebank (PDT 2.0). Mor?ce?s tag ging accuracy was 95.12%, 0.3% better than the n-gram tagger. A hybrid system based on four different tagging methods reached an accuracy of 95.68%. Because of the different corpora used andthe different amounts of lexical information avail able, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Mor?ce e.g. is optimized for Czech POS tagging. The German tagging results are, to the best ofour knowledge, the first published results for fine grained POS tagging with the Tiger tagset. 783","Our tagger combines two ideas, the decompositionof the probability of complex POS tags into a prod uct of feature probabilities, and the estimation of the conditional probabilities with decision trees. A similar idea was previously presented in Kempe (1994), but apparently never applied again. The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method out-performed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.Schmid (1994) and M`arquez (1999) used decision trees for the estimation of contextual tag prob abilities, but without a decomposition of the tagprobability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a genera tive model.Provost & Domingos (2003) noted that well known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al,1984) fail to produce accurate probability esti mates. They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction). Ferri et al (2003) describe a more complex backoffsmoothing method. Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0). Another difference is that we used N twoclass trees with normalization to predict the prob abilities of N classes. These two-class trees can be pruned with a fixed pruning threshold. Hence there is no need to put aside training data for parameter tuning. An open question is whether the SVMTool (orother discriminatively trained taggers) could outperform the presented tagger if the same decompo sition of POS tags and the same context size was used. We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train theSVMTool (or other discriminatively trained taggers) without such a restriction given the difficul ties to train it with the standard context size.Czech POS tagging has been extensively stud ied in the past (Haji?c and Vidov?a-Hladk?a, 1998; Haji?c et al, 2001; Votrubec, 2006). Spoustov etal. (2007) compared several POS taggers includ ing an n-gram tagger and a discriminatively trained tagger (Mor?ce), and evaluated them on the PragueDependency Treebank (PDT 2.0). Mor?ce?s tag ging accuracy was 95.12%, 0.3% better than the n-gram tagger. A hybrid system based on four different tagging methods reached an accuracy of 95.68%. Because of the different corpora used andthe different amounts of lexical information avail able, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas Mor?ce e.g. is optimized for Czech POS tagging. The German tagging results are, to the best ofour knowledge, the first published results for fine grained POS tagging with the Tiger tagset. 783"
74,"This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used. We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy. Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).","This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used. We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy. Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations). Over recent years, many Natural Language Pro cessing (NLP) techniques have been developedthat might benefit from knowledge of distribu tionally similar words, i.e., words that occur in similar contexts. For example, the sparse dataproblem can make it difficult to construct language models which predict combinations of lex ical events. Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of dis tributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be orientedtowards a particular task such as language mod elling (Dagan et al, 1999; Lee, 1999). The first approach is not ideal since it assumes that the goal of distributional similarity methods is topredict semantic similarity and that the semantic resource used is a valid gold standard. Further, the second approach is clearly advanta geous when one wishes to apply distributional similarity methods in a particular application area. However, it is not at all obvious that oneuniversally best measure exists for all applica tions (Weeds and Weir, 2003). Thus, applying adistributional similarity technique to a new ap plication necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm. We propose a shift in focus from attemptingto discover the overall best distributional sim ilarity measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures. This will make it possible to predict in advanceof any experimental evaluation which distributional similarity measures might be most appro priate for a particular application. Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methodsdo not seem to offer any obvious way to distinguish between the semantic relations of syn onymy, antonymy and hyponymy. Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., ?Xs and other Ys? is used as evidence that X is a hyponym of Y. Our work explores the connection between relativefrequency, distributional generality and seman tic generality with promising results. The rest of this paper is organised as follows.In Section 2, we present ten distributional simi larity measures that have been proposed for use in NLP. In Section 3, we analyse the variation in neighbour sets returned by these measures. In Section 4, we take one fundamental statisticalproperty (word frequency) and analyse correla tion between this and the nearest neighbour setsgenerated. In Section 5, we relate relative fre quency to a concept of distributional generalityand the semantic relation of hyponymy. In Sec tion 6, we consider the effects that this has on a potential application of distributional similarity techniques, which is judging compositionality of collocations. In this section, we introduce some basic con cepts and then discuss the ten distributional similarity measures used in this study. The co-occurrence types of a target word are the contexts, c, in which it occurs and these have associated frequencies which may be used to form probability estimates. In our work, theco-occurrence types are always grammatical de pendency relations. For example, in Sections 3 to 5, similarity between nouns is derived fromtheir co-occurrences with verbs in the direct object position. In Section 6, similarity between verbs is derived from their subjects and objects. The k nearest neighbours of a target word w are the k words for which similarity with w is greatest. Our use of the term similarity measure encompasses measures which should strictly bereferred to as distance, divergence or dissimilar ity measures. An increase in distance correlates with a decrease in similarity. However, eithertype of measure can be used to find the k near est neighbours of a target word.Table 1 lists ten distributional similarity mea sures. The cosine measure (Salton and McGill, 1983) returns the cosine of the angle between two vectors. The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the ?-skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. The KL divergence,or relative entropy, D(p||q), between two prob ability distribution functions p and q is defined (Cover and Thomas, 1991) as the ?inefficiency of assuming that the distribution is q when the true distribution is p?: D(p||q) = ? c p log p q .However, D(p||q) = ? if there are any con texts c for which p(c) > 0 and q(c) = 0. Thus,this measure cannot be used directly on maxi mum likelihood estimate (MLE) probabilities.One possible solution is to use the JS diver gence measure, which measures the cost of usingthe average distribution in place of each individual distribution. Another is the ?-skew diver gence measure, which uses the p distribution tosmooth the q distribution. The value of the pa rameter ? controls the extent to which the KL divergence is approximated. We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001). The confusion probability (Sugawara et al, 1985) is an estimate of the probability that one word can be substituted for another. Words w1 and w2 are completely confusable if we are equally as likely to see w2 in a given context as we are to see w1 in that context. Jaccard?s coefficient (Salton and McGill,1983) calculates the proportion of features be longing to either word that are shared by both words. In the simplest case, the features of a word are defined as the contexts in which it has been seen to occur. simja+mi is a variant (Lin, 1998) in which the features of a word are thosecontexts for which the pointwise mutual infor mation (MI) between the word and the context is positive, where MI can be calculated usingI(c, w) = log P (c|w)P (c) . The related Dice Coeffi cient (Frakes and Baeza-Yates, 1992) is omitted here since it has been shown (van Rijsbergen, 1979) that Dice and Jaccard?s Coefficients are monotonic in each other. Lin?s Measure (Lin, 1998) is based on his information-theoretic similarity theorem, whichstates, ?the similarity between A and B is measured by the ratio between the amount of in formation needed to state the commonality of A and B and the information needed to fully describe what A and B are.? The final three measures are settings in the additive MI-based Co-occurrence Retrieval Model (AMCRM) (Weeds and Weir, 2003; Weeds, 2003). We can measure the precisionand the recall of a potential neighbour?s re trieval of the co-occurrences of the target word,where the sets of required and retrieved co occurrences (F (w1) and F (w2) respectively) are those co-occurrences for which MI is positive. Neighbours with both high precision and high recall retrieval can be obtained by computing Measure Function cosine simcm(w2, w1) = ? c P (c|w1).P (c|w2) ?? c P (c|w1)2 ? c P (c|w2)2 Jens.-Shan. distjs(w2, w1) = 12 ( D ( p||p+q2 ) +D ( q||p+q2 )) where p = P (c|w1) and q = P (c|w2) ?-skew dist?(w2, w1) = D (p||(?.q + (1? ?).p)) where p = P (c|w1) and q = P (c|w2) conf. prob. simcp(w2|w1) = ? c P (w1|c).P (w2|c).P (c) P (w1) Jaccard?s simja(w2, w1) = |F (w1)?F (w2)| |F (w1)?F (w2)| where F (w) = {c : P (c|v) > 0} Jacc.+MI simja+mi(w2,W1) = |F (w1)?F (w2)| |F (w1)?F (w2)| where F (w) = {c : I(c, w) > 0} Lin?s simlin(w2, w1) = ? F (w1)?F (w2) (I(c,w1)+I(c,w2)) ? F (w1) I(c,w1)+ ? F (w2) I(c,w2) where F (w) = {c : I(c, w) > 0} precision simP(w2, w1) = ? F (w1)?F (w2) I(c,w2) ? F (w2) I(c,w2) where F (w) = {c : I(c, w) > 0} recall simR(w2, w1) = ? F (w1)?F (w2) I(c,w1) ? F (w1) I(c,w1) where F (w) = {c : I(c, w) > 0} harm. mean simhm(w2, w1) = 2.simP (w2,w1).simR(w2,w1) simP (w2,w1)+simR(w2,w1) where F (w) = {c : I(c, w) > 0} Table 1: Ten distributional similarity measures their harmonic mean (or F-score). We have described a number of ways of calculating distributional similarity. We now con sider whether there is substantial variation ina word?s distributionally nearest neighbours ac cording to the chosen measure. We do this by calculating the overlap between neighbour setsfor 2000 nouns generated using different mea sures from direct-object data extracted from the British National Corpus (BNC). 3.1 Experimental set-up. The data from which sets of nearest neighbours are derived is direct-object data for 2000 nouns extracted from the BNC using a robust accurate statistical parser (RASP) (Briscoe and Carroll, 2002). For reasons of computational efficiency,we limit ourselves to 2000 nouns and directobject relation data. Given the goal of comparing neighbour sets generated by different mea sures, we would not expect these restrictions to affect our findings. The complete set of 2000 nouns (WScomp) is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency: WShigh contains the 1000 most frequently occurring nouns (frequency > 500), and WSlow contains the nouns ranked 3001-4000 (frequency ? 100). By excludingmid-frequency nouns, we obtain a clear sepa ration between high and low frequency nouns.The complete data-set consists of 1,596,798 cooccurrence tokens distributed over 331,079 co occurrence types. From this data, we computedthe similarity between every pair of nouns according to each distributional similarity mea sure. We then generated ranked sets of nearest neighbours (of size k = 200 and where a word is excluded from being a neighbour of itself) for each word and each measure.For a given word, we compute the overlap between neighbour sets using a comparison tech nique adapted from Lin (1998). Given a word w, each word w? in WScomp is assigned a rankscore of k ? rank if it is one of the k near est neighbours of w using measure m and zero otherwise. If NS(w,m) is the vector of such scores for word w and measure m, then theoverlap, C(NS(w,m1),NS(w,m2)), of two neigh bour sets is the cosine between the two vectors: C(NS(w,m1),NS(w,m2)) = ? w? rm1(w ?, w)? rm2(w ?, w) ?k i=1 i2 The overlap score indicates the extent to which sets share members and the extent to whichthey are in the same order. To achieve an over lap score of 1, the sets must contain exactly the same items in exactly the same order. An overlap score of 0 is obtained if the sets do not contain any common items. If two sets share roughly half their items and these shared items are dispersed throughout the sets in a roughlysimilar order, we would expect the overlap be tween sets to be around 0.5. cm js ? cp ja ja+mi lin cm 1.0(0.0) 0.69(0.12) 0.53(0.15) 0.33(0.09) 0.26(0.12) 0.28(0.15) 0.32(0.15) js 0.69(0.12) 1.0(0.0) 0.81(0.10) 0.46(0.31) 0.48(0.18) 0.49(0.20) 0.55(0.16) ? 0.53(0.15) 0.81(0.10) 1.0(0.0) 0.61(0.08) 0.4(0.27) 0.39(0.25) 0.48(0.19) cp 0.33(0.09) 0.46(0.31) 0.61(0.08) 1.0(0.0) 0.24(0.24) 0.20(0.18) 0.29(0.15) ja 0.26(0.12) 0.48(0.18) 0.4(0.27) 0.24(0.24) 1.0(0.0) 0.81(0.08) 0.69(0.09) ja+mi 0.28(0.15) 0.49(0.20) 0.39(0.25) 0.20(0.18) 0.81(0.08) 1.0(0.0) 0.81(0.10) lin 0.32(0.15) 0.55(0.16) 0.48(0.19) 0.29(0.15) 0.69(0.09) 0.81(0.10) 1.0(0.0) Table 2: Cross-comparison of first seven similarity measures in terms of mean overlap of neighbour sets and corresponding standard deviations. P R hm cm 0.18(0.10) 0.31(0.13) 0.30(0.14) js 0.19(0.12) 0.55(0.18) 0.51(0.18) ? 0.08(0.08) 0.74(0.14) 0.41(0.23) cp 0.03(0.04) 0.57(0.10) 0.25(0.18) ja 0.36(0.30) 0.38(0.30) 0.74(0.14) ja+mi 0.42(0.30) 0.40(0.31) 0.86(0.07) lin 0.46(0.25) 0.52(0.22) 0.95(0.039)Table 3: Mean overlap scores for seven simi larity measures with precision, recall and the harmonic mean in the AMCRM. 3.2 Results. Table 2 shows the mean overlap score between every pair of the first seven measures in Table 1 calculated over WScomp. Table 3 shows the mean overlap score between each of these measures and precision, recall and the harmonic mean inthe AMCRM. In both tables, standard devia tions are given in brackets and boldface denotes the highest levels of overlap for each measure. For compactness, each measure is denoted by its subscript from Table 1. Although overlap between most pairs of measures is greater than expected if sets of 200 neighbours were generated randomly from WScomp (in this case, average overlap would be 0.08 and only the overlap between the pairs (?,P) and (cp,P) is not significantly greaterthan this at the 1% level), there are substantial differences between the neighbour sets gen erated by different measures. For example, for many pairs, neighbour sets do not appear to have even half their members in common. We have seen that there is a large variation inneighbours selected by different similarity mea sures. In this section, we analyse how neighboursets vary with respect to one fundamental statis tical property ? word frequency. To do this, we measure the bias in neighbour sets towards high frequency nouns and consider how this varies depending on whether the target noun is itself a high frequency noun or low frequency noun. 4.1 Measuring bias. If a measure is biased towards selecting high frequency words as neighbours, then we would ex pect that neighbour sets for this measure wouldbe made up mainly of words from WShigh. Fur ther, the more biased the measure is, the more highly ranked these high frequency words will tend to be. In other words, there will be highoverlap between neighbour sets generated con sidering all 2000 nouns as potential neighbours and neighbour sets generated considering just the nouns in WShigh as potential neighbours. In the extreme case, where all of a noun?s k nearestneighbours are high frequency nouns, the over lap with the high frequency noun neighbour set will be 1 and the overlap with the low frequency noun neighbour set will be 0. The inverse is, ofcourse, true if a measure is biased towards se lecting low frequency words as neighbours. If NSwordset is the vector of neighbours (and associated rank scores) for a given word, w, andsimilarity measure, m, and generated considering just the words in wordset as potential neigh bours, then the overlap between two neighboursets can be computed using a cosine (as be fore). If Chigh = C(NScomp,NShigh) and Clow =C(NScomp,NSlow), then we compute the bias towards high frequency neighbours for word w us ing measure m as: biashighm(w) = Chigh Chigh+Clow The value of this normalised score lies in the range [0,1] where 1 indicates a neighbour set completely made up of high frequency words, 0 indicates a neighbour set completely made up oflow frequency words and 0.5 indicates a neighbour set with no biases towards high or low fre quency words. This score is more informative than simply calculating the proportion of high high freq. low freq. target nouns target nouns cm 0.90 0.87 js 0.94 0.70 ? 0.98 0.90 cp 1.00 0.99 ja 0.99 0.21 ja+mi 0.95 0.14 lin 0.85 0.38 P 0.12 0.04 R 0.99 0.98 hm 0.92 0.28 Table 4: Mean value of biashigh according to measure and frequency of target noun. and low frequency words in each neighbour set because it weights the importance of neighbours by their rank in the set. Thus, a large numberof high frequency words in the positions clos est to the target word is considered more biased than a large number of high frequency words distributed throughout the neighbour set. 4.2 Results. Table 4 shows the mean value of the biashigh score for every measure calculated over the set of high frequency nouns and over the set of low frequency nouns. The standard deviations (not shown) all lie in the range [0,0.2]. Any deviation from 0.5 of greater than 0.0234 is significant at the 1% level. For all measures and both sets of target nouns, there appear to be strong tendencies toselect neighbours of particular frequencies. Further, there appears to be three classes of mea sures: those that select high frequency nouns as neighbours regardless of the frequency of thetarget noun (cm, js, ?, cp andR); those that select low frequency nouns as neighbours regard less of the frequency of the target noun (P); and those that select nouns of a similar frequency to the target noun (ja, ja+mi, lin and hm).This can also be considered in terms of distri butional generality. By definition, recall preferswords that have occurred in more of the con texts that the target noun has, regardless of whether it occurs in other contexts as well i.e., it prefers distributionally more general words. The probability of this being the case increasesas the frequency of the potential neighbour increases and so, recall tends to select high fre quency words. In contrast, precision prefers words that have occurred in very few contextsthat the target word has not i.e., it prefers distributionally more specific words. The prob ability of this being the case increases as the frequency of the potential neighbour decreases and so, precision tends to select low frequencywords. The harmonic mean of precision and re call prefers words that have both high precision and high recall. The probability of this beingthe case is highest when the words are of sim ilar frequency and so, the harmonic mean will tend to select words of a similar frequency. In this section, we consider the observed fre quency effects from a semantic perspective.The concept of distributional generality in troduced in the previous section has parallels with the linguistic relation of hyponymy, where a hypernym is a semantically more general term and a hyponym is a semantically more specificterm. For example, animal is an (indirect1) hypernym of dog and conversely dog is an (indi rect) hyponym of animal. Although one can obviously think of counter-examples, we would generally expect that the more specific term dog can only be used in contexts where animal can be used and that the more general term animal might be used in all of the contexts where dogis used and possibly others. Thus, we might ex pect that distributional generality is correlated with semantic generality ? a word has high recall/low precision retrieval of its hyponyms?co-occurrences and high precision/low recall re trieval of its hypernyms? co-occurrences. Thus, if n1 and n2 are related and P(n2, n1) >R(n2, n1), we might expect that n2 is a hy ponym of n1 and vice versa. However, having discussed a connection between frequency and distributional generality, we might also expect to find that the frequency of the hypernymic term is greater than that of the hyponymicterm. In order to test these hypotheses, we ex tracted all of the possible hyponym-hypernym pairs (20, 415 pairs in total) from our list of 2000 nouns (using WordNet 1.6). We then calculatedthe proportion for which the direction of the hy ponymy relation could be accurately predicted by the relative values of precision and recall andthe proportion for which the direction of the hy ponymy relation could be accurately predictedby relative frequency. We found that the direc tion of the hyponymy relation is correlated in the predicted direction with the precision-recall 1There may be other concepts in the hypernym chain between dog and animal e.g. carnivore and mammal.values in 71% of cases and correlated in the pre dicted direction with relative frequency in 70% of cases. This supports the idea of a three-waylinking between distributional generality, rela tive frequency and semantic generality. We now consider the impact that this has on a potential application of distributional similarity methods. In its most general sense, a collocation is a habitual or lexicalised word combination. How ever, some collocations such as strong tea arecompositional, i.e., their meaning can be determined from their constituents, whereas oth ers such as hot dog are not. Both types areimportant in language generation since a sys tem must choose between alternatives but onlynon-compositional ones are of interest in language understanding since only these colloca tions need to be listed in the dictionary. Baldwin et al (2003) explore empiricalmodels of compositionality for noun-noun com pounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate sev eral tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. They ex tract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents(e.g. rip) from the BNC using RASP and cal culate the value of simlin between each phrasal verb and its simplex constituent. The test simplexscore is used to rank the phrasal verbs according to their similarity with their simplexconstituent. This ranking is correlated with hu man judgements of the compositionality of the phrasal verbs using Spearman?s rank correlationcoefficient. The value obtained (0.0525) is dis appointing since it is not statistically significant(the probability of this value under the null hy pothesis of ?no correlation? is 0.3).2 However, Haspelmath (2002) notes that a compositional collocation is not just similar to one of its constituents ? it can be considered tobe a hyponym of its head constituent. For ex ample, ?strong tea? is a type of ?tea? and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better. Measure rs P (rs) under H0 simlin 0.0525 0.2946 precision -0.160 0.0475 recall 0.219 0.0110 harmonic mean 0.011 0.4562 Table 5: Correlation with compositionality for different similarity measures rip up? is a way of ?ripping?. Thus, we hypothesised that a distributional measure which tends to select more generalterms as neighbours of the phrasal verb (e.g. re call) would do better than measures that tend to select more specific terms (e.g. precision) or measures that tend to select terms of a similar specificity (e.g simlin or the harmonic mean of precision and recall). Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003). We now see significant correlation between compositionality judgements and distributional similarity of thephrasal verb and its head constituent. The cor relation using the recall measure is significant at the 5% level; thus we can conclude that if the simplex verb has high recall retrieval of the phrasal verb?s co-occurrences, then the phrasal is likely to be compositional. The correlation score using the precision measure is negative since we would not expect the simplex verb to be a hyponym of the phrasal verb and thus, ifthe simplex verb does have high precision re trieval of the phrasal verb?s co-occurrences, it is less likely to be compositional. Finally, we obtained a very similar result (0.217) by ranking phrasals according to their inverse relative frequency with their simplex constituent (i.e., freq(simplex)freq(phrasal) ). Thus, it would seem that the three-way connection betweendistributional generality, hyponymy and rela tive frequency exists for verbs as well as nouns.","In its most general sense, a collocation is a habitual or lexicalised word combination. How ever, some collocations such as strong tea arecompositional, i.e., their meaning can be determined from their constituents, whereas oth ers such as hot dog are not. Both types areimportant in language generation since a sys tem must choose between alternatives but onlynon-compositional ones are of interest in language understanding since only these colloca tions need to be listed in the dictionary. Baldwin et al (2003) explore empiricalmodels of compositionality for noun-noun com pounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate sev eral tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. They ex tract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents(e.g. rip) from the BNC using RASP and cal culate the value of simlin between each phrasal verb and its simplex constituent. The test simplexscore is used to rank the phrasal verbs according to their similarity with their simplexconstituent. This ranking is correlated with hu man judgements of the compositionality of the phrasal verbs using Spearman?s rank correlationcoefficient. The value obtained (0.0525) is dis appointing since it is not statistically significant(the probability of this value under the null hy pothesis of ?no correlation? is 0.3).2 However, Haspelmath (2002) notes that a compositional collocation is not just similar to one of its constituents ? it can be considered tobe a hyponym of its head constituent. For ex ample, ?strong tea? is a type of ?tea? and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better. Measure rs P (rs) under H0 simlin 0.0525 0.2946 precision -0.160 0.0475 recall 0.219 0.0110 harmonic mean 0.011 0.4562 Table 5: Correlation with compositionality for different similarity measures rip up? is a way of ?ripping?. Thus, we hypothesised that a distributional measure which tends to select more generalterms as neighbours of the phrasal verb (e.g. re call) would do better than measures that tend to select more specific terms (e.g. precision) or measures that tend to select terms of a similar specificity (e.g simlin or the harmonic mean of precision and recall). Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003). We now see significant correlation between compositionality judgements and distributional similarity of thephrasal verb and its head constituent. The cor relation using the recall measure is significant at the 5% level; thus we can conclude that if the simplex verb has high recall retrieval of the phrasal verb?s co-occurrences, then the phrasal is likely to be compositional. The correlation score using the precision measure is negative since we would not expect the simplex verb to be a hyponym of the phrasal verb and thus, ifthe simplex verb does have high precision re trieval of the phrasal verb?s co-occurrences, it is less likely to be compositional. Finally, we obtained a very similar result (0.217) by ranking phrasals according to their inverse relative frequency with their simplex constituent (i.e., freq(simplex)freq(phrasal) ). Thus, it would seem that the three-way connection betweendistributional generality, hyponymy and rela tive frequency exists for verbs as well as nouns."
75,"Automated identification of diverse sen timent types can be beneficial for manyNLP systems such as review summariza tion and public media analysis. In some ofthese systems there is an option of assign ing a sentiment value to a single sentence or a very short text. In this paper we propose a supervised sentiment classification framework whichis based on data from Twitter, a popu lar microblogging service. By utilizing50 Twitter tags and 15 smileys as sen timent labels, this framework avoids theneed for labor intensive manual annotation, allowing identification and classifi cation of diverse sentiment types of shorttexts. We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences. The quality of the senti ment identification was also confirmed byhuman judges. We also explore dependencies and overlap between different sen timent types represented by smileys and Twitter hashtags.","Automated identification of diverse sen timent types can be beneficial for manyNLP systems such as review summariza tion and public media analysis. In some ofthese systems there is an option of assign ing a sentiment value to a single sentence or a very short text. In this paper we propose a supervised sentiment classification framework whichis based on data from Twitter, a popu lar microblogging service. By utilizing50 Twitter tags and 15 smileys as sen timent labels, this framework avoids theneed for labor intensive manual annotation, allowing identification and classifi cation of diverse sentiment types of shorttexts. We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences. The quality of the senti ment identification was also confirmed byhuman judges. We also explore dependencies and overlap between different sen timent types represented by smileys and Twitter hashtags. A huge amount of social media including news,forums, product reviews and blogs contain nu merous sentiment-based sentences. Sentiment is defined as ?a personal belief or judgment that ?* Both authors equally contributed to this paper.is not founded on proof or certainty?1. Senti ment expressions may describe the mood of thewriter (happy/sad/bored/grateful/...) or the opin ion of the writer towards some specific entity (X is great/I hate X, etc.). Automated identification of diverse sentimenttypes can be beneficial for many NLP systems such as review summarization systems, dia logue systems and public media analysis systems. Sometimes it is directly requested by the user toobtain articles or sentences with a certain senti ment value (e.g Give me all positive reviews of product X/ Show me articles which explain why movie X is boring). In some other cases obtaining sentiment value can greatly enhance information extraction tasks like review summarization. Whilethe majority of existing sentiment extraction sys tems focus on polarity identification (e.g., positive vs. negative reviews) or extraction of a handful of pre-specified mood labels, there are many useful and relatively unexplored sentiment types. Sentiment extraction systems usually require an extensive set of manually supplied sentiment words or a handcrafted sentiment-specific dataset. With the recent popularity of article tagging, some social media types like blogs allow users to add sentiment tags to articles. This allows to use blogsas a large user-labeled dataset for sentiment learning and identification. However, the set of sentiment tags in most blog platforms is somewhat re stricted. Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007).With the recent popularity of the Twitter micro blogging service, a huge amount of frequently 1WordNet 2.1 definitions. 241self-standing short textual sentences (tweets) became openly available for the research community. Many of these tweets contain a wide vari ety of user-defined hashtags. Some of these tagsare sentiment tags which assign one or more senti ment values to a tweet. In this paper we propose away to utilize such tagged Twitter data for classi fication of a wide variety of sentiment types from text. We utilize 50 Twitter tags and 15 smileys assentiment labels which allow us to build a classifier for dozens of sentiment types for short tex tual sentences. In our study we use four different feature types (punctuation, words, n-grams and patterns) for sentiment classification and evaluate the contribution of each feature type for this task.We show that our framework successfully identi fies sentiment types of the untagged tweets. We confirm the quality of our algorithm using human judges. We also explore the dependencies and overlap between different sentiment types represented by smileys and Twitter tags. Section 2 describes related work. Section 3 details classification features and the algorithm, while Section 4 describes the dataset and labels. Automated and manual evaluation protocols and results are presented in Section 5, followed by a short discussion. Sentiment analysis tasks typically combine twodifferent tasks: (1) Identifying sentiment expres sions, and (2) determining the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002;Riloff, 2003; Whitelaw et al, 2005) use lexical re sources and decide whether a sentence expressesa sentiment by the presence of lexical items (sen timent words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al, 2005; Bloom et al, 2007; McDonald et al, 2007; Titov and McDonald, 2008a; Melville et al, 2009). It was suggested that sentiment words may havedifferent senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihal cea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identifyevaluative sentiment expressions and their polar ity. Another line of works aims at identifying abroader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addi tion to) positive or negative evaluations. Mihalcea and Liu (2006) derive lists of words and phrases with happiness factor from a corpus of blog posts, where each post is annotated by the blogger with a mood label. Balog et al (2006) use the mood annotation of blog posts coupled with news datain order to discover the events that drive the dom inant moods expressed in blogs. Mishne (2005) used an ontology of over 100 moods assigned to blog posts to classify blog texts according tomoods. While (Mishne, 2005) classifies a blog entry (post), (Mihalcea and Liu, 2006) assign a hap piness factor to specific words and expressions. Mishne used a much broader range of moods. Strapparava and Mihalcea (2008) classify blogposts and news headlines to six sentiment cate gories.While most of the works on sentiment analysis focus on full text, some works address senti ment analysis in the phrasal and sentence level, see (Yu and Hatzivassiloglou, 2003; Wilson et al,2005; McDonald et al, 2007; Titov and McDon ald, 2008a; Titov and McDonald, 2008b; Wilson et al, 2009; Tsur et al, 2010) among others. Only a few studies analyze the sentiment and polarity of tweets targeted at major brands. Jansenet al (2009) used a commercial sentiment analyzer as well as a manually labeled corpus. Davi dov et al (2010) analyze the use of the #sarcasmhashtag and its contribution to automatic recognition of sarcastic tweets. To the best of our knowledge, there are no works employing Twitter hashtags to learn a wide range of emotions and the re lations between the different emotions. 242 Below we propose a set of classification featuresand present the algorithm for sentiment classifica tion. 3.1 Classification features. We utilize four basic feature types for sentimentclassification: single word features, n-gram fea tures, pattern features and punctuation features.For the classification, all feature types are com bined into a single feature vector. 3.1.1 Word-based and n-gram-based features Each word appearing in a sentence serves as a binary feature with weight equal to the inverted count of this word in the Twitter corpus. We also took each consecutive word sequence containing2?5 words as a binary n-gram feature using a similar weighting strategy. Thus n-gram features al ways have a higher weight than features of their component words, and rare words have a higher weight than common words. Words or n-gramsappearing in less than 0.5% of the training set sen tences do not constitute a feature. ASCII smileys and other punctuation sequences containing two or more consecutive punctuation symbols were used as single-word features. Word features alsoinclude the substituted meta-words for URLs, ref erences and hashtags (see Subsection 4.1). 3.1.2 Pattern-based featuresOur main feature type is based on surface pat terns. For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). We classified words into high-frequency words (HFWs) and content words (CWs). A word whose corpus frequency is more (less) than FH (FC) is considered to be a HFW(CW).We estimate word frequency from the train ing set rather than from an external corpus. Unlike (Davidov and Rappoport, 2006), we consider allsingle punctuation characters or consecutive se quences of punctuation characters as HFWs. We also consider URL, REF, and HASHTAG tags as HFWs for pattern extraction. We define a pattern as an ordered sequence of high frequency words and slots for content words. Following (Davidov and Rappoport, 2008), the FH and FC thresholds were set to 1000 words per million (upper bound for FC) and 100 words per million (lower bound for FH )2. The patterns allow 2?6 HFWs and 1?5 slots forCWs. To avoid collection of patterns which capture only a part of a meaningful multiword ex pression, we require patterns to start and to end with a HFW. Thus a minimal pattern is of the form [HFW] [CW slot] [HFW]. For each sentenceit is possible to generate dozens of different pat terns that may overlap. As with words and n-gram features, we do not treat as features any patterns which appear in less than 0.5% of the training set sentences. Since each feature vector is based on a singlesentence (tweet), we would like to allow approximate pattern matching for enhancement of learn ing flexibility. The value of a pattern feature is estimated according the one of the following four scenarios3: ? ???????????????????????? 1 count(p) : Exact match ? all the pattern components appear in the sentence in correct order without any additional words. count(p) : Sparse match ? same as exact match but additional non-matching words can be inserted between pattern components. ??n N?count(p) : Incomplete match ? only n > 1 of N pattern components appear in the sentence, while some non-matching words can be inserted in-between. At least one of the appearing components should be a HFW. 0 : No match ? nothing or only a single pattern component appears in the sentence. 0 ? ? 1 and 0 ? ? 1 are parameters we use to assign reduced scores for imperfect matches.Since the patterns we use are relatively long, ex act matches are uncommon, and taking advantageof partial matches allows us to significantly re duce the sparsity of the feature vectors. We used ? = ? = 0.1 in all experiments.This pattern based framework was proven effi cient for sarcasm detection in (Tsur et al, 2010; 2Note that the FH and FC bounds allow overlap between some HFWs and CWs. See (Davidov and Rappoport, 2008) for a short discussion. 3As with word and n-gram features, the maximal featureweight of a pattern p is defined as the inverse count of a pat tern in the complete Twitter corpus. 243 Davidov et al, 2010). 3.1.3 Efficiency of feature selection Since we avoid selection of textual features which have a training set frequency below 0.5%, we perform feature selection incrementally, on each stage using the frequencies of the features obtained during the previous stages. Thus first we estimate the frequencies of single words in the training set, then we only consider creationof n-grams from single words with sufficient frequency, finally we only consider patterns composed from sufficiently frequent words and n grams. 3.1.4 Punctuation-based features In addition to pattern-based features we used the following generic features: (1) Sentence length in words, (2) Number of ?!? characters in the sentence, (3) Number of ??? characters in the sentence, (4) Number of quotes in the sentence, and (5) Number of capitalized/all capitals wordsin the sentence. All these features were normal ized by dividing them by the (maximal observed value times averaged maximal value of the other feature groups), thus the maximal weight of each of these features is equal to the averaged weight of a single pattern/word/n-gram feature. 3.2 Classification algorithm. In order to assign a sentiment label to new exam ples in the test set we use a k-nearest neighbors(kNN)-like strategy. We construct a feature vec tor for each example in the training and the test set. We would like to assign a sentiment class toeach example in the test set. For each feature vec tor V in the test set, we compute the Euclidean distance to each of the matching vectors in the training set, where matching vectors are defined as ones which share at least one pattern/n-gram/word feature with v.Let ti, i = 1 . . . k be the k vectors with low est Euclidean distance to v4 with assigned labels Li, i = 1 . . . k. We calculate the mean distance d(ti, v) for this set of vectors and drop from the set up to five outliers for which the distance was more then twice the mean distance. The label assigned 4We used k = 10 for all experiments. to v is the label of the majority of the remaining vectors. If a similar number of remaining vectors have different labels, we assigned to the test vector the most frequent of these labels according to their frequency in the dataset. If there are no matching vectors found for v, we assigned the default ?no sentiment? label since there is significantly more non-sentiment sentences than sentiment sentences in Twitter. In our experiments we used an extensive Twit ter data collection as training and testing sets. In our training sets we utilize sentiment hashtags andsmileys as classification labels. Below we de scribe this dataset in detail. 4.1 Twitter dataset. We have used a Twitter dataset generously pro vided to us by Brendan O?Connor. This dataset includes over 475 million tweets comprising roughly 15% of all public, non-?low quality? tweets created from May 2009 to Jan 2010.Tweets are short sentences limited to 140 UTF 8 characters. All non-English tweets and tweets which contain less than 5 proper English words5 were removed from the dataset. Apart of simple text, tweets may contain URLaddresses, references to other Twitter users (ap pear as @<user>) or a content tags (also called hashtags) assigned by the tweeter (#<tag>)which we use as labels for our supervised clas sification framework. Two examples of typical tweets are: ?#ipad #sucks and 6,510 people agree. See more on Ipad sucks page: http://j.mp/4OiYyg??, and ?Pay nomind to those who talk behind ur back, it sim ply means that u?re 2 steps ahead. #ihatequotes?. Note that in the first example the hashtagged words are a grammatical part of the sentence (itbecomes meaningless without them) while #ihate qoutes of the second example is a mere sentiment label and not part of the sentence. Also note that hashtags can be composed of multiple words (with no spaces). 5Identification of proper English words was based on an available WN-based English dictionary 244 Category # of tags % agreement Strong sentiment 52 87 Likely sentiment 70 66 Context-dependent 110 61 Focused 45 75 No sentiment 3564 99 Table 1: Annotation results (2 judges) for the 3852 mostfrequent tweeter tags. The second column displays the av erage number of tags, and the last column shows % of tags annotated similarly by two judges. During preprocessing, we have replaced URL links, hashtags and references by URL/REF/TAG meta-words. This substitution obviously had some effect on the pattern recognition phase (see Section 3.1.2), however, our algorithm is robust enough to overcome this distortion. 4.2 Hashtag-based sentiment labels. The Twitter dataset contains above 2.5 million dif ferent user-defined hashtags. Many tweets include more than a single tag and 3852 ?frequent? tags appear in more than 1000 different tweets. Two human judges manually annotated these frequenttags into five different categories: 1 ? strong sen timent (e.g #sucks in the example above), 2 ?most likely sentiment (e.g., #notcute), 3 ? contextdependent sentiment (e.g., #shoutsout), 4 ? fo cused sentiment (e.g., #tmobilesucks where the target of the sentiment is part of the hashtag), and 5 ? no sentiment (e.g. #obama). Table 1 shows annotation results and the percentage of similarly assigned values for each category. We selected 50 hashtags annotated ?1? or ?2?by both judges. For each of these tags we automatically sampled 1000 tweets resulting in 50000 la beled tweets. We avoided sampling tweets which include more than one of the sampled hashtags. As a no-sentiment dataset we randomly sampled 10000 tweets with no hashtags/smileys from thewhole dataset assuming that such a random sam ple is unlikely to contain a significant amount of sentiment sentences. 4.3 Smiley-based sentiment labels. While there exist many ?official? lists of possibleASCII smileys, most of these smileys are infrequent or not commonly accepted and used as sen timent indicators by online communities. We used the Amazon Mechanical Turk (AMT) service in order to obtain a list of the most commonly used and unambiguous ASCII smileys. We asked each of ten AMT human subjects to provide at least 6 commonly used ASCII mood-indicating smileystogether with one or more single-word descrip tions of the smiley-related mood state. From the obtained list of smileys we selected a subset of 15 smileys which were (1) provided by at least threehuman subjects, (2) described by at least two human subject using the same single-word descrip tion, and (3) appear at least 1000 times in our Twitter dataset. We then sampled 1000 tweets foreach of these smileys, using these smileys as sentiment tags in the sentiment classification frame work described in the previous section. The purpose of our evaluation was to learn how well our framework can identify and distinguishbetween sentiment types defined by tags or smileys and to test if our framework can be successfully used to identify sentiment types in new un tagged sentences. 5.1 Evaluation using cross-validation. In the first experiment we evaluated the consistency and quality of sentiment classification us ing cross-validation over the training set. Fullyautomated evaluation allowed us to test the performance of our algorithm under several dif ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-, Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/? stands for utilization/omission of the followingfeature types: Pn:punctuation, W:Word, M:n grams (M stands for ?multi?), Pt:patterns. FULL stands for utilization of all feature types. In this experimental setting, the training set was divided to 10 parts and a 10-fold cross validation test is executed. Each time, we use 9 parts as thelabeled training data for feature selection and con struction of labeled vectors and the remaining part is used as a test set. The process was repeated tentimes. To avoid utilization of labels as strong fea tures in the test set, we removed all instances of involved label hashtags/smileys from the tweets used as the test set. 245 Setup Smileys Hashtags random 0.06 0.02 Pn+W-M-Pt- 0.16 0.06 Pn+W+M-Pt- 0.25 0.15 Pn+W+M+Pt- 0.29 0.18 Pn-W-M-Pt+ 0.5 0.26 FULL 0.64 0.31 Table 2: Multi-class classification results for smileys andhashtags. The table shows averaged harmonic f-score for 10 fold cross validation. 51 (16) sentiment classes were used for hashtags (smileys). Multi-class classification. Under multi-class classification we attempt to assign a single label (51 labels in case of hashtags and 16 labels in case of smileys) to each of vectors in the test set. Note that the random baseline for this task is 0.02 (0.06)for hashtags (smileys). Table 2 shows the perfor mance of our framework for these tasks. Results are significantly above the random baseline and definitely nontrivial considering theequal class sizes in the test set. While still relatively low (0.31 for hashtags and 0.64 for smi leys), we observe much better performance forsmileys which is expected due to the lower num ber of sentiment types. The relatively low performance of hashtags can be explained by ambiguity of the hashtags andsome overlap of sentiments. Examination of clas sified sentences reveals that many of them can be reasonably assigned to more than one of the available hashtags or smileys. Thus a tweet ?I?mreading stuff that I DON?T understand again! ha haha...wth am I doing? may reasonably matchtags #sarcasm, #damn, #haha, #lol, #humor, #an gry etc. Close examination of the incorrectly classified examples also reveals that substantialamount of tweets utilize hashtags to explicitly in dicate the specific hashtagged sentiment, in these cases that no sentiment value could be perceived by readers unless indicated explicitly, e.g. ?De Blob game review posted on our blog. #fun?. Obviously, our framework fails to process such cases and captures noise since no sentiment datais present in the processed text labeled with a spe cific sentiment label.Binary classification. In the binary classification experiments, we classified a sentence as either appropriate for a particular tag or as not bear Hashtags Avg #hate #jealous #cute #outrageous Pn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53 Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6 Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64 Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69 FULL 0.8 0.83 0.76 0.71 0.78 Smileys Avg :) ; ) X( : d Pn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65 Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69 Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69 Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72 FULL 0.86 0.87 0.9 0.74 0.81Table 3: Binary classification results for smileys and hashtags. Avg column shows averaged harmonic f-score for 10fold cross validation over all 50(15) sentiment hashtags (smi leys). ing any sentiment6. For each of the 50 (15) labelsfor hashtags (smileys) we have performed a bi nary classification when providing as training/testsets only positive examples of the specific senti ment label together with non-sentiment examples. Table 3 shows averaged results for this case and specific results for selected tags. We can see thatour framework successfully identifies diverse sentiment types. Obviously the results are much bet ter than those of multi-class classification, and the observed > 0.8 precision confirms the usefulnessof the proposed framework for sentiment classifi cation of a variety of different sentiment types. We can see that even for binary classification settings, classification of smiley-labeled sentencesis a substantially easier task compared to classifi cation of hashtag-labeled tweets. Comparing the contributed performance of different feature typeswe can see that punctuation, word and pattern features, each provide a substantial boost for classi fication quality while we observe only a marginalboost when adding n-grams as classification features. We can also see that pattern features contribute the performance more than all other fea tures together. 5.2 Evaluation with human judges. In the second set of experiments we evaluated our framework on a test set of unseen and untaggedtweets (thus tweets that were not part of the train 6Note that this is a useful application in itself, as a filter that extracts sentiment sentences from a corpus for further focused study/processing. 246 ing data), comparing its output to tags assigned by human judges. We applied our framework with its FULL setting, learning the sentiment tags fromthe training set for hashtags and smileys (sepa rately) and executed the framework on the reduced Tweeter dataset (without untagged data) allowingit to identify at least five sentences for each senti ment class.In order to make the evaluation harsher, we re moved all tweets containing at least one of the relevant classification hashtags (or smileys). For each of the resulting 250 sentences for hashtags,and 75 sentences for smileys we generated an ?as signment task?. Each task presents a human judgewith a sentence and a list of ten possible hash tags. One tag from this list was provided by ouralgorithm, 8 other tags were sampled from the re maining 49 (14) available sentiment tags, and the tenth tag is from the list of frequent non-sentiment tags (e.g. travel or obama). The human judge was requested to select the 0-2 most appropriate tags from the list. Allowing assignment of multiple tags conforms to the observation that even short sentences may express several different sentimenttypes and to the observation that some of the selected sentiment tags might express similar senti ment types. We used the Amazon Mechanical Turk service to present the tasks to English-speaking subjects.Each subject was given 50 tasks for Twitter hash tags or 25 questions for smileys. To ensure the quality of assignments, we added to each test fivemanually selected, clearly sentiment bearing, as signment tasks from the tagged Twitter sentences used in the training set. Each set was presented to four subjects. If a human subject failed to provide the intended ?correct? answer to at least two of the control set questions we reject him/her from the calculation. In our evaluation the algorithmis considered to be correct if one of the tags se lected by a human judge was also selected by thealgorithm. Table 4 shows results for human judge ment classification. The agreement score for this task was ? = 0.41 (we consider agreement when at least one of two selected items are shared). Table 4 shows that the majority of tags selectedby humans matched those selected by the algo rithm. Precision of smiley tags is substantially Setup % Correct % No sentiment Control Smileys 84% 6% 92% Hashtags 77% 10% 90%Table 4: Results of human evaluation. The second col umn indicates percentage of sentences where judges find noappropriate tags from the list. The third column shows per formance on the control set. Hashtags #happy #sad #crazy # bored#sad 0.67 - - #crazy 0.67 0.25 - #bored 0.05 0.42 0.35 #fun 1.21 0.06 1.17 0.43 Smileys :) ; ) : ( X(; ) 3.35 - - : ( 3.12 0.53 - X( 1.74 0.47 2.18 : S 1.74 0.42 1.4 0.15 Table 5: Percentage of co-appearance of tags in tweeter corpus. higher than of hashtag labels, due to the lessernumber of possible smileys and the lesser ambi guity of smileys in comparison to hashtags. 5.3 Exploration of feature dependencies. Our algorithm assigns a single sentiment type for each tweet. However, as discussed above, some sentiment types overlap (e.g., #awesome and #amazing). Many sentences may express several types of sentiment (e.g., #fun and #scary in ?Oh My God http://goo.gl/fb/K2N5z #entertainment #fun #pictures #photography #scary #teaparty?). We would like to estimate such inter-sentiment dependencies and overlap automatically from the labeled data. We use two different methods for overlap estimation: tag co-occurrence and feature overlap. 5.3.1 Tag co-occurrenceMany tweets contain more than a single hashtag or a single smiley type. As mentioned, we ex clude such tweets from the training set to reduce ambiguity. However such tag co-appearances canbe used for sentiment overlap estimation. We cal culated the relative co-occurrence frequencies of some hashtags and smileys. Table 5 shows some of the observed co-appearance ratios. As expected some of the observed tags frequently co-appear with other similar tags. 247 Hashtags #happy #sad #crazy # bored#sad 12.8 - - #crazy 14.2 3.5 - #bored 2.4 11.1 2.1 #fun 19.6 2.1 15 4.4 Smileys :) ; ) : ( X(; ) 35.9 - - : ( 31.9 10.5 - X( 8.1 10.2 36 : S 10.5 12.6 21.6 6.1 Table 6: Percentage of shared features in feature vectors for different tags. Interestingly, it appears that a relatively high ratio of co-appearance of tags is with opposite meanings (e.g., ?#ilove eating but #ihate feeling fat lol? or ?happy days of training going to end in a few days #sad #happy?). This is possibly due to frequently expressed contrast sentiment types in the same sentence ? a fascinating phenomenareflecting the great complexity of the human emo tional state (and expression). 5.3.2 Feature overlapIn our framework we have created a set of fea ture vectors for each of the Twitter sentiment tags. Comparison of shared features in feature vector sets allows us to estimate dependencies betweendifferent sentiment types even when direct tag cooccurrence data is very sparse. A feature is considered to be shared between two different senti ment labels if for both sentiment labels there is at least a single example in the training set whichhas a positive value of this feature. In order to automatically analyze such dependencies we calcu late the percentage of sharedWord/n-gram/Pattern features between different sentiment labels. Table 6 shows the observed feature overlap values for selected sentiment tags. We observe the trend of results obtained by comparison of shared feature vectors is similar to those obtained by means of label co-occurrence, although the numbers of the shared features arehigher. These results, demonstrating the patternbased similarity of conflicting, sometimes contradicting, emotions are interesting from a psycho logical and cognitive perspective.","The purpose of our evaluation was to learn how well our framework can identify and distinguishbetween sentiment types defined by tags or smileys and to test if our framework can be successfully used to identify sentiment types in new un tagged sentences. 5.1 Evaluation using cross-validation. In the first experiment we evaluated the consistency and quality of sentiment classification us ing cross-validation over the training set. Fullyautomated evaluation allowed us to test the performance of our algorithm under several dif ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-, Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/? stands for utilization/omission of the followingfeature types: Pn:punctuation, W:Word, M:n grams (M stands for ?multi?), Pt:patterns. FULL stands for utilization of all feature types. In this experimental setting, the training set was divided to 10 parts and a 10-fold cross validation test is executed. Each time, we use 9 parts as thelabeled training data for feature selection and con struction of labeled vectors and the remaining part is used as a test set. The process was repeated tentimes. To avoid utilization of labels as strong fea tures in the test set, we removed all instances of involved label hashtags/smileys from the tweets used as the test set. 245 Setup Smileys Hashtags random 0.06 0.02 Pn+W-M-Pt- 0.16 0.06 Pn+W+M-Pt- 0.25 0.15 Pn+W+M+Pt- 0.29 0.18 Pn-W-M-Pt+ 0.5 0.26 FULL 0.64 0.31 Table 2: Multi-class classification results for smileys andhashtags. The table shows averaged harmonic f-score for 10 fold cross validation. 51 (16) sentiment classes were used for hashtags (smileys). Multi-class classification. Under multi-class classification we attempt to assign a single label (51 labels in case of hashtags and 16 labels in case of smileys) to each of vectors in the test set. Note that the random baseline for this task is 0.02 (0.06)for hashtags (smileys). Table 2 shows the perfor mance of our framework for these tasks. Results are significantly above the random baseline and definitely nontrivial considering theequal class sizes in the test set. While still relatively low (0.31 for hashtags and 0.64 for smi leys), we observe much better performance forsmileys which is expected due to the lower num ber of sentiment types. The relatively low performance of hashtags can be explained by ambiguity of the hashtags andsome overlap of sentiments. Examination of clas sified sentences reveals that many of them can be reasonably assigned to more than one of the available hashtags or smileys. Thus a tweet ?I?mreading stuff that I DON?T understand again! ha haha...wth am I doing? may reasonably matchtags #sarcasm, #damn, #haha, #lol, #humor, #an gry etc. Close examination of the incorrectly classified examples also reveals that substantialamount of tweets utilize hashtags to explicitly in dicate the specific hashtagged sentiment, in these cases that no sentiment value could be perceived by readers unless indicated explicitly, e.g. ?De Blob game review posted on our blog. #fun?. Obviously, our framework fails to process such cases and captures noise since no sentiment datais present in the processed text labeled with a spe cific sentiment label.Binary classification. In the binary classification experiments, we classified a sentence as either appropriate for a particular tag or as not bear Hashtags Avg #hate #jealous #cute #outrageous Pn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53 Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6 Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64 Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69 FULL 0.8 0.83 0.76 0.71 0.78 Smileys Avg :) ; ) X( : d Pn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65 Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69 Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69 Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72 FULL 0.86 0.87 0.9 0.74 0.81Table 3: Binary classification results for smileys and hashtags. Avg column shows averaged harmonic f-score for 10fold cross validation over all 50(15) sentiment hashtags (smi leys). ing any sentiment6. For each of the 50 (15) labelsfor hashtags (smileys) we have performed a bi nary classification when providing as training/testsets only positive examples of the specific senti ment label together with non-sentiment examples. Table 3 shows averaged results for this case and specific results for selected tags. We can see thatour framework successfully identifies diverse sentiment types. Obviously the results are much bet ter than those of multi-class classification, and the observed > 0.8 precision confirms the usefulnessof the proposed framework for sentiment classifi cation of a variety of different sentiment types. We can see that even for binary classification settings, classification of smiley-labeled sentencesis a substantially easier task compared to classifi cation of hashtag-labeled tweets. Comparing the contributed performance of different feature typeswe can see that punctuation, word and pattern features, each provide a substantial boost for classi fication quality while we observe only a marginalboost when adding n-grams as classification features. We can also see that pattern features contribute the performance more than all other fea tures together. 5.2 Evaluation with human judges. In the second set of experiments we evaluated our framework on a test set of unseen and untaggedtweets (thus tweets that were not part of the train 6Note that this is a useful application in itself, as a filter that extracts sentiment sentences from a corpus for further focused study/processing. 246 ing data), comparing its output to tags assigned by human judges. We applied our framework with its FULL setting, learning the sentiment tags fromthe training set for hashtags and smileys (sepa rately) and executed the framework on the reduced Tweeter dataset (without untagged data) allowingit to identify at least five sentences for each senti ment class.In order to make the evaluation harsher, we re moved all tweets containing at least one of the relevant classification hashtags (or smileys). For each of the resulting 250 sentences for hashtags,and 75 sentences for smileys we generated an ?as signment task?. Each task presents a human judgewith a sentence and a list of ten possible hash tags. One tag from this list was provided by ouralgorithm, 8 other tags were sampled from the re maining 49 (14) available sentiment tags, and the tenth tag is from the list of frequent non-sentiment tags (e.g. travel or obama). The human judge was requested to select the 0-2 most appropriate tags from the list. Allowing assignment of multiple tags conforms to the observation that even short sentences may express several different sentimenttypes and to the observation that some of the selected sentiment tags might express similar senti ment types. We used the Amazon Mechanical Turk service to present the tasks to English-speaking subjects.Each subject was given 50 tasks for Twitter hash tags or 25 questions for smileys. To ensure the quality of assignments, we added to each test fivemanually selected, clearly sentiment bearing, as signment tasks from the tagged Twitter sentences used in the training set. Each set was presented to four subjects. If a human subject failed to provide the intended ?correct? answer to at least two of the control set questions we reject him/her from the calculation. In our evaluation the algorithmis considered to be correct if one of the tags se lected by a human judge was also selected by thealgorithm. Table 4 shows results for human judge ment classification. The agreement score for this task was ? = 0.41 (we consider agreement when at least one of two selected items are shared). Table 4 shows that the majority of tags selectedby humans matched those selected by the algo rithm. Precision of smiley tags is substantially Setup % Correct % No sentiment Control Smileys 84% 6% 92% Hashtags 77% 10% 90%Table 4: Results of human evaluation. The second col umn indicates percentage of sentences where judges find noappropriate tags from the list. The third column shows per formance on the control set. Hashtags #happy #sad #crazy # bored#sad 0.67 - - #crazy 0.67 0.25 - #bored 0.05 0.42 0.35 #fun 1.21 0.06 1.17 0.43 Smileys :) ; ) : ( X(; ) 3.35 - - : ( 3.12 0.53 - X( 1.74 0.47 2.18 : S 1.74 0.42 1.4 0.15 Table 5: Percentage of co-appearance of tags in tweeter corpus. higher than of hashtag labels, due to the lessernumber of possible smileys and the lesser ambi guity of smileys in comparison to hashtags. 5.3 Exploration of feature dependencies. Our algorithm assigns a single sentiment type for each tweet. However, as discussed above, some sentiment types overlap (e.g., #awesome and #amazing). Many sentences may express several types of sentiment (e.g., #fun and #scary in ?Oh My God http://goo.gl/fb/K2N5z #entertainment #fun #pictures #photography #scary #teaparty?). We would like to estimate such inter-sentiment dependencies and overlap automatically from the labeled data. We use two different methods for overlap estimation: tag co-occurrence and feature overlap. 5.3.1 Tag co-occurrenceMany tweets contain more than a single hashtag or a single smiley type. As mentioned, we ex clude such tweets from the training set to reduce ambiguity. However such tag co-appearances canbe used for sentiment overlap estimation. We cal culated the relative co-occurrence frequencies of some hashtags and smileys. Table 5 shows some of the observed co-appearance ratios. As expected some of the observed tags frequently co-appear with other similar tags. 247 Hashtags #happy #sad #crazy # bored#sad 12.8 - - #crazy 14.2 3.5 - #bored 2.4 11.1 2.1 #fun 19.6 2.1 15 4.4 Smileys :) ; ) : ( X(; ) 35.9 - - : ( 31.9 10.5 - X( 8.1 10.2 36 : S 10.5 12.6 21.6 6.1 Table 6: Percentage of shared features in feature vectors for different tags. Interestingly, it appears that a relatively high ratio of co-appearance of tags is with opposite meanings (e.g., ?#ilove eating but #ihate feeling fat lol? or ?happy days of training going to end in a few days #sad #happy?). This is possibly due to frequently expressed contrast sentiment types in the same sentence ? a fascinating phenomenareflecting the great complexity of the human emo tional state (and expression). 5.3.2 Feature overlapIn our framework we have created a set of fea ture vectors for each of the Twitter sentiment tags. Comparison of shared features in feature vector sets allows us to estimate dependencies betweendifferent sentiment types even when direct tag cooccurrence data is very sparse. A feature is considered to be shared between two different senti ment labels if for both sentiment labels there is at least a single example in the training set whichhas a positive value of this feature. In order to automatically analyze such dependencies we calcu late the percentage of sharedWord/n-gram/Pattern features between different sentiment labels. Table 6 shows the observed feature overlap values for selected sentiment tags. We observe the trend of results obtained by comparison of shared feature vectors is similar to those obtained by means of label co-occurrence, although the numbers of the shared features arehigher. These results, demonstrating the patternbased similarity of conflicting, sometimes contradicting, emotions are interesting from a psycho logical and cognitive perspective."
76,We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models.,"We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models. Language models (LM) are applied in many natural language processing applications, such as speech recognition and machine translation, to encapsulate syntactic, semantic and pragmatic information. For systems which learn from given data we frequently observe a severe drop in performance when moving to a new genre or new domain. In speech recognition a number of adaptation techniques have been developed to cope with this situation. In statistical machine translation we have a similar situation, i.e. estimate the model parameter from some data, and use the system to translate sentences which may not be well covered by the training data. Therefore, the potential of adaptation techniques needs to be explored for machine translation applications. Statistical machine translation is based on the noisy channel model, where the translation hypothesis is searched over the space defined by a translation model and a target language (Brown et al, 1993). Statistical machine translation can be formulated as follows: )()|(maxarg)|(maxarg* tPtsPstPt tt ?== where t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model. The argmax operation is the search, which is done by the decoder. In the current study we modify the target language model P(t), to represent the test data better, and thereby improve the translation quality. (Janiszek, et al 2001) list the following approaches to language model adaptation: ? Linear interpolation of a general and a domain specific model (Seymore, Rosenfeld, 1997). Back off of domain specific probabilities with those of a specific model (Besling, Meier, 1995). Retrieval of documents pertinent to the new domain and training a language model on-line with those data (Iyer, Ostendorf, 1999, Mahajan et. al. 1999). Maximum entropy, minimum discrimination adaptation (Chen, et. al., 1998). Adaptation by linear transformation of vectors of bigram counts in a reduced space (DeMori, Federico, 1999). Smoothing and adaptation in a dual space via latent semantic analysis, modeling long-term semantic dependencies, and trigger combinations. (J. Bellegarda, 2000). Our approach can be characterized as unsupervised data augmentation by retrieval of relevant documents from large monolingual corpora, and interpolation of the specific language model, build from the retrieved data, with a background language model. To be more specific, the following steps are carried out to do the language model adaptation. First, a baseline statistical machine translation system, using a large general language model, is applied to generate initial translations. Then these translations hypotheses are reformulated as queries to retrieve similar sentences from a very large text collection. A small domain specific language model is build using the retrieved sentences and linearly interpolated with the background language model. This new interpolated language model in applied in a second decoding run to produce the final translations. There are a number of interesting questions pertaining to this approach: ? Which information can and should used to generate the queries: the first-best translation only, or also translation alternatives. How should we construct the queries, just as simple bag-of-words, or can we incorporate more structure to make them more powerful. How many documents should be retrieved to build the specific language models, and on what granularity should this be done, i.e. what is a document in the information retrieval process. The paper is structured as follows: section 2 outlines the sentence retrieval approach, and three bag-of-words query models are designed and explored; structured query models are introduced in section 3. In section 4 we present translation experiments are presented for the different query. Finally, summary is given in section 5. Our language model adaptation is an unsupervised data augmentation approach guided by query models. Given a baseline statistical machine translation system, the language model adaptation is done in several steps shown as follows: ? Generate a set of initial translation hypotheses H = {h1 ?hn} for source sentences s, using either the baseline MT system with the background language model or only the translation model ? Use H to build query ? Use query to retrieve relevant sentences from the large corpus ? Build specific language models from retrieved sentences ? Interpolate the specific language model with the background language ? Re-translate sentences s with adapted language model Figure-1: Adaptation Algorithm The specific language model )|( hwP iA and the general background model )|( hwP iB are combined using linear interpolation: )|()1()|()|(? hwPhwPhwP iAiBi ?? ?+= (1) The interpolation factor ? can be simply estimated using cross validation or a grid search. As an alternative to using translations for the baseline system, we will also describe an approach, which uses partial translations of the source sentence, using the translation model only. In this case, no full translation needs to be carried out in the first step; only information from the translation model is used. Our approach focuses on query model building, using different levels of knowledge representations from the hypothesis set or from the translation model itself. The quality of the query models is crucial to the adapted language model?s performance. Three bag-of-words query models are proposed and explained in the following sections. 2.1 Sentence Retrieval Process. In our sentence retrieval process, the standard tf/idf (term frequency and inverse document frequency) term weighting scheme is used. The queries are built from the translation hypotheses. We follow (Eck, et al, 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories. Both the query and the sentences in the text corpus are converted into vectors by assigning a term weight to each word. Then the cosine similarity is calculated proportional to the inner product of the two vectors. All sentences are ranked according to their similarity with the query, and the most similar sentences are used as the data for building the specific language model. In our experiments we use different numbers of similar sentences, ranting from one to several thousand. 2.2 Bag-of-words Query Models. Different query models are designed to guide the data augmentation efficiently. We first define ?bag-of-words? models, based on different levels of knowledge collected from the hypotheses of the statistical machine translation engine. 2.2.1 First-best Hypothesis as a Query Model The first-best hypothesis is the Viterbi path in the search space returned from the statistical machine translation decoder. It is the optimal hypothesis the statistical machine translation system can generate using the given translation and language model, and restricted by the applied pruning strategy. Ignoring word order, the hypothesis is converted into a bag-of-words representation, which is then used as a query: }|),{(),,( 1211 TiiilT VwfwwwwQ ?== L where iw is a word in the vocabulary 1TV of the Top 1 hypothesis. if is the frequency of iw ?s occurrence in the hypothesis. The first-best hypothesis is the actual translation we want to improve, and usually it captures enough correct word translations to secure a sound adaptation process. But it can miss some informative translation words, which could lead to better-adapted language models. 2.2.2 N-Best Hypothesis List as a Query Model Similar to the first-best hypothesis, the n-best hypothesis list is converted into a bag-of-words representation. Words which occurred in several translation hypotheses are simply repeated in the bag-of-words representations. }|),{( ),,;;,,( ,2,1,,12,11,1 1 TNiii lNNNlTN Vwfw wwwwwwQ N ?= = LLL where TNV is the combined vocabulary from all n best hypotheses and if is the frequency of iw ?s occurrence in the n-best hypothesis list. TNQ has several good characteristics: First it contains translation candidates, and thus is more informative than 1TQ . In addition, the confidently translated words usually occur in every hypothesis in the n-best list, therefore have a stronger impact on the retrieval result due to the higher term frequency (tf) in the query. Thirdly, most of the hypotheses are only different from each other in one word or two. This means, there is not so much noise and variance introduced in this query model. 2.2.3 Translation Model as a Query Model To fully leverage the available knowledge from the translation system, the translation model can be used to guide the language model adaptation process. As introduced in section 1, the translation model represents the full knowledge of translating words, as it encodes all possible translations candidates for a given source sentence. Thus the query model based on the translation model, has potential advantages over both 1TQ and TNQ . To utilize the translation model, all the n-grams from the source sentence are extracted, and the corresponding candidate translations are collected from the translation model. These are then converted into a bag-of-words representation as follows: }|),{( ),,;;,,( ,2,1,,2,1, 1111 TMiii nsssnsssTM Vwfw wwwwwwQ IIII ?= = LLL where is is a source n-gram, and I is the number of n-grams in the source sentence. jsiw , is a candidate target word as translation of is . Thus the translation model is converted into a collection of target words as a bag-of-word query model. There is no decoding process involved to build TMQ . This means TMQ does not incorporate any background language model information at all, while both 1TQ and TNQ implicitly use the background language model to prune the words in the query. Thus TMQ is a generalization, and 1TQ and TNQ are pruned versions. This also means TMQ is subject to more noise. Word proximity and word order is closely related to syntactic and semantic characteristics. However, it is not modeled in the query models presented so far, which are simple bag-of-words representations. Incorporating syntactic and semantic information into the query models can potentially improve the effectiveness of LM adaptation. The word-proximity and word ordering information can be easily extracted from the first best hypothesis, the n-best hypothesis list, and the translation lattice built from the translation model. After extraction of the information, structured query models are proposed using the structured query language, described in the Section 3.1. 3.1 Structured Query Language. This query language essentially enables the use of proximity operators (ordered and unordered windows) in queries, so that it is possible to model the syntactic and semantic information encoded in phrases, n-grams, and co-occurred word pairs. The InQuery implementation (Lemur 2003) is applied. So far 16 operators are defined in InQuery to model word proximity (ordered, unordered, phrase level, and passage level). Four of these operators are used specially for our language model adaptation: Sum Operator: #sum( 1t ? nt ) The terms or nodes ( 1t ? nt ) are treated as having equal influence on the final retrieval result. The belief values provided by the arguments of the sum are averaged to produce the belief value of the #sum node. Weighted Sum Operator: #wsum( 11 : tw , ?) The terms or nodes ( 1t ? nt ) contribute unequally to the final result according to the weight ( iw ) associated with each it . Ordered Distance Operator: #N( 1t ? nt ) The terms must be found within N words of each other in the text in order to contribute to the document's belief value. An n-gram phrase can be modeled as an ordered distance operator with N=n. Unordered Distance Operator: #uwN( 1t ? nt ) The terms contained must be found in any order within a window of N words in order for this operator to contribute to the belief value of the document. 3.2 Structured Query Models. Given the representation power of the structured query language, the Top-1 hypothesis, Top-N Best hypothesis list, and the translation lattice can be converted into three Structured Query Models respectively. For first-best and n-best hypotheses, we collect related target n-grams of a given source word according to the alignments generated in the Viterbi decoding process. While for the translation lattice, similar to the construction of TMQ , we collect all the source n-grams, and translate them into target n-grams. In either case, we get a set of target n-grams for each source word. The structured query model for the whole source sentence is a collection of such subsets of target n grams. },,,{ 21 Isssst. tttQ vLvv= is t v is a set of target n-grams for the source word is : }}{;},{;},{{ 311211 LLL v gramiiigramiigramis ttttttt i ?+??+?= In our experiments, we consider up to trigram for better retrieval efficiency, but higher order n-grams could be used as will. The second simplification is that every source word is equally important, thus each n-gram subset is t v will have an equal contribution to the final retrieval results. The last simplification is each n-gram within the set of is t v has an equal weight, i.e. we do not use the translation probabilities of the translation model. If the system is a phrase-based translation system, we can encode the phrases using the ordered distance operator (#N) with N equals to the number of the words of that phrase, which is denoted as the #phrase operator in InQuery implementation. The 2-grams and 3-grams can be encoded using this operator too. Thus our final structured query model is a sum operator over a set of nodes. Each node corresponds to a source word. Usually each source word has a number of translation candidates (unigrams or phrases). Each node is a weighted sum over all translation candidates weighted by their frequency in the hypothesis set. An example is shown below, where #phrase indicates the use of the ordered distance operator with varying n: #q=#sum( #wsum(2 eu 2 #phrase(european union) ) #wsum(12 #phrase(the united states) 1 american 1 #phrase(an american) ) #wsum(4 are 1 is ) #wsum(8 markets 3 market)) #wsum(7 #phrase(the main) 5 primary ) ); Experiments are carried out on a standard statistical machine translation task defined in the NIST evaluation in June 2002. There are 878 test sentences in Chinese, and each sentence has four human translations as references. NIST score (NIST 2002) and Bleu score (Papineni et. al. 2002) of mteval version 9 are reported to evaluate the translation quality. 4.1 Baseline Translation System. Our baseline system (Vogel et al, 2003) gives scores of 7.80 NIST and 0.1952 Bleu for Top-1 hypothesis, which is comparable to the best results reported on this task. For the baseline system, we built a translation model using 284K parallel sentence pairs, and a trigram language model from a 160 million words general English news text collection. This LM is the background model to be adapted. With the baseline system, the n-best hypotheses list and the translation lattice are extracted to build the query models. Experiments are carried out on the adapted language model using the three bag-of words query models: 1TQ , TNQ and TMQ , and the corresponding structured query models. 4.2 Data: GigaWord Corpora. The so-called GigaWord corpora (LDC, 2003) are very large English news text collections. There are four distinct international sources of English newswire: AFE Agence France Press English Service APW Associated Press Worldstream English Service NYT The New York Times Newswire Service XIE The Xinhua News Agency English Service Table-1 shows the size of each part in word counts. AFE APW NYT XIE 170,969K 539,665K 914,159K 131,711K Table-1: Number of words in the different GigaWord corpora As the Lemur toolkit could not handle the two large corpora (APW and NYT) we used only 200 million words from each of these two corpora. In the preprocessing all words are lowercased and punctuation is separated. There is no explicit removal of stop words as they usually fade out by tf.idf weights, and our experiments showed not positive effects when removing stop words. 4.3 Bag-of-Words Query Models. Table-2 shows the size of 1TQ , TNQ and TMQ in terms of number of tokens in the 878 queries: 1TQ TNQ TMQ || Q 25,861 231,834 3,412,512 Table-2: Query size in number of tokens As words occurring several times are reduced to word-frequency pairs, the size of the queries generated from the 100-best translation lists is only 9 times as big as the queries generated from the first-best translations. The queries generated from the translation model contain many more translation alternatives, summing up to almost 3.4 million tokens. Using the lattices the whole information of the translation model is kept. 4.3.1 Results for Query 1TQ In the first experiment we used the first-best translations to generate the queries. For each of the 4 corpora different numbers of similar sentences (1, 10, 100, and 1000) were retrieved to build specific language models. Figure-2 shows the language model adaptation after tuning the interpolation factor ? by a grid search over [0,1]. Typically ? is around 0.80. 1-Best/NIST Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline 1-Best/BLEU-Scores 0.1900 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure-2: NIST and Bleu scores 1TQ We see that each corpus gives an improvement over the baseline. The best NIST score is 7.94, and the best Bleu score is 0.2018. Both best scores are realized using top 100 relevant sentences corpus per source sentence mined from the AFE. 4.3.2 Results for Query TNQ Figure-3 shows the results for the query model TNQ . The best results are 7.99 NIST score, and 0.2022 Bleu score. These improvements are statistically significant. Both scores are achieved at the same settings as those in 1TQ , i.e. using top 100 retrieved relevant sentences mined from the AFE corpus. 100-Best/NIST-Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline 100-Best/BLEU-Scores 0.1900 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure-3: NIST and Bleu scores from TNQ Using the translation alternatives to retrieve the data for language model adaptation gives an improvement over using the first-best translation only for query construction. Using only one translation hypothesis to build an adapted language model has the tendency to reinforce that translation. 4.3.3 Results for Query TMQ The third bag-of-words query model uses all translation alternatives for source words and source phrases. Figure-4 shows the results of this query model TMQ . The best results are 7.91 NIST score and 0.1995 Bleu. For this query model best results were achieved using the top 1000 relevant sentences mined from the AFE corpus per source sentence. The improvement is not as much as the other two query models. The reason is probably that all translation alternatives, even wrong translations resulting from errors in the word and phrase alignment, contribute alike to retrieve similar sentences. Thereby, an adapted language model is built, which reinforces not only good translations, but also bad translations. All the three query models showed improvements over the baseline system in terms of NIST and Bleu scores. The best bag-of-words query model is TNQ built from the N-Best list. It provides a good balance between incorporating translation alternatives in the language model adaptation process and not reinforcing wrong translations. Lattice/NIST-Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Lattice/BLEU-Scores 0.1900 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure-4: NIST and Bleu scores from TMQ 4.4 Structured Query Models. The next series of experiments was done to study if using word order information in constructing the queries could help to generate more effective adapted language models. By using the structured query language we converted the same first-best hypothesis, the 100-best list, and the translation lattice into structured query models. Results are reported for the AFE corpus only, as this corpus gave best translation scores. Figure-5 shows the results for all three structured query models, built from the first-best hypothesis (?1-Best?), the 100 best hypotheses list (?100 Best?), and translation lattice (?TM-Lattice?). Using these query models, different numbers of most similar sentences, ranging from 100 to 4000, where retrieved from the AFE corpus. The given baseline results are the best results achieved from the corresponding bag-of-words query models. Consistent improvements were observed on NIST and Bleu scores. Again, optimal interpolation factors to interpolate the specific language models with the background language model were used, which typically were in the range of [0.6, 0.7]. Structured query models give most improvements when using more sentences for language model adaptation. The effect is more pronounced for Bleu then for NIST score. Structured query/NIST-Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 8.0500 8.1000 8.1500 Baseline Top100 Top500 Top1000 Top2000 Top4000 1-Best 100-Best TM-Lattice Structured query/BLEU-Scores 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 0.2060 0.2080 Baseline Top100 Top500 Top1000 Top2000 Top4000 1-Best 100-Best TM-Lattice Figure-5: NIST and Bleu scores from the structured query models The really interesting result is that the structured query model TMQ gives now the best translation results. Adding word order information to the queries obviously helps to reduce the noise in the retrieved data by selecting sentences, which are closer to the good translations, The best results using the adapted language models are NIST score 8.12 for using the 2000 most similar sentences, whereas Bleu score goes up to 0.2068 when using 4000 sentences for language model adaptation. 4.5 Example. Table-3 shows translation examples for the 17th Chinese sentence in the test set. We applied the baseline system (Base), the bag-of-word query model (Hyp1), and the structured query model (Hyp2) using AFE corpus. Ref The police has already blockade the scene of the explosion. Base At present, the police had cordoned off the explosion. Hyp1 At present, police have sealed off the explosion. Hyp2 Currently, police have blockade on the scene of the explosion. Table-3 Translation examples 4.6 Oracle Experiment Finally, we run an oracle experiments to see how much improvement could be achieved if we only selected better data for the specific language models. We converted the four available reference translations into structured query models and retrieved the top 4000 relevant sentences from AFE corpus for each source sentence. Using these language models, interpolated with the background language model gave a NIST score of 8.67, and a Bleu score of 0.2228. This result indicates that there is room for further improvements using this language model adaptation technique. The oracle experiment suggests that better initial translations lead to better language models and thereby better 2nd iteration translations. This lead to the question if we can iterate the retrieval process several times to get further improvement, or if the observed improvement results form using for (good) translations, which have more diversity than the translations in an n-best list. On the other side the oracle experiment also shows that the optimally expected improvement is limited by the translation model and decoding algorithm used in the current SMT system.","Experiments are carried out on a standard statistical machine translation task defined in the NIST evaluation in June 2002. There are 878 test sentences in Chinese, and each sentence has four human translations as references. NIST score (NIST 2002) and Bleu score (Papineni et. al. 2002) of mteval version 9 are reported to evaluate the translation quality. 4.1 Baseline Translation System. Our baseline system (Vogel et al, 2003) gives scores of 7.80 NIST and 0.1952 Bleu for Top-1 hypothesis, which is comparable to the best results reported on this task. For the baseline system, we built a translation model using 284K parallel sentence pairs, and a trigram language model from a 160 million words general English news text collection. This LM is the background model to be adapted. With the baseline system, the n-best hypotheses list and the translation lattice are extracted to build the query models. Experiments are carried out on the adapted language model using the three bag-of words query models: 1TQ , TNQ and TMQ , and the corresponding structured query models. 4.2 Data: GigaWord Corpora. The so-called GigaWord corpora (LDC, 2003) are very large English news text collections. There are four distinct international sources of English newswire: AFE Agence France Press English Service APW Associated Press Worldstream English Service NYT The New York Times Newswire Service XIE The Xinhua News Agency English Service Table-1 shows the size of each part in word counts. AFE APW NYT XIE 170,969K 539,665K 914,159K 131,711K Table-1: Number of words in the different GigaWord corpora As the Lemur toolkit could not handle the two large corpora (APW and NYT) we used only 200 million words from each of these two corpora. In the preprocessing all words are lowercased and punctuation is separated. There is no explicit removal of stop words as they usually fade out by tf.idf weights, and our experiments showed not positive effects when removing stop words. 4.3 Bag-of-Words Query Models. Table-2 shows the size of 1TQ , TNQ and TMQ in terms of number of tokens in the 878 queries: 1TQ TNQ TMQ || Q 25,861 231,834 3,412,512 Table-2: Query size in number of tokens As words occurring several times are reduced to word-frequency pairs, the size of the queries generated from the 100-best translation lists is only 9 times as big as the queries generated from the first-best translations. The queries generated from the translation model contain many more translation alternatives, summing up to almost 3.4 million tokens. Using the lattices the whole information of the translation model is kept. 4.3.1 Results for Query 1TQ In the first experiment we used the first-best translations to generate the queries. For each of the 4 corpora different numbers of similar sentences (1, 10, 100, and 1000) were retrieved to build specific language models. Figure-2 shows the language model adaptation after tuning the interpolation factor ? by a grid search over [0,1]. Typically ? is around 0.80. 1-Best/NIST Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline 1-Best/BLEU-Scores 0.1900 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure-2: NIST and Bleu scores 1TQ We see that each corpus gives an improvement over the baseline. The best NIST score is 7.94, and the best Bleu score is 0.2018. Both best scores are realized using top 100 relevant sentences corpus per source sentence mined from the AFE. 4.3.2 Results for Query TNQ Figure-3 shows the results for the query model TNQ . The best results are 7.99 NIST score, and 0.2022 Bleu score. These improvements are statistically significant. Both scores are achieved at the same settings as those in 1TQ , i.e. using top 100 retrieved relevant sentences mined from the AFE corpus. 100-Best/NIST-Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline 100-Best/BLEU-Scores 0.1900 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure-3: NIST and Bleu scores from TNQ Using the translation alternatives to retrieve the data for language model adaptation gives an improvement over using the first-best translation only for query construction. Using only one translation hypothesis to build an adapted language model has the tendency to reinforce that translation. 4.3.3 Results for Query TMQ The third bag-of-words query model uses all translation alternatives for source words and source phrases. Figure-4 shows the results of this query model TMQ . The best results are 7.91 NIST score and 0.1995 Bleu. For this query model best results were achieved using the top 1000 relevant sentences mined from the AFE corpus per source sentence. The improvement is not as much as the other two query models. The reason is probably that all translation alternatives, even wrong translations resulting from errors in the word and phrase alignment, contribute alike to retrieve similar sentences. Thereby, an adapted language model is built, which reinforces not only good translations, but also bad translations. All the three query models showed improvements over the baseline system in terms of NIST and Bleu scores. The best bag-of-words query model is TNQ built from the N-Best list. It provides a good balance between incorporating translation alternatives in the language model adaptation process and not reinforcing wrong translations. Lattice/NIST-Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Lattice/BLEU-Scores 0.1900 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure-4: NIST and Bleu scores from TMQ 4.4 Structured Query Models. The next series of experiments was done to study if using word order information in constructing the queries could help to generate more effective adapted language models. By using the structured query language we converted the same first-best hypothesis, the 100-best list, and the translation lattice into structured query models. Results are reported for the AFE corpus only, as this corpus gave best translation scores. Figure-5 shows the results for all three structured query models, built from the first-best hypothesis (?1-Best?), the 100 best hypotheses list (?100 Best?), and translation lattice (?TM-Lattice?). Using these query models, different numbers of most similar sentences, ranging from 100 to 4000, where retrieved from the AFE corpus. The given baseline results are the best results achieved from the corresponding bag-of-words query models. Consistent improvements were observed on NIST and Bleu scores. Again, optimal interpolation factors to interpolate the specific language models with the background language model were used, which typically were in the range of [0.6, 0.7]. Structured query models give most improvements when using more sentences for language model adaptation. The effect is more pronounced for Bleu then for NIST score. Structured query/NIST-Scores 7.7500 7.8000 7.8500 7.9000 7.9500 8.0000 8.0500 8.1000 8.1500 Baseline Top100 Top500 Top1000 Top2000 Top4000 1-Best 100-Best TM-Lattice Structured query/BLEU-Scores 0.1920 0.1940 0.1960 0.1980 0.2000 0.2020 0.2040 0.2060 0.2080 Baseline Top100 Top500 Top1000 Top2000 Top4000 1-Best 100-Best TM-Lattice Figure-5: NIST and Bleu scores from the structured query models The really interesting result is that the structured query model TMQ gives now the best translation results. Adding word order information to the queries obviously helps to reduce the noise in the retrieved data by selecting sentences, which are closer to the good translations, The best results using the adapted language models are NIST score 8.12 for using the 2000 most similar sentences, whereas Bleu score goes up to 0.2068 when using 4000 sentences for language model adaptation. 4.5 Example. Table-3 shows translation examples for the 17th Chinese sentence in the test set. We applied the baseline system (Base), the bag-of-word query model (Hyp1), and the structured query model (Hyp2) using AFE corpus. Ref The police has already blockade the scene of the explosion. Base At present, the police had cordoned off the explosion. Hyp1 At present, police have sealed off the explosion. Hyp2 Currently, police have blockade on the scene of the explosion. Table-3 Translation examples 4.6 Oracle Experiment Finally, we run an oracle experiments to see how much improvement could be achieved if we only selected better data for the specific language models. We converted the four available reference translations into structured query models and retrieved the top 4000 relevant sentences from AFE corpus for each source sentence. Using these language models, interpolated with the background language model gave a NIST score of 8.67, and a Bleu score of 0.2228. This result indicates that there is room for further improvements using this language model adaptation technique. The oracle experiment suggests that better initial translations lead to better language models and thereby better 2nd iteration translations. This lead to the question if we can iterate the retrieval process several times to get further improvement, or if the observed improvement results form using for (good) translations, which have more diversity than the translations in an n-best list. On the other side the oracle experiment also shows that the optimally expected improvement is limited by the translation model and decoding algorithm used in the current SMT system."
77,We present a system for the semantic role la beling task. The system combines a machine learning technique with an inference procedurebased on integer linear programming that supports the incorporation of linguistic and struc tural constraints into the decision process. Thesystem is tested on the data provided in CoNLL 2004 shared task on semantic role labeling and achieves very competitive results.,"We present a system for the semantic role la beling task. The system combines a machine learning technique with an inference procedurebased on integer linear programming that supports the incorporation of linguistic and struc tural constraints into the decision process. Thesystem is tested on the data provided in CoNLL 2004 shared task on semantic role labeling and achieves very competitive results. Semantic parsing of sentences is believed to be animportant task toward natural language understand ing, and has immediate applications in tasks such information extraction and question answering. We study semantic role labeling(SRL). For each verb in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles,such as Agent, Patient or Instrument, and their ad juncts, such as Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and Ma`rquez, 2003) which consists of a portion of thePropBank corpus, allowing us to compare the per formance of our approach with other systems. Previous approaches to the SRL task have madeuse of a full syntactic parse of the sentence in or der to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003;Pradhan et al, 2003; Pradhan et al, 2004; Sur deanu et al, 2003). In this work, following the CoNLL-2004 shared task definition, we assume thatthe SRL system takes as input only partial syn tactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of basedchunks and clauses (Tjong Kim Sang and Buch holz, 2000; Tjong Kim Sang and De?jean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expecthigh levels of performance from either purely man ual classifiers or purely learned classifiers. Rather, supplemental linguistic information must be used to support and correct a learning system. So far,machine learning approaches to SRL have incorpo rated linguistic information only implicitly, via theclassifiers? features. The key innovation in our ap proach is the development of a principled method tocombine machine learning techniques with linguistic and structural constraints by explicitly incorpo rating inference into the decision process. In the machine learning part, the system we present here is composed of two phases. First, a set of argument candidates is produced using twolearned classifiers?one to discover beginning po sitions and one to discover end positions of each argument type. Hopefully, this phase discovers a small superset of all arguments in the sentence (foreach verb). In a second learning phase, the candi date arguments from the first phase are re-scored using a classifier designed to determine argument type, given a candidate argument.Unfortunately, it is difficult to utilize global prop erties of the sentence into the learning phases.However, the inference level it is possible to incorporate the fact that the set of possible rolelabelings is restricted by both structural and lin guistic constraints?for example, arguments cannotstructurally overlap, or, given a predicate, some ar gument structures are illegal. The overall decision problem must produce an outcome that consistent with these constraints. We encode the constraints aslinear inequalities, and use integer linear programming(ILP) as an inference procedure to make a final decision that is both consistent with the con straints and most likely according to the learningsystem. Although ILP is generally a computationally hard problem, there are efficient implementations that can run on thousands of variables and constraints. In our experiments, we used the commer cial ILP package (Xpress-MP, 2003), and were able to process roughly twenty sentences per second. The goal of the semantic-role labeling task is to dis cover the verb-argument structure for a given input sentence. For example, given a sentence ? I left my pearls to my daughter-in-law in my will?, the goal is to identify different arguments of the verb left which yields the output:[A0 I] [V left ] [A1 my pearls] [A2 to my daughter in-law] [AM-LOC in my will]. Here A0 represents the leaver, A1 represents the thing left, A2 represents the benefactor, AM-LOC is an adjunct indicating the location of the action, and V determines the verb. Following the definition of the PropBank, and CoNLL-2004 shared task, there are six different types of arguments labelled as A0-A5 and AA. These labels have different semantics for each verbas specified in the PropBank Frame files. In addi tion, there are also 13 types of adjuncts labelled as AM-XXX where XXX specifies the adjunct type.In some cases, an argument may span over differ ent parts of a sentence, the label C-XXX is used to specify the continuity of the arguments, as shown in the example below. [A1 The pearls] , [A0 I] [V said] , [C-A1 were left to my daughter-in-law]. Moreover in some cases, an argument might be a relative pronoun that in fact refers to the actual agentoutside the clause. In this case, the actual agent is la beled as the appropriate argument type, XXX, while the relative pronoun is instead labeled as R-XXX. For example, [A1 The pearls] [R-A1 which] [A0 I] [V left] , [A2 to my daughter-in-law] are fake. See the details of the definition in Kingsbury and Palmer (2002) and Carreras and Ma`rquez (2003). Our semantic role labeling system consists of two phases. The first phase finds a subset of arguments from all possible candidates. The goal here is tofilter out as many as possible false argument candidates, while still maintaining high recall. The sec ond phase focuses on identifying the types of thoseargument candidates. Since the number of candi dates is much fewer, the second phase is able to use slightly complicated features to facilitate learning a better classifier. This section first introduces the learning system we use and then describes how we learn the classifiers in these two phases. 3.1 SNoW Learning Architecture. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learningtasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predic tions or argument type predictions, in this case) arerepresented as linear functions over a common fea ture space. It incorporates several improvements over the basic Winnow multiplicative update rule. In particular, a regularization term is added, which has the effect of trying to separate the data with a thick separator (Grove and Roth, 2001; Hang et al,2002). In the work presented here we use this regu larization with a fixed parameter. Experimental evidence has shown that SNoW activations are monotonic with the confidence in the prediction. Therefore, it can provide a goodsource of probability estimation. We use soft max (Bishop, 1995) over the raw activation values as conditional probabilities, and also the score of the target. Specifically, suppose the number of classes is n, and the raw activation values of class i is acti. The posterior estimation for class i is derived by the following equation. score(i) = pi = e acti ? 1?j?n eactj The score plays an important role in different places. For example, the first phase uses the scoresto decide which argument candidates should be filtered out. Also, the scores output by the second phase classifier are used in the inference procedure to reason for the best global labeling. 3.2 First Phase: Find Argument Candidates. The first phase is to predict the argument candidates of a given sentence that correspond to the active verb. Unfortunately, it turns out that it is difficult to predict the exact arguments accurately. Therefore, the goal here is to output a superset of the correct arguments by filtering out unlikely candidates.Specifically, we learn two classifiers, one to de tect beginning argument locations and the otherto detect end argument locations. Each multi class classifier makes predictions over forty-three classes?thirty-two argument types, ten continuousargument types, and one class to detect not begin ning/not end. Features used for these classifiers are: ? Word feature includes the current word, two words before and two words after. Part-of-speech tag (POS) feature includes the POS tags of all words in a window of size two. Chunk feature includes the BIO tags for chunks of all words in a window of size two. Predicate lemma & POS tag show the lemma form and POS tag of the active predicate. Voice feature is the voice (active/passive) of the current predicate. This is extracted with a simple rule: a verb is identified as passive if it follows a to-be verb in the same phrase chunk and its POS tag is VBN(past participle) or it immediately follows a noun phrase. Position feature describes if the current word is before or after the predicate. Chunk pattern encodes the sequence of chunks from the current words to the predicate. Clause tag indicates the boundary of clauses. Clause path feature is a path formed from a semi-parsed tree containing only clauses and chunks. Each clause is named with the chunk preceding it. The clause path is the path from predicate to target word in the semi-parse tree. Clause position feature is the position of the target word relative to the predicate in the semi-parse tree containing only clauses. Thereare four configurations ? target word and pred icate share the same parent, target word parent is an ancestor of predicate, predicate parent is an ancestor of target word, or otherwise.Because each argument consists of a single be ginning and a single ending, these classifiers can be used to construct a set of potential arguments (by combining each predicted begin with each predicted end after it of the same type). Although this phase identifies typed arguments (i.e. labeled with argument types), the second phasewill re-score each phrase using phrase-based classifiers ? therefore, the goal of the first phase is sim ply to identify non-typed phrase candidates. In thistask, we achieves 98.96% and 88.65% recall (overall, without verb) on the training and the develop ment set, respectively. Because these are the onlycandidates passed to the second phase, the final sys tem performance is upper-bounded by 88.65%. 3.3 Second Phase: Argument Classification. The second phase of our system assigns the final argument classes to (a subset) of the argument can didates supplied from the first phase. Again, theSNoW learning architecture is used to train a multi class classifier to label each argument to one of the argument types, plus a special class?no argument(null). Training examples are created from the argu ment candidates supplied from the first phase using the following features: ? Predicate lemma & POS tag, voice, position, clause Path, clause position, chunk pattern Same features as those in the first phase.? Word & POS tag from the argument, includ ing the first,last,and head1 word and tag.? Named entity feature tells if the target argu ment is, embeds, overlaps, or is embedded in a named entity with its type. Chunk tells if the target argument is, embeds, overlaps, or is embedded in a chunk with its type. Lengths of the target argument, in the numbers of words and chunks separately. Verb class feature is the class of the active predicate described in PropBank Frames. Phrase type uses simple heuristics to identify the target argument as VP, PP, or NP. Sub-categorization describes the phrase structure around the predicate. We separate the clause where the predicate is in into three parts?the predicate chunk, segments before and after the predicate, and use the sequence of phrase types of these three segments. Baseline features identified not in the main verb chunk as AM-NEG and modal verb in the main verb chunk as AM-MOD. Clause coverage describes how much of the local clause (from the predicate) is covered by the target argument.? Chunk pattern length feature counts the num ber of patterns in the argument.? Conjunctions join every pair of the above fea tures as new features. Boundary words & POS tag include twowords/tags before and after the target argu ment. Bigrams are pairs of words/tags in the window from two words before the target to the first word of the target, and also from the last word to two words after the argument. 1We use simple rules to first decide if a candidate phrase type is VP, NP, or PP. The headword of an NP phrase is the right-most noun. Similarly, the left-most verb/proposition of a VP/PP phrase is extracted as the headword ? Sparse collocation picks one word/tag from the two words before the argument, the first word/tag, the last word/tag of the argument, and one word/tag from the two words after the argument to join as features. Although the predictions of the second-phaseclassifier can be used directly, the labels of argu ments in a sentence often violate some constraints. Therefore, we rely on the inference procedure to make the final predictions. Ideally, if the learned classifiers are perfect, arguments can be labeled correctly according to the classifiers? predictions. In reality, labels assigned to ar guments in a sentence often contradict each other,and violate the constraints arising from the struc tural and linguistic information. In order to resolve the conflicts, we design an inference procedure thattakes the confidence scores of each individual argument given by the second-phase classifier as in put, and outputs the best global assignment that also satisfies the constraints. In this section we firstintroduce the constraints and the inference prob lem in the semantic role labeling task. Then, wedemonstrate how we apply integer linear program ming(ILP) to reason for the global label assignment. 4.1 Constraints over Argument Labeling. Formally, the argument classifier attempts to assign labels to a set of arguments, S1:M , indexed from 1 to M . Each argument Si can take any label from a set of argument labels, P , and the indexed set of arguments can take a set of labels, c1:M ? PM . If we assume that the classifier returns a score, score(Si = ci), corresponding to the likelihood ofseeing label ci for argument Si, then, given a sentence, the unaltered inference task is solved by max imizing the overall score of the arguments, c?1:M = argmax c1:M?PM score(S1:M = c1:M ) = argmax c1:M?PM M? i=1 score(Si = ci). (1) In the presence of global constraints derived from linguistic information and structural considerations,our system seeks for a legitimate labeling that max imizes the score. Specifically, it can be viewed asthe solution space is limited through the use of a filter function, F , that eliminates many argument labelings from consideration. It is interesting to con trast this with previous work that filters individual phrases (see (Carreras and Ma`rquez, 2003)). Here, we are concerned with global constraints as well as constraints on the arguments. Therefore, the final labeling becomes c?1:M = argmax c1:M?F(PM ) M? i=1 score(Si = ci) (2)The filter function used considers the following con straints: 1. Arguments cannot cover the predicate except. those that contain only the verb or the verb and the following word. 2. Arguments cannot overlap with the clauses. (they can be embedded in one another). 3. If a predicate is outside a clause, its arguments. cannot be embedded in that clause. 4. No overlapping or embedding arguments.. 7. If there is C-V, then there should be a sequence. of consecutive V, A1, and C-V pattern. For ex ample, when split is the verb in ?split it up?, the A1 argument is ?it? and C-V argument is ?up?. 8. If there is an R-XXX argument, then there has. to be an XXX argument. That is, if an ar gument is a reference to some other argument XXX, then this referenced argument must exist in the sentence. 9. If there is a C-XXX argument, then there has. to be an XXX argument; in addition, the C XXX argument must occur after XXX. This is stricter than the previous rule because the order of appearance also needs to be considered. 10. Given the predicate, some argument classes. are illegal (e.g. predicate ?stalk? can take only A0 or A1). This linguistic information can be found in PropBank Frames. We reformulate the constraints as linear (in)equalities by introducing indicator variables. The optimization problem (Eq. 2) is solved using ILP. 4.2 Using Integer Linear Programming. As discussed previously, a collection of potential arguments is not necessarily a valid semantic label ing since it must satisfy all of the constraints. In this context, inference is the process of finding the best (according to Equation 1) valid semantic labels that satisfy all of the specified constraints. We take a similar approach that has been previously used for entity/relation recognition (Roth and Yih, 2004), and model this inference procedure as solving an ILP. An integer linear program(ILP) is basically the same as a linear program. The cost function and the (in)equality constraints are all linear in terms of thevariables. The only difference in an ILP is the vari ables can only take integers as their values. In our inference problem, the variables are in fact binary. A general binary integer programming problem can be stated as follows. Given a cost vector p ? <d, a set of variables, z = (z1, . . . , zd) and cost matrices C1 ? <t1 ? <d,C2 ? <t2?<d , where t1 and t2 are the numbers of inequality and equality constraints and d is the number of binary variables. The ILP solution z? is the vector that maximizes the cost function, z? = argmax z?{0,1}d p ? z, subject to C1z ? b1, and C2z = b2, where b1,b2 ? <d, and for all z ? z, z ? {0, 1}.To solve the problem of Equation 2 in this set ting, we first reformulate the original cost function?Mi=1 score(Si = ci) as a linear function over sev eral binary variables, and then represent the filter function F using linear inequalities and equalities. We set up a bijection from the semantic labeling to the variable set z. This is done by setting z to a set of indicator variables. Specifically, let zic = [Si = c] be the indicator variable that represents whether or not the argument type c is assigned to Si, and let pic = score(Si = c). Equation 1 can then be written as an ILP cost function as argmax z?{0,1}d M? i=1 |P|? c=1 piczic, subject to |P|? c=1 zic = 1 ?zic ? z, which means that each argument can take only one type. Note that this new constraint comes from thevariable transformation, and is not one of the con straints used in the filter function F .Constraints 1 through 3 can be evaluated on a per argument basis ? the sake of efficiency, arguments that violate these constraints are eliminated even before given the second-phase classifier. Next, we show how to transform the constraints in the filter function into the form of linear (in)equalities over z, and use them in this ILP setting. Constraint 4: No overlapping or embedding If arguments Sj1 , . . . , Sjk occupy the same word in asentence, then this constraint restricts only one ar guments to be assigned to an argument type. In other words, k ? 1 arguments will be the special class null, which means the argument candidate is not a legitimate argument. If the special class null is represented by the symbol ?, then for every set ofsuch arguments, the following linear equality repre sents this constraint. k? i=1 zji? = k ? 1 Constraint 5: No duplicate argument classesWithin the same sentence, several types of argu ments cannot appear more than once. For example, a predicate can only take one A0. This constraint can be represented using the following inequality. M? i=1 ziA0 ? 1 Constraint 6: Exactly one V argument For each verb, there is one and has to be one V argument,which represents the active verb. Similarly, this con straint can be represented by the following equality. M? i=1 ziV = 1Constraint 7: V?A1?C-V pattern This constraint is only useful when there are three consec utive candidate arguments in a sentence. Suppose arguments Sj1 , Sj2 , Sj3 are consecutive. If Sj3 isC-V, then Sj1 and Sj2 have to be V and A1, respec tively. This if-then constraint can be represented by the following two linear inequalities. zj3C-V ? zj1V, and zj3C-V ? zj2A1 Constraint 8: R-XXX arguments Suppose the referenced argument type is A0 and the reference type is R-A0. The linear inequalities that represent this constraint are: ?m ? {1, . . . ,M} : M? i=1 ziA0 ? zmR-A0 If there are ? reference argument pairs, then the total number of inequalities needed is ?M .Constraint 9: C-XXX arguments This constraint is similar to the reference argument constraints. The difference is that the continued argu ment XXX has to occur before C-XXX. Assumethat the argument pair is A0 and C-A0, and argu ment Sji appears before Sjk if i ? k. The linear inequalities that represent this constraint are: ?m ? {2, . . . ,M} : j?1? i=1 zjiA0 ? zmR-A0 Constraint 10: Illegal argument types Given aspecific verb, some argument types should never oc cur. For example, most verbs don?t have arguments A5. This constraint is represented by summing all the corresponding indicator variables to be 0. M? i=1 ziA5 = 0Using ILP to solve this inference problem en joys several advantages. Linear constraints are very general, and are able to represent many types of constraints. Previous approaches usually relyon dynamic programming to resolve non over lapping/embedding constraints (i.e., Constraint 4)when the data is sequential, but are unable to han dle other constraints. The ILP approach is flexibleenough to handle constraints regardless of the structure of the data. Although solving an ILP problem is NP-hard, with the help of todays commer cial numerical packages, this problem can usually be solved very fast in practice. For instance, it onlytakes about 10 minutes to solve the inference prob lem for 4305 sentences on a Pentium-III 800 MHz machine in our experiments. Note that ordinarysearch methods (e.g., beam search) are not neces sarily faster than solving an ILP problem and do not guarantee the optimal solution. 5 Experimental Results. The system is evaluated on the data provided in the CoNLL-2004 semantic-role labeling shared task which consists of a portion of PropBank corpus.The training set is extracted from TreeBank (Mar cus et al, 1993) section 15?18, the development set,used in tuning parameters of the system, from sec tion 20, and the test set from section 21. We first compare this system with the basic taggerthat we have, the CSCL shallow parser from (Punyakanok and Roth, 2001), which is equivalent to us ing the scoring function from the first phase with only the non-overlapping/embedding constraints. In Prec. Rec. F?=1 1st-phase, non-overlap 70.54 61.50 65.71 1st-phase, All Const. 70.97 60.74 65.46 2nd-phase, non-overlap 69.69 64.75 67.13 2nd-phase, All Const. 71.96 64.93 68.26 Table 1: Summary of experiments on the development set. All results are for overall performance. Precision Recall F?=1 Without Inference 86.95 87.24 87.10 With Inference 88.03 88.23 88.13 Table 2: Results of second phase phrase prediction and inference assuming perfect boundary detection inthe first phase. Inference improves performance by re stricting label sequences rather than restricting structuralproperties since the correct boundaries are given. All re sults are for overall performance on the development set. addition, we evaluate the effectiveness of using only this constraint versus all constraints, as in Sec. 4. Table 1 shows how additional constraints over thestandard non-overlapping constraints improve per formance on the development set. The argument scoring is chosen from either the first phase or the second phase and each is evaluated by considering simply the non-overlapping/embedding constraint or the full set of linguistic constraints. To make a fair comparison, parameters were set separately to optimize performance when using the first phase results. In general, using all constraints increasesF?=1 by about 1% in this system, but slightly de creases the performance when only the first phaseclassifier is used. Also, using the two-phase archi tecture improves both precision and recall, and the enhancement reflected in F?=1 is about 2.5%. It is interesting to find out how well the secondphase classifier can perform given perfectly seg mented arguments. This evaluates the quality of the argument classifier, and also provides a conceptual upper bound. Table 2 first shows the results without using inference (i.e. F(PM ) = PM ). The secondrow shows adding inference to the phrase classifica tion can further improve F?=1 by 1%. Finally, the overall result on the official test set is given in Table 3. Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. For a fair comparison, our system was among the best at CoNLL-04, where the best system (Hacioglu et al, 2004) achieve a 69.49 F1 score. We show that linguistic information is useful for se mantic role labeling, both in extracting features and Dist. Prec. Rec. F?=1 Overall 100.00 70.07 63.07 66.39 A0 26.87 81.13 77.70 79.38 A1 35.73 74.21 63.02 68.16 A2 7.44 54.16 41.04 46.69 A3 1.56 47.06 26.67 34.04 A4 0.52 71.43 60.00 65.22 AM-ADV 3.20 39.36 36.16 37.69 AM-CAU 0.51 45.95 34.69 39.53 AM-DIR 0.52 42.50 34.00 37.78 AM-DIS 2.22 52.00 67.14 58.61 AM-EXT 0.15 46.67 50.00 48.28 AM-LOC 2.38 33.47 34.65 34.05 AM-MNR 2.66 45.19 36.86 40.60 AM-MOD 3.51 92.49 94.96 93.70 AM-NEG 1.32 85.92 96.06 90.71 AM-PNC 0.89 32.79 23.53 27.40 AM-TMP 7.78 59.77 56.89 58.30 R-A0 1.66 81.33 76.73 78.96 R-A1 0.73 58.82 57.14 57.97 R-A2 0.09 100.00 22.22 36.36 R-AM-TMP 0.15 54.55 42.86 48.00 Table 3: Results on the test set. deriving hard constraints on the output. We also demonstrate that it is possible to use integer linear programming to perform inference that incorporates a wide variety of hard constraints, which would be difficult to incorporate using existing methods. In addition, we provide further evidence supporting the use of scoring arguments over scoring argument boundaries for complex tasks. In the future, we planto use the full PropBank corpus to see the improvement when more training data is provided. In addition, we would like to explore the possibility of integer linear programming approach using soft constraints. As more constraints are considered, we ex pect the overall performance to improve.","We show that linguistic information is useful for se mantic role labeling, both in extracting features and Dist. Prec. Rec. F?=1 Overall 100.00 70.07 63.07 66.39 A0 26.87 81.13 77.70 79.38 A1 35.73 74.21 63.02 68.16 A2 7.44 54.16 41.04 46.69 A3 1.56 47.06 26.67 34.04 A4 0.52 71.43 60.00 65.22 AM-ADV 3.20 39.36 36.16 37.69 AM-CAU 0.51 45.95 34.69 39.53 AM-DIR 0.52 42.50 34.00 37.78 AM-DIS 2.22 52.00 67.14 58.61 AM-EXT 0.15 46.67 50.00 48.28 AM-LOC 2.38 33.47 34.65 34.05 AM-MNR 2.66 45.19 36.86 40.60 AM-MOD 3.51 92.49 94.96 93.70 AM-NEG 1.32 85.92 96.06 90.71 AM-PNC 0.89 32.79 23.53 27.40 AM-TMP 7.78 59.77 56.89 58.30 R-A0 1.66 81.33 76.73 78.96 R-A1 0.73 58.82 57.14 57.97 R-A2 0.09 100.00 22.22 36.36 R-AM-TMP 0.15 54.55 42.86 48.00 Table 3: Results on the test set. deriving hard constraints on the output. We also demonstrate that it is possible to use integer linear programming to perform inference that incorporates a wide variety of hard constraints, which would be difficult to incorporate using existing methods. In addition, we provide further evidence supporting the use of scoring arguments over scoring argument boundaries for complex tasks. In the future, we planto use the full PropBank corpus to see the improvement when more training data is provided. In addition, we would like to explore the possibility of integer linear programming approach using soft constraints. As more constraints are considered, we ex pect the overall performance to improve."
78,"This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.","This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.","This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP."
79,"We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework.","We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework. The empiricist revolution in computational linguistics has dramatically shifted the accepted boundary between what kinds of knowledge are best supplied by humans and what kinds are best learned from data, with much of the human supplied knowledge now being in the form of annotations of data. As we look to the future, we expect that relatively unsupervised methods will grow in applicability, reducing the need for expensive human annotation of data. With respect to part-of-speech tagging, we believe that the way forward from the relatively small number of languages for which we can currently identify parts of speech in context with reasonable accuracy will make use of unsupervised methods that require only an untagged corpus and a lexicon of words and their possible parts of speech. We believe this based on the fact that such lexicons exist for many more languages (in the form of conventional dictionaries) than extensive human-tagged training corpora exist for. Unsupervised part-of-speech tagging, as defined above, has been attempted using a variety of learning algorithms (Brill 1995, Church, 1988, Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, Merialdo 1991). While this makes unsupervised part-of-speech tagging a relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners. In this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging. In addition, we explore two new ideas for improving tagging accuracy. First, we explore an HMM approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on building bidirectionality into graphical models (Lafferty et. al. 2001, Toutanova et. al. 2003). Second we describe a method for sequential unsupervised training of tag sequence and lexical probabilities in an HMM, which we observe leads to improved accuracy over simultaneous training with certain types of models. In section 2, we provide a brief description of the methods we evaluate and review published results. Section 3 describes the contextualized variation on HMM tagging that we have explored. In Section 4 we provide a direct comparison of several unsupervised part-of-speech taggers, which is followed by Section 5, in which we present a new method for training with suboptimal lexicons. In section 6, we revisit our new approach to HMM tagging, this time, in the supervised framework. A common formulation of an unsupervised part-of speech tagger takes the form of a hidden Markov model (HMM), where the states correspond to part-of-speech tags, ti, and words, wi, are emitted each time a state is visited. The training of HMM? based taggers involves estimating lexical probabilities, P(wi|ti), and tag sequence probabilities, P(ti | ti-1 ... ti-n). The ultimate goal of HMM training is to find the model that maximizes the probability of a given training text, which can be done easily using the forward-backward, or Baum-Welch algorithm (Baum et al1970, Bahl, Jelinek and Mercer, 1983). These model probabilities are then used in conjunction with the Viterbi algorithm (Viterbi, 1967) to find the most probable sequence of part-of-speech tags for a given sentence. When estimating tag sequence probabilities, an HMM tagger, such as that described in Merialdo (1991), typically takes into account a history consisting of the previous two tags -- e.g. we compute P(ti | ti-1, ti-2). Kupiec (1992) describes a modified trigram HMM tagger in which he computes word classes for which lexical probabilities are then estimated, instead of computing probabilities for individual words. Words contained within the same equivalence classes are those which possess the same set of possible parts of speech. Another highly-accurate method for part-of speech tagging from unlabelled data is Brill?s unsupervised transformation-based learner (UTBL) (Brill, 1995). Derived from his supervised transformation-based tagger (Brill, 1992), UTBL uses information from the distribution of unambiguously tagged data to make informed labeling decisions in ambiguous contexts. In contrast to the HMM taggers previously described, which make use of contextual information coming from the left side only, UTBL considers both left and right contexts. Reported tagging accuracies for these methods range from 87% to 96%, but are not directly comparable. Kupiec?s HMM class-based tagger, when trained on a sample of 440,000 words of the original Brown corpus, obtained a test set accuracy of 95.7%. Brill assessed his UTBL tagger using 350,000 words of the Brown corpus for training, and found that 96% of words in a separate 200,000-word test set could be tagged correctly. Furthermore, he reported test set accuracy of 95.1% for the UTBL tagger trained on 120,000 words of Penn Treebank and tested on a separate test set of 200,000 words taken from the same corpus. Finally, using 1 million words from the Associated Press for training, Merialdo?s trigram tagger was reported to have an accuracy of 86.6%. This tagger was assessed using a tag set other than that which is employed by the Penn Treebank. Unfortunately none of these results can be directly compared to the others, as they have used different, randomized and irreproducible splits of training and test data (Brill and Kupiec), different tag sets (Merialdo) or different corpora altogether. The HMM taggers we have discussed so far are similar in that they use condition only on left context when estimating probabilities of tag sequences. Recently, Toutanova et al (2003) presented a supervised conditional Markov Model part-of-speech tagger (CMM) which exploited information coming from both left and right contexts. Accuracy on the Penn Treebank using two tags to the left as features in addition to the current tag was 96.10%. When using tag to the left and tag to the right as features in addition to the current tag, accuracy improved to 96.55%. Lafferty et al (2001) also compared the accuracies of several supervised part-of-speech tagging models, while examining the effect of directionality in graphical models. Using a 50% 50% train-test split of the Penn Treebank to assess HMMs, maximum entropy Markov models (MEMMs) and conditional random fields (CRFs), they found that CRFs, which make use of observation features from both the past and future, outperformed HMMs which in turn outperformed MEMMs. In a traditional HMM tagger, the probability of transitioning into a state representing tag ti is computed based on the previous two tags ti-1 and ti 2, and the probability of a word wi is conditioned only on the current tag ti. This formulation ignores dependencies that may exist between a word and the part-of-speech tags of the words which precede and follow it. For example, verbs which subcategorize strongly for a particular part-of speech but can also be tagged as nouns or pronouns (e.g. ?thinking that?) may benefit from modeling dependencies on future tags. To model this relationship, we now estimate the probability of a word wi based on tags ti-1 and ti-+1. This change in structure, which we will call a contextualized HMM, is depicted in Figure 1. This type of structure is analogous to context-dependent phone models used in acoustic modeling for speech recognition (e.g.Young, 1999, Section 4.3). 3.1 Model Definition. In order to build both left and right-context into an HMM part-of-speech tagger, we reformulate the Figure 1: Graphical Structure of Traditional HMM Tagger (top) and Contextualized HMM Tagger (bottom) trigram HMM model traditionally described as ? = ???? ?= n i iiiiiiiii twtwtpttwtwwpTWp 1 111111 )..|()...|(),( by replacing the approximation: )|()..|( )|()...|( 1211 1111 ???? = = iiiiiiii iiiii tttptwtwtp twptwtwwp with the approximation: )|()..|( )|()...|( 1211 111111 ???? = = iiiiiiii iiiiiii tttptwtwtp tttwptwtwwp Given that we are using an increased context size during the estimation of lexical probabilities, thus fragmenting the data, we have found it desirable to smooth these estimates, for which we use a standard absolute discounting scheme (Ney, Essen and Knesser, 1994). 4.1 Corpora and Lexicon Construction. For our comparison of unsupervised tagging methods, we implemented the HMM taggers described in Merialdo (1991) and Kupiec (1992), as well as the UTBL tagger described in Brill (1995). We also implemented a version of the contextualized HMM using the type of word classes utilized in the Kupiec model. The algorithms were trained and tested using version 3 of the Penn Treebank, using the training, development, and test split described in Collins (2002) and also employed by Toutanova et al (2003) in testing their supervised tagging algorithm. Specifically, we allocated sections 00 18 for training, 19-21 for development, and 22-24 for testing. To avoid the problem of unknown words, each learner was provided with a lexicon constructed from tagged versions of the full Treebank. We did not begin with any estimates of the likelihoods of tags for words, but only the knowledge of what tags are possible for each word in the lexicon, i.e., something we could obtain from a manually-constructed dictionary. 4.2 The Effect of Lexicon Construction on. Tagging Accuracy To our surprise, we found initial tag accuracies of all methods using the full lexicon extracted from the Penn Treebank to be significantly lower than previously reported. We discovered this was due to several factors. One issue we noticed which impacted tagging accuracy was that of a frequently occurring word (a) The/VB Lyneses/NNP ,/, of/IN Powder/NNP Springs/NNP ,/, Ga./NNP ,/, have/VBP filed/VBN suit/NN in/IN Georgia/NNP state/NN court/NN against/IN Stuart/NNP James/NNP ,/, *-1/-NONE- alleging/VBG fraud/NN ./. (b) Last/JJ week/NN CBS/NNP Inc./NNP cancelled/VBD ``/`` The/NNP People/NNP Next/NNP Door/NNP ./. ''/'' (c) a/SYM -/: Discounted/VBN rate/NN ./. Figure 2: Manually-Tagged Examples being mistagged during Treebank construction, as shown in the example in Figure 2a. Since we are not starting out with any known estimates for probabilities of tags given a word, the learner considers this tag to be just as likely as the word?s other, more probable, possibilities. In another, more frequently occurring scenario, human annotators have chosen to tag all words in multi word names, such as titles, with the proper-noun tag, NNP (Figure 2b). This has the effect of adding noise to the set of tags for many closed-class words. Finally, we noticed that a certain number of frequently occurring words (e.g. a, to, of) are sometimes labeled with infrequently occurring tags (e.g. SYM, RB), as exemplified in Figure 2c. In the case of the HMM taggers, where we begin with uniform estimates of both the state transition probabilities and the lexical probabilities, the learner finds it difficult to distinguish between more and less probable tag assignments. We later discovered that previous implementations of UTBL involved limiting which possible part of speech assignments were placed into the lexicon1, which was not explicitly detailed in the published reports. We then simulated, in a similar fashion, the construction of higher quality lexicons by using relative frequencies of tags for each word from the tagged Treebank to limit allowable word-tag assignments. That is, tags that appeared the tag of a particular word less than X% of the time were omitted from the set of possible tags for that word. We varied this threshold until accuracy did not significantly change on our set of heldout data. The effect of thresholding tags based on relative frequency in the training set is shown for our set of part-of-speech taggers in the curve in Figure 3. As shown in Table 1, the elimination of noisy possible part-of-speech assignments raised accuracy back into the realm of previously published results. The best test set accuracies for the learners in the class of HMM taggers are 1 Eric Brill, Personal Communication 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0 0.1 0.2 0.3 Threshold Ta g A cc u ra c y Merialdo Trigram Contextual Trigram Kupiec Trigram UTBL Figure 3: The effect of lexicon construction on unsupervised part-of-speech taggers 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0 1 2 3 4 5 Iteration Ta g A cc u ra cy Contextual Trigram Kupiec Trigram Merialdo Trigram Figure 4: Test Accuracy of HMMs using Optimzed Lexicons plotted against the number of training iterations in Figure 4. Lexicons While placing informed limitations on the tags that can be included in a lexicon can dramatically improve results, it is dependent on some form of supervision ? either from manually tagged data or by a human editor who post-filters an automatically constructed list. In the interest of being as unsupervised as possible, we sought to find a way to cope with the noisy aspects of the unfiltered lexicon described in the previous section. We suspected that in order to better control the training of lexical probabilities, having a stable model of state transition probabilities would be of help. We stabilized this model in two ways. Unfiltered Lexicon Optimized Lexicon Merialdo HMM 71.9 93.9 Contextualized HMM 76.9 94.0 Kupiec HMM 77.1 95.9 UTBL 77.2 95.9 Contextualized HMM with Classes 77.2 95.9 Table 1: Tag Accuracy of Unsupervised POS Taggers 5.1 Using Unambiguous Tag Sequences To. Initialize Contextual Probabilities First, we used our unfiltered lexicon along with our tagged corpus to extract non-ambiguous tag sequences. Specifically, we looked for trigrams in which all words contained at most one possible part-of-speech tag. We then used these n-grams and their counts to bias the initial estimates of state transitions in the HMM taggers. This approach is similar to that described in Ratnaparhki (1998), who used unambiguous phrasal attachments to train an unsupervised prepositional phrase attachment model. 5.2 HMM Model Training Revised. Second, we revised the training paradigm for HMMs, in which lexical and transition probabilities are typically estimated simultaneously. We decided to train the transition model probabilities first, keeping the lexical probabilities constant and uniform. Using the estimates initially biased by the method previously mentioned, we train the transition model until it reaches convergence on a heldout set. We then use this model, keeping it fixed, to train the lexical probabilities, until they eventually converge on heldout data. 5.3 Results. We implemented this technique for the Kupiec, Merialdo and Contextualized HMM taggers. From our training data, we were able to extract data for on the order of 10,000 unique unambiguous tag sequences which were then be used for better initializing the state transition probabilities. As shown in Table 2, this method improved tagging accuracy of the Merialdo and contextual taggers over traditional simultaneous HMM training, reducing error by 0.4 in the case of Merialdo and 0.7 for the contextual HMM part-of-speech tagger. HMM Tagger Simultaneous Model Training Sequential Model Training Merialdo 93.9 94.3 Contextualized 94.0 94.7 Kupiec 95.9 95.9 Table 2: Effects of HMM Training on Tagger Accuracy In this paradigm, tagging accuracy of the Kupiec HMM did not change. As one more way to assess the potential benefit from using left and right context in an HMM tagger, we tested our tagging model in the supervised framework, using the same sections of the Treebank previously allocated for unsupervised training, development and testing. In addition to comparing against a baseline tagger, which always chooses a word?s most frequent tag, we implemented and trained a version of a standard HMM trigram tagger. For further comparison, we evaluated these part of speech taggers against Toutanova et als supervised dependency-network based tagger, which currently achieves the highest accuracy on this dataset to date. The best result for this tagger, at 97.24%, makes use of both lexical and tag features coming from the left and right sides of the target. We also chose to examine this tagger?s results when using only <ti, t i-1, t i+1> as feature templates, which represents the same amount of context built into our contextualized tagger. As shown in Table 3, incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%, relatively reducing error rate by 17.4%. With the contextualized tagger we witness a small improvement in accuracy over the current state of the art when using the same amount of context. It is important to note that this accuracy can be obtained without the intensive training required by Toutanova et. al?s log-linear models. This result falls only slightly below the full-blown training intensive dependency-based conditional model. We have presented a comprehensive evaluation of several methods for unsupervised part-of-speech tagging, comparing several variations of hidden Markov model taggers and unsupervised transformation-based learning using the same corpus and same lexicons. We discovered that the Supervised Tagger Test Accuracy Baseline 92.19 Standard HMM 95.87 Contextualized HMM 96.59 Dependency Using LR tag features 96.55 Dependency Best Feature Set 97.24 Table 3: Comparison of Supervised Taggers quality of the lexicon made available to unsupervised learner made the greatest difference to tagging accuracy. Filtering the possible part-of speech assignments contained in a basic lexicon automatically constructed from the commonly used Penn Treebank improved results by as much as 22%. This finding highlights the importance of the need for clean dictionaries whether they are constructed by hand or automatically when we seek to be fully unsupervised. In addition, we presented a variation on HMM model training in which the tag sequence and lexical probabilities are estimated in sequence. This helped stabilize training when estimation of lexical probabilities can be noisy. Finally, we experimented with using left and right context in the estimation of lexical probabilities, which we refer to as a contextualized HMM. Without supervision, this new HMM structure improved results slightly compared to a simple trigram tagger as described in Merialdo, which takes into account only the current tag in predicting the lexical item. With supervision, this model achieves state of the art results without the lengthy training procedure involved in other high performing models. In the future, we will consider making an increase the context-size, which helped Toutanova et al (2003).","We have presented a comprehensive evaluation of several methods for unsupervised part-of-speech tagging, comparing several variations of hidden Markov model taggers and unsupervised transformation-based learning using the same corpus and same lexicons. We discovered that the Supervised Tagger Test Accuracy Baseline 92.19 Standard HMM 95.87 Contextualized HMM 96.59 Dependency Using LR tag features 96.55 Dependency Best Feature Set 97.24 Table 3: Comparison of Supervised Taggers quality of the lexicon made available to unsupervised learner made the greatest difference to tagging accuracy. Filtering the possible part-of speech assignments contained in a basic lexicon automatically constructed from the commonly used Penn Treebank improved results by as much as 22%. This finding highlights the importance of the need for clean dictionaries whether they are constructed by hand or automatically when we seek to be fully unsupervised. In addition, we presented a variation on HMM model training in which the tag sequence and lexical probabilities are estimated in sequence. This helped stabilize training when estimation of lexical probabilities can be noisy. Finally, we experimented with using left and right context in the estimation of lexical probabilities, which we refer to as a contextualized HMM. Without supervision, this new HMM structure improved results slightly compared to a simple trigram tagger as described in Merialdo, which takes into account only the current tag in predicting the lexical item. With supervision, this model achieves state of the art results without the lengthy training procedure involved in other high performing models. In the future, we will consider making an increase the context-size, which helped Toutanova et al (2003)."
80,"Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms. In the past, the four tasks have been treated independently, using a widevariety of algorithms. These four seman tic classes, however, are a tiny sample of the full range of semantic phenomena, andwe cannot afford to create ad hoc algo rithms for each semantic phenomenon; weneed to seek a unified approach. We propose to subsume a broad range of phenom ena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.","Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms. In the past, the four tasks have been treated independently, using a widevariety of algorithms. These four seman tic classes, however, are a tiny sample of the full range of semantic phenomena, andwe cannot afford to create ad hoc algo rithms for each semantic phenomenon; weneed to seek a unified approach. We propose to subsume a broad range of phenom ena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology. A pair of words (petrify:stone) is analogous to another pair (vaporize:gas) when the semantic re lations between the words in the first pair are highly similar to the relations in the second pair. Two words (levied and imposed) are synonymousin a context (levied a tax) when they can be interchanged (imposed a tax), they are are antony mous when they have opposite meanings (black c ? 2008, National Research Council of Canada (NRC).Licensed to the Coling 2008 Organizing Committee for pub lication in Coling 2008 and for re-publishing in any form or medium. and white), and they are associated when they tend to co-occur (doctor and hospital).On the surface, it appears that these are four distinct semantic classes, requiring distinct NLP al gorithms, but we propose a uniform approach to all four. We subsume synonyms, antonyms, and associations under analogies. In essence, we say that X and Y are antonyms when the pair X:Y is analogous to the pair black:white, X and Y are synonyms when they are analogous to the pair levied:imposed, and X and Y are associated when they are analogous to the pair doctor:hospital. There is past work on recognizing analogies(Reitman, 1965), synonyms (Landauer and Dumais, 1997), antonyms (Lin et al, 2003), and asso ciations (Lesk, 1969), but each of these four tasks has been examined separately, in isolation from the others. As far as we know, the algorithm proposed here is the first attempt to deal with all four tasks using a uniform approach. We believe that it isimportant to seek NLP algorithms that can han dle a broad range of semantic phenomena, becausedeveloping a specialized algorithm for each phe nomenon is a very inefficient research strategy.It might seem that a lexicon, such as Word Net (Fellbaum, 1998), contains all the information we need to handle these four tasks. However, weprefer to take a corpus-based approach to seman tics. Veale (2004) used WordNet to answer 374 multiple-choice SAT analogy questions, achievingan accuracy of 43%, but the best corpus-based ap proach attains an accuracy of 56% (Turney, 2006). Another reason to prefer a corpus-based approachto a lexicon-based approach is that the former re quires less human labour, and thus it is easier to extend to other languages.In Section 2, we describe our algorithm for rec ognizing analogies. We use a standard supervised 905 machine learning approach, with feature vectorsbased on the frequencies of patterns in a large cor pus. We use a support vector machine (SVM) to learn how to classify the feature vectors (Platt, 1998; Witten and Frank, 1999). Section 3 presents four sets of experiments. We apply our algorithm for recognizing analogies to multiple-choice analogy questions from the SAT college entrance test, multiple-choice synonym questions from the TOEFL (test of English as aforeign language), ESL (English as a second language) practice questions for distinguishing syn onyms and antonyms, and a set of word pairs thatare labeled similar, associated, and both, devel oped for experiments in cognitive psychology.We discuss the results of the experiments in Section 4. The accuracy of the algorithm is competitive with other systems, but the strength of the al gorithm is that it is able to handle all four tasks, with no tuning of the learning parameters to the particular task. It performs well, although it iscompeting against specialized algorithms, devel oped for single tasks.Related work is examined in Section 5 and limitations and future work are considered in Sec tion 6. We conclude in Section 7. An analogy, A:B::C:D, asserts that A is to B as C is to D; for example, traffic:street::water:riverbed asserts that traffic is to street as water is to riverbed; that is, the semantic relations between traffic and street are highly similar to the semantic relations between water and riverbed. We may view the task of recognizing word analogies as a problem of classifying word pairs (see Table 1). Word pair Class label carpenter:wood artisan:material mason:stone artisan:material potter:clay artisan:material glassblower:glass artisan:material traffic:street entity:carrier water:riverbed entity:carrier packets:network entity:carrier gossip:grapevine entity:carrierTable 1: Examples of how the task of recogniz ing word analogies may be viewed as a problem of classifying word pairs. We approach this as a standard classificationproblem for supervised machine learning. The al gorithm takes as input a training set of word pairs with class labels and a testing set of word pairs without labels. Each word pair is represented as a vector in a feature space and a supervised learning algorithm is used to classify the feature vectors. The elements in the feature vectors are based on the frequencies of automatically defined patterns in a large corpus. The output of the algorithm is anassignment of labels to the word pairs in the test ing set. For some of the experiments, we selecta unique label for each word pair; for other ex periments, we assign probabilities to each possible label for each word pair. For a given word pair, such as mason:stone, the first step is to generate morphological variations,such as masons:stones. In the following experi ments, we use morpha (morphological analyzer)and morphg (morphological generator) for mor phological processing (Minnen et al, 2001).1 The second step is to search in a large corpus for all phrases of the following form: ?[0 to 1 words] X [0 to 3 words] Y [0 to 1 words]? In this template, X:Y consists of morphologicalvariations of the given word pair, in either order; for example, mason:stone, stone:mason, masons:stones, and so on. A typical phrase for ma son:stone would be ?the mason cut the stone with?. We then normalize all of the phrases that are found, by using morpha to remove suffixes. The template we use here is similar to Turney (2006), but we have added extra context wordsbefore the X and after the Y . Our morpholog ical processing also differs from Turney (2006).In the following experiments, we search in a cor pus of 5 ? 1010 words (about 280 GB of plain text), consisting of web pages gathered by a web crawler.2 To retrieve phrases from the corpus, weuse Wumpus (Bu?ttcher and Clarke, 2005), an effi cient search engine for passage retrieval from large corpora.3 The next step is to generate patterns from allof the phrases that were found for all of the in put word pairs (from both the training and testingsets). To generate patterns from a phrase, we re place the given word pairs with variables, X and Y , and we replace the remaining words with a wild card symbol (an asterisk) or leave them as they are. 1http://www.informatics.susx.ac.uk/research/groups/nlp/ carroll/morph.html. 2The corpus was collected by Charles Clarke, University of Waterloo. We can provide copies on request. 3http://www.wumpus-search.org/. 906 For example, the phrase ?the mason cut the stone with? yields the patterns ?the X cut * Y with?, ?* X * the Y *?, and so on. If a phrase contains n words, then it yields 2(n?2) patterns.Each pattern corresponds to a feature in the feature vectors that we will generate. Since a typical input set of word pairs yields millions of pat terns, we need to use feature selection, to reduce the number of patterns to a manageable quantity. For each pattern, we count the number of input word pairs that generated the pattern. For example, ?* X cut * Y *? is generated by both mason:stone and carpenter:wood. We then sort the patterns in descending order of the number of word pairs that generated them. If there are N input word pairs (and thus N feature vectors, including both the training and testing sets), then we select the topkN patterns and drop the remainder. In the fol lowing experiments, k is set to 20. The algorithm is not sensitive to the precise value of k.The reasoning behind the feature selection al gorithm is that shared patterns make more useful features than rare patterns. The number of features (kN ) depends on the number of word pairs (N ), because, if we have more feature vectors, then we need more features to distinguish them. Turney (2006) also selects patterns based on the numberof pairs that generate them, but the number of se lected patterns is a constant (8000), independent of the number of input word pairs. The next step is to generate feature vectors, one vector for each input word pair. Each of the N feature vectors has kN elements, one element for each selected pattern. The value of an element in a vector is given by the logarithm of the frequency in the corpus of the corresponding pattern for the given word pair. For example, suppose the given pair is mason:stone and the pattern is ?* X cut * Y *?. We look at the normalized phrases that we collected for mason:stone and we count how many match this pattern. If f phrases match thepattern, then the value of this element in the fea ture vector is log(f +1) (we add 1 because log(0)is undefined). Each feature vector is then normal ized to unit length. The normalization ensures that features in vectors for high-frequency word pairs (traffic:street) are comparable to features in vectors for low-frequency word pairs (water:riverbed).Now that we have a feature vector for each in put word pair, we can apply a standard supervised learning algorithm. In the following experiments, we use a sequential minimal optimization (SMO)support vector machine (SVM) with a radial basis function (RBF) kernel (Platt, 1998), as implemented in Weka (Waikato Environment for Knowl edge Analysis) (Witten and Frank, 1999).4 The algorithm generates probability estimates for each class by fitting logistic regression models to the outputs of the SVM. We disable the normalizationoption in Weka, since the vectors are already nor malized to unit length. We chose the SMO RBF algorithm because it is fast, robust, and it easily handles large numbers of features.For convenience, we will refer to the above algo rithm as PairClass. In the following experiments, PairClass is applied to each of the four problemswith no adjustments or tuning to the specific prob lems. Some work is required to fit each probleminto the general framework of PairClass (supervised classification of word pairs) but the core al gorithm is the same in each case. This section presents four sets of experiments, with analogies, synonyms, antonyms, and associations. We explain how each task is treated as a problem of classifying analogous word pairs, we give the experimental results, and we discuss past work on each of the four tasks. 3.1 SAT Analogies. In this section, we apply PairClass to the taskof recognizing analogies. To evaluate the performance, we use a set of 374 multiple-choice ques tions from the SAT college entrance exam. Table 2 shows a typical question. The target pair is called the stem. The task is to select the choice pair that is most analogous to the stem pair. Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood Table 2: An example of a question from the 374 SAT analogy questions. The problem of recognizing word analogies wasfirst attempted with a system called Argus (Reit 4http://www.cs.waikato.ac.nz/ml/weka/. 907man, 1965), using a small hand-built semantic network with a spreading activation algorithm. Turney et al (2003) used a combination of 13 independent modules. Veale (2004) used a spread ing activation algorithm with WordNet (in effect, treating WordNet as a semantic network). Turney (2006) used a corpus-based algorithm.We may view Table 2 as a binary classification problem, in which mason:stone and carpen ter:wood are positive examples and the remaining word pairs are negative examples. The difficulty is that the labels of the choice pairs must be hidden from the learning algorithm. That is, the training set consists of one positive example (the stem pair)and the testing set consists of five unlabeled exam ples (the five choice pairs). To make this task more tractable, we randomly choose a stem pair from one of the 373 other SAT analogy questions, andwe assume that this new stem pair is a negative ex ample, as shown in Table 3. Word pair Train or test Class label mason:stone train positive tutor:pupil train negative teacher:chalk test hidden carpenter:wood test hidden soldier:gun test hidden photograph:camera test hidden book:word test hidden Table 3: How to fit a SAT analogy question into the framework of supervised pair classification. To answer the SAT question, we use PairClass to estimate the probability that each testing example is positive, and we guess the testing example with the highest probability. Learning from a trainingset with only one positive example and one nega tive example is difficult, since the learned model can be highly unstable. To increase the stability, we repeat the learning process 10 times, using adifferent randomly chosen negative training exam ple each time. For each testing word pair, the 10 probability estimates are averaged together. This is a form of bagging (Breiman, 1996). PairClass attains an accuracy of 52.1%. Forcomparison, the ACL Wiki lists 12 previously published results with the 374 SAT analogy ques tions.5 Only 2 of the 12 algorithms have higher accuracy. The best previous result is an accuracy of 56.1% (Turney, 2006). Random guessing would 5For more information, see SAT Analogy Questions (State of the art) at http://aclweb.org/aclwiki/. yield an accuracy of 20%. The average senior high school student achieves 57% correct (Turney, 2006). 3.2 TOEFL Synonyms. Now we apply PairClass to the task of recogniz ing synonyms, using a set of 80 multiple-choicesynonym questions from the TOEFL (test of En glish as a foreign language). A sample question is shown in Table 4. The task is to select the choice word that is most similar in meaning to the stem word. Stem: levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed Table 4: An example of a question from the 80 TOEFL questions. Synonymy can be viewed as a high degree of semantic similarity. The most common way tomeasure semantic similarity is to measure the dis tance between words in WordNet (Resnik, 1995; Jiang and Conrath, 1997; Hirst and St-Onge, 1998). Corpus-based measures of word similarityare also common (Lesk, 1969; Landauer and Du mais, 1997; Turney, 2001).We may view Table 4 as a binary classifica tion problem, in which the pair levied:imposed is a positive example of the class synonymous and the other possible pairings are negative examples, as shown in Table 5. Word pair Class label levied:imposed positive levied:believed negative levied:requested negative levied:correlated negative Table 5: How to fit a TOEFL question into the framework of supervised pair classification. The 80 TOEFL questions yield 320 (80 ? 4) word pairs, 80 labeled positive and 240 labelednegative. We apply PairClass to the word pairs us ing ten-fold cross-validation. In each random fold, 90% of the pairs are used for training and 10% are used for testing. For each fold, the model that is learned from the training set is used to assign probabilities to the pairs in the testing set. With 908ten separate folds, the ten non-overlapping test ing sets cover the whole dataset. Our guess foreach TOEFL question is the choice with the high est probability of being positive, when paired with the corresponding stem. PairClass attains an accuracy of 76.2%. Forcomparison, the ACL Wiki lists 15 previously published results with the 80 TOEFL synonym questions.6 Of the 15 algorithms, 8 have higher accu racy and 7 have lower. The best previous resultis an accuracy of 97.5% (Turney et al, 2003), ob tained using a hybrid of four different algorithms. Random guessing would yield an accuracy of 25%. The average foreign applicant to a US university achieves 64.5% correct (Landauer and Dumais, 1997). 3.3 Synonyms and Antonyms. The task of classifying word pairs as either syn onyms or antonyms readily fits into the framework of supervised classification of word pairs. Table 6shows some examples from a set of 136 ESL (En glish as a second language) practice questions that we collected from various ESL websites. Word pair Class label galling:irksome synonyms yield:bend synonyms naive:callow synonyms advise:suggest synonyms dissimilarity:resemblance antonyms commend:denounce antonyms expose:camouflage antonyms unveil:veil antonyms Table 6: Examples of synonyms and antonyms from 136 ESL practice questions. Lin et al (2003) distinguish synonyms from antonyms using two patterns, ?from X to Y ? and ?either X or Y ?. When X and Y are antonyms, they occasionally appear in a large corpus in oneof these two patterns, but it is very rare for syn onyms to appear in these patterns. Our approach is similar to Lin et al (2003), but we do not rely on hand-coded patterns; instead, PairClass patterns are generated automatically.Using ten-fold cross-validation, PairClass at tains an accuracy of 75.0%. Always guessing the majority class would result in an accuracy of 65.4%. The average human score is unknown and 6For more information, see TOEFL Synonym Questions (State of the art) at http://aclweb.org/aclwiki/. there are no previous results for comparison. 3.4 Similar, Associated, and Both. A common criticism of corpus-based measures ofword similarity (as opposed to lexicon-based mea sures) is that they are merely detecting associations(co-occurrences), rather than actual semantic similarity (Lund et al, 1995). To address this criti cism, Lund et al (1995) evaluated their algorithm for measuring word similarity with word pairs that were labeled similar, associated, or both. Theselabeled pairs were originally created for cogni tive psychology experiments with human subjects(Chiarello et al, 1990). Table 7 shows some ex amples from this collection of 144 word pairs (48 pairs in each of the three classes). Word pair Class label table:bed similar music:art similar hair:fur similar house:cabin similar cradle:baby associated mug:beer associated camel:hump associated cheese:mouse associated ale:beer both uncle:aunt both pepper:salt both frown:smile both Table 7: Examples of word pairs labeled similar, associated, or both. Lund et al (1995) did not measure the accuracy of their algorithm on this three-class classification problem. Instead, following standard practice incognitive psychology, they showed that their al gorithm?s similarity scores for the 144 word pairs were correlated with the response times of human subjects in priming tests. In a typical priming test, a human subject reads a priming word (cradle) and is then asked to complete a partial word (complete bab as baby). The time required to perform thetask is taken to indicate the strength of the cogni tive link between the two words (cradle and baby).Using ten-fold cross-validation, PairClass at tains an accuracy of 77.1% on the 144 word pairs. Since the three classes are of equal size, guessing the majority class and random guessing both yield an accuracy of 33.3%. The average human score is unknown and there are no previous results for comparison. 909 The four experiments are summarized in Tables 8 and 9. For the first two experiments, where there are previous results, PairClass is not the best, butit performs competitively. For the second two ex periments, PairClass performs significantly abovethe baselines. However, the strength of this ap proach is not its performance on any one task, but the range of tasks it can handle.As far as we know, this is the first time a standard supervised learning algorithm has been ap plied to any of these four problems. The advantageof being able to cast these problems in the frame work of standard supervised learning problems isthat we can now exploit the huge literature on su pervised learning. Past work on these problems has required implicitly coding our knowledge ofthe nature of the task into the structure of the algo rithm. For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con tains a theory of synonymy (Landauer and Dumais, 1997). The problem with this approach is that it can be very difficult to work out how to modify the algorithm if it does not behave the way we want.On the other hand, with a supervised learning algo rithm, we can put our knowledge into the labeling of the feature vectors, instead of putting it directly into the algorithm. This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual? or ?simulated? negative example). In effect, a single example (mason:stone) becomes a sui generis; it con stitutes a class of its own. It may be possible to apply the machinery of supervised learning toother problems that apparently call for unsupervised learning (for example, clustering or measur ing similarity), by using this sui generis device. One of the first papers using supervised ma chine learning to classify word pairs was Rosarioand Hearst?s (2001) paper on classifying nounmodifier pairs in the medical domain. For ex ample, the noun-modifier expression brain biopsy was classified as Procedure. Rosario and Hearst(2001) constructed feature vectors for each nounmodifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources. They then trained a neu ral network to distinguish 13 classes of semantic relations, such as Cause, Location, Measure, andInstrument. Nastase and Szpakowicz (2003) explored a similar approach to classifying general domain noun-modifier pairs, using WordNet and Roget?s Thesaurus as lexical resources. Turney and Littman (2005) used corpus-based features for classifying noun-modifier pairs. Their features were based on 128 hand-coded patterns. They used a nearest-neighbour learning algorithm to classify general-domain noun-modifier pairsinto 30 different classes of semantic relations. Tur ney (2006) later addressed the same problem using 8000 automatically generated patterns.One of the tasks in SemEval 2007 was the clas sification of semantic relations between nominals (Girju et al, 2007). The problem is to classifysemantic relations between nouns and noun com pounds in the context of a sentence. The task attracted 14 teams who created 15 systems, all of which used supervised machine learning with features that were lexicon-based, corpus-based, or both.PairClass is most similar to the algorithm of Tur ney (2006), but it differs in the following ways:? PairClass does not use a lexicon to find syn onyms for the input word pairs. One of our goals in this paper is to show that a pure corpus-based algorithm can handle synonymswithout a lexicon. This considerably simpli fies the algorithm. PairClass uses a support vector machine (SVM) instead of a nearest neighbour (NN) learning algorithm. PairClass does not use the singular value decomposition (SVD) to smooth the feature vectors. It has been our experience that SVD is not necessary with SVMs. PairClass generates probability estimates,whereas Turney (2006) uses a cosine mea sure of similarity. Probability estimates canbe readily used in further downstream pro cessing, but cosines are less useful.? The automatically generated patterns in PairClass are slightly more general than the pat terns of Turney (2006). The morphological processing in PairClass (Minnen et al, 2001) is more sophisticated than in Turney (2006). 910 Experiment Number of vectors Number of features Number of classes SAT Analogies 2,244 (374 ? 6) 44,880 (2, 244 ? 20) 374 TOEFL Synonyms 320 (80 ? 4) 6,400 (320 ? 20) 2 Synonyms and Antonyms 136 2,720 (136 ? 20) 2 Similar, Associated, and Both 144 2,880 (144 ? 20) 3 Table 8: Summary of the four tasks. See Section 3 for explanations. Experiment Accuracy Best previous Human Baseline Rank SAT Analogies 52.1% 56.1% 57.0% 20.0% 2 higher out of 12 TOEFL Synonyms 76.2% 97.5% 64.5% 25.0% 8 higher out of 15 Synonyms and Antonyms 75.0% none unknown 65.4% none Similar, Associated, and Both 77.1% none unknown 33.3% none Table 9: Summary of experimental results. See Section 3 for explanations. However, we believe that the main contribution of this paper is not PairClass itself, but the extension of supervised word pair classification beyond theclassification of noun-modifier pairs and semantic relations between nominals, to analogies, syn onyms, antonyms, and associations. As far as we know, this has not been done before. The main limitation of PairClass is the need for a large corpus. Phrases that contain a pair of wordstend to be more rare than phrases that contain either of the members of the pair, thus a large cor pus is needed to ensure that sufficient numbers of phrases are found for each input word pair. The size of the corpus has a cost in terms of disk spaceand processing time. In the future, as hardware im proves, this will become less of an issue, but there may be ways to improve the algorithm, so that a smaller corpus is sufficient.Another area for future work is to apply Pair Class to more tasks. WordNet includes more thana dozen semantic relations (e.g., synonyms, hy ponyms, hypernyms, meronyms, holonyms, and antonyms). PairClass should be applicable to allof these relations. Other potential applications in clude any task that involves semantic relations, such as word sense disambiguation, informationretrieval, information extraction, and metaphor in terpretation.","The main limitation of PairClass is the need for a large corpus. Phrases that contain a pair of wordstend to be more rare than phrases that contain either of the members of the pair, thus a large cor pus is needed to ensure that sufficient numbers of phrases are found for each input word pair. The size of the corpus has a cost in terms of disk spaceand processing time. In the future, as hardware im proves, this will become less of an issue, but there may be ways to improve the algorithm, so that a smaller corpus is sufficient.Another area for future work is to apply Pair Class to more tasks. WordNet includes more thana dozen semantic relations (e.g., synonyms, hy ponyms, hypernyms, meronyms, holonyms, and antonyms). PairClass should be applicable to allof these relations. Other potential applications in clude any task that involves semantic relations, such as word sense disambiguation, informationretrieval, information extraction, and metaphor in terpretation."
81,"In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detectionwebsites over twitter data. In our experi ments, we show that since our features areable to capture a more abstract representation of tweets, our solution is more ef fective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources.","In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detectionwebsites over twitter data. In our experi ments, we show that since our features areable to capture a more abstract representation of tweets, our solution is more ef fective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources. Twitter is one of the most popular social network websites and has been growing at a very fast pace. The number of Twitter users reached an estimated75 million by the end of 2009, up from approx imately 5 million in the previous year. Through the twitter platform, users share either informationor opinions about personalities, politicians, prod ucts, companies, events (Prentice and Huffman, 2008) etc. This has been attracting the attention of different communities interested in analyzing its content. Sentiment detection of tweets is one of the basicanalysis utility functions needed by various applications over twitter data. Many systems and ap proaches have been implemented to automatically detect sentiment on texts (e.g., news articles, Web reviews and Web blogs) (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al, 2005; Wilson et al, 2005). Most of theseapproaches use the raw word representation (n grams) as features to build a model for sentiment detection and perform this task over large pieces of texts. However, the main limitation of usingthese techniques for the Twitter context is mes sages posted on Twitter, so-called tweets, are veryshort. The maximum size of a tweet is 140 char acters. In this paper, we propose a 2-step sentiment analysis classification method for Twitter, whichfirst classifies messages as subjective and ob jective, and further distinguishes the subjectivetweets as positive or negative. To reduce the la beling effort in creating these classifiers, instead of using manually annotated data to compose thetraining data, as regular supervised learning ap proaches, we leverage sources of noisy labels asour training data. These noisy labels were pro vided by a few sentiment detection websites over twitter data. To better utilize these sources, we verify the potential value of using and combining them, providing an analysis of the provided labels, examine different strategies of combining these sources in order to obtain the best outcome; and, propose a more robust feature set that captures a more abstract representation of tweets, composedby meta-information associated to words and spe cific characteristics of how tweets are written. By using it, we aim to handle better: the problem of lack of information on tweets, helping on thegeneralization process of the classification algo rithms; and the noisy and biased labels provided by those websites.The remainder of this paper is organized as fol lows. In Section 2, we provide some context about messages on Twitter and about the websites used as label sources. We introduce the features used in the sentiment detection and also provide a deep analysis of the labels generated by those sources in Section 3. We examine different strategies of 36 combining these sources and present an extensive experimental evaluation in Section 4. Finally, we discuss previous works related to ours in Section 5and conclude in Section 6, where we outline direc tions and future work. In this section, we give some context about Twitter messages and the sources used for our data-driven approach. Tweets. The Twitter messages are called tweets. There are some particular features that can be usedto compose a tweet (Figure 1 illustrates an ex ample): ?RT? is an acronym for retweet, which means the tweet was forwarded from a previous post; ?@twUser? represents that this message is areply to the user ?twUser?; ?#obama? is a tag provided by the user for this message, so-called hash tag; and ?http://bit.ly/9K4n9p? is a link to someexternal source. Tweets are limited to 140 charac ters. Due to this lack of information in terms of words present in a tweet, we explore some of the tweet features listed above to boost the sentiment detection, as we will show in detail in Section 3.Data Sources. We collected data from 3 different websites that provide almost real-time sentiment detection for tweets: Twendz, Twitter Sen timent and TweetFeel. To collect data, we issued a query containing a common stopword ?of?, as we are interested in collecting generic data, and retrieved tweets from these sites for three weeks,archiving the returned tweets along with their sen timent labels. Table 1 shows more details aboutthese sources. Two of the websites provide 3 class detection: positive, negative and neutral and one of them just 2-class detection. One thing tonote is our crawling process obtained a very dif ferent number of tweets from each website. Thismight be a result of differences among their sam pling processes of Twitter stream or some kind of filtering process to output. For instance, a sitemay only present the tweets it has more confi dence about their sentiment. In Section 3, we present a deep analysis of the data provided by these sources, showing if they are useful to build a sentiment classification. RT @twUser: Obama is the first U.S. president not to have seen a new state added in his lifetime. http://bit.ly/9K4n9p #obama Figure 1: Example of a tweet. Our goal is to categorize a tweet into one of the three sentiment categories: positive, neutral ornegative. Similar to (Pang and Lee, 2004; Wil son et al, 2005), we implement a 2-step sentimentdetection framework. The first step targets on dis tinguishing subjective tweets from non-subjective tweets (subjectivity detection). The second onefurther classifies the subjective tweets into posi tive and negative, namely, the polarity detection.Both classifiers perform prediction using an ab stract representation of the sentences as features, as we show later in this section. 3.1 Features. A variety of features have been exploited on the problem of sentiment detection (Pang and Lee, 2004; Pang et al, 2002; Wiebe et al, 1999; Wiebeand Riloff, 2005; Riloff et al, 2006) including unigrams, bigrams, part-of-speech tags etc. A natural choice would be to use the raw word represen tation (n-grams) as features, since they obtained good results in previous works (Pang and Lee, 2004; Pang et al, 2002) that deal with large texts.However, as we want to perform sentiment detection on very short messages (tweets), this strategy might not be effective, as shown in our ex periments. In this context, we are motivated to develop an abstract representation of tweets. Wepropose the use of two sets of features: metainformation about the words on tweets and char acteristics of how tweets are written. Meta-features. Given a word in a tweet, we mapit to its part-of-speech using a part-of-speech dic tionary1. Previous approaches (Wiebe and Riloff,2005; Riloff et al, 2003) have shown that the ef fectiveness of using POS tags for this task. Theintuition is certain POS tags are good indicators for sentiment tagging. For example, opinion messages are more likely containing adjec 1The pos dictionary we used in this paper is available at: http://wordlist.sourceforge.net/pos-readme. 37 Data sources URL # Tweets Sentiments Twendz http://twendz.waggeneredstrom.com/ 254081 pos/neg/neutral Twitter Sentiment http://twittersentiment.appspot.com/ 79696 pos/neg/neutral TweetFeel http://www.tweetfeel.com/ 13122 pos/neg Table 1: Information about the 3 data sources. tives or interjections. In addition to POS tags, we map the word to its prior subjectivity (weak and strong subjectivity), also used by (Wiebe and Riloff, 2005), and polarity (positive, negative andneutral). The prior polarity is switched from pos itive to negative or vice-versa when a negative expression (as, e.g., ?don?t?, ?never?) precedes the word. We obtained the prior subjectivity and polarity information from subjectivity lexicon of about 8,000 words used in (Riloff and Wiebe, 2003)2. Although this is a very comprehensive list, slang and specific Web vocabulary are not present on it, e.g., words as ?yummy? or ?ftw?. For this reason, we collected popular words used on online discussions from many online sources and added them to this list.Tweet Syntax Features. We exploited the syn tax of the tweets to compose our features. Theyare: retweet; hashtag; reply; link, if the tweet contains a link; punctuation (exclamation and questions marks); emoticons (textual expression rep resenting facial expressions); and upper cases (the number of words that starts with upper case in the tweet).The frequency of each feature in a tweet is di vided by the number of the words in the tweet. 3.2 Subjectivity Classifier. As we mentioned before, the first step in our tweet sentiment detection is to predict the subjectivity ofa given tweet. We decided to create a single clas sifier by combining the objectivity sentences from Twendz and Twitter Sentiment (objectivity class) and the subjectivity sentences from all 3 sources.As we do not know the quality of the labels pro vided by these sources, we perform a cleaning process over this data to assure some reasonable quality. These are the steps: 1. Disagreement removal: we remove the. 2The subjectivity lexicon is available at http://www.cs.pitt.edu/mpqa/ tweets that are disagreed between the data sources in terms of subjectivity; 2. Same user?s messages: we observed that the. users with the highest number of messages in our dataset are usually those ones that postsome objective messages, for example, ad vertising some product or posting some job recruiting information. For this reason, we allowed in the training data only one message from the same user. As we show later, this boosts the classification performance, mainlybecause it removes tweets labeled as subjective by the data sources but are in fact objec tive; 3. Top opinion words: to clean the objective. training set, we remove from this set tweets that contain the top-n opinion words in the subjectivity training set, e.g., words as cool, suck, awesome etc. As we show in Section 4, this process is in fact able to remove certain noisy in the training data,leading to a better performing subjectivity classi fier. To illustrate which of the proposed features are more effective for this task, the top-5 features in terms of information gain, based on our trainingdata, are: positive polarity, link, strong subjec tive, upper case and verbs. Three of them aremeta-information (positive polarity, strong sub jective and verbs) and the other two are tweet syntax features (link and upper case). Here is a typical example of a objective tweet in which the user pointed an external link and used manyupper case words: ?Starbucks Expands Pay-ByIPhone Pilot to 1,000 Stores?Starbucks cus tomers with Apple iPhones or iPod touches can .. http://oohja.com/x9UbC?. 38 3.3 Polarity Classifier. The second step of our sentiment detection approach is polarity classification, i.e., predict ing positive or negative sentiment on subjectivetweets. In this section, first we analyze the qual ity of the polarity labels provided by the three sources, and whether their combination has the potential to bring improvement. Second, wepresent some modifications in the proposed fea tures that are more suitable for this task. 3.3.1 Analysis of the Data Sources The 3 data sources used in this work provide some kind of polarity labels (see Table 1). Two questions we investigate regarding these sources are: (1) how useful are these polarity labels? and (2) does combining them bring improvement in accuracy?We take the following aspects into considera tion:? Labeler quality: if the labelers have low quality, combine them might not bring much im provement (Sheng et al, 2008). In our case, each source is treated as a labeler; ? Number of labels provided by the labelers:if the labels are informative, i.e., the prob ability of them being correct is higher than 0.5, the more the number of labels, the higher is the performance of a classifier built from them (Sheng et al, 2008); ? Labeler bias: the labeled data provided by the labelers might be only a subset of the real data distribution. For instance, labelers might be interested in only providing labels that they are more confident about;? Different labeler bias: if labelers make simi lar mistakes, the combination of them might not bring much improvement. We provide an empirical analysis of these datasets to address these points. First, we measurethe polarity detection quality of a source by calcu lating the probability p of a label from this source being correct. We use the data manually labeled for assessing the classifiers? performance (testing data, see Section 4) to obtain the correct labels of Data sources Quality Entropy Twendz 0.77 8.3 TwitterSentiment 0.82 7.9 TweetFeel 0.89 7.5 Table 2: Quality of the labels and entropy of thetweets provided by each data source for the polar ity detection. a data sample. Table 2 shows their values. We can conclude from these numbers that the 3 sources provide a reasonable quality data. This means that combining them might bring some improvement to the polarity detection instead of, for instance, using one of them in isolation. An aspect that is overlooked by quality is the bias of the data. For instance, by examining the data from TwitterFeel,we found out that only 4 positive words (?awe some?,?rock?,?love? and ?beat?) cover 95% of their positive examples and only 6 negative words (?hate?,?suck?,?wtf?,?piss?,?stupid? and ?fail?) cover 96% of their negative set. Clearly, the data provided by this source is biased towards thesewords. This is probably the reason why this website outputs such fewer number of tweets com pared to the other websites (see Table 1) as well as why its data has the smallest entropy among the sources (see Table 2). The quality of the data and its individual bias have certainly impact in the combination of labels. However, there is other important aspect that oneneeds to consider: different bias between the labelers. For instance, if labelers a and b make similar decisions, we expect that combining their labels would not bring much improvement. There fore, the diversity of labelers is a key element incombining them (Polikar, 2006). One way to mea sure this is by calculating the agreement between the labels produced by the labelers. We use the kappa coefficient (Cohen, 1960) to measure thedegree of agreement between two sources. Ta ble 3 presents the coefficients for each par of data source. All the coefficients are between 0.4 and0.6, which represents a moderate agreement be tween the labelers (Landis and Koch, 1977). This means that in fact the sources provide different bias regarding polarity detection. 39 Data sources Kappa Twendz/TwitterSentiment 0.58 TwitterSentiment/TweetFeel 0.58 Twendz/TweetFeel 0.44 Table 3: Kappa coefficient between pairs of sources.From this analysis we can conclude that com bining the labels provided by the 3 sources canimprove the performance of the polarity detection instead of using one of them in isolation be cause they provide diverse labels (moderate kappa agreement) of reasonable quality, although thereis some issues related to bias of the labels pro vided by them. In our experimental evaluation in Section 4, we present results obtained by different strategies of combining these sources that confirm these findings. 3.3.2 Polarity Features The features used in the polarity detection are the same ones used in the subjectivity detection. However, as one would expect the set of the most discriminative features is different between the two tasks. For subjectivity detection, the top-5 features in terms of information gain, based on the training data, are: negative polarity, positive polarity, verbs, good emoticons and upper case. For this task, the meta-information of the words (negative polarity, positive polarity and verbs) is more important than specific features from Twitter (good emoticons and upper case), whereas for the subjectivity detection, tweet syntax features have a higher relevance. This analysis show that prior polarity is very important for this task. However, one limitation of using it from a generic list is its values might not hold for some specific scenario. For instance,the polarity of the word ?spot? is positive accord ing to this list. However, looking at our training data almost half of the occurrences of this word appears in the positive set and the other half inthe negative set. Thus, it is not correct to as sume that prior polarity of ?spot? is 1 for thisparticular data. This example illustrates our strat egy to weight the prior polarities: for each wordw with prior polarity defined by the list, we cal culate the prior polarity of w, pol(w), based on the distribution of w in the positive and negative sets. Thus, polpos(w) = count(w, pos)/count(w) and polneg(w) = 1? polpos(w). We assume thepolarity of a word is associated with the polar ity of the sentence, which seems to be reasonable since we are dealing with very short messages. Although simple, this strategy is able to improve the polarity detection, as we show in Section 4. We have performed an extensive performance evaluation of our solution for twitter sentimentdetection. Besides analyzing its overall perfor mance, our goals included: examining different strategies to combine the labels provided by the sources; comparing our approach to previous onesin this area; and evaluating how robust our solu tion is to the noisy and biased data described in Section 3. 4.1 Experimental Setup. Data Sets. For the subjectivity detection, afterthe cleansing processing (see Section 3), the train ing data contains about 200,000 tweets (roughly 100,000 tweets were labeled by the sources as subjective ones and 100,000 objective ones), and for polarity detection, 71046 positive and 79628negative tweets. For test data, we manually labeled 1,000 tweets as positive, negative and neu tral. We also built a development set (1,000 tweets) to tune the parameters of the classification algorithms.Approaches. For both tasks, subjectivity and po larity detection, we compared our approach with previous ones reported in the literature. Detailed explanation about them are as follows: ? ReviewSA: this is the approach proposed by Pang and Lee (Pang and Lee, 2004)for sentiment analysis in regular online reviews. It performs the subjectivity detection on a sentence-level relying on the proximity between sentences to detect subjectivity. The set of sentences predicted as subjec tive is then classified as negative or positive in terms of polarity using the unigrams that 40compose the sentences. We used the imple mentation provided by LingPipe (LingPipe, 2008); ? Unigrams: Pang et al (Pang et al, 2002) showed unigrams are effective for sentiment detection in regular reviews. Based on that, we built unigram-based classifiers for the subjectivity and polarity detections over thetraining data. Another approach that uses un igrams is the one used by TwitterSentiment website. For polarity detection, they select the positive examples for the training data from the tweets containing good emoticonsand negative examples from tweets contain ing bad emoticons. (Go et al, 2009). We built a polarity classifier using this approach (Unigrams-TS). TwitterSA: TwitterSA exploits the features described in Section 3 in this paper. Forthe subjectivity detection, we trained a classifier from the two available sources, using the cleaning process described in Sec tion 3 to remove noise in the training data, TwitterSA(cleaning), and other classifiertrained from the original data, TwitterSA(no cleaning). For the polarity detection task, we built a few classifiers to compare theirperformances: TwitterSA(single) and Twit terSA(weights) are two classifiers we trained using combined data from the 3 sources. The only difference is TwitterSA(weights) uses the modification of weighting the priorpolarity of the words based on the training data. TwitterSA(voting) and TwitterSA(maxconf) combine classification out puts from 3 classifiers respectively trained from each source. TwitterSA(voting) usesmajority voting to combine them and TwitterSA(maxconf) picks the one with maxi mum confidence score.We use Weka (Witten and Frank, 2005) to create the classifiers. We tried different learning al gorithms available on Weka and SVM obtainedthe best results for Unigrams and TwitterSA. Experimental results reported in this section are ob tained using SVM. 4.2 Subjectivity Detection Evaluation. Table 4 shows the error rates obtained by the different subjectivity detection approaches. TwitterSA achieved lower error rate than both Unigrams and ReviewSA. As a result, these numbers confirm that features inferred from meta information of words and specific syntax featuresfrom tweets are better indicators of the subjectivity than unigrams. Another advantage of our approach is since it uses only 20 features, the training and test times are much faster than using thousands of features like Unigrams. One of the reasons why TwitterSA obtained such a good performance was the process of data cleansing (see Sec tion 3). The label quality provided by the sources for this task was very poor: 0.66 for Twendz and 0.68 for TwitterSentiment. By cleaning the data,the error decreased from 19.9, TwitterSA(nocleaning), to 18.1, TwitterSA(cleaning). Regard ing ReviewSA, its lower performance is expected since tweets are composed by single sentences and ReviewSA relies on the proximity between sentences to perform subjectivity detection. We also investigated the influence of the size oftraining data on classification performance. Fig ure 2 plots the error rates obtained by TwitterSAand Unigrams versus the number of training ex amples. The curve corresponding to TwitterSA showed that it achieved good performances evenwith a small training data set, and kept almost constant as more examples were added to the training data, whereas for Unigrams the error rate de creased. For instance, with only 2,000 tweets as training data, TwitterSA obtained 20% of error rate whereas Unigrams 34.5%. These numbers show that our generic representation of tweets produces models that are able to generalize even with a few examples. 4.3 Polarity Detection Evaluation. We provide the results for polarity detectionin Table 5. The best performance was ob tained by TwitterSA(maxconf), which combines results of the 3 classifiers, respectively trained from each source, by taking the output by themost confident classifier, as the final prediction. TwitterSA(maxconf) was followed by TwitterSA(weights) and TwitterSA(single), both cre 41 ated from a single training data. This result shows that computing the prior polarity of the words based on the training data TwitterSA(weights)brings some improvement for this task. Twit terSA(voting) obtained the highest error rate among the TwitterSA approaches. This implies that, in our scenario, the best way of combining the merits of the individual classifiers is by using a confidence score approach.Unigrams also achieved comparable perfor mances. However, when reducing the size of thetraining data, the performance gap between Twit terSA and Unigrams is much wider. Figure 3shows the error rate of both approaches3 in function of the training size. Similar to subjectivity detection, the training size does not have much influ ence in the error rate for TwitterSA. However forUnigrams, it decreased significantly as the train ing size increased. For instance, for a training size with 2,000 tweets, the error rate for Unigrams was 46% versus 23.8% for our approach. As forsubjectivity detection, this occurs because our features are in fact able to capture a more general rep resentation of the tweets.Another advantage of TwitterSA over Uni grams is that it produces more robust models. Toillustrate this, we present the error rates of Uni grams and TwitterSA where the training data is composed by data from each source in isolation.For the TweetFeel website, where data is very bi ased (see Section 3), Unigrams obtained an error rate of 44.5% whereas over a sample of the same size of the combined training data (Figure 3), itobtained an error rate of around 30%. Our ap proach also performed worse over this data thanthe general one, but still had a reasonable er ror rate, 25.1%. Regarding the Twendz website, which is the noisiest one (Section 3), Unigrams also obtained a poor performance comparing itagainst its performance over a sample of the general data with a same size (see Table 5 and Fig ure 3). Our approach, on the other hand, was not much influenced by the noise (22.9% on noisy data and around 20% on the sample of same sizeof the general data). Finally, since the data qual ity provided by TwitterSentiment is better than the3For this experiment, we used the TwitterSA(single) con figuration. Approach Error rate TwitterSA(cleaning) 18.1 TwitterSA(no-cleaning) 19.9 Unigrams 27.6 ReviewSA 32 Table 4: Results for subjectivity detection. Approach Error rate TwitterSA(maxconf) 18.7 TwitterSA(weights) 19.4 TwitterSA(single) 20 TwitterSA(voting) 22.6 Unigrams 20.9 ReviewSA 21.7 Unigrams-TS 24.3 Table 5: Results for polarity detection. Site Training Size TwitterSA Unigrams TweetFeel 13120 25.1 44.5 Twendz 78025 22.9 32.3 TwitterSentiment 59578 22 23.4 Table 6: Training data size for each source and error rates obtained by classifiers built from them. 0 5 10 15 20 25 30 35 40 0 20000 40000 60000 80000 100000 120000 140000 160000 180000 200000Error Rate Training Size UnigramsTwitterSA Figure 2: Influence of the training data size in the error rate of subjectivity detection using Unigrams and TwitterSA. previous sources (Table 2), there was not much impact over both classifiers created from it.From this analysis over real data, we can con clude that our approach produces (1) an effective polarity classifier even when only a small number of training data is available; (2) a robust model tobias and noise in the training data; and (3) combining data sources with such distinct characteris tics, as our data analysis in Section 3 pointed out, is effective. 42 0 10 20 30 40 50 0 20000 40000 60000 80000 100000 120000 140000 160000Error Rate Training Size UnigramsTwitterSA Figure 3: Influence of the training data size in the error rate of polarity detection using Unigrams and TwitterSA. There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005). Most of these ap proaches try to perform this task on large texts, ase.g., newspaper articles and movie reviews. An other common characteristic of some of them isthe use of n-grams as features to create their mod els. For instance, Pang and Lee (Pang and Lee, 2004) explores the fact that sentences close in a text might share the same subjectivity to create a better subjectivity detector and, similar to (Pang etal., 2002), uses unigrams as features for the polar ity detection. However, these approaches do not obtain a good performance on detecting sentimenton tweets, as we showed in Section 4, mainly be cause tweets are very short messages. In addition to that, since they use a raw word representation, they are more sensible to bias and noise, and needa much higher number of examples in the train ing data than our approach to obtain a reasonable performance. The Web sources used in this paper and some other websites provide sentiment detection for tweets. A great limitation to evaluate them is they do not make available how their classification was built. One exception is TwitterSentiment (Go et al., 2009), for instance, which considers tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and builds a classifier using unigrams and bigrams as features. We showed in Section 4 that our approach works better than theirs for this problem, obtaining lower error rates.","There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005). Most of these ap proaches try to perform this task on large texts, ase.g., newspaper articles and movie reviews. An other common characteristic of some of them isthe use of n-grams as features to create their mod els. For instance, Pang and Lee (Pang and Lee, 2004) explores the fact that sentences close in a text might share the same subjectivity to create a better subjectivity detector and, similar to (Pang etal., 2002), uses unigrams as features for the polar ity detection. However, these approaches do not obtain a good performance on detecting sentimenton tweets, as we showed in Section 4, mainly be cause tweets are very short messages. In addition to that, since they use a raw word representation, they are more sensible to bias and noise, and needa much higher number of examples in the train ing data than our approach to obtain a reasonable performance. The Web sources used in this paper and some other websites provide sentiment detection for tweets. A great limitation to evaluate them is they do not make available how their classification was built. One exception is TwitterSentiment (Go et al., 2009), for instance, which considers tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and builds a classifier using unigrams and bigrams as features. We showed in Section 4 that our approach works better than theirs for this problem, obtaining lower error rates."
82,"Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem. This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition. State-of-the-art perfor mance is obtained.","Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem. This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition. State-of-the-art perfor mance is obtained. Unlike English and other western languages, many Asian languages such as Chinese, Japanese, and Thai, do not delimit words by white-space. Wordsegmentation is therefore a key precursor for language processing tasks in these languages. For Chinese, there has been significant research on find ing word boundaries in unsegmented sequences(see (Sproat and Shih, 2002) for a review). Un fortunately, building a Chinese word segmentation system is complicated by the fact that there is no standard definition of word boundaries in Chinese. Approaches to Chinese segmentation fall roughly into two categories: heuristic dictionary-based methods and statistical machine learning methods.In dictionary-based methods, a predefined dictio nary is used along with hand-generated rules for segmenting input sequence (Wu, 1999). Howeverthese approaches have been limited by the impossibility of creating a lexicon that includes all possible Chinese words and by the lack of robust statistical inference in the rules. Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000). Many current approaches suffer from either lackof exact inference over sequences or difficulty in incorporating domain knowledge effectively into seg mentation. Domain knowledge is either not used, used in a limited way, or used in a complicated way spread across different components. For example,the N-gram generative language modeling based ap proach of Teahan et al(2000) does not use domainknowledge. Gao et al(2003) uses class-based language for word segmentation where some word cat egory information can be incorporated. Zhang et al (2003) use a hierarchical hidden Markov Model to incorporate lexical knowledge. A recent advance in this area is Xue (2003), in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules. Maximum entropy models give tremendousflexibility to incorporate arbitrary features. How ever, a traditional maximum entropy tagger, as used in Xue (2003), labels characters without consideringdependencies among the predicted segmentation labels that is inherent in the state transitions of finite state sequence models. Linear-chain conditional random fields (CRFs) (Lafferty et al, 2001) are models that address both issues above. Unlike heuristic methods, they are principled probabilistic finite state models onwhich exact inference over sequences can be ef ficiently performed. Unlike generative N-gram or hidden Markov models, they have the ability to straightforwardly combine rich domain knowledge, for example in this paper, in the form of multiple readily-available lexicons. Furthermore, they arediscriminatively-trained, and are often more accurate than generative models, even with the same fea tures. In their most general form, CRFs are arbitrary undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. In the linear-chainspecial case we use here, they can be roughly un derstood as discriminatively-trained hidden Markovmodels with next-state transition functions represented by exponential models (as in maximum en tropy classifiers), and with great flexibility to viewthe observation sequence in terms of arbitrary, over lapping features, with long-range dependencies, and at multiple levels of granularity. These beneficialproperties suggests that CRFs are a promising ap proach for Chinese word segmentation.New word detection is one of the most impor tant problems in Chinese information processing.Many machine learning approaches have been pro posed (Chen and Bai, 1998; Wu and Jiang, 2000; Nie et al, 1995). New word detection is normally considered as a separate process from segmentation.However, integrating them would benefit both seg mentation and new word detection. CRFs provide aconvenient framework for doing this. They can pro duce not only a segmentation, but also confidence in local segmentation decisions, which can be usedto find new, unfamiliar character sequences sur rounded by high-confidence segmentations. Thus, our new word detection is not a stand-alone process, but an integral part of segmentation. Newly detected words are re-incorporated into our word lexicon,and used to improve segmentation. Improved seg mentation can then be further used to improve new word detection. Comparing Chinese word segmentation accuracyacross systems can be difficult because many re search papers use different data sets and different ground-rules. Some published results claim 98% or99% segmentation precision and recall, but these ei ther count only the words that occur in the lexicon, or use unrealistically simple data, lexicons that haveextremely small (or artificially non-existant) outof-vocabulary rates, short sentences or many numbers. A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made compar isons easier. The competition provided four datasets with significantly different segmentation guidelines, and consistent train-test splits. The performance ofparticipating system varies significantly across different datasets. Our system achieves top performance in two of the runs, and a state-of-the-art per formance on average. This indicates that CRFs are a viable model for robust Chinese word segmentation. Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al, 2001). A common special-case graph structure is a linear chain, which corresponds to a finite state machine, and is suitablefor sequence labeling. A linear-chain CRF with parameters ? = {?1, ...} defines a conditional proba bility for a state (label) sequence y = y1...yT (for example, labels indicating where words start or have their interior) given an input sequence x = x1...xT (for example, the characters of a Chinese sentence) to be P?(y|x) = 1Zx exp ( T? t=1 ? k ?kfk(yt?1, yt,x, t) ) , (1) where Zx is the per-input normalization that makes the probability of all state sequences sum to one;fk(yt?1, yt,x, t) is a feature function which is of ten binary-valued, but can be real-valued, and ?k is a learned weight associated with feature fk. The feature functions can measure any aspect of a statetransition, yt?1 ? yt, and the entire observation se quence, x, centered at the current time step, t. For example, one feature function might have value 1when yt?1 is the state START, yt is the state NOT START, and xt is a word appearing in a lexicon of people?s first names. Large positive values for ?kindicate a preference for such an event; large nega tive values make the event unlikely. The most probable label sequence for an input x, y? = argmaxy P?(y|x),can be efficiently determined using the Viterbi algorithm (Rabiner, 1990). An N -best list of labeling sequences can also be obtained using modi fied Viterbi algorithm and A* search (Schwartz and Chow, 1990). The parameters can be estimated by maximum likelihood?maximizing the conditional probabilityof a set of label sequences, each given their cor responding input sequences. The log-likelihood of training set {(xi, yi) : i = 1, ...M} is written L? = ? i logP?(yi|xi) = ? i ( T? t=1 ? k ?kfk(yt?1, yt,x, t)? logZxi ) . Traditional maximum entropy learning algorithms, such as GIS and IIS (della Pietra et al, 1995), canbe used to train CRFs. However, our implemen tation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003). The gradient of the likelihood is ?P?(y|x)/??k = ? i,t fk(yt?1, y(i)t ,x(i), t) ? ? i,y,t P?(y|x(i))fk(yt?1, yt,x(i), t) CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. 2.1 Regularization in CRFs. To avoid over-fitting, log-likelihood is usually penalized by some prior distribution over the parameters. A commonly used prior is a zero-mean Gaussian. With a Gaussian prior, log-likelihood is penal ized as follows. L? = ? i logP?(yi|xi)? k ?2k 2?2k (2) where ?2k is the variance for feature dimension k. The variance can be feature dependent. However for simplicity, constant variance is often used for all features. We experiment an alternate version ofGaussian prior in which the variance is feature dependent. We bin features by frequency in the train ing set, and let the features in the same bin share the same variance. The discounted value is set to be ?k dck/Me??2 where ck is the count of features, M is the bin size set by held out validation, and dae is the ceiling function. See Peng and McCallum (2004) for more details and further experiments. 2.2 State transition features. Varying state-transition structures with different Markov order can be specified by different CRF feature functions, as determined by the number ofoutput labels y examined together in a feature func tion. We define four different state transition feature functions corresponding to different Markov orders.Higher-order features capture more long-range de pendencies, but also cause more data sparseness problems and require more memory for training. The best Markov order for a particular application can be selected by held-out cross-validation. 1. First-order: Here the inputs are examined in. the context of the current state only. The feature functions are represented as f(yt,x).There are no separate parameters for state tran sitions. 2. First-order+transitions: Here we add parame-. ters corresponding to state transitions. The fea ture functions used are f(yt,x), f(yt?1, yt).context of the current and previous states. Fea ture function are represented as f(yt?1, yt,x). 4. Third-order: Here inputs are examined in. the context of the current, and two previous states. Feature function are represented as f(yt?2, yt?1, yt,x). We cast the segmentation problem as one of se quence tagging: Chinese characters that begin a new word are given the START tag, and characters in the middle and at the end of words are given theNONSTART tag. The task of segmenting new, un segmented test data becomes a matter of assigning a sequence of tags (labels) to the input sequence of Chinese characters. Conditional random fields are configured as a linear-chain (finite state machine) for this purpose,and tagging is performed using the Viterbi algorithm to efficiently find the most likely label se quence for a given character sequence. 3.1 Lexicon features as domain knowledge. One advantage of CRFs (as well as traditional maximum entropy models) is its flexibility in using arbitrary features of the input. To explore this advantage, as well as the importance of domain knowledge, we use many open features from external re sources. To specifically evaluate the importance ofdomain knowledge beyond the training data, we divide our features into two categories: closed fea tures and open features, (i.e., features allowed in thecompetition?s ?closed test? and ?open test? respec tively). The open features include a large word list (containing single and multiple-character words), a character list, and additional topic or part-of-speech character lexicons obtained from various sources. The closed features are obtained from training data alone, by intersecting the character list obtainedfrom training data with corresponding open lexi cons. Many lexicons of Chinese words and characters are available from the Internet and other sources. Besides the word list and character list, our lexiconsinclude 24 lists of Chinese words and characters obtained from several Internet sites1 cleaned and augmented by a local native Chinese speaker indepen dently of the competition data. The list of lexicons used in our experiments is shown in Figure 1. 3.2 Feature conjunctions. Since CRFs are log-linear models, feature conjunctions are required to form complex, non-linear de cision boundaries in the original feature space. We 1http://www.mandarintools.com, ftp://xcin.linux.org.tw/pub/xcin/libtabe, http://www.geocities.com/hao510/wordlist noun (e.g.,?,?) verb (e.g.,?) adjective (e.g.,?,?) adverb (e.g.,!,?) auxiliary (e.g.,,?) preposition (e.g.,?) number (e.g.,,) negative (e.g.,X,:) determiner (e.g.,?,?,Y) function (e.g. ?,?) letter (English character) punctuation (e.g., # $) last name (e.g.,K) foreign name (e.g.,?) maybe last-name (e.g.,?,[) plural character (e.g.,?,?) pronoun (e.g.,fi,?,?) unit character (e.g.,G,?) country name (e.g.,?,?) Chinese place name (e.g.,?) organization name title suffix (e.g.,?,?) title prefix (e.g.,,?) date (e.g.,#,?,?) Figure 1: Lexicons used in our experiments C?2: second previous character in lexicon C?1: previous character in lexicon C1: next character in lexicon C2: second next character in lexicon C0C1: current and next character in lexicon C?1C0: current and previous character in lexicon C?2C?1: previous two characters in lexicon C?1C0C1: previous, current, and next character in the lexicon Figure 2: Feature conjunctions used in experiments use feature conjunctions in both the open and closed tests, as listed Figure 2. Since no vocabulary list could ever be complete,new word (unknown word) identification is an im portant issue in Chinese segmentation. Unknownwords cause segmentation errors in that these outof-vocabulary words in input text are often in correctly segmented into single-character or otheroverly-short words (Chen and Bai, 1998). Tradi tionally, new word detection has been considered as a standalone process. We consider here new word detection as an integral part of segmentation, aimingto improve both segmentation and new word detec tion: detected new words are added to the word list lexicon in order to improve segmentation; improved segmentation can potentially further improve new word detection. We measure the performance ofnew word detection by its improvements on seg mentation. Given a word segmentation proposed by the CRF, we can compute a confidence in each segment. We detect as new words those that are not in the existing word list, yet are either highly confident segments, or low confident segments that are surrounded by high confident words. A confidence threshold of 0.9 is determined by cross-validation.Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004). The standard forward-backward algorithm (Rabiner, 1990) calculates Zx, the total like lihood of all label sequences y given a sequence x. Constrained forward-backward algorithm calculates Z ?x, total likelihood of all paths passing through a constrained segment (in our case, a sequence of characters starting with a START tag followed by a few NONSTART tags before the next START tag). The confidence in this segment is then Z ? x Zx , a real number between 0 and 1.In order to increase recall of new words, we consider not only the most likely (Viterbi) segmen tation, but the segmentations in the top N most likely segmentations (an N -best list), and detect new words according to the above criteria in all N segmentations.Many errors can be corrected by new word detection. For example, person name ????? hap pens four times. In the first pass of segmentation, two of them are segmented correctly and the other two are mistakenly segmented as ?? (they are segmented differently because Viterbi algorithm decodes based on context.). However, ????? is identified as a new word and added to the word list lexicon. In the second pass of segmentation, the other two mistakes are corrected. To make a comprehensive evaluation, we use allfour of the datasets from a recent Chinese word segmentation bake-off competition (Sproat and Emer son, 2003). These datasets represent four different segmentation standards. A summary of the datasets is shown in Table 1. The standard bake-off scoring program is used to calculate precision, recall, F1, and OOV word recall. 5.1 Experimental design. Since CTB and PK are provided in the GB encod ing while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible. Note that this conversion could potentially worsen performance slightly due to a few conversion errors. We use cross-validation to choose Markov-order and perform feature selection. Thus, each training set is randomly split?80% used for training and theremaining 20% for validation?and based on vali dation set performance, choices are made for model structure, prior, and which word lexicons to include. The choices of prior and model structure shown in Table 2 are used for our final testing. We conduct closed and open tests on all four datasets. The closed tests use only material from the training data for the particular corpus being tested.Open tests allows using other material, such as lexicons from Internet. In open tests, we use lexi cons obtained from various resources as described Corpus Abbrev. Encoding #Train words #Test Words OOV rate (%) UPenn Chinese Treebank CTB GB 250K 40K 18.1 Beijing University PK GB 1.1M 17K 6.9 Hong Kong City U HK Big 5 240K 35K 7.1 Academia Sinica AS Big 5 5.8M 12K 2.2 Table 1: Datasets statistics bin-Size M Markov order CTB 10 first-order + transitions PK 15 first-order + transitions HK 1 first-order AS 15 first-order + transitions Table 2: Optimal prior and Markov order setting in Section 3.1. In addition, we conduct cross-dataset tests, in which we train on one dataset and test on other datasets. 5.2 Overall results. Final results of CRF based segmentation with newword detection are summarized in Table 3. The up per part of the table contains the closed test results, and the lower part contains the open test results. Each entry is the performance of the given metric (precision, recall, F1, and Roov) on the test set. Closed Precision Recall F1 Roov CTB 0.828 0.870 0.849 0.550 PK 0.935 0.947 0.941 0.660 HK 0.917 0.940 0.928 0.531 AS 0.950 0.962 0.956 0.292 Open Precision Recall F1 Roov CTB 0.889 0.898 0.894 0.619 PK 0.941 0.952 0.946 0.676 HK 0.944 0.948 0.946 0.629 AS 0.953 0.961 0.957 0.403 Table 3: Overall results of CRF segmentation on closed and open tests To compare our results against other systems, we summarize the competition results reported in (Sproat and Emerson, 2003) in Table 4. XXc and XXo indicate the closed and open runs on datasetXX respectively. Entries contain the F1 perfor mance of each participating site on different runs, with the best performance in bold. Our results are in the last row. Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results. Column OUR-AVG is the average F1 performance of our system over the same datasets.Comparing performance across systems is diffi cult since none of those systems reported results on all eight datasets (open and closed runs on 4 datasets). Nevertheless, several observations could be made from Table 4. First, no single system achieved best results in all tests. Only one site (S01)achieved two best runs (CTBc and PKc) with an av erage of 91.8% over 6 runs. S01 is one of the best segmentation systems in mainland China (Zhang et al., 2003). We also achieve two best runs (ASo and HKc), with a comparable average of 91.9% over the same 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg mentation guidelines. We also notice that the worstresults were obtained on CTB dataset for all systems. This is due to significant inconsistent segmen tation in training and testing (Sproat and Emerson, 2003). We verify this by another test. We randomly split the training data into 80% training and 20%testing, and run the experiments for 3 times, result ing in a testing F1 of 97.13%. Third, consider a comparison of our results with site S12, who use a sliding-window maximum entropy model (Xue, 2003). They participated in two datasets, with an average of 93.8%. Our average over the same two runs is 94.2%. This gives some empirical evidenceof the advantages of linear-chain CRFs over sliding window maximum entropy models, however, this comparison still requires further investigation sincethere are many factors that could affect the performance such as different features used in both sys tems. To further study the robustness of our approach to segmentation, we perform cross-testing?that is, training on one dataset and testing on other datasets. Table 5 summarizes these results, in which the rows are the training datasets and the columns are thetesting datasets. Not surprisingly, cross testing re sults are worse than the results using the same ASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVG S01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9 S02 87.4 91.2 89.3 87.2 S03 87.2 82.9 88.6 92.5 87.8 93.6 S04 93.9 93.7 93.8 94.4 S05 94.2 73.2 89.4 85.6 91.5 S06 94.5 82.9 92.4 92.4 90.6 91.9 S07 94.0 94.0 94.6 S08 90.4 95.6 93.6 93.8 93.4 94.0 S09 96.1 94.6 95.4 94.9 S10 83.1 90.1 94.7 95.9 91.0 90.8 S11 90.4 88.4 87.9 88.6 88.8 93.6 S12 95.9 91.6 93.8 94.2 95.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7 Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-off competition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is the winner of that run; column SITE-AVG contains the average performance of the site over the runs in which it participated, where a bold entry indicates that this site performs better than our system; column OUR-AVG is the average of our system over the same runs, where a bolded entry indicates our system performs better than the other site; the last row is the performance of our system over all the runs and the overall average. source as training due to different segmentationpolicies, with an exception on CTB where mod els trained on other datasets perform better than the model trained on CTB itself. This is due to the data problem mentioned above. Overall, CRFs perform robustly well across all datasets. From both Table 3 and 5, we see, as expected,improvement from closed tests to open tests, indicating the significant contribution of domain knowl edge lexicons. Closed CTB PK HK AS CTB 0.822 0.810 0.815 PK 0.816 0.824 0.830 HK 0.790 0.807 0.825 AS 0.890 0.844 0.864 Open CTB PK HK AS CTB 0.863 0.870 0.894 PK 0.852 0.862 0.871 HK 0.861 0.871 0.889 AS 0.898 0.867 0.871 Table 5: Crossing test of CRF segmentation 5.3 Effects of new word detection. Table 6 shows the effect of new word detection on the closed tests. An interesting observation is CTB PK HK AS w/o NWD 0.792 0.934 0.916 0.956 NWD 0.849 0.941 0.928 0.946 Table 6: New word detection effects: w/o NWD is the results without new word detection and NWD is the results with new word detection. that the improvement is monotonically related to the OOV rate (OOV rates are listed in Table 1). This is desirable because new word detection is most needed in situations that have high OOV rate. At low OOV rate, noisy new word detection can result in worse performance, as seen in the AS dataset. 5.4 Error analysis and discussion. Several typical errors are observed in error anal ysis. One typical error is caused by inconsistent segmentation labeling in the test set. This is mostnotorious in CTB dataset. The second most typical error is in new, out-of-vocabulary words, especially proper names. Although our new word detec tion fixes many of these problems, it is not effectiveenough to recognize proper names well. One solution to this problem could use a named entity ex tractor to recognize proper names; this was found to be very helpful in Wu (2003). One of the most attractive advantages of CRFs (and maximum entropy models in general) is its the flexibility to easily incorporate arbitrary features,here in the form domain-knowledge-providing lex icons. However, obtaining these lexicons is not a trivial matter. The quality of lexicons can affect the performance of CRFs significantly. In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcomingof CRF-based segmenters is that it requires signifi cantly longer training time. However, training is a one-time process, and testing time is still linear in the length of the input.","To make a comprehensive evaluation, we use allfour of the datasets from a recent Chinese word segmentation bake-off competition (Sproat and Emer son, 2003). These datasets represent four different segmentation standards. A summary of the datasets is shown in Table 1. The standard bake-off scoring program is used to calculate precision, recall, F1, and OOV word recall. 5.1 Experimental design. Since CTB and PK are provided in the GB encod ing while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible. Note that this conversion could potentially worsen performance slightly due to a few conversion errors. We use cross-validation to choose Markov-order and perform feature selection. Thus, each training set is randomly split?80% used for training and theremaining 20% for validation?and based on vali dation set performance, choices are made for model structure, prior, and which word lexicons to include. The choices of prior and model structure shown in Table 2 are used for our final testing. We conduct closed and open tests on all four datasets. The closed tests use only material from the training data for the particular corpus being tested.Open tests allows using other material, such as lexicons from Internet. In open tests, we use lexi cons obtained from various resources as described Corpus Abbrev. Encoding #Train words #Test Words OOV rate (%) UPenn Chinese Treebank CTB GB 250K 40K 18.1 Beijing University PK GB 1.1M 17K 6.9 Hong Kong City U HK Big 5 240K 35K 7.1 Academia Sinica AS Big 5 5.8M 12K 2.2 Table 1: Datasets statistics bin-Size M Markov order CTB 10 first-order + transitions PK 15 first-order + transitions HK 1 first-order AS 15 first-order + transitions Table 2: Optimal prior and Markov order setting in Section 3.1. In addition, we conduct cross-dataset tests, in which we train on one dataset and test on other datasets. 5.2 Overall results. Final results of CRF based segmentation with newword detection are summarized in Table 3. The up per part of the table contains the closed test results, and the lower part contains the open test results. Each entry is the performance of the given metric (precision, recall, F1, and Roov) on the test set. Closed Precision Recall F1 Roov CTB 0.828 0.870 0.849 0.550 PK 0.935 0.947 0.941 0.660 HK 0.917 0.940 0.928 0.531 AS 0.950 0.962 0.956 0.292 Open Precision Recall F1 Roov CTB 0.889 0.898 0.894 0.619 PK 0.941 0.952 0.946 0.676 HK 0.944 0.948 0.946 0.629 AS 0.953 0.961 0.957 0.403 Table 3: Overall results of CRF segmentation on closed and open tests To compare our results against other systems, we summarize the competition results reported in (Sproat and Emerson, 2003) in Table 4. XXc and XXo indicate the closed and open runs on datasetXX respectively. Entries contain the F1 perfor mance of each participating site on different runs, with the best performance in bold. Our results are in the last row. Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results. Column OUR-AVG is the average F1 performance of our system over the same datasets.Comparing performance across systems is diffi cult since none of those systems reported results on all eight datasets (open and closed runs on 4 datasets). Nevertheless, several observations could be made from Table 4. First, no single system achieved best results in all tests. Only one site (S01)achieved two best runs (CTBc and PKc) with an av erage of 91.8% over 6 runs. S01 is one of the best segmentation systems in mainland China (Zhang et al., 2003). We also achieve two best runs (ASo and HKc), with a comparable average of 91.9% over the same 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg mentation guidelines. We also notice that the worstresults were obtained on CTB dataset for all systems. This is due to significant inconsistent segmen tation in training and testing (Sproat and Emerson, 2003). We verify this by another test. We randomly split the training data into 80% training and 20%testing, and run the experiments for 3 times, result ing in a testing F1 of 97.13%. Third, consider a comparison of our results with site S12, who use a sliding-window maximum entropy model (Xue, 2003). They participated in two datasets, with an average of 93.8%. Our average over the same two runs is 94.2%. This gives some empirical evidenceof the advantages of linear-chain CRFs over sliding window maximum entropy models, however, this comparison still requires further investigation sincethere are many factors that could affect the performance such as different features used in both sys tems. To further study the robustness of our approach to segmentation, we perform cross-testing?that is, training on one dataset and testing on other datasets. Table 5 summarizes these results, in which the rows are the training datasets and the columns are thetesting datasets. Not surprisingly, cross testing re sults are worse than the results using the same ASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVG S01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9 S02 87.4 91.2 89.3 87.2 S03 87.2 82.9 88.6 92.5 87.8 93.6 S04 93.9 93.7 93.8 94.4 S05 94.2 73.2 89.4 85.6 91.5 S06 94.5 82.9 92.4 92.4 90.6 91.9 S07 94.0 94.0 94.6 S08 90.4 95.6 93.6 93.8 93.4 94.0 S09 96.1 94.6 95.4 94.9 S10 83.1 90.1 94.7 95.9 91.0 90.8 S11 90.4 88.4 87.9 88.6 88.8 93.6 S12 95.9 91.6 93.8 94.2 95.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7 Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-off competition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is the winner of that run; column SITE-AVG contains the average performance of the site over the runs in which it participated, where a bold entry indicates that this site performs better than our system; column OUR-AVG is the average of our system over the same runs, where a bolded entry indicates our system performs better than the other site; the last row is the performance of our system over all the runs and the overall average. source as training due to different segmentationpolicies, with an exception on CTB where mod els trained on other datasets perform better than the model trained on CTB itself. This is due to the data problem mentioned above. Overall, CRFs perform robustly well across all datasets. From both Table 3 and 5, we see, as expected,improvement from closed tests to open tests, indicating the significant contribution of domain knowl edge lexicons. Closed CTB PK HK AS CTB 0.822 0.810 0.815 PK 0.816 0.824 0.830 HK 0.790 0.807 0.825 AS 0.890 0.844 0.864 Open CTB PK HK AS CTB 0.863 0.870 0.894 PK 0.852 0.862 0.871 HK 0.861 0.871 0.889 AS 0.898 0.867 0.871 Table 5: Crossing test of CRF segmentation 5.3 Effects of new word detection. Table 6 shows the effect of new word detection on the closed tests. An interesting observation is CTB PK HK AS w/o NWD 0.792 0.934 0.916 0.956 NWD 0.849 0.941 0.928 0.946 Table 6: New word detection effects: w/o NWD is the results without new word detection and NWD is the results with new word detection. that the improvement is monotonically related to the OOV rate (OOV rates are listed in Table 1). This is desirable because new word detection is most needed in situations that have high OOV rate. At low OOV rate, noisy new word detection can result in worse performance, as seen in the AS dataset. 5.4 Error analysis and discussion. Several typical errors are observed in error anal ysis. One typical error is caused by inconsistent segmentation labeling in the test set. This is mostnotorious in CTB dataset. The second most typical error is in new, out-of-vocabulary words, especially proper names. Although our new word detec tion fixes many of these problems, it is not effectiveenough to recognize proper names well. One solution to this problem could use a named entity ex tractor to recognize proper names; this was found to be very helpful in Wu (2003). One of the most attractive advantages of CRFs (and maximum entropy models in general) is its the flexibility to easily incorporate arbitrary features,here in the form domain-knowledge-providing lex icons. However, obtaining these lexicons is not a trivial matter. The quality of lexicons can affect the performance of CRFs significantly. In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcomingof CRF-based segmenters is that it requires signifi cantly longer training time. However, training is a one-time process, and testing time is still linear in the length of the input."
83,"In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, pars ing and training times are still relatively long. To determine why, we analyzed thetime usage of a dependency parser. We il lustrate that the mapping of the features onto their weights in the support vectormachine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive percep tron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training. This has lead to a higher accuracy. We could further increase theparsing and training speed with a paral lel feature extraction and a parallel parsing algorithm. We are convinced that the HashKernel and the parallelization can be ap plied successful to other NLP applicationsas well such as transition based depen dency parsers, phrase structrue parsers, and machine translation.","In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, pars ing and training times are still relatively long. To determine why, we analyzed thetime usage of a dependency parser. We il lustrate that the mapping of the features onto their weights in the support vectormachine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive percep tron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training. This has lead to a higher accuracy. We could further increase theparsing and training speed with a paral lel feature extraction and a parallel parsing algorithm. We are convinced that the HashKernel and the parallelization can be ap plied successful to other NLP applicationsas well such as transition based depen dency parsers, phrase structrue parsers, and machine translation. Highly accurate dependency parsers have high de mands on resources and long parsing times. The training of a parser frequently takes several days and the parsing of a sentence can take on averageup to a minute. The parsing time usage is impor tant for many applications. For instance, dialog systems only have a few hundred milliseconds toanalyze a sentence and machine translation sys tems, have to consider in that time some thousandtranslation alternatives for the translation of a sen tence. Parsing and training times can be improved by methods that maintain the accuracy level, or methods that trade accuracy against better parsing times. Software developers and researchers areusually unwilling to reduce the quality of their ap plications. Consequently, we have to consider atfirst methods to improve a parser, which do not in volve an accuracy loss, such as faster algorithms,faster implementation of algorithms, parallel al gorithms that use several CPU cores, and feature selection that eliminates the features that do not improve accuracy. We employ, as a basis for our parser, the secondorder maximum spanning tree dependency pars ing algorithm of Carreras (2007). This algorithmfrequently reaches very good, or even the best la beled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajic? et al, 2009). We combined thisparsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al, 2003; McDon ald et al, 2005; Crammer et al, 2006). A parser build out of these two algorithms provides a good baseline and starting point to improve upon the parsing and training times. The rest of the paper is structured as follows. In Section 2, we describe related work. In section 3, we analyze the time usage of the components of 89the parser. In Section 4, we introduce a new Kernel that resolves some of the bottlenecks and im proves the performance. In Section 5, we describethe parallel parsing algorithms which nearly allowed us to divide the parsing times by the number of cores. In Section 6, we determine the opti mal setting for the Non-Projective ApproximationAlgorithm. In Section 7, we conclude with a sum mary and an outline of further research. The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al, 2004; Attardi, 2006).Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst casequadratic complexity and an expected linear pars ing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used abeam search with a latent variable machine learn ing technique. Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as ?factors?. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling. This algo rithm uses in addition to the first order factors: theedges to those children which are closest to the de pendent. The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4). Johansson and Nugues (2008) reduced the needed number of loops over the edge labels by using only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination.The transition based parsers have a lower com plexity. Nevertheless, the reported run times inthe last shared tasks were similar to the maxi mum spanning tree parsers. For a transition based parser, Gesmundo et al (2009) reported run times between 2.2 days for English and 4.7 days forCzech for the joint training of syntactic and se mantic dependencies. The parsing times were about one word per second, which speeds upquickly with a smaller beam-size, although the ac curacy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). We built a baseline parser to measure the time usage. The baseline parser resembles the architec ture of McDonald and Pereira (2006). It consists of the second order parsing algorithm of Carreras(2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. The features are listed in Table 4. As in McDonald et al (2005), the parser stores the features of each training example in a file. In each epoch of the training, the feature file is read, and the weights are calculated and stored in an array. This procedure is up to 5 times faster than computing the features each time anew. But the parser has to maintain large arrays: for the weights of the sentence and the training file. Therefore, the parser needs 3GB of main memoryfor English and 100GB of disc space for the train ing file. The parsing time is approximately 20% faster, since some of the values did not have to be recalculated. Algorithm 1 illustrates the training algorithm in pseudo code. is the set of training examples where an example is a pair (xi, yi) of a sentence and the corresponding dependency structure. ??wand ??v are weight vectors. The first loop ex tracts features from the sentence xi and maps the features to numbers. The numbers are grouped into three vectors for the features of all possible edges ?h,d, possible edges in combination withsiblings ?h,d,s and in combination with grandchil 90 te+s tr tp ta rest total te pars. train. sent. feat. LAS UAS Chinese 4582 748 95 - 3 846 3298 3262 84h 22277 8.76M 76.88 81.27 English 1509 168 12.5 20 1.5 202 1223 1258 38.5h 39279 8.47M 90.14 92.45 German 945 139 7.7 17.8 1.5 166 419 429 26.7h 36020 9.16M 87.64 90.03 Spanish 3329 779 36 - 2 816 2518 2550 16.9h 14329 5.51M 86.02 89.54 Table 1: te+s is the elapsed time in milliseconds to extract and store the features, tr to read the features and to calculate the weight arrays, tp to predict the projective parse tree, ta to apply the non-projective approximation algorithm, rest is the time to conduct the other parts such as the update function, train. is the total training time per instance (tr + tp + ta+rest ), and te is the elapsed time to extract the features. The next columns illustrate the parsing time in milliseconds per sentence for the test set, training time in hours, the number of sentences in the training set, the total number of features in million, the labeled attachment score of the test set, and the unlabeled attachment score. Algorithm 1: Training ? baseline algorithm ? = {(xi, yi)}Ii=1 // Training data??w = 0,??v = 0 ? = E ? I // passive-aggresive update weight for i = 1 to I tss+e; extract-and-store-features(xi); tes+e; for n = 1 to E // iteration over the training epochs for i = 1 to I // iteration over the training examples k ? (n? 1) ? I + i ? = E ? I ? k + 2 // passive-aggressive weight tsr,k; A = read-features-and-calc-arrays(i,??w ) ; ter,k tsp,k; yp = predicte-projective-parse-tree(A);tep,k tsa,k; ya = non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average dren ?h,d,g where h, d, g, and s are the indexes of the words included in xi. Finally, the method stores the feature vectors on the hard disc. The next two loops build the main part of the training algorithm. The outer loop iterates over the number of training epochs, while the innerloop iterates over all training examples. The on line training algorithm considers a single training example in each iteration. The first function in the loop reads the features and computes the weights A for the factors in the sentence xi. A is a set of weight arrays. A = {??w ? ??f h,d,??w ? ??f h,d,s,??w ? ??f h,d,g} The parsing algorithm uses the weight arrays to predict a projective dependency structure yp. The non-projective approximation algorithm has as input the dependency structure and the weightarrays. It rearranges the edges and tries to in crease the total score of the dependency structure. This algorithm builds a dependency structure ya,which might be non-projective. The training al gorithm updates ??w according to the difference between the predicted dependency structures ya and the reference structure yi. It updates ??v as well, whereby the algorithm additionally weights the updates by ?. Since the algorithm decreases ? in each round, the algorithm adapts the weights more aggressively at the beginning (Crammer etal., 2006). After all iterations, the algorithm com putes the average of ??v , which reduces the effect of overfitting (Collins, 2002). We have inserted into the training algorithm functions to measure the start times ts and the end times te for the procedures to compute andstore the features, to read the features, to predict the projective parse, and to calculate the nonprojective approximation. We calculate the aver age elapsed time per instance, as the average over all training examples and epochs: tx = ?E?I k=1 t e x,k?tsx,k E?I . We use the training set and the test set of theCoNLL shared task 2009 for our experiments. Ta ble 1 shows the elapsed times in 11000 seconds (milliseconds) of the selected languages for the procedure calls in the loops of Algorithm 1. We had to measure the times for the feature extractionin the parsing algorithm, since in the training al gorithm, the time can only be measured together with the time for storing the features. The table contains additional figures for the total training time and parsing scores.1 The parsing algorithm itself only required, to our surprise, 12.5 ms (tp) for a English sentence 1We use a Intel Nehalem i7 CPU 3.33 Ghz. With turbo mode on, the clock speed was 3.46 Ghz. 91 on average, while the feature extraction needs 1223 ms. To extract the features takes about100 times longer than to build a projective dependency tree. The feature extraction is already implemented efficiently. It uses only numbers to rep resent features which it combines to a long integer number and then maps by a hash table2 to a 32bit integer number. The parsing algorithm uses the integer number as an index to access the weights in the vectors ??w and ??v .The complexity of the parsing algorithm is usu ally considered the reason for long parsing times.However, it is not the most time consuming component as proven by the above analysis. There fore, we investigated the question further, askingwhat causes the high time consumption of the fea ture extraction? In our next experiment, we left out the mapping of the features to the index of the weight vectors.The feature extraction takes 88 ms/sentence with out the mapping and 1223 ms/sentence with the mapping. The feature?index mapping needs 93% of the time to extract the features and 91% of thetotal parsing time. What causes the high time con sumption of the feature?index mapping?The mapping has to provide a number as an in dex for the features in the training examples and to filter out the features of examples built, while theparser predicts the dependency structures. The al gorithm filters out negative features to reduce the memory requirement, even if they could improve the parsing result. We will call the features built due to the training examples positive features and the rest negative features. We counted 5.8 timesmore access to negative features than positive fea tures.We now look more into the implementation details of the used hash table to answer the pre viously asked question. The hash table for the feature?index mapping uses three arrays: one for the keys, one for the values and a status array to indicate the deleted elements. If a program storesa value then the hash function uses the key to cal culate the location of the value. Since the hashfunction is a heuristic function, the predicted lo cation might be wrong, which leads to so-called 2We use the hash tables of the trove library: http://sourceforge.net/projects/trove4j. hash misses. In such cases the hash algorithm has to retry to find the value. We counted 87% hash misses including misses where the hash had to retry several times. The number of hash misseswas high, because of the additional negative fea tures. The CPU cache can only store a small amount of the data from the hash table. Therefore, the memory controller has frequently to transfer data from the main memory into the CPU. This procedure is relatively slow. We traced down the high time consumption to the access of the key and the access of the value. Successive accessesto the arrays are fast, but the relative random ac cesses via the hash function are very slow. Thelarge number of accesses to the three arrays, be cause of the negative features, positive features and because of the hash misses multiplied by the time needed to transfer the data into the CPU are the reason for the high time consumption.We tried to solve this problem with Bloom filters, larger hash tables and customized hash func tions to reduce the hash misses. These techniquesdid not help much. However, a substantial im provement did result when we eliminated the hash table completely, and directly accessed the weight vectors ??w and ??v with a hash function. This led us to the use of Hash Kernels. A Hash Kernel for structured data uses a hash function h : J ? {1...n} to index ?, cf. Shi etal. (2009). maps the observations X to a feature space. We define ?(x, y) as the numeric fea ture representation indexed by J . Let ?k(x, y) = ?j(x, y) the hash based feature?index mapping,where h(j) = k. The process of parsing a sen tence xi is to find a parse tree yp that maximizes a scoring function argmaxyF (xi, y). The learning problem is to fit the function F so that the errors of the predicted parse tree y are as low as possible. The scoring function of the Hash Kernel is F (x, y) = ??w ? ?(x, y) where ??w is the weight vector and the size of ??w is n. Algorithm 2 shows the update function of the Hash Kernel. We derived the update function from the update function of MIRA (Crammer et 92 Algorithm 2: Update of the Hash Kernel // yp = arg maxyF (xi, y) update(??w,??v , xi, yi, yp, ?) ? = ?(yi, yp) // number of wrong labeled edges if ? > 0 then ??u ? (?(xi, yi)? ?(xi, yp)) ? = ??(F (xt,yi)?F (xi,yp))||??u ||2??w ? ??w + ? ? ??u ??v ? ~v + ? ? ??u return ??w , ??v al., 2006). The parameters of the function are the weight vectors ??w and ??v , the sentence xi, the gold dependency structure yi, the predicted dependency structure yp, and the update weight ?. The function ? calculates the number ofwrong labeled edges. The update function updates the weight vectors, if at least one edge is la beled wrong. It calculates the difference ??u of the feature vectors of the gold dependency structure ?(xi, yi) and the predicted dependency structure?(xi, yp). Each time, we use the feature represen tation ?, the hash function h maps the features to integer numbers between 1 and |??w |. After that the update function calculates the margin ? and updates ??w and ??v respectively. Algorithm 3 shows the training algorithm forthe Hash Kernel in pseudo code. A main dif ference to the baseline algorithm is that it does not store the features because of the required time which is needed to store the additional negative features. Accordingly, the algorithm first extracts the features for each training instance, then maps the features to indexes for the weight vector with the hash function and calculates the weight arrays. Algorithm 3: Training ? Hash Kernel for n? 1 to E // iteration over the training epochs for i? 1 to I // iteration over the training exmaples k ? (n? 1) ? I + i ? ? E ? I ? k + 2 // passive-aggressive weight tse,k; A? extr.-features-&-calc-arrays(i,??w ) ; tee,k tsp,k; yp? predicte-projective-parse-tree(A);tep,k tsa,k; ya? non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average For different j, the hash function h(j) might generate the same value k. This means that the hash function maps more than one feature to thesame weight. We call such cases collisions. Col lisions can reduce the accuracy, since the weights are changed arbitrarily. This procedure is similar to randomization of weights (features), which aims to save space by sharing values in the weight vector (Blum., 2006; Rahimi and Recht, 2008). The Hash Kernel shares values when collisions occur that can be considered as an approximation of the kernel function, because a weight might be adapted due to more than one feature. If the approximation works well then we would need only a relatively small weight vector otherwise we need a larger weight vector to reduce the chance of collisions. In an experiments, we compared two hash functions and different hash sizes. We selected for the comparison a standard hash function (h1) and a custom hash function (h2). The idea for the custom hash function h2 is not to overlap the values of the feature sequence number and the edge label with other values. These values are stored at the beginning of a long number, which represents a feature. h1 ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|3 h2 ? |(l xor ((l >> 13) ? 0xffffffffffffe000) xor ((l >> 24) ? 0xffffffffffff0000) xor ((l >> 33) ? 0xfffffffffffc0000) xor ((l >> 40) ? 0xfffffffffff00000)) % size | vector size h1 #(h1) h2 #(h2) 411527 85.67 0.41 85.74 0.41 3292489 87.82 3.27 87.97 3.28 10503061 88.26 8.83 88.35 8.77 21006137 88.19 12.58 88.41 12.53 42012281 88.32 12.45 88.34 15.27 115911564? 88.32 17.58 88.39 17.34 179669557 88.34 17.65 88.28 17.84Table 2: The labeled attachment scores for differ ent weight vector sizes and the number of nonzero values in the feature vectors in millions. Not a prime number. Table 2 shows the labeled attachment scores for selected weight vector sizes and the number of nonzero weights. Most of the numbers in Table2 are primes, since they are frequently used to obtain a better distribution of the content in hash ta 3>> n shifts n bits right, and % is the modulo operation. 93bles. h2 has more nonzero weights than h1. Nevertheless, we did not observe any clear improve ment of the accuracy scores. The values do not change significantly for a weight vector size of 10 million and more elements. We choose a weightvector size of 115911564 values for further exper iments since we get more non zero weights and therefore fewer collisions. te tp ta r total par. trai. Chinese 1308 - 200 3 1511 1184 93h English 379 21.3 18.2 1.5 420 354 46h German 209 12 15.3 1.7 238 126 24h Spanish 1056 - 39 2 1097 1044 44h Table 3: The time in milliseconds for the featureextraction, projective parsing, non-projective ap proximation, rest (r), the total training time perinstance, the average parsing (par.) time in mil liseconds for the test set and the training time in hours 0 1 2 3 0 5000 10000 15000 Spanish Figure 1: The difference of the labeled attachment score between the baseline parser and the parser with the Hash Kernel (y-axis) for increasing large training sets (x-axis). Table 3 contains the measured times for the Hash Kernel as used in Algorithm 2. The parserneeds 0.354 seconds in average to parse a sen tence of the English test set. This is 3.5 times faster than the baseline parser. The reason for that is the faster feature mapping of the Hash Kernel.Therefore, the measured time te for the feature ex traction and the calculation of the weight arrays are much lower than for the baseline parser. The training is about 19% slower since we could no longer use a file to store the feature indexes of the training examples because of the large number of negative features. We counted about twice the number of nonzero weights in the weight vector of the Hash Kernel compared to the baseline parser.For instance, we counted for English 17.34 Mil lions nonzero weights in the Hash Kernel and 8.47 Millions in baseline parser and for Chinese 18.28 Millions nonzero weights in the Hash Kernel and 8.76 Millions in the baseline parser. Table 6 shows. the scores for all languages of the shared task2009. The attachment scores increased for all languages. It increased most for Catalan and Span ish. These two corpora have the smallest training sets. We searched for the reason and found thatthe Hash Kernel provides an overproportional ac curacy gain with less training data compared to MIRA. Figure 1 shows the difference between the labeled attachment score of the parser with MIRA and the Hash Kernel for Spanish. The decreasing curve shows clearly that the Hash Kernel providesan overproportional accuracy gain with less train ing data compared to the baseline. This provides an advantage for small training corpora.However, this is probably not the main rea son for the high improvement, since for languageswith only slightly larger training sets such as Chinese the improvement is much lower and the gra dient at the end of the curve is so that a huge amount of training data would be needed to make the curve reach zero. Current CPUs have up to 12 cores and we will see soon CPUs with more cores. Also graphiccards provide many simple cores. Parsing algo rithms can use several cores. Especially, the tasks to extract the features and to calculate the weightarrays can be well implemented as parallel algo rithm. We could also successful parallelize theprojective parsing and the non-projective approximation algorithm. Algorithm 4 shows the paral lel feature extraction in pseudo code. The mainmethod prepares a list of tasks which can be per formed in parallel and afterwards it creates thethreads that perform the tasks. Each thread re moves from the task list an element, carries out the task and stores the result. This procedure is repeated until the list is empty. The main method waits until all threads are completed and returns the result. For the parallel algorithms, Table 5 shows the elapsed times depend on the number of 94 # Standard Features # Linear Features Linear G. Features Sibling Features 1 l,hf ,hp,d(h,d) 14 l,hp,h+1p,dp,d(h,d) 44 l,gp,dp,d+1p,d(h,d) 99 l,sl,hp,d(h,d)?r(h,d) 2 l,hf ,d(h,d) 15 l,hp,d-1p,dp,d(h,d) 45 l,gp,dp,d-1p,d(h,d) 100 l,sl,dp,d(h,d)?r(h,d) 3 l,hp,d(h,d) 16 l,hp,dp,d+1p,d(h,d) 46 l,gp,g+1p,d-1p,dp,d(h,d) 101 l,hl,dp,d(h,d)?r(h,d) 4 l,df ,dp,d(h,d) 17 l,hp,h+1p,d-1p,dp,d(h,d) 47 l,g-1p,gp,d-1p,dp,d(h,d) 102 l,dl,sp,d(h,d)?r(h,d) 5 l,hp,d(h,d) 18 l,h-1p,h+1p,d-1p,dp,d(h,d) 48 l,gp,g+1p,dp,d+1p,d(h,d) 75 l,?dm,?sm,d(h,d) 6 l,dp,d(h,d) 19 l,hp,h+1p,dp,d+1p,d(h,d) 49 l,g-1p,gp,dp,d+1p,d(h,d) 76 l,?hm,?sm,d(h,s) 7 l,hf ,hp,df ,dp,d(h,d) 20 l,h-1p,hp,dp,d-1p,d(h,d) 50 l,gp,g+1p,hp,d(h,d) Linear S. Features 8 l,hp,df ,dp,d(h,d) Grandchild Features 51 l,gp,g-1p,hp,d(h,d) 58 l,sp,s+1p,hp,d(h,d) 9 l,hf ,df ,dp,d(h,d) 21 l,hp,dp,gp,d(h,d,g) 52 l,gp,hp,h+1p,d(h,d) 59 l,sp,s-1p,hp,d(h,d) 10 l,hf ,hp,df ,d(h,d) 22 l,hp,gp,d(h,d,g) 53 l,gp,hp,h-1p,d(h,d) 60 l,sp,hp,h+1p,d(h,d) 11 l,hf ,df ,hp,d(h,d) 23 l,dp,gp,d(h,d,g) 54 l,gp,g+1p,h-1p,hp,d(h,d) 61 l,sp,hp,h-1p,d(h,d) 12 l,hf ,df ,d(h,d) 24 l,hf ,gf ,d(h,d,g) 55 l,g-1p,gp,h-1p,hp,d(h,d) 62 l,sp,s+1p,h-1p,d(h,d) 13 l,hp,dp,d(h,d) 25 l,df ,gf ,d(h,d,g) 56 l,gp,g+1p,hp,h+1p,d(h,d) 63 l,s-1p,sp,h-1p,d(h,d) 77 l,hl,hp,d(h,d) 26 l,gf ,hp,d(h,d,g) 57 l,g-1p,gp,hp,h+1p,d(h,d) 64 l,sp,s+1p,hp,d(h,d) 78 l,hl,d(h,d) 27 l,gf ,dp,d(h,d,g) Sibling Features 65 l,s-1p,sp,hp,h+1p,d(h,d) 79 l,hp,d(h,d) 28 l,hf ,gp,d(h,d,g) 30 l,hp,dp,sp,d(h,d) ?r(h,d) 66 l,sp,s+1p,dp,d(h,d) 80 l,dl,dp,d(h,d) 29 l,df ,gp,d(h,d,g) 31 l,hp,sp,d(h,d)?r(h,d) 67 l,sp,s-1p,dp,d(h,d) 81 l,dl,d(h,d) 91 l,hl,gl,d(h,d,g) 32 l,dp,sp,d(h,d)?r(h,d) 68 sp,dp,d+1p,d(h,d) 82 l,dp,d(h,d) 92 l,dp,gp,d(h,d,g) 33 l,pf ,sf ,d(h,d)?r(h,d) 69 sp,dp,d-1p,d(h,d) 83 l,dl,hp,dp,hl,d(h,d) 93 l,gl,hp,d(h,d,g) 34 l,pp,sf ,d(h,d)?r(h,d) 70 sp,s+1p,d-1p,dp,d(h,d) 84 l,dl,hp,dp,d(h,d) 94 l,gl,dp,d(h,d,g) 35 l,sf ,pp,d(h,d)?r(h,d) 71 s-1p,sp,d-1p,dp,d(h,d) 85 l,hl,dl,dp,d(h,d) 95 l,hl,gp,d(h,d,g) 36 l,sf ,dp,d(h,d)?r(h,d) 72 sp,s+1p,dp,d+1p,d(h,d) 86 l,hl,hp,dp,d(h,d) 96 l,dl,gp,d(h,d,g) 37 l,sf ,dp,d(h,d)?r(h,d) 73 s-1p,sp,dp,d+1p,d(h,d) 87 l,hl,dl,hp,d(h,d) 74 l,?dm,?gm,d(h,d) 38 l,df ,sp,d(h,d)?r(h,d) Special Feature 88 l,hl,dl,d(h,d) Linear G. Features 97 l,hl,sl,d(h,d)?r(h,d) 39 ?l,hp,dp,xpbetween h,d 89 l,hp,dp,d(h,d) 42 l,gp,g+1p,dp,d(h,d) 98 l,dl,sl,d(h,d)?r(h,d) 41 l,?hm,?dm,d(h,d) 43 l,gp,g-1p,dp,d(h,d) Table 4: Features Groups. l represents the label, h the head, d the dependent, s a sibling, and g a grandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance. used cores. The parsing time is 1.9 times fasteron two cores and 3.4 times faster on 4 cores. Hy per threading can improve the parsing times again and we get with hyper threading 4.6 faster parsingtimes. Hyper threading possibly reduces the over head of threads, which contains already our single core version. Algorithm 4: Parallel Feature Extraction A // weight arrays extract-features-and-calc-arrays(xi) data-list? {} // thread-save data list for w1 ? 1 to |xi| for w2 ? 1 to |xi| data-list? data-list ?{(w1, w2)} c? number of CPU cores for t? 1 to c Tt ? create-array-thread(t, xi,data-list) start array-thread Tt// start thread t for t? 1 to c join Tt// wait until thread t is finished A? A ? collect-result(Tt) return A // array-thread T d? remove-first-element(data-list) if d is empty then end-thread ... // extract features and calculate part d of A Cores te tp ta rest total pars. train. 1 379 21.3 18.2 1.5 420 354 45.8h 2 196 11.7 9.2 2.1 219 187 23.9h 3 138 8.9 6.5 1.6 155 126 16.6h 4 106 8.2 5.2 1.6 121 105 13.2h 4+4h 73.3 8.8 4.8 1.3 88.2 77 9.6hTable 5: Elapsed times in milliseconds for differ ent numbers of cores. The parsing time (pars.) are expressed in milliseconds per sentence and the training (train.) time in hours. The last row shows the times for 8 threads on a 4 core CPU with Hyper-threading. For these experiment, we set the clock speed to 3.46 Ghz in order to have the same clock speed for all experiments. ThresholdFor non-projective parsing, we use the NonProjective Approximation Algorithm of McDon ald and Pereira (2006). The algorithm rearranges edges in a dependency tree when they improve the score. Bohnet (2009) extended the algorithm by a threshold which biases the rearrangement of the edges. With a threshold, it is possible to gain a higher percentage of correct dependency links. We determined a threshold in experiments for Czech, English and German. In the experiment,we use the Hash Kernel and increase the thresh 95 System Average Catalan Chinese Czech English German Japanese Spanish Top CoNLL 09 85.77(1) 87.86(1) 79.19(4) 80.38(1) 89.88(2) 87.48(2) 92.57(3) 87.64(1) Baseline Parser 85.10 85.70 76.88 76.93 90.14 87.64 92.26 86.12 this work 86.33 87.45 76.99 80.96 90.33 88.06 92.47 88.13 Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al (2009), (2) Bohnet (2009), (3) Che et al. (2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel. The numbers in bold face mark the top scores. We used for Catalan, Chinese, Japanese and Spanish the projective parsing algorithm. old at the beginning in small steps by 0.1 and later in larger steps by 0.5 and 1.0. Figure 2 showsthe labeled attachment scores for the Czech, En glish and German development set in relation to the rearrangement threshold. The curves for all languages are a bit volatile. The English curve is rather flat. It increases a bit until about 0.3and remains relative stable before it slightly decreases. The labeled attachment score for Ger man and Czech increases until 0.3 as well and thenboth scores start to decrease. For English a thresh old between 0.3 and about 2.0 would work well. For German and Czech, a threshold of about 0.3is the best choice. We selected for all three lan guages a threshold of 0.3. 74 76 78 80 82 84 86 88 0 1 2 3 4 5 Czech English GermanFigure 2: English, German, and Czech labeled at tachment score (y-axis) for the development set in relation to the rearrangement threshold (x-axis).","ThresholdFor non-projective parsing, we use the NonProjective Approximation Algorithm of McDon ald and Pereira (2006). The algorithm rearranges edges in a dependency tree when they improve the score. Bohnet (2009) extended the algorithm by a threshold which biases the rearrangement of the edges. With a threshold, it is possible to gain a higher percentage of correct dependency links. We determined a threshold in experiments for Czech, English and German. In the experiment,we use the Hash Kernel and increase the thresh 95 System Average Catalan Chinese Czech English German Japanese Spanish Top CoNLL 09 85.77(1) 87.86(1) 79.19(4) 80.38(1) 89.88(2) 87.48(2) 92.57(3) 87.64(1) Baseline Parser 85.10 85.70 76.88 76.93 90.14 87.64 92.26 86.12 this work 86.33 87.45 76.99 80.96 90.33 88.06 92.47 88.13 Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al (2009), (2) Bohnet (2009), (3) Che et al. (2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel. The numbers in bold face mark the top scores. We used for Catalan, Chinese, Japanese and Spanish the projective parsing algorithm. old at the beginning in small steps by 0.1 and later in larger steps by 0.5 and 1.0. Figure 2 showsthe labeled attachment scores for the Czech, En glish and German development set in relation to the rearrangement threshold. The curves for all languages are a bit volatile. The English curve is rather flat. It increases a bit until about 0.3and remains relative stable before it slightly decreases. The labeled attachment score for Ger man and Czech increases until 0.3 as well and thenboth scores start to decrease. For English a thresh old between 0.3 and about 2.0 would work well. For German and Czech, a threshold of about 0.3is the best choice. We selected for all three lan guages a threshold of 0.3. 74 76 78 80 82 84 86 88 0 1 2 3 4 5 Czech English GermanFigure 2: English, German, and Czech labeled at tachment score (y-axis) for the development set in relation to the rearrangement threshold (x-axis)."
84,"In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.","In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. The field of research in natural language processing (NLP) applications for L2 language is constantly growing. This is largely driven by the ex panding population of L2 English speakers, whose varying levels of ability may require different types of NLP tools from those designed primarily for native speakers of the language. These include applications for use by the individual and within instructional contexts. Among the key tools are error-checking applications, focusing particularly on areas which learners find the most challenging. Prepositions and determiners are known to be oneof the most frequent sources of error for L2 En glish speakers, a finding supported by our analysisof a small error-tagged corpus we created (determiners 17% of errors, prepositions 12%). There fore, in developing a system for automatic error detection in L2 writing, it seems desirable to focus on these problematic, and very common, parts of speech (POS).This paper gives a brief overview of the prob lems posed by these POS and of related work. We c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. then present our proposed approach on both L1 and L2 data and discuss the results obtained so far. 2.1 Prepositions. Prepositions are challenging for learners because they can appear to have an idiosyncratic behaviour which does not follow any predictable pattern even across nearly identical contexts. For example, we say I study in Boston but I study at MIT; or He is independent of his parents, but dependent on his son. As it is hard even for L1 speakers to articulatethe reasons for these differences, it is not surprising that learners find it difficult to master preposi tions. 2.2 Determiners. Determiners pose a somewhat different problem from prepositions as, unlike them, their choice is more dependent on the wider discourse contextthan on individual lexical items. The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context. For example, wecan say boys like sport or the boys like sport, depending on whether we are making a general state ment about all boys or referring to a specific group.Equally, both she ate an apple and she ate the ap ple are grammatically well-formed sentences, butonly one may be appropriate in a given context, de pending on whether the apple has been mentioned previously. Therefore, here, too, it is very hard tocome up with clear-cut rules predicting every pos sible kind of occurrence. 169 Although in the past there has been some research on determiner choice in L1 for applications such as generation and machine translation output, work to date on automatic error detection in L2 writing hasbeen fairly limited. Izumi et al (2004) train a maximum entropy classifier to recognise various er rors using contextual features. They report results for different error types (e.g. omission - precision 75.7%, recall 45.67%; replacement - P 31.17%, R 8%), but there is no break-down of results byindividual POS. Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy. Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter. They report 80% precision and 30% recall. Finally, Gamon etal. (2008) use a complex system including a decision tree and a language model for both preposi tion and determiner errors, while Yi et al (2008)propose a web count-based system to correct de terminer errors (P 62%, R 41%).The work presented here displays some similar ities to the papers mentioned above in its use of a maximum entropy classifier and a set of features.However, our feature set is more linguistically sophisticated in that it relies on a full syntactic analysis of the data. It includes some semantic compo nents which we believe play a role in correct class assignment. determiners 4.1 Feature set. The approach proposed in this paper is based on the belief that although it is difficult to formulatehard and fast rules for correct preposition and determiner usage, there is enough underlying regularity of characteristic syntactic and semantic con texts to be able to predict usage to an acceptabledegree of accuracy. We use a corpus of grammat ically correct English to train a maximum entropyclassifier on examples of correct usage. The classifier can therefore learn to associate a given preposition or determiner to particular contexts, and re liably predict a class when presented with a novel instance of a context for one or the other. The L1 source we use is the British National Head noun ?apple? Number singular Noun type count Named entity? no WordNet category food, plant Prep modification? yes, ?on? Object of Prep? no Adj modification? yes, ?juicy? Adj grade superlative POS ?3 VV, DT, JJS, IN, DT, NN Table 1: Determiner feature set for Pick the juiciest apple on the tree. POS modified verb Lexical item modified ?drive? WordNet Category motion Subcat frame pp to POS of object noun Object lexical item ?London? Named entity? yes, type = location POS ?3 NNP, VBD, NNP Grammatical relation iobj Table 2: Preposition feature set for John drove to London.Corpus (BNC) as we believe this offers a represen tative sample of different text types. We represent training and testing items as vectors of values for linguistically motivated contextual features. Our feature vectors include 18 feature categories for determiners and 13 for prepositions; the main ones are illustrated in Table 1 and Table 2 respectively. Further determiner features note whether the nounis modified by a predeterminer, possessive, nu meral, and/or a relative clause, and whether it ispart of a ?there is. . . ? phrase. Additional preposi tion features refer to the grade of any adjectives or adverbs modified (base, comparative, superlative) and to whether the items modified are modified by more than one PP 1 . In De Felice and Pulman (2007), we described some of the preprocessing required and offered some motivation for this approach. As for ourchoice of features, we aim to capture all the ele ments of a sentence which we believe to have an effect on preposition and determiner choice, and which can be easily extracted automatically - this is a key consideration as all the features derivedrely on automatic processing of the text. Grammatical relations refer to RASP-style grammatical re lations between heads and complements in which the preposition occurs (see e.g. (Briscoe et al, 1 A full discussion of each feature, including motivation for its inclusion and an assessment of its contribution to the model, is found in De Felice (forthcoming). 170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)). Semantic word type information is takenfrom WordNet lexicographer classes, 40 broad se mantic categories which all nouns and verbs in WordNet belong to 2 (e.g. ?verb of motion?, ?noun denoting food?), while the POStags are from the Penn Treebank tagset - we note the POS of three words either side of the target word 3 . For each. occurrence of a preposition or determiner in the corpus, we obtain a feature vector consisting ofthe preposition or determiner and its context, de scribed in terms of the features noted above. 5.1 Prepositions. At the moment, we restrict our analysis to the nine most frequent prepositions in the data: at, by, for, from, in, of, on, to, and with, to ensure a sufficient amount of data for training. This gives a training dataset comprising 8,898,359 instances. We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance. Before testing our model on learner data, it is important to ascertain that it can correctlyassociate prepositions to a given context in gram matical, well-edited data. We therefore tested themodel on a section of the BNC not used in train ing, section J. Our best result to date is 70.06% accuracy (test set size: 536,193). Table 3 relates our results to others reported in the literature on comparable tasks. The baseline refers to always choosing the most frequent option, namely of.We can see that our model?s performance com pares favourably to the best results in the literature, although direct comparisons are hard to draw sincedifferent groups train and test on different preposi tion sets and on different types of data (British vs. American English, BNC vs. news reports, and so 2 No word sense disambiguation was performed at this stage. 3 In NPs with a null determiner, the target is the head noun. 4 Developed by James Curran. Proportion of training data Precision Recall of 27.83% (2,501,327) 74.28% 90.47% to 20.64% (1,855,304) 85.99% 81.73% in 17.68% (1,589,718) 60.15% 67.60% for 8.01% (720,369) 55.47% 43.78% on 6.54% (587,871) 58.52% 45.81% with 6.03% (541,696) 58.13% 46.33% at 4.72% (424,539) 57.44% 52.12% by 4.69% (421,430) 63.83% 56.51% from 3.86% (347,105) 59.20% 32.07% Table 4: L1 results - individual prepositions on). Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be. The figure reported here refers to the latter task,as it is the most similar to the one we are evalu ating. Additionally, Chodorow et al also discusssome modifications to their model which can in crease accuracy; the result noted here is the one more directly comparable to our own approach. 5.1.1 Further discussion To fully assess the model?s performance on the L1data, it is important to consider factors such as performance on individual prepositions, the relation ship between training dataset size and accuracy, and the kinds of errors made by the model.Table 4 shows the classifier?s performance on in dividual prepositions together with the size of their training datasets. At first glance, a clear correlationappears between the amount of data seen in training and precision and recall, as evidenced for ex ample by of or to, for which the classifier achievesa very high score. In other cases, however, the cor relation is not so clear-cut. For example by has one of the smallest data sets in training but higher scores than many of the other prepositions, whilefor is notable for the opposite reason, namely hav ing a large dataset but some of the lowest scores. The absence of a definite relation between dataset size and performance suggests that theremight be a cline of ?learnability? for these prepo sitions: different prepositions? contexts may be more or less uniquely identifiable, or they mayhave more or fewer senses, leading to less confusion for the classifier. One simple way of verify ing the latter case is by looking at the number of senses assigned to the prepositions by a resource 171 Target prep Confused with at by for from in of on to with at xx 4.65% 10.82% 2.95% 36.83% 19.46% 9.17% 10.28% 5.85% by 6.54% xx 8.50% 2.58% 41.38% 19.44% 5.41% 10.04% 6.10% for 8.19% 3.93% xx 1.91% 25.67% 36.12% 5.60% 11.29% 7.28% from 6.19% 4.14% 6.72% xx 26.98% 26.74% 7.70% 16.45% 5.07% in 7.16% 9.28% 10.68% 3.01% xx 43.40% 10.92% 8.96% 6.59% of 3.95% 2.00% 18.81% 3.36% 40.21% xx 9.46% 14.77% 7.43% on 5.49% 3.85% 8.66% 2.29% 32.88% 27.92% xx 12.20% 6.71% to 9.77% 3.82% 11.49% 3.71% 24.86% 27.95% 9.43% xx 8.95% with 3.66% 4.43% 12.06% 2.24% 28.08% 26.63% 6.81% 16.10% xx Table 5: Confusion matrix for L1 data - prepositions such as the Oxford English Dictionary. However, we find no good correlation between the two as the preposition with the most senses is of (16), and that with the fewest is from (1), thus negating the idea that fewer senses make a preposition easierto learn. The reason may therefore be found else where, e.g. in the lexical properties of the contexts. A good picture of the model?s errors can be had by looking at the confusion matrix in Table 5,which reports, for each preposition, what the clas sifier?s incorrect decision was. Analysis of these errors may establish whether they are related to thedataset size issue noted above, or have a more lin guistically grounded explanation.From the table, the frequency effect appears evi dent: in almost every case, the three most frequentwrong choices are the three most frequent prepo sitions, to, of, and in, although interestingly not inthat order, in usually being the first choice. Conversely, the less frequent prepositions are less of ten suggested as the classifier?s choice. This effectprecludes the possibility at the moment of draw ing any linguistic conclusions. These may only be gleaned by looking at the errors for the three more frequent prepositions. We see for example that there seems to be a strong relation between of and for, the cause of which is not immediately clear: perhaps they both often occur within noun phrases(e.g. book of recipes, book for recipes). More pre dictable is the confusion between to and from, andbetween locative prepositions such as to and at, al though the effect is less strong for other potentially confusable pairs such as in and at or on. Table 6 gives some examples of instances where the classifier?s chosen preposition differs from thatfound in the original text. In most cases, the clas sifier?s suggestion is also grammatically correct, Classifier choice Correct phrase demands of the sector demands for. condition for development condition of. travel to speed travel at. look at the USA look to. .Table 6: Examples of classifier errors on preposi tion L1 task Author Accuracy Baseline 59.83% Han et al 06 83.00% Gamon et al 08 86.07% Turner and Charniak 07 86.74% Our model 92.15% Table 7: Classifier performance - L1 determiners but the overall meaning of the phrases changes somewhat. For example, while the demands of the sector are usually made by the sector itself, the demands for the sector suggest that someoneelse may be making them. These are subtle dif ferences which it may be impossible to capture without a more sophisticated understanding of the wider context. The example with travel, on the other hand, yields an ungrammatical result. We assume thatthe classifier has acquired a very strong link be tween the lexical item travel and the preposition tothat directs it towards this choice (cf. also the ex ample of look at/to). This suggests that individual lexical items play an important role in preposition choice along with other more general syntactic and semantic properties of the context. 172 %of training data Prec. Recall a 9.61% (388,476) 70.52% 53.50% the 29.19% (1,180,435) 85.17% 91.51% null 61.20% (2,475,014) 98.63% 98.79% Table 8: L1 results - individual determiners 5.2 Determiners. For the determiner task, we also consider only the three most frequent cases (a, the, null), which gives us a training dataset consisting of 4,043,925 instances. We achieve accuracy of 92.15% on theL1 data (test set size: 305,264), as shown in Table 7. Again, the baseline refers to the most fre quent class, null. The best reported results to date on determiner selection are those in Turner and Charniak (2007). Our model outperforms their n-gram languagemodel approach by over 5%. Since the two approaches are not tested on the same data this com parison is not conclusive, but we are optimistic that there is a real difference in accuracy since the type of texts used are not dissimilar. As in the case of the prepositions, it is interesting to see whether this high performance is equally distributed across thethree classes; this information is reported in Ta ble 8. Here we can see that there is a very strongcorrelation between amount of data seen in training and precision and recall. The indefinite arti cle?s lower ?learnability?, and its lower frequency appears not to be peculiar to our data, as it is also found by Gamon et al among others.The disparity in training is a reflection of the dis tribution of determiners in the English language. Perhaps if this imbalance were addressed, the model would more confidently learn contexts of use for a, too, which would be desirable in view of using this information for error correction. On theother hand, this would create a distorted represen tation of the composition of English, which maynot be what we want in a statistical model of lan guage. We plan to experiment with smaller scale, more similar datasets to ascertain whether the issue is one of training size or of inherent difficulty in learning about the indefinite article?s occurrence.In looking at the confusion matrix for determin ers (Table 9), it is interesting to note that for theclassifier?s mistakes involving a or the, the erroneous choice is in the almost always the other de terminer rather than the null case. This suggeststhat the frequency effect is not so strong as to over Target det Confused with a the null a xx 92.92% 7.08% the 80.66% xx 19.34% null 14.51% 85.49% xx Table 9: Confusion matrix for L1 determiners ride any true linguistic information the model has acquired, otherwise the predominant choice wouldalways be the null case. On the contrary, these results show that the model is indeed capable of distinguishing between contexts which require a determiner and those which do not, but requires fur ther fine tuning to perform better in knowing which of the two determiner options to choose. Perhaps the introduction of a discourse dimension might assist in this respect. We plan to experiment withsome simple heuristics: for example, given a se quence ?Determiner Noun?, has the noun appeared in the preceding few sentences? If so, we might expect the to be the correct choice rather than a. 6.1 Working with L2 text. To evaluate the model?s performance on learner data, we use a subsection of the Cambridge Learner Corpus (CLC) 5 . We envisage our model to. eventually be of assistance to learners in analysingtheir writing and identifying instances of preposi tion or determiner usage which do not correspond to what it has been trained to expect; the more probable instance would be suggested as a more appropriate alternative. In using NLP tools and techniques which have been developed with and for L1 language, a loss of performance on L2 data is to be expected. These methods usually expect grammatically well-formed input; learner text is often ungrammatical, misspelled, and different in content and structure from typical L1 resources such as the WSJ and the BNC. 6.2 Prepositions. For the preposition task, we extract 2523 instances of preposition use from the CLC (1282 correct, 1241 incorrect) and ask the classifier to mark them 5 The CLC is a computerised database of contemporary written learner English (currently over 25m words). It wasdeveloped jointly by Cambridge ESOL and Cambridge Uni versity Press. The Cambridge Error Coding System has been developed and applied manually to the data by Cambridge University Press. 173 Instance type Accuracy Correct 66.7% Incorrect 70%Table 10: Accuracy on L2 data - prepositions. Ac curacy on incorrect instances refers to the classifier successfully identifying the preposition in the text as not appropriate for that context. as correct or incorrect. The results from this taskare presented in Table 10. These first results sug gest that the model is fairly robust: the accuracy rate on the correct data, for example, is not much lower than that on the L1 data. In an application designed to assist learners, it is important to aim to reduce the rate of false alarms - cases where the original is correct, but the model flags an error - toa minimum, so it is positive that this result is com paratively high. Accuracy on error identification is at first glance even more encouraging. However, ifwe look at the suggestions the model makes to re place the erroneous preposition, we find that theseare correct only 51.5% of the time, greatly reduc ing its usefulness. 6.2.1 Further discussion A first analysis of the classifier?s decisions and itserrors points to various factors which could be im pairing its performance. Spelling mistakes in theinput are one of the most immediate ones. For ex ample, in the sentence I?m Franch, responsable on the computer services, the classifier is not able to suggest a correct alternative to the erroneous on:since it does not recognise the adjective as a misspelling of responsible, it loses the information associated with this lexical feature, which could po tentially determine the preposition choice. A more complex problem arises when poor grammar in the input misleads the parser so thatthe information it gives for a sentence is incor rect, especially as regards PP attachment. In this example, I wold like following equipment to my speech: computer, modem socket and microphone, the missing the leads the parser to treat following as a verb, and believes it to be the verb to which the preposition is attached. It therefore suggests from as a correction, which is a reasonable choice given the frequency of phrases such as to follow from. However, this was not what the PP was meant to modify: impaired performance from the parser could be a significant negative factor in the model?s performance. It would be interesting to test themodel on texts written by students of different lev els of proficiency, as their grammar may be more error-free and more likely to be parsed correctly. Alternatively, we could modify the parser so as to skip cases where it requires several attempts before producing a parse, as these more challenging casescould be indicative of very poorly structured sentences in which misused prepositions are depen dent on more complex errors.A different kind of problem impacting our accu racy scores derives from those instances where theclassifier selects a preposition which can be cor rect in the given context, but is not the correct one in that particular case. In the example I received a beautiful present at my birthday, the classifier identifies the presence of the error, and suggests the grammatically and pragmatically appropriate correction for. The corpus annotators, however, indicate on as the correct choice. Since we use their annotations as the benchmark against which to evaluate the model, this instance is counted as the classifier being wrong because it disagrees with the annotators. A better indication of the model?sperformance may be to independently judge its de cisions, to avoid being subject to the annotators?bias. Finally, we are beginning to look at the rela tions between preposition errors and other types oferror such as verb choice, and how these are anno tated in the data. An overview of the classifier?s error patterns forthe data in this task shows that they are largely similar to those observed in the L1 data. This sug gests that the gap in performance between L1 and L2 is due more to the challenges posed by learner text than by inherent shortcomings in the model, and therefore that the key to better performance is likely to lie in overcoming these problems. In future work we plan to use L2 data where someof the spelling errors and non-preposition or deter miner errors have been corrected so that we can see which of the other errors are worth focussing on first. 6.3 Determiners. Our work on determiner error correction is still in the early stages. We follow a similar procedure to the prepositions task, selecting a number of both correct and incorrect instances. On the former (set size 2000) accuracy is comparable to that on L1data: 92.2%. The danger of false alarms, then, ap pears not to be as significant as for the prepositions 174 task. On the incorrect instances (set size ca. 1200), however, accuracy is less than 10%. Preliminary error analysis shows that the modelis successful at identifying cases of misused deter miner, e.g. a for the or vice versa, doing so in overtwo-thirds of cases. However, by far the most fre quent error type for determiners is not confusion between indefinite and definite article, but omitting an article where one is needed. At the moment, themodel detects very few of these errors, no doubt in fluenced by the preponderance of null cases seen in training. Furthermore, some of the issues raised earlier in discussing the application of NLP tools to L2 language hold for this task, too. In addition to those, though, in this task more than for prepositions we believe that differences intext type between the training texts - the BNC and the testing material - learner essays - has a sig nificant negative effect on the model. In this task,the lexical items play a crucial role in class assign ment. If the noun in question has not been seen in training, the classifier may be unable to make an informed choice. Although the BNC comprises a wide variety of texts, there may not be a sufficient number covering topics typical of learner essays, such as ?business letters? or ?postcards to penpals?.Also, the BNC was created with material from almost 20 years ago, and learners writing in contem porary English may use lexical items which are notvery frequently seen in the BNC. A clear exam ple of this discrepancy is the noun internet, which requires the definite article in English, but not inseveral other languages, leading to countless sen tences such as I saw it in internet, I booked it on internet, and so on. This is one of the errors themodel never detects: a fact which is not surpris ing when we consider that this noun occurs only four times in the whole of the training data. It may be therefore necessary to consider using alternative sources of training data to overcome this problem and improve the classifier?s performance. In developing this model, our first aim was not to create something which learns like a human, butsomething that works in the best and most effi cient possible way. However, it is interesting to see whether human learners and classifiers display similar patterns of errors in preposition choice.This information has twofold value: as well as being of pedagogical assistance to instructors of En glish L2, were the classifier to display student-like error patterns, insights into ?error triggers? could be derived from the L2 pedagogical literature to improve the classifier. The analysis of the typesof errors made by human learners yields some insights which might be worthy of further investi gation. A clear one is the confusion between the three locative and temporal prepositions at, in, and on (typical sentence: The training programme will start at the 1st August). This type of error is made often by both learners and the model on both types of data, suggesting that perhaps further attentionto features might be necessary to improve discrim ination between these three prepositions.There are also interesting divergences. For ex ample, a common source of confusion in learners is between by and from, as in I like it becauseit?s from my favourite band. However, this confu sion is not very frequent in the model, a difference which could be explained either by the fact that, as noted above, performance on from is very low and so the classifier is unlikely to suggest it, or that in training the contexts seen for by are sufficiently distinctive that the classifier is not misled like the learners. Finally, a surprising difference comes from looking at what to is confused with. The model often suggests at where to would be correct. This is perhaps not entirely unusual as both can occur with locative complements (one can go to a placeor be at a place) and this similarity could be con fusing the classifier. Learners, however, although they do make this kind of mistake, are much more hampered by the confusion between for and to, as in She was helpful for me or This is interesting for you. In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense. We can hypothesise that the classifier is less distracted by these cases because the effect of the lexical features is stronger. A more detailed discussion of the issues arising from the comparison of confusion pairs cannot be had here. However, in noting both divergences and similarities between the two learners, human and machine, we may be able to derive useful insights into the way the learning processes operate, and what factors could be more or less important for them. 175","In developing this model, our first aim was not to create something which learns like a human, butsomething that works in the best and most effi cient possible way. However, it is interesting to see whether human learners and classifiers display similar patterns of errors in preposition choice.This information has twofold value: as well as being of pedagogical assistance to instructors of En glish L2, were the classifier to display student-like error patterns, insights into ?error triggers? could be derived from the L2 pedagogical literature to improve the classifier. The analysis of the typesof errors made by human learners yields some insights which might be worthy of further investi gation. A clear one is the confusion between the three locative and temporal prepositions at, in, and on (typical sentence: The training programme will start at the 1st August). This type of error is made often by both learners and the model on both types of data, suggesting that perhaps further attentionto features might be necessary to improve discrim ination between these three prepositions.There are also interesting divergences. For ex ample, a common source of confusion in learners is between by and from, as in I like it becauseit?s from my favourite band. However, this confu sion is not very frequent in the model, a difference which could be explained either by the fact that, as noted above, performance on from is very low and so the classifier is unlikely to suggest it, or that in training the contexts seen for by are sufficiently distinctive that the classifier is not misled like the learners. Finally, a surprising difference comes from looking at what to is confused with. The model often suggests at where to would be correct. This is perhaps not entirely unusual as both can occur with locative complements (one can go to a placeor be at a place) and this similarity could be con fusing the classifier. Learners, however, although they do make this kind of mistake, are much more hampered by the confusion between for and to, as in She was helpful for me or This is interesting for you. In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense. We can hypothesise that the classifier is less distracted by these cases because the effect of the lexical features is stronger. A more detailed discussion of the issues arising from the comparison of confusion pairs cannot be had here. However, in noting both divergences and similarities between the two learners, human and machine, we may be able to derive useful insights into the way the learning processes operate, and what factors could be more or less important for them. 175"
85,"We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.","We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships. The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization, search, and dialog has been highlighted by a number of recent efforts (Barzilay & McKeown 2001; Shinyama et al 2002; Lee & Barzilay 2003; Lin & Pantel 2001). While several different learning methods have been applied to this problem, all share a need for large amounts of data in the form of pairs or sets of strings that are likely to exhibit lexical and/or structural paraphrase alternations. One approach1 1 An alternative approach involves identifying anchor points--pairs of words linked in a known way--and collecting the strings that intervene. (Shinyama, et al 2002; Lin & Pantel 2001). Since our interest is in that has been successfully used is edit distance, a measure of similarity between strings. The assumption is that strings separated by a small edit distance will tend to be similar in meaning: The leading indicators measure the economy? The leading index measures the economy?. Lee & Barzilay (2003), for example, use Multi Sequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts. Their goal is to extract sentential templates that can be used in high-precision generation of paraphrase alter nations within a limited domain. Our goal here is rather different: our interest lies in constructing a monolingual broad-domain corpus of pairwise aligned sentences. Such data would be amenable to conventional statistical machine translation (SMT) techniques (e.g., those discussed in Och & Ney 2003).2 In what follows we compare two strategies for unsupervised construction of such a corpus, one employing string similarity and the other associating sentences that may overlap very little at the string level. We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text. We show that although the edit distance corpus is well-suited as training data for the alignment algorithms currently used in SMT, it is an incomplete source of information about paraphrase relations, which exhibit many of the characteristics of comparable bilingual corpora or free translations. Many of the more complex alternations that characterize monolingual paraphrase, such as large-scale lexical alternations and constituent reorderings, are not readily learning sentence level paraphrases, including major constituent reorganizations, we do not address this approach here. 2 Barzilay & McKeown (2001) consider the possibility of using SMT machinery, but reject the idea because of the noisy, comparable nature of their dataset. captured by edit distance techniques, which conflate semantic similarity with formal similarity. We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques. Our two paraphrase datasets are distilled from a corpus of news articles gathered from thousands of news sources over an extended period. While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, Shinyama et al 2002; Barzilay and Lee 2003) have been restricted to at most two news sources. Our work represents what we believe to be the first attempt to exploit the explosion of news coverage on the Web, where a single event can generate scores or hundreds of different articles within a brief period of time. Some of these articles represent minor rewrites of an original AP or Reuters story, while others represent truly distinct descriptions of the same basic facts. The massive redundancy of information conveyed with widely varying surface strings is a resource begging to be exploited. Figure 1 shows the flow of our data collection process. We begin with sets of pre-clustered URLs which point to news articles on the Web, representing thousands of different news sources. The clustering algorithm takes into account the full text of each news article, in addition to temporal cues, to produce a set of topically and temporally related articles. Our method is believed to be independent of the specific clustering technology used. The story text is isolated from a sea of advertisements and other miscellaneous text through use of a supervised HMM. Altogether we collected 11,162 clusters in an 8 month period, assembling 177,095 articles with an average of 15.8 articles per cluster. The clusters are generally coherent in topic and focus. Discrete events like disasters, business announcements, and deaths tend to yield tightly focused clusters, while ongoing stories like the SARS crisis tend to produce less focused clusters. While exact duplicate articles are filtered out of the clusters, many slightly-rewritten variants remain. 2.1 Extracting Sentential Paraphrases. Two separate techniques were employed to extract likely pairs of sentential paraphrases from these clusters. The first used string edit distance, counting the number of lexical deletions and insertions needed to transform one string into another. The second relied on a discourse-based heuristic, specific to the news genre, to identify likely paraphrase pairs even when they have little superficial similarity. A simple edit distance metric (Levenshtein 1966) was used to identify pairs of sentences within a cluster that are similar at the string level. First, each sentence was normalized to lower case and paired with every other sentence in the cluster. Pairings that were identical or differing only by punctuation were rejected, as were those where the shorter sentence in the pair was less than two thirds the length of the longer, this latter constraint in effect placing an upper bound on edit distance relative to the length of the sentence. Pairs that had been seen before in either order were also rejected. Filtered in this way, our dataset yields 139K non identical sentence pairs at a Levenshtein distance of n ? 12. 3 Mean Levenshtein distance was 5.17, and mean sentence length was 18.6 words. We will refer to this dataset as L12. 3.1.1 First sentences The second extraction technique was specifically intended to capture paraphrases which might contain very different sets of content words, word order, and so on. Such pairs are typically used to illustrate the phenomenon of paraphrase, but precisely because their surface dissimilarity renders automatic discovery difficult, they have generally not been the focus of previous computational approaches. In order to automatically identify sentence pairs of this type, we have attempted to take advantage of some of the unique characteristics of the dataset. The topical clustering is sufficiently precise to ensure that, in general, articles in the same cluster overlap significantly in overall semantic content. Even so, any arbitrary pair of sentences from different articles within a cluster is unlikely to exhibit a paraphrase relationship: The Phi-X174 genome is short and compact. This is a robust new step that allows us to make much larger pieces. To isolate just those sentence pairs that represent likely paraphrases without requiring significant string similarity, we exploited a common journalistic convention: the first sentence or two of 3A maximum Levenshtein distance of 12 was selected for the purposes of this paper on the basis of experiments with corpora extracted at various edit distances. a newspaper article typically summarize its content. One might reasonably expect, therefore, that initial sentences from one article in a cluster will be paraphrases of the initial sentences in other articles in that cluster. This heuristic turns out to be a powerful one, often correctly associating sentences that are very different at the string level: In only 14 days, US researchers have created an artificial bacteria-eating virus from synthetic genes. An artificial bacteria-eating virus has been made from synthetic genes in the record time of just two weeks. Also consider the following example, in which related words are obscured by different parts of speech: Chosun Ilbo, one of South Korea's leading newspapers, said North Korea had finished developing a new ballistic missile last year and was planning to deploy it. The Chosun Ilbo said development of the new missile, with a range of up to %%number%% kilometres (%%number%% miles), had been completed and deployment was imminent. A corpus was produced by extracting the first two sentences of each article, then pairing these across documents within each cluster. We will refer to this collection as the F2 corpus. The combination of the first-two sentences heuristic plus topical article clusters allows us to take advantage of meta-information implicit in our corpus, since clustering exploits lexical information from the entire document, not just the few sentences that are our focus. The assumption that two first sentences are semantically related is thus based in part on linguistic information that is external to the sentences themselves. Sometimes, however, the strategy of pairing sentences based on their cluster and position goes astray. This would lead us to posit a paraphrase relationship where there is none: Terence Hope should have spent most of yesterday in hospital performing brain surgery. A leading brain surgeon has been suspended from work following a dispute over a bowl of soup. To prevent too high an incidence of unrelated sentences, one string-based heuristic filter was found useful: a pair is discarded if the sentences do not share at least 3 words of 4+ characters. This constraint succeeds in filtering out many unrelated pairs, although it can sometimes be too restrictive, excluding completely legitimate paraphrases: There was no chance it would endanger our planet, astronomers said. NASA emphasized that there was never danger of a collision. An additional filter ensured that the word count of the shorter sentence is at least one-half that of the longer sentence. Given the relatively long sentences in our corpus (average length 18.6 words), these filters allowed us to maintain a degree of semantic relatedness between sentences. Accordingly, the dataset encompasses many paraphrases that would have been excluded under a more stringent edit-distance threshold, for example, the following non-paraphrase pair that contain an element of paraphrase: A staggering %%number%% million Americans have been victims of identity theft in the last five years , according to federal trade commission survey out this week. In the last year alone, %%number%% million people have had their identity purloined. Nevertheless, even after filtering in these ways, a significant amount of unfiltered noise remains in the F2 corpus, which consisted of 214K sentence pairs. Out of a sample of 448 held-out sentence pairs, 118 (26.3%) were rated by two independent human evaluators as sentence-level paraphrases, while 151 (33.7%) were rated as partial paraphrases. The remaining ~40% were assessed as News article clusters: URLs Download URLs, Isolate content (HMM), Sentence separate Textual content of articles Select and filter first sentence pairs Approximately parallel monolingual corpus Figure 1. Data collection unrelated. 4 Thus, although the F2 data set is nominally larger than the L12 data set, when the noise factor is taken into account, the actual number of full paraphrase sentences in this data set is estimated to be in the region of 56K sentences, with a further estimated 72K sentences containing some paraphrase material that might be a potential source of alignment. Some of these relations captured in this data can be complex. The following pair, for example, would be unlikely to pass muster on edit distance grounds, but nonetheless contains an inversion of deep semantic roles, employing different lexical items. The Hartford Courant reported %%day%% that Tony Bryant said two friends were the killers. A lawyer for Skakel says there is a claim that the murder was carried out by two friends of one of Skakel's school classmates, Tony Bryan. The F2 data also retains pairs like the following that involve both high-level semantic alternations and long distance dependencies: Two men who robbed a jeweller's shop to raise funds for the Bali bombings were each jailed for %%number%% years by Indonesian courts today. An Indonesian court today sentenced two men to %%number%% years in prison for helping finance last year's terrorist bombings in Bali by robbing a jewelry store. These examples do not by any means exhaust the inventory of complex paraphrase types that are commonly encountered in the F2 data. We encounter, among other things, polarity alternations, including those involving long distance dependencies, and a variety of distributed paraphrases, with alignments spanning widely separated elements. 3.2 Word Error Alignment Rate. An objective scoring function was needed to compare the relative success of the two data collection strategies sketched in 2.1.1 and 2.1.2. Which technique produces more data? Are the types of data significantly different in character or utility? In order to address such questions, we used word Alignment Error Rate (AER), a metric borrowed from the field of statistical machine translation (Och & Ney 2003). AER measures how accurately an automatic algorithm can align words in corpus of parallel sentence pairs, with a human 4 This contrasts with 16.7% pairs assessed as unrelated in a 10,000 pair sampling of the L12 data. tagged corpus of alignments serving as the gold standard. Paraphrase data is of course monolingual, but otherwise the task is very similar to the MT alignment problem, posing the same issues with one-to-many, many-to-many, and one/many-to null word mappings. Our a priori assumption was that the lower the AER for a corpus, the more likely it would be to yield learnable information about paraphrase alternations. We closely followed the evaluation standards established in Melamed (2001) and Och & Ney (2000, 2003). Following Och & Ney?s methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required). Differences were then highlighted and the annotators were asked to review these cases. Finally we combined the two annotations into a single gold standard in the following manner: if both annotators agreed that an alignment should be SURE, then the alignment was marked as sure in the gold-standard; otherwise the alignment was marked as POSSIBLE. To compute Precision, Recall, and Alignment Error Rate (AER) for the twin datasets, we used exactly the formulae listed in Och & Ney (2003). Let A be the set of alignments in the comparison, S be the set of SURE alignments in the gold standard, and P be the union of the SURE and POSSIBLE alignments in the gold standard. Then we have: || ||precision A PA ? = || || recall S SA ? = || ||AER SA SAPA + ?+? = We held out a set of news clusters from our training data and randomly extracted two sets of sentence pairs for blind evaluation. The first is a set of 250 sentence pairs extracted on the basis of an edit distance of 5 ? n ? 20, arbitrarily chosen to allow a range of reasonably divergent candidate pairs. These sentence pairs were checked by an independent human evaluator to ensure that they contained paraphrases before they were tagged for alignments. The second set comprised 116 sentence pairs randomly selected from the set of first-two sentence pairs. These were likewise hand vetted by independent human evaluators. After an initial training pass and refinement of the linking specification, interrater agreement measured in terms of AER5 was 93.1% for the edit distance test set versus 83.7% for the F2 test set, suggestive of the greater variability in the latter data set. 3.3 Data Alignment. Each corpus was used as input to the word alignment algorithms available in Giza++ (Och & Ney 2000). Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000). Giza++ accepts as input a corpus of sentence pairs and produces as output a Viterbi alignment of that corpus as well as the parameters for the model that produced those alignments. While these models have proven effective at the word alignment task (Mihalcea & Pedersen 2003), there are significant practical limitations in their output. Most fundamentally, all alignments have either zero or one connection to each target word. Hence they are unable to produce the many-to many alignments required to identify correspondences with idioms and other phrasal chunks. To mitigate this limitation on final mappings, we follow the approach of Och (2000): we align once in the forward direction and again in the backward direction. These alignments can subsequently be recombined in a variety of ways, 5 The formula for AER given here and in Och & Ney (2003) is intended to compare an automatic alignment against a gold standard alignment. However, when comparing one human against another, both comparison and reference distinguish between SURE and POSSIBLE links. Because the AER is asymmetric (though each direction differs by less than 5%), we have presented the average of the directional AERs. such as union to maximize recall or intersection to maximize precision. Och also documents a method for heuristically recombining the unidirectional alignments intended to balance precision and recall. In our experience, many alignment errors are present in one side but not the other, hence this recombination also serves to filter noise from the process. Table 1 shows the results of training translation models on data extracted by both methods and then tested on the blind data. The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words). The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data. The nearly comparable numbers for the two training data sets, at 13.2% and 14.7% respectively, suggest that the L12 training corpus provides no substantive advantage over the F2 data when tested on the more complex test data. This is particularly striking given the noise inherent in the F2 training data. To explore some of the differences between the training sets, we hand-examined a random sample of sentence pairs from each corpus type. The most common paraphrase alternations that we observed fell into the following broad categories: ? Elaboration: Sentence pairs can differ in total information content, with an added word, phrase or clause in one sentence that has no Training Data Type: L12 F2 L12 F2 Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic Precision 87.46% 86.44% 85.07% 84.16% Recall 89.52% 82.64% 88.70% 86.55% AER 11.58% 15.41% 13.24% 14.71% Identical word precision 89.36% 88.79% 92.92% 93.41% Identical word recall 89.50% 83.10% 93.49% 92.47% Identical word AER 10.57% 14.14% 6.80% 7.06% Non-Identical word precision 76.99% 71.86% 60.54% 53.69% Non-Identical word recall 90.22% 69.57% 59.50% 50.41% Non-Identical word AER 20.88% 28.57% 39.81% 47.46% Table 1. Precision, recall, and alignment error rates (AER) for F2 and L12 counterpart in the other (e.g. the NASDAQ / the tech-heavy NASDAQ). Phrasal: An entire group of words in one sentence alternates with one word or a phrase in the other. Some are non-compositional idioms (has pulled the plug on / is dropping plans for); others involve different phrasing (electronically / in electronic form, more than a million people / a massive crowd). Spelling: British/American sources system atically differ in spellings of common words (colour / color); other variants also appear (email / e-mail). Synonymy: Sentence pairs differ only in one or two words (e.g. charges / accusations), suggesting an editor?s hand in modifying a single source sentence. Anaphora: A full NP in one sentence corresponds to an anaphor in the other (Prime Minister Blair / He). Cases of NP anaphora (ISS / the Atlanta-based security company) are also common in the data, but in quantifying paraphrase types we restricted our attention to the simpler case of pronominal anaphora. Reordering: Words, phrases, or entire constituents occur in different order in two related sentences, either because of major syntactic differences (e.g. topicalization, voice alternations) or more local pragmatic choices (e.g. adverb or prepositional phrase placement). These categories do not cover all possible alternations between pairs of paraphrased sentences; moreover, categories often overlap in the same sequence of words. It is common, for example, to find instances of clausal Reordering combined with Synonymy. Figure 2 shows a hand-aligned paraphrase pair taken from the F2 data. This pair displays one Spelling alternation (defence / defense), one Reordering (position of the ?since? phrase), and one example of Elaboration (terror attacks occurs in only one sentence). To quantify the differences between L12 and F2, we randomly chose 100 sentence pairs from each dataset and counted the number of times each phenomenon was encountered. A given sentence pair might exhibit multiple instances of a single phenomenon, such as two phrasal paraphrase changes or two synonym replacements. In this case all instances were counted. Lower-frequency changes that fell outside of the above categories were not tallied: for example, the presence or absence of a definite article (had authority / had the authority) in Figure 2 was ignored. After summing all alternations in each sentence pair, we calculated the average number of occurrences of each paraphrase type in each data set. The results are shown in Table 2. Several major differences stand out between the two data sets. First, the F2 data is less parallel, as evidenced by the higher percentage of Elaborations found in those sentence pairs. Loss of parallelism, however, is offset by greater diversity of paraphrase types encountered in the F2 data. Phrasal alternations are more than 4x more common, and Reorderings occur over 20x more frequently. Thus while string difference methods may produce relatively clean training data, this is achieved at the cost of filtering out common (and interesting) paraphrase relationships.","To explore some of the differences between the training sets, we hand-examined a random sample of sentence pairs from each corpus type. The most common paraphrase alternations that we observed fell into the following broad categories: ? Elaboration: Sentence pairs can differ in total information content, with an added word, phrase or clause in one sentence that has no Training Data Type: L12 F2 L12 F2 Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic Precision 87.46% 86.44% 85.07% 84.16% Recall 89.52% 82.64% 88.70% 86.55% AER 11.58% 15.41% 13.24% 14.71% Identical word precision 89.36% 88.79% 92.92% 93.41% Identical word recall 89.50% 83.10% 93.49% 92.47% Identical word AER 10.57% 14.14% 6.80% 7.06% Non-Identical word precision 76.99% 71.86% 60.54% 53.69% Non-Identical word recall 90.22% 69.57% 59.50% 50.41% Non-Identical word AER 20.88% 28.57% 39.81% 47.46% Table 1. Precision, recall, and alignment error rates (AER) for F2 and L12 counterpart in the other (e.g. the NASDAQ / the tech-heavy NASDAQ). Phrasal: An entire group of words in one sentence alternates with one word or a phrase in the other. Some are non-compositional idioms (has pulled the plug on / is dropping plans for); others involve different phrasing (electronically / in electronic form, more than a million people / a massive crowd). Spelling: British/American sources system atically differ in spellings of common words (colour / color); other variants also appear (email / e-mail). Synonymy: Sentence pairs differ only in one or two words (e.g. charges / accusations), suggesting an editor?s hand in modifying a single source sentence. Anaphora: A full NP in one sentence corresponds to an anaphor in the other (Prime Minister Blair / He). Cases of NP anaphora (ISS / the Atlanta-based security company) are also common in the data, but in quantifying paraphrase types we restricted our attention to the simpler case of pronominal anaphora. Reordering: Words, phrases, or entire constituents occur in different order in two related sentences, either because of major syntactic differences (e.g. topicalization, voice alternations) or more local pragmatic choices (e.g. adverb or prepositional phrase placement). These categories do not cover all possible alternations between pairs of paraphrased sentences; moreover, categories often overlap in the same sequence of words. It is common, for example, to find instances of clausal Reordering combined with Synonymy. Figure 2 shows a hand-aligned paraphrase pair taken from the F2 data. This pair displays one Spelling alternation (defence / defense), one Reordering (position of the ?since? phrase), and one example of Elaboration (terror attacks occurs in only one sentence). To quantify the differences between L12 and F2, we randomly chose 100 sentence pairs from each dataset and counted the number of times each phenomenon was encountered. A given sentence pair might exhibit multiple instances of a single phenomenon, such as two phrasal paraphrase changes or two synonym replacements. In this case all instances were counted. Lower-frequency changes that fell outside of the above categories were not tallied: for example, the presence or absence of a definite article (had authority / had the authority) in Figure 2 was ignored. After summing all alternations in each sentence pair, we calculated the average number of occurrences of each paraphrase type in each data set. The results are shown in Table 2. Several major differences stand out between the two data sets. First, the F2 data is less parallel, as evidenced by the higher percentage of Elaborations found in those sentence pairs. Loss of parallelism, however, is offset by greater diversity of paraphrase types encountered in the F2 data. Phrasal alternations are more than 4x more common, and Reorderings occur over 20x more frequently. Thus while string difference methods may produce relatively clean training data, this is achieved at the cost of filtering out common (and interesting) paraphrase relationships."
86,"The unique properties of lree-adjoining grammars (TAG) present a challenge for the application of 'FAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of nat- ural h'mguage. We present a variant of ""FAGs, called synchronous TAGs, which chmacterize correspondences between languages. ""lq\]e formalism's intended usage is to relate expressions of natural anguages to their associ- ated semantics represented in a logical tbrm language, or to their translates in another natural anguage; in sum- mary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mention- ing primarily in passing some computational issues that tu:ise in its interpretation.","The unique properties of lree-adjoining grammars (TAG) present a challenge for the application of 'FAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of nat- ural h'mguage. We present a variant of ""FAGs, called synchronous TAGs, which chmacterize correspondences between languages. ""lq\]e formalism's intended usage is to relate expressions of natural anguages to their associ- ated semantics represented in a logical tbrm language, or to their translates in another natural anguage; in sum- mary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mention- ing primarily in passing some computational issues that tu:ise in its interpretation. Tree-adjoining rammars (TAG) constitute a grammat- ical formalism with attractive properties for the strong characterization f the syntax of natural angtmges, that is, characterization of the analysis trees of the expres- sions in the language (Kroch and Joshi, 1985; Kroch, 1989)) Among these properties are that o The domain of locality in TAGs is larger than lot formalisms lhat augment context-free grammars (such as lexical-functkmal, or generalized or head- driven phrase-structure grammar), and ? The statements of dependencies and recursion pos- sibilities in a tree are factored, the former following from primitive dependencies in elementary trees, the latter a consequence of an operatkm of adjunc- tion of trees. These unique properties of TAGs present a challenge tot the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpre- tation or automatic tr~mslation of natural anguage. The slandm'd methods of moving beyond syntax to interpre- tation make use in one way or another of the compo- sitional structure of the analysis tree that is manifested in the tree's derivation. Any version of compositional 1We assume familiarity throughout the paper with previous work on TAGs. See, for instance, the introduction by Joshi (1987). semantics, or syntax.directed translation relies on such a methodology to some extent. However, in the case of TAGs, the compositional structure of the tree is not miro rored by its derivational structure, so that a method for constructing semantics based on the compositional syn- tactic structure will be inherently nonderivational, that is, construction of the semantics will be independent of the derivation of the tree, and therefore subsequent. On the other hand, a method mirroring the deriva- tional structure will not necessarily be compositional with respect to tile derived structures of expressions. AI+ tl~ough such a method would be quite different from ttle primarily compositional methods previously postulated, it may have advantages, given that certain aspects of language seem to be noncompositional. (See Section 4.) In this paper, we present a varim~t of TAGs, called synchronous TAGs, which characterize correstxmdences between languages. The formalism's intended usage is to relate expressions of natural anguages to their asso- ciated semantics represented in a logical form language, or to their translations in another natural language; in summary, we intend the formalism to allow TAGs to be used beyond their role in syntax proper. We also discuss its application to concrete xamples, and mention some computational issues that arise in its interpretation. mal Description Language interpretation tasks can be thought of as asso- ciating a syntactic analysis of a sentence with some other stmcture,---a logical form representation r an analysis of a target language sentence, perhaps. Synchronous TAGs are defined so as to make such associations explicit. The original language and its associated structures are both defined by grammars tated in a TAG formalism; the two TAGs are synchronous in the sense that adjunction and substitution operations are applied simultaneously to related nodes in pairs of trees, one for each language. For convenience, we will call the two languages ource and target languages, although the formalism is not in- herently directional. As an example, consider the task of relating a frag- ment of English with a simple representation of its predicate-argument structure. A synchronous TAG for this purpose is given in Figure 1. Each element of the 1 253 NP V ~ R ~ T T \ V NP$ hates' / / I I George george' N Jb rocco l i ) \ br~coli \[ vP F \ p, A vp F,\] violently violently' I cooked cooked' I Figure 1: A sample synchronous TAG. synchronous TAG is a pair consisting of two elemen- tar2,' trees, one from tlie source language (English) and one from the target (logical form \[LF\]). Nodes, one from each tree, may be linked; ~ such links are depicted graph- ically as thick lines. If we project the pairs onto their first or second components (ignoring the cross links), the projections are TAGs for an English fragment and an LF fragment, respectively, qhese grammars are themselves written in a particular variant of TAGs; the choice of this base formalism, as we will call it, is free. In the case at hand, we have chosen single-component lexicalized TAGs with adjunction and substitution (Schabes et el., 1988). Later examples are built on other bases. The elementary operation in a synchronous TAG is su- pervenient on the elementary operations in the base for- malism. A derivation step from a pair of trees (cq, a2) proceeds as follows: 1. . Nondeterministically choose a link in the pair con- necting two nodes (say, nl in cq and no in c~2). Nondeterministically choose a pair of trees (3~, 32) in the grammar. Form the resultant pair </3t(oq, nl), ;32(~2, n2)). where 3(c~, n) is the result of performing a primi- tive operation in the base formalism on a at node n using 3 (e.g., adjoining or substituting 3 into at n). 3 2We will generalize the links later to allow sets of nodes from one tree to be linked to sets from the other. 3The definition allows for the operations performed on the first Synchronous TAG derivation then proceods by choos~ ing a pair of initial trees (cq, o~2) that is an element of the grammar, and repeatedly applying derivation steps as above. As an example, suppose we start with the tree pair c~ in Figure 1. 4 We choose the link from the subject NP to T and the tree pair fl to apply to its nodes. The resultant, by synchronous substitution, is the tree pair: i Ny T T,\ I I I \George V "" / ~P, hates' georgeJ / Note that the links from a are preserved in the resul- tant pair cq except for the chosen link, which has no counterpart in the result. Using tree pair 7 on the remaining link from NP to T in oq yields o~ 2 \] NP VP~. .~ ~ R T .~. T \ George y ~P hare'george')broccoli' \ hates broccoli This pairing manifests the correspondence b tween the sentence ""George hates broccoli"" and its logical form hates' (george' , broccoli') (as written in a more tradi- tional notation). Here we see that the links in the opera? tor trees (those in 7) are preserved in the resultant pair, accounting for the sole remaining link. Tile trees in 7 are linked in this way so that other tree pairs can modify the N. We can continue the derivation, using 5 and ~ to gen- erate the pair given in Figure 2 thereby associating the meaning violently' ( hates' (george', cooked'( broccol i') ) ) ) with the sentence ""George hates cooked broccoli vio- lently."" A subtle issue mises with respect o link updating in the resultant pair if two links impinge on the same node. When one of the links is chosen and an adjunction per- formed at the node, the other link must appear in the resultant. The question as to whether that link should now end at the root or foot of the adjoined tree can be re- solved in several ways. Although the choice of method does not affect any of the examples in this paper, we mention our current resolution of this problem here. If the remaining link is connected initially to the top of and second trees to differ, one being a substitution and the other an adjunetion, for example. aWe uge standard TAG notation, marking foot nodes in auxiliary trees with '*' and nodes where substitution is m occur with '1/. The nonterminal names in the logical form grammar e mnemonic for Formula, Relation (or function) symbol, Term, and Quantifier. 254 2 F George VP ADVP violently' T ~ ~ hates N,,....._ /cooked"" broccoli' I cooked broccoli Figure 2: Derived tree pair for ""George hates cooked broccoli violently."" the node serving as the adjunction site, it will connect to the top of the root node of the adjoined auxiliary nee after the adjunction has been performed; conversely, if it is connected initially to the bottom of the node, it will connect o the bottom of the foot node of the auxiliary tree. In all of the examples in this paper, the links may be thought of as connecting to the tops of nodes. The issue has important ramifications. For instance, the link updating process allows for different derivations of a single derivation in the source language to correspond to derivations of different derivations in the ""target lan~ guage; that is, derivation order in synchronous TAGs is in this respect crucial, unlike in the base TAG for- malisms. We rely on this property in the analysis of quantifier scope in Section 4.2. We turn to the question of why, in augmenting TAGs for the purposes of encoding semantic information, it is preferable to use the synchronous TAG method over more conventional methods, such as semantic rules in- volving logical operations (as in Montague grammar or generalized phrase-structure grammar) or complex- feature-structure encodings (as in unification-based or logic grammar formalisms), First, the arguments for factoring recursion and depen- dencies as TAGs do for the syntax of natural anguage have their counterparts in the semantics. The structure of TAGs allows syntactic dependencies--agreement, sub- categorization, and so forth--to be localized in the prim- itives of a grammar, the elementary trees. This is most dramatically evident in the case of long-distance depen- dencies, such as that between a wh-phrase and its as- sociated gap. Similarly, using TAGs to construct logi- cal forms allows the localization of semantic dependen- cies in the logical forms of natural language xpressions, dependencies such as the signature requirements (argu- ment type and arity) of function and relation symbols, and even the long-distance dependencies between a wh- quantifier and its associated bound variable. With other methods of semantics, these dependencies cannot be lo- calized; the semantic aspects of filler-gap dependencies must be passed among the features of various nodes in a parse tree or otherwise distributed over the entire deriva- tion. Second, the use of the synchronous TAG augmenta- tion allows ,an even more radical reduction in the role of features in a TAG grammar. Because of the extended domain of locality that TAGs possess, the role of features and unification is reduced from its role in context-free based systems. Only finite-valued features are needed, with the possible exception of a feature whose value encodes an expression's logical form. In removing the conslz'uction of logical forms from the duties delegateA to features, we can maintain a strictly finiteovalued-- and therefore formally dispensable---feature system Ibr TAGs. As a side note, we mention a ramification of the syn- chronous TAG analysis concerning the claim of Ka- plan and Zaenen (1989) that the paths over which long-distance dependencies operate (in the f-structure of lexieal-functional grammatical theory) form a regu- lar language. Vijay-Shanker and Joshi (1989) provide an argument that this claim follows from several as- sumptions concerning how a feature system for TAGs might be constrained. Vijay-Shanker (personal commu- nication) has noted that by placing a simple assumption on the elementary trees in the logical form component of a synchronous TAG, the proof of this claim becomes immediate. Any TAG in which all foot nodes are im- mediate children of their associated root generates a tree path language that is regular. ~Thus, a synchronous TAG (like the grammar presented in Figure 1) whose semantic component forms a TAG with this property necessarily obeys the regular language constraint on long-distance semantic dependencies. To exemplify the formalism's utility, we briefly and in- formally describe its application to the semantics of id- ioms and quantifiers. A companion paper (Abeill6 et al, 1990) uses a mapping between two TAGs for automatic translation between natural anguages, and constitutes a further application of the synchronous TAG concept. 5This is a folk theorem whose straighlforward proof is left as an exercise for the reader, 3 255 More expansive descriptions of these analyses will be forthcoming in joint work with Anne Abeilld (idioms and translation) and Anthony Kroch (quantifiers). 4,1 Id ioms Abeill6 and Schabes (1989) note that lexicalized TAGs are an appropriate r presentation language for idiomatic constructions, as their expanded omain of locality can account for many syntactic properties of idioms. It seems natural to generalize beyond syntax, as they do, to the claim that lexicalized 'FAGs allow one to deal with semantic noncompositionality. Their argument to this claim is based on an intuition that semantics de- pends on the TAG derivation structure, an intuition that synchronous TAGs makes precise. For example, the id- iomatic construction ""kick the bucket"" cashes out as the following tree pair, under its idiomatic interpretation: a3 d}e' $ whereas the literal usage of ""kick"" is associated with a tree pair similar to that of ""hates"" in Figure 1. Two derivations of the sentence ""George kicked the bucket"" are possible, each using a different one of these two elementary tree pairs, but both yielding identical de- rived constituency trees for the English. They will be associated, of course, with two different readings, cor- responding to the idiomatic (die'(yeorge')) and literal (kick'(george ~,bucket')) interpretations, respectively. All of the arguments for the TAG analysis of idioms and light verb constructions can then be maintained in a formalism that allows for semantics for them as well. In particular, ? Discontinuous syntactic onstituents can be seman- tic'ally localized. Nonstandard long-distance dependencies are stat- able without resort to reanalysis. Both frozen and flexible idioms can be easily char- acterized. 4.2 Quant i f ie rs. In order to characterize quantifier scoping possibilities, we use a synchronous TAG whose base formalism is multi-component TAGs (Joshi, 1987), in which the prim- itive operation is incorporation (by multiple substitutions and adjunctions) of a set of elementary trees at once. In synchronous multi-component TAGs, the links between trees connect, in general, a set of nodes in one tree with a set in another. In particular, an NP will be linked both to a formula in the semantics (the quantifier's scope) and a term (the position bound by the quantifier). We will begin a derivation with just such a pair of elementat3, trees, depicted as at in Figure 3. To distinguish two separate links from a single link among several nodes, we use a coindexing--rather than graphical~-notation f r links. Thus, the subject NP node on the left is linked with both the F and first T node on the right, as indicated by the boxed index 1. The inteqgretation f such ""hyper-links"" is that when a pair is chosen to operate at the link, it must have sets of the correct sizes as its left and right component (1 and 2 in the case at hmad) and the sets are simultaneously used at the various nodes as in a multi-component ""lAG. For instance, a quantifiable noun will be paired with a set of two trees: 6 politician R T x politician Applying the latter multi-component tree pair fll to the initial tree pair a l , we derive the next stage in the deriva- tion o~2. We have highlighted the link being operated on at this and later steps by using thick lines for the index boxes of the selected link. The determiner can be introduced with the simple pair leading to the derivation step a3. Completing the deriva- tion using analogous elementary tree pairs, we might generate the final tree pair a4 of Figure 3. This final pairing associates the meaning By : vegetablc' (y).Vx : politician' (z).hates' ( z, y) with the sentence ""Every politician hates some veg- etable."" It should be clear that in a structure such as this with multiple NPs, the order of substitution of NPs de- termines the relative scope of the quantifiers, although it has no effect whatsoever on the syntactic structure. De- veloping this line of reasoning has led to several detailed predictions of this analysis of quantifier scope, which is beyond this paper's purview. In summary, however, the analysis is slightly more restrictive than that of Hobbs and Shieber (1987), making predictions regarding the scope of topicalized or wh-moved constituents, relative scope of embedded quantifiers, and possibly even syn- tactic structure of complex NPs. The synchronous TAG formalism is inherently nondirec- tional. Derivation is not defined in terms of constructing 6The subscript x on certain nodes is the value of a feature on the nodes corresponding to the variable bound by the quantifier. The technique of using metavariables to encode object variables is familiar from the logic and unification-based grammar literatures, Variable renaming with respect o these variables proceeds as usual. 256 4 % I S V NP~\] 1 hates NP V NP~ I politician hates mm F \ j - -T '~.. NINF / IIiq~, ~x F R T x R T x NT, I"" I 1 politician' hates' % % ( f S NP D N V NPDI I I t every politician hates S . - - t i ' - -""- - - - . NP D N V NP every politician hates D N I I a vegetable F F VR T x R T x ry lT / I I '~ / politician"" hates' / \ 9Y F F vegetable V ,R T x R T x T, / I I Y/ politician' hates"" / Figure 3: Sample synchronous TAG derivation steps for ""Every politician hates a vegetable."" a tin'get expression from a source or vice versa. Thus, it can be used to characterize both of these mappings. Furthermore, the existence of a parsing algorithm for the base formalism of a synchronous TAG is a sufficient condition for interpreting a synchronous TAG grammar. Schabes and Joshi (1988) and Vijay-Shanker and Joshi (1985) provide parsing algorithms for TAGs that could serw:: to parse the base formalism of a synchronous TAG. Given such an algorithm, semantic interpretation can be performed by parsing the sentence according to the source grammar; the pairings then determine a deriva- tion in the target language for tile logical form. Gen- eration from a logical form proceeds by the converse process of parsing the logical form expression thereby determining the derivation for the natural anguage sen- fence. Machine translation proceeds akmg similar lines by mapping two 'FAGs directly (Abeill6 et al, 1990), In previous work, one of us noted that generation ac- cording to an augmented context-free grammar can be made more efficient by requiring the grammar to be se- mantically monotonic (Shieber, 1988); the derived se- mantics for an expression must include, in an appropri- ate sense, the semantic material of all its subconstituents. It is interesting to note that synchronous ""FAGs are in- herently semantically monotonic. Furthermore, it is rea- sonable to require that the semantic omponent of a syn- chronous TAG t~ lexicalized (in the sense of Schabes et al. (1988)), allowing for more efficient parsing accord- ing to the semantic grammar and, consequenlly, more efficient generation. In the case of augmented context- free grammars, the semantic monotonicity requirement precludes ""lexicalization"" of the semantics. It is not possible to require nontrivial semantics to be associated with each lexical item. In summary, just as lexicaliza- lion of the syntactic grammar aids parsing (Schabes and Joshi, 1990), so lexicalization of the semantic gra.,nmz:r aids generation. Tile description of parsing and germration above rnay seem to imply that these processes cannot be pcrlormcd incrementally, that is, an entire source derivation must be recovered before the corresponding target derivation can be computed. The issue deserves clarification. In the case wltere the synchronous TAG is order- independent ( hat is, the order of derivation in one TAG does not effect the result in the other, as when no two links share an endpoint) there is a one-to-one mapping between the source and target derivation. When par- tial source derivations are recognized by the parser, the corresponding partial target derivation (for example se- mantic inteq)retation) can be incrementally compuled: as the input is read from left to right, interpretations of the partial target derivations corresponding to partial source derivations can be combined in one step to buikl a larger partial target derivation. 5 257 When the synchronous TAG is order-sensitive, how- ever, there may be a many-to-many correspondence b - tween source derivations and target derivations. This is the case, for instance, in a grammar in which alterna- tive quantifier scopings may be generated for a single sentence. In this case, it is unclear what should even be meant by incremental computation. For instance, mid- way in parsing a sentence, at a point at which a single quantified NP has been analyzed, the incremental inter- pretation could not possibly represent all possible scop- ings that that quantifier might end up taking, as it is not known what the quantifier might be required to scope with respect o. At the point in the parse where the scoping decision can be made, it is not clear whether an inerementality requirement would mean that the variant scopings must all be explicitly generated at that point, or only implicitly generable. With respect o synchronous TAGs, these considera- tions are reflected in choice of parsing algorithm. Ef- ficiency of parsing necessitates that only one canonical derivation (say leftmost or rightmost) need to be com- puted; all other derivations yield the same object. Stan- dard parsing algorithms for both TAGs and CFGs rely on this optimization. If incrementality requires that we generate xplicit representations of all possible interpre- tations (i.e., target derivations) of the string seen so far, then this optimization cannot be used, and parsing will be highly inefficient. If the representation can be left im- plicit, the optimization can be maintained, but retrieval of explicit representations will be combinatorially more complex.","The synchronous TAG formalism is inherently nondirec- tional. Derivation is not defined in terms of constructing 6The subscript x on certain nodes is the value of a feature on the nodes corresponding to the variable bound by the quantifier. The technique of using metavariables to encode object variables is familiar from the logic and unification-based grammar literatures, Variable renaming with respect o these variables proceeds as usual. 256 4 % I S V NP~\] 1 hates NP V NP~ I politician hates mm F \ j - -T '~.. NINF / IIiq~, ~x F R T x R T x NT, I"" I 1 politician' hates' % % ( f S NP D N V NPDI I I t every politician hates S . - - t i ' - -""- - - - . NP D N V NP every politician hates D N I I a vegetable F F VR T x R T x ry lT / I I '~ / politician"" hates' / \ 9Y F F vegetable V ,R T x R T x T, / I I Y/ politician' hates"" / Figure 3: Sample synchronous TAG derivation steps for ""Every politician hates a vegetable."" a tin'get expression from a source or vice versa. Thus, it can be used to characterize both of these mappings. Furthermore, the existence of a parsing algorithm for the base formalism of a synchronous TAG is a sufficient condition for interpreting a synchronous TAG grammar. Schabes and Joshi (1988) and Vijay-Shanker and Joshi (1985) provide parsing algorithms for TAGs that could serw:: to parse the base formalism of a synchronous TAG. Given such an algorithm, semantic interpretation can be performed by parsing the sentence according to the source grammar; the pairings then determine a deriva- tion in the target language for tile logical form. Gen- eration from a logical form proceeds by the converse process of parsing the logical form expression thereby determining the derivation for the natural anguage sen- fence. Machine translation proceeds akmg similar lines by mapping two 'FAGs directly (Abeill6 et al, 1990), In previous work, one of us noted that generation ac- cording to an augmented context-free grammar can be made more efficient by requiring the grammar to be se- mantically monotonic (Shieber, 1988); the derived se- mantics for an expression must include, in an appropri- ate sense, the semantic material of all its subconstituents. It is interesting to note that synchronous ""FAGs are in- herently semantically monotonic. Furthermore, it is rea- sonable to require that the semantic omponent of a syn- chronous TAG t~ lexicalized (in the sense of Schabes et al. (1988)), allowing for more efficient parsing accord- ing to the semantic grammar and, consequenlly, more efficient generation. In the case of augmented context- free grammars, the semantic monotonicity requirement precludes ""lexicalization"" of the semantics. It is not possible to require nontrivial semantics to be associated with each lexical item. In summary, just as lexicaliza- lion of the syntactic grammar aids parsing (Schabes and Joshi, 1990), so lexicalization of the semantic gra.,nmz:r aids generation. Tile description of parsing and germration above rnay seem to imply that these processes cannot be pcrlormcd incrementally, that is, an entire source derivation must be recovered before the corresponding target derivation can be computed. The issue deserves clarification. In the case wltere the synchronous TAG is order- independent ( hat is, the order of derivation in one TAG does not effect the result in the other, as when no two links share an endpoint) there is a one-to-one mapping between the source and target derivation. When par- tial source derivations are recognized by the parser, the corresponding partial target derivation (for example se- mantic inteq)retation) can be incrementally compuled: as the input is read from left to right, interpretations of the partial target derivations corresponding to partial source derivations can be combined in one step to buikl a larger partial target derivation. 5 257 When the synchronous TAG is order-sensitive, how- ever, there may be a many-to-many correspondence b - tween source derivations and target derivations. This is the case, for instance, in a grammar in which alterna- tive quantifier scopings may be generated for a single sentence. In this case, it is unclear what should even be meant by incremental computation. For instance, mid- way in parsing a sentence, at a point at which a single quantified NP has been analyzed, the incremental inter- pretation could not possibly represent all possible scop- ings that that quantifier might end up taking, as it is not known what the quantifier might be required to scope with respect o. At the point in the parse where the scoping decision can be made, it is not clear whether an inerementality requirement would mean that the variant scopings must all be explicitly generated at that point, or only implicitly generable. With respect o synchronous TAGs, these considera- tions are reflected in choice of parsing algorithm. Ef- ficiency of parsing necessitates that only one canonical derivation (say leftmost or rightmost) need to be com- puted; all other derivations yield the same object. Stan- dard parsing algorithms for both TAGs and CFGs rely on this optimization. If incrementality requires that we generate xplicit representations of all possible interpre- tations (i.e., target derivations) of the string seen so far, then this optimization cannot be used, and parsing will be highly inefficient. If the representation can be left im- plicit, the optimization can be maintained, but retrieval of explicit representations will be combinatorially more complex."
87,"Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Baker, J.K. 1979. Trainable grammars tbr speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presentacd at the 97 ~h Meeting of the Acoustical Society of Amer- ica, MIT, Cambridge, MA, June. llooth, Taylor R. and Richard A. Thoml)son. 1973. Applying probability measures to abstract languages. IEEE 7)'aasactions on Computers, C-22(5):442-450, May. Booth, T. 1969. Probabilistic representation f formal languages. In Tenth Annual IEEE Symposium on Switching and Automata Theory, October. Chomsky, N., 1964. Syntactic Structures, chapter 2-3, pages 13-18. Mouton. Gazdar, G. 1985. Applicability of indexed gr,'unmars to natural anguages. Technical Report CSLI-85-34, Center for Study of Language and Information. tlempttill, Charles T., John J. Godfrey, and George IL Doddington. 1990. The ATIS spoken language sys- tems pilot corpus. In DARPA Speech and Natural Laaguage Workshop, Hidden Valley, Pennsylvania, June. Jelinek, F., J. D. Lafferty, and R. L. Mercer. 1990. Ba- sic methods of probabilistic ontext free grammars. Technical Report RC 16374 (72684), IBM, Yorktown Heights, New York 10598. Joshi, Aravind K. and Yves Schabes. 1991. Tree- adjoiuing grammars and lexiealized grammars. In Maurice Nivat and Andreas Podelski, editors, Defin- ability and Recognizability ofSets of Trees. Elsevier. Forthcoming. Joshi, Aravind K., K. Vijay-Simnker, and David Weir. 1991. The convergence of mildly context-sensitive gramnmtical formalisms, in Peter Sells, Stuart Shieber, and Tom Wasow, editors, Foundational Is- sues in Natural Language Processing. MIT Press, Cambridge MA. Joshi, Aravind K. 1987. An Introduction to Tree Ad- joining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Beujamins, Amster- dana. Lari, K. and S. J. Young. 1990. The estimation of stochastic ontext-free grmnmars using the Inside- Outside algorithm. Computer Speech and Language, 4:35-56. ACRES DE COL1NG-92, NANTES, 23-28 AO~r 1992 4 3 1 PROr'.. OI: COLING-92, NANTES, AUG. 23-28, 1992 Pereira, Fernando and Yves Schabes. 1992. Inside- outside reest imation from partial ly bracketed cor- pora. In 20 th Meeting of the Association for Compu- tational Linguistics (ACL '9~), Newark, Delaware. Prat t , Fletcher. 1942. Secret and urgent, the story of codes and ciphers. Blue Ribbon Books. Resnik, Philip. 1991. Lexicalized tree-adjoining ram- mar for distr ibutional analysis. In Penn Review of Linguistics, Spring. Schabes, Yves, Anne Abeill~, and Aravind K. Joshi. 1988. Pars ing strategies with ' lexicalized' grarnmars: Application to tree adjoining gra~mnars. In Proceed- ings of the 1~ lh International Conference on Compu- tational Linguistics (COLING'88}, Budapest, Hun- gary, August . Sehabes, Yves. 1990. Mathematical nd Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Uni- versity of Pennsylvania, Philadelphia, PA, August. Available as technical report (MS-CIS-90-48, L INC LAB179) from the Department of Computer Science. Schabes, Yves. 1991. An inside-outside algor i thm for est imat ing the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Manuscript. Shannon, C. E. 1948. A mathemat ica l theory of communicat ion. The Bell System Technical Journal, 27(3):379-423. Shannon, C. E. 1951. Predict ion and entropy of printed english. The Bell System Technical Journal, 30:50- 64. Vi jay-Shanker, K. and David J. Weir. 1991. Pars ing constrained grammar formalisms. In preparation. Vi jay-Shanker, K. 1987. A Study of ?lbee Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvmfia. A Comput ing the Ins ide P rob- ab i l i t i es In the following, the inside and outside probabilities are re\]ative to the input string w. 3 t"" stands for the the set of foot nodes, S for the set of nodes on which substitution can occur, ~ for the set of root nodes of initial trees, and ,4 for the set of non-terminal nodes of auxiliary trees. The inside probability can be computed bottom-up with the following recurrence quations. For all node v/found in an elementary tree, it can be shown that: 1. If b\[$r/\] ~ a, I(b,7, i , - , - , I ) = dl if / = i+ 1 and if. a = w~ +1, 0 otherwise. 2. \] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and if k = l, 0 otherwise.","Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Baker, J.K. 1979. Trainable grammars tbr speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presentacd at the 97 ~h Meeting of the Acoustical Society of Amer- ica, MIT, Cambridge, MA, June. llooth, Taylor R. and Richard A. Thoml)son. 1973. Applying probability measures to abstract languages. IEEE 7)'aasactions on Computers, C-22(5):442-450, May. Booth, T. 1969. Probabilistic representation f formal languages. In Tenth Annual IEEE Symposium on Switching and Automata Theory, October. Chomsky, N., 1964. Syntactic Structures, chapter 2-3, pages 13-18. Mouton. Gazdar, G. 1985. Applicability of indexed gr,'unmars to natural anguages. Technical Report CSLI-85-34, Center for Study of Language and Information. tlempttill, Charles T., John J. Godfrey, and George IL Doddington. 1990. The ATIS spoken language sys- tems pilot corpus. In DARPA Speech and Natural Laaguage Workshop, Hidden Valley, Pennsylvania, June. Jelinek, F., J. D. Lafferty, and R. L. Mercer. 1990. Ba- sic methods of probabilistic ontext free grammars. Technical Report RC 16374 (72684), IBM, Yorktown Heights, New York 10598. Joshi, Aravind K. and Yves Schabes. 1991. Tree- adjoiuing grammars and lexiealized grammars. In Maurice Nivat and Andreas Podelski, editors, Defin- ability and Recognizability ofSets of Trees. Elsevier. Forthcoming. Joshi, Aravind K., K. Vijay-Simnker, and David Weir. 1991. The convergence of mildly context-sensitive gramnmtical formalisms, in Peter Sells, Stuart Shieber, and Tom Wasow, editors, Foundational Is- sues in Natural Language Processing. MIT Press, Cambridge MA. Joshi, Aravind K. 1987. An Introduction to Tree Ad- joining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Beujamins, Amster- dana. Lari, K. and S. J. Young. 1990. The estimation of stochastic ontext-free grmnmars using the Inside- Outside algorithm. Computer Speech and Language, 4:35-56. ACRES DE COL1NG-92, NANTES, 23-28 AO~r 1992 4 3 1 PROr'.. OI: COLING-92, NANTES, AUG. 23-28, 1992 Pereira, Fernando and Yves Schabes. 1992. Inside- outside reest imation from partial ly bracketed cor- pora. In 20 th Meeting of the Association for Compu- tational Linguistics (ACL '9~), Newark, Delaware. Prat t , Fletcher. 1942. Secret and urgent, the story of codes and ciphers. Blue Ribbon Books. Resnik, Philip. 1991. Lexicalized tree-adjoining ram- mar for distr ibutional analysis. In Penn Review of Linguistics, Spring. Schabes, Yves, Anne Abeill~, and Aravind K. Joshi. 1988. Pars ing strategies with ' lexicalized' grarnmars: Application to tree adjoining gra~mnars. In Proceed- ings of the 1~ lh International Conference on Compu- tational Linguistics (COLING'88}, Budapest, Hun- gary, August . Sehabes, Yves. 1990. Mathematical nd Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Uni- versity of Pennsylvania, Philadelphia, PA, August. Available as technical report (MS-CIS-90-48, L INC LAB179) from the Department of Computer Science. Schabes, Yves. 1991. An inside-outside algor i thm for est imat ing the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Manuscript. Shannon, C. E. 1948. A mathemat ica l theory of communicat ion. The Bell System Technical Journal, 27(3):379-423. Shannon, C. E. 1951. Predict ion and entropy of printed english. The Bell System Technical Journal, 30:50- 64. Vi jay-Shanker, K. and David J. Weir. 1991. Pars ing constrained grammar formalisms. In preparation. Vi jay-Shanker, K. 1987. A Study of ?lbee Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvmfia. A Comput ing the Ins ide P rob- ab i l i t i es In the following, the inside and outside probabilities are re\]ative to the input string w. 3 t"" stands for the the set of foot nodes, S for the set of nodes on which substitution can occur, ~ for the set of root nodes of initial trees, and ,4 for the set of non-terminal nodes of auxiliary trees. The inside probability can be computed bottom-up with the following recurrence quations. For all node v/found in an elementary tree, it can be shown that: 1. If b\[$r/\] ~ a, I(b,7, i , - , - , I ) = dl if / = i+ 1 and if. a = w~ +1, 0 otherwise. 2. \] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and if k = l, 0 otherwise.","Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Baker, J.K. 1979. Trainable grammars tbr speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presentacd at the 97 ~h Meeting of the Acoustical Society of Amer- ica, MIT, Cambridge, MA, June. llooth, Taylor R. and Richard A. Thoml)son. 1973. Applying probability measures to abstract languages. IEEE 7)'aasactions on Computers, C-22(5):442-450, May. Booth, T. 1969. Probabilistic representation f formal languages. In Tenth Annual IEEE Symposium on Switching and Automata Theory, October. Chomsky, N., 1964. Syntactic Structures, chapter 2-3, pages 13-18. Mouton. Gazdar, G. 1985. Applicability of indexed gr,'unmars to natural anguages. Technical Report CSLI-85-34, Center for Study of Language and Information. tlempttill, Charles T., John J. Godfrey, and George IL Doddington. 1990. The ATIS spoken language sys- tems pilot corpus. In DARPA Speech and Natural Laaguage Workshop, Hidden Valley, Pennsylvania, June. Jelinek, F., J. D. Lafferty, and R. L. Mercer. 1990. Ba- sic methods of probabilistic ontext free grammars. Technical Report RC 16374 (72684), IBM, Yorktown Heights, New York 10598. Joshi, Aravind K. and Yves Schabes. 1991. Tree- adjoiuing grammars and lexiealized grammars. In Maurice Nivat and Andreas Podelski, editors, Defin- ability and Recognizability ofSets of Trees. Elsevier. Forthcoming. Joshi, Aravind K., K. Vijay-Simnker, and David Weir. 1991. The convergence of mildly context-sensitive gramnmtical formalisms, in Peter Sells, Stuart Shieber, and Tom Wasow, editors, Foundational Is- sues in Natural Language Processing. MIT Press, Cambridge MA. Joshi, Aravind K. 1987. An Introduction to Tree Ad- joining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Beujamins, Amster- dana. Lari, K. and S. J. Young. 1990. The estimation of stochastic ontext-free grmnmars using the Inside- Outside algorithm. Computer Speech and Language, 4:35-56. ACRES DE COL1NG-92, NANTES, 23-28 AO~r 1992 4 3 1 PROr'.. OI: COLING-92, NANTES, AUG. 23-28, 1992 Pereira, Fernando and Yves Schabes. 1992. Inside- outside reest imation from partial ly bracketed cor- pora. In 20 th Meeting of the Association for Compu- tational Linguistics (ACL '9~), Newark, Delaware. Prat t , Fletcher. 1942. Secret and urgent, the story of codes and ciphers. Blue Ribbon Books. Resnik, Philip. 1991. Lexicalized tree-adjoining ram- mar for distr ibutional analysis. In Penn Review of Linguistics, Spring. Schabes, Yves, Anne Abeill~, and Aravind K. Joshi. 1988. Pars ing strategies with ' lexicalized' grarnmars: Application to tree adjoining gra~mnars. In Proceed- ings of the 1~ lh International Conference on Compu- tational Linguistics (COLING'88}, Budapest, Hun- gary, August . Sehabes, Yves. 1990. Mathematical nd Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Uni- versity of Pennsylvania, Philadelphia, PA, August. Available as technical report (MS-CIS-90-48, L INC LAB179) from the Department of Computer Science. Schabes, Yves. 1991. An inside-outside algor i thm for est imat ing the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Manuscript. Shannon, C. E. 1948. A mathemat ica l theory of communicat ion. The Bell System Technical Journal, 27(3):379-423. Shannon, C. E. 1951. Predict ion and entropy of printed english. The Bell System Technical Journal, 30:50- 64. Vi jay-Shanker, K. and David J. Weir. 1991. Pars ing constrained grammar formalisms. In preparation. Vi jay-Shanker, K. 1987. A Study of ?lbee Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvmfia. A Comput ing the Ins ide P rob- ab i l i t i es In the following, the inside and outside probabilities are re\]ative to the input string w. 3 t"" stands for the the set of foot nodes, S for the set of nodes on which substitution can occur, ~ for the set of root nodes of initial trees, and ,4 for the set of non-terminal nodes of auxiliary trees. The inside probability can be computed bottom-up with the following recurrence quations. For all node v/found in an elementary tree, it can be shown that: 1. If b\[$r/\] ~ a, I(b,7, i , - , - , I ) = dl if / = i+ 1 and if. a = w~ +1, 0 otherwise. 2. \] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and if k = l, 0 otherwise."
88,"Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences i to identify the words. The difficulties of identifying words include (l) the identification of com- plex words, such as Determinative-Measure, redupli- cations, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmenta- tions. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate. The statistical data supports that the maximal match- ing algorithm is the most effective heuristics.","Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences i to identify the words. The difficulties of identifying words include (l) the identification of com- plex words, such as Determinative-Measure, redupli- cations, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmenta- tions. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate. The statistical data supports that the maximal match- ing algorithm is the most effective heuristics. Chinese sentences arc cx)mposed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences is to identify the words( i.e. segment the character strings of the sentences into word strings). Most of the current Chinese natural language processing systems include a processor for word iden- tification. Also there are many word segmentation techniques been developed. Usually they use a lexicon with a large set of entries to match input sentences \[2,10,12,13,14,21\]. It is very often that there are many l~)ssible different successful matchings. Therefore the major focus for word identification were on thc resolu- tion of ambiguities. However many other important as- pects, such as what should be done, in what depth and what are considered to be the correct identifications were totally ignored. High identification rates are claimed to be achieved, but none of them were mea- sured under equal bases. There is no agreement in what extend words are considered to be correctly iden- tified. For instance, compounds occur very often in Chi- nese text, but none of the existing systems except ours pay much attention to identify them. Proper name is another type of words which cannot be listed exhaus- tively in the lexicon. Therefore simple matching algo- rithms can not successfully identify either compounds or proper names. In this paper, we like to raise the ptx~blems and the difficulties in identifying words and suggest the possible solutions.","Chinese sentences arc cx)mposed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences is to identify the words( i.e. segment the character strings of the sentences into word strings). Most of the current Chinese natural language processing systems include a processor for word iden- tification. Also there are many word segmentation techniques been developed. Usually they use a lexicon with a large set of entries to match input sentences \[2,10,12,13,14,21\]. It is very often that there are many l~)ssible different successful matchings. Therefore the major focus for word identification were on thc resolu- tion of ambiguities. However many other important as- pects, such as what should be done, in what depth and what are considered to be the correct identifications were totally ignored. High identification rates are claimed to be achieved, but none of them were mea- sured under equal bases. There is no agreement in what extend words are considered to be correctly iden- tified. For instance, compounds occur very often in Chi- nese text, but none of the existing systems except ours pay much attention to identify them. Proper name is another type of words which cannot be listed exhaus- tively in the lexicon. Therefore simple matching algo- rithms can not successfully identify either compounds or proper names. In this paper, we like to raise the ptx~blems and the difficulties in identifying words and suggest the possible solutions."
89,"Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration i the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed.","Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration i the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed. The work on merging strategies from unification grammars and categorial grammars has its origins in several research efforst that have been pursued in parallel. One of them is the grammar development on the PATR system (Shieber et al, 1983; Shieber, 1984) at SRI. For quite a while now I have been using the excellent facilities of PATR for the design and testing of experimental\[ CUGs. Such grammars currently run on two PATR implementations: Stuart Shieber's Zetalisp version on the Symbolics 3600 and Lauri Karttunen's Interlisp-D w:rsion on the XEROX 1109. The work on CUGs has influenced our efforts to develop a larger PATR grammar, and will do so even more in the future. On the theoretical side, this work is part of ongoing research on such topics as word order variation, modification, and German syntax within projects at SRI and CSLI (Stanford University). The structure of the paper eflects the diverse nature of the enterprise. In the first section, I will introduce the basic notions of CUGs and demonstrate them through examples in PATR notation. The second section discusses the motivation for this work and some of its theoretical implications. The third section sketches a linguistically motivated CUG framework with a strong lexical syntax that accomodates word order variation. The paper concludes with a brief discussion of possible CUG approaches tolong-distance d pendencies. 1. Basic Notions of Categorial Unification. Grammars 1.2. Unif ication Grammars and Categorial. Grammars Both terms, unification grammar (UG) and categorial grammar (CG), stand for whole families of related grammar formalisms whose basic notions are widely known.l Yet, for the characterization f the class of formalisms I want to discuss, it will be useful to review the most central concepts of both UG and CG. Unification grammar formalisms employ complex feature structures as their syntactic representations. These structures encode partial information about constituents. Either term or graph unification is utilized as the main operation for checking, propagating, and merging of the information in these complex representations. Most unification grammars also use the complex feature structures for the linking of syntactic and semantic information. In traditional categorial grammars, all information about possible syntactic ombinations of constituents is encoded in their categories. Those grammars allow only binary combinations. One of the two combined constituents, the functor, encodes the combination funtion, the other constituent serves as the argument to this function. Instead ot7 phrase structure rules, the grammar contains one or, in some formalisms, two combination rules that combine a functor and an argument by applying the function encoded in the functor to the argument constituent. Most categorial grammars only combine constituents whose terminal strings concatenate in the input string, but this need not be so. In most categorial grammar formalisms, it is assumed that the syntactic functor-argument structure in the corresponding compositional semantics. 187 There are usually two types of grammatical categories in a categorial grammar, basic and derived ones. Basic categories are just category symbols, derived categories are functions from one (derived or basic) category to another. A derived category that encodes a function from category A to category B might be written B/A if the functor combines with an argument to its right or B~, if it expects the argument to its left. Thus, if we assume just two basic categories, N and S, then N/S, S/N, N\S, S\N, (S\N)/N, (N/S\(S\(N/N)), etc. are also categories. Not all of these categories will ever occur in the derivation of sentences. The set of actually occurring categories depends on the lexical categories of the language. Assume the following simple sample grammar: (2) Basic categories: N, S lexical categories: N (Paul, Peter) (S\N)fN (likes) The grammar is used for the sample derivation in (3): (3) Peter likes Paul N (S\N)fin N SkN S It should be clear from my brief description that the defining characteristics of unification grammar have nothing to do with the ones of categorial grammar. We will see that the properties of both grammar types actually complement each other quite wetl. 1.2. A Sample CUG in PATR Notat ion Since the first categorial unification grammars were written in the PATR formalism and tested on the PATR systems implemented at SRI, and since PATR is especially well suited for the emulation of other grammar formalisms, I will use its notation. The representations in PATR are directed acyclic graphs (DAGs) 2 . Rules have two parts, a head and a body. The head is a context-free rewrite rule and the body is a DAG. Here is an example, a simple rule that forms a sentence by combining anoun phrase with a verb phrase. 188 (4) head XO -~ X1, X2 body in unification otation <X0 cat> = S <X1 cat> = NP <X2cat> = VP <X1 agr> = <X2agr> body in graph notation xo r S NP The rule states that two constituents X1 and X2 can combine to form a constituent X0 if the terminal string covered by X1 immediately precedes the terminal string of X2 and if the DAGs of X0, X1, and X2 unify with the X0, X1, and X2 subgraphs of the rule body, respectively. I will now show the most straight-forward encoding of a categorial grammar in this notation. There are two types of constituent graphs. Constituent graphs for basic categories are of the following form: (5) N S Of course, there might be more features associated with the constituent: (6) /oe 7 N S Finite 3 Sg Derived constituents have graphs of the following form: (7) arg (t0b) Backward Functional Application (BFA) value -~ functor argument < value > = < functor va l> <argument> = <functor a rg> <f lmctor d i r> :--: Left. This is the graph associated with the VP likes Paul: in graph notation: (8) ,. /~ Left /~ agr ca~//pers / form cat/pers~nu m S Finite N 3 Sg It corresponds to the derived-category s mboh (9) S \ N form : Finite pers : 3 num: Sg (10a) and (10b) are the rules that combine constituents. As in tradit ional categorial grammars, two such rules sufice. (10a) Forward Functional Application (FFA) value -~ functor argument <va lue> = <functorva l> <argument> = <functorarg> <functor d i r> = Right. in graph notation: val u e~J -~~'~. / funct? r l . ~rgu Right ment val u e ~- - J J -~-~rg u ment / Left If Backward Functional Application is used to combine the constituents Peter and likes Paul, the result is a finite sentence. However, if the same rule is applied to the identical constituents likes Paul and likes Paul, again a finite sentence is obtained. '\]\['his is so because the graph for likes Paul actually unifies with the value of arg in the same graph. This can be easily remedied by modifying the graph for the VP slightly. By st ipulat ing that the argument must not have an unfilled argument position, one can rule out derivcd categories as subject arguments tbr the VP: (II) /0o-i /?e?Tum S Finite N 3 Sg 1.3. Extens ions to the Basic Formal i sm. In this subsection \[want to discuss very briefly a few extensions of' the basic model that make it more suitable for the encoding of natural- language rammars. The first one is the sorting of fimctors according to their own syntactic category. This move might be described alternat ively as defining the type of a constituent as being defined by both a set of syntactic (and semantic) 189 attributes and a function from categories to categories. This function is also expressed as the value of an attribute. For a basic category the value of the function attribute is NIL. The following graph is a simplified example of a functor category (prenominal djective in a language with case and number agreement within the NP). ~ ~/ ~ ~'~unction ca;~/ ~s: :m - -~gr The combination rules need accordingly. This is the modified functional application. to be changed rule of forward value -~ functor argument <value> = <functor function val> < argument > = < functor function arg > <functor function dir> = Right. In a traditional categorial grammar, a derived category is exhaustively described by the argument and value categories. But often, syntacticians want to make more fine grained distinctions. An example is VP modification. In a traditional categorial grammar, two different VP modifiers, lets say an adverb and an adverbial clause, would receive the same translation. (12) Peter called him angrily N (S\N)fN N (S\N)/(S~q) (13) Peter called him at work N (S\N)/N N (S\N)/(S~aN) 190 But what should be the category for very? If it receives the category ((S\N)\(S\N))/((S\N)\(S~N)) to allow the derivation of (14), the ungrammatical sentence (15) is also permitted. (14) Peter called him very angrily N (S\N)/N N ((S\N)\(SLN))/ (S\N)/(S~X\[) ((S\N)\(S~N')) (15) *Peter called him very N (S\N)/N N ((S\N)\(S~))/ ((S\N)\(S\N)) at work (S\N)/(S~) If functor categories are permitted to carry features of their own that are not necessarily bound to to any features of their argument and value categories, this problem disappears. Adverbs and adverbial clauses could receive different features even if their categories encode the same combination function. Another solution to the problem involves the encoding of the difference in the value part of the functor. Yet this solution is not only unintuitive but also contradicts a linguistic generalization. It is unintuitive because there is no difference in the distribution of the resulting VPs. The only difference holds between the modifiers themselves. The gene~:alization that is violated by the encoding of the difference in the value subgraphs is the endocentricity of the VP. The modified VP shares all syntactic features with its head, the lower VP. Yet the feature that indicates the difference between adverbs and adverbial phrases could not be in both the argument and the value parts of the functor, otherwise iterations of the two types of modifiers as they occur in the following pair of sentences would be ruled out. (16a) Peter called him very angrily at work. (16b) Peter called him at work very angrily. Another augmentation is based on the PATR strategy for linking syntax and semantics. Most grammars written in PATR use the constituent graphs also for encoding semantic information. Every constituent has an attribute called trans or semantics. The value of this attribute contains minimally the internal semantic fnnction-argument structure of the constituent, but may also encode additional semantic information. The separate encoding of the semantics allows for a compositional semantics even in construction in which syntactic and semantic structure divert as in certain raising constructions. The following graph for a ficticious prenominal adjective that was introduced earlier contains translation attributes for the functor, the argument and the value. The meaning of the adjective is indicated by the atom Red. cat ~ / functi% ~rans Adj Acc ing ~._~g Red At first glance, the lexical graphs--even the ones that are used in the highly simplified examples--seem to exhibit an excessive degree of complexity and redundancy. However, the lexical approach to syntax is built on the assumption that the lexicon is structured. To create a lexicon that is structured according to linguistic generalizations, weintroduced lexical templates early on in the development of PATR. Templates are graphs that contain structure shared by a class of lexical entries. Lexical graphs can be partially or fully defined in terms of templates, which themselves can be defined in terms of templates. If a template name appeam in the definition of some graph, the graph is simply unified with the graph denoted by the template. The next augmentation is already built into the formalism. Categorial grammarians have recognized the limitations of fimctional application as the sole mode of combining constituents for a long time. One of the obvious extensions to classical categorial grammar was the utilization of functional composition as a further combination mode. A good example of a categorial grammar that employs both functional application and functional composition is Steedman (1985). Forward functional composition permits the following combination ofcategories: (21) A/B + B/C = A/C The resulting category inherits the argument place for C from the argument B/C. Neither Steedman's nor any other CG I am aware of permits functional composition i its full generality. In order to prevent overgeneration, functional composition as well as other combination modes that are discussed by Steedman are restricted to apply to certain categories only. This somehow violates the spirit of a categorial grammar. Steedman's combination rules, for instance, are net universal. In CUG, functional composition is subsumed under functional application. It is the functor category that determines whether simple functional application, or functional composition, or either one may take place. Conjunction is a good case for demonstrating the versatility. Consider the following sentences: 3 (22a) Peter andPaul like bananas. (22b) Peter likes bananas and Paul likes oranges. (22c) Peter likes and buys bananas. The conjunction and may combine two simple argument categories (22a), two functors with one unfilled argument position (22b), or two functors with more than one unfilled argument position (22c). If the conjuncts have unfilled argument positions, the conjoined phrase needs to inherit them through functional composition. The simplified lexical graph for and is given under (23). In order to avoid a thicket of crossing edges, I have expressed some of the relevant bindings by indices. 191 (23) c ~ .. r ( The most appealing feature of this way of utilizing functional composition is that no additional combinators are required. No restriction on such a rule need to be formulated. It is only the lexical entries for functors that either demand, permit, or forbid functional composition. Extensions to the formalism that I have experimented with that cannot be discussed in the frame of this paper are the use of multiple stacks for leftward and rightward arguments and the DCG-like encoding of the ordering positions in the graphs. In Sections 3. and 4., I will discuss further extensions ofthe formalism and specific linguistic analyses. The following section contains a summary of the motivations for working on and with CUG and the main objectives of this work. Both terms, unification grammar and categorial grammar are used for classes of grammar formalisms, for individual grammar formalisms, and finally for grammars that are written in these formalisms. In addition, they might also be used by linguists to denote linguistic theories that are built around or on top of such a formalism. This is the type of terminological overloading that linguists have learned to live with--or at least gotten accustomed to. As I indicated in the previous section, I consider CUG to stand for a family of grammar formalisms that might be described as the intersection of categorial and 192 unification grammar formalisms. What has been proposed so far is therefore not a new grammar formalism and even less a linguistic framework. The proposal is simply to further explore the usefulness and formal properties of subclasses of CUG. This proposal can be supported by a number of reasons. Both types of formalisms have clear advantages. Categorial grammars have been hailed for their conceptual c arity and their potentials for l inking syntax and semantics. The fact that they have been around for a long time and that they are currently enjoying a renaissance in the works of Steedman, Bach, Dowty, and many others demonstrates their virtues. Unification grammars are spreading last and lend themselves to powerfifl but efficient computer implementations. Traditionally, categorial grammars have been lacking syntactic sophistication. In a functor category such as A/B, only domain and range of the function are specified but nothing is said about bow they are related; how, for instance, the features of the argument influence the features of the value. The graph notation expresses the relation between argument and value categories quite well; it is expressed in a set of bindings between subgraphs ofthe two categories. In the context of this discussion, some remarks are in order on the specific role PATR has played for the experiments with CUGs. The philosophy behind the development of PATR has been to provide a tool for writing, testing, and comparing grammars of very different types in a powerful formalism with well-understood formal properties and a well-defined semantics (Shieber 1984). Thus PATR could be useful for writing grammars, designing rammar formalisms, and for exploring classes of such formalisms. The work on exploring categorial unification formalisms has not only benefitted from the features of PATR but it has in a way also influenced the development of the PATR formalism. It was, for instance, essential for the writing of categorial grammars to allow category variables in the context-free phrase structure part of the rules. How else could one formulate the rules of functional application. The implementation f this facility through Stuart Shieber, however, raised interesting problems in connection with the prediction aspect of the Earley-parser. Original Earley prediction works on category symbols. An answer to these problems was presented by Shieber (1985) who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer. Another example for the influence of the CUG efforts on the development of PATR is a new template notation introduced by Lauri Karttunen in his Interlisp-D version of PATR. Since categorial grammars exhibit an extensive embedding of categories within other categories, it is useful to unify templates not only with the whole lexical DAG but also with its categorial subgraphs. The @-notation permits this use of templates (Karttunen, 1986)3","Both terms, unification grammar and categorial grammar are used for classes of grammar formalisms, for individual grammar formalisms, and finally for grammars that are written in these formalisms. In addition, they might also be used by linguists to denote linguistic theories that are built around or on top of such a formalism. This is the type of terminological overloading that linguists have learned to live with--or at least gotten accustomed to. As I indicated in the previous section, I consider CUG to stand for a family of grammar formalisms that might be described as the intersection of categorial and 192 unification grammar formalisms. What has been proposed so far is therefore not a new grammar formalism and even less a linguistic framework. The proposal is simply to further explore the usefulness and formal properties of subclasses of CUG. This proposal can be supported by a number of reasons. Both types of formalisms have clear advantages. Categorial grammars have been hailed for their conceptual c arity and their potentials for l inking syntax and semantics. The fact that they have been around for a long time and that they are currently enjoying a renaissance in the works of Steedman, Bach, Dowty, and many others demonstrates their virtues. Unification grammars are spreading last and lend themselves to powerfifl but efficient computer implementations. Traditionally, categorial grammars have been lacking syntactic sophistication. In a functor category such as A/B, only domain and range of the function are specified but nothing is said about bow they are related; how, for instance, the features of the argument influence the features of the value. The graph notation expresses the relation between argument and value categories quite well; it is expressed in a set of bindings between subgraphs ofthe two categories. In the context of this discussion, some remarks are in order on the specific role PATR has played for the experiments with CUGs. The philosophy behind the development of PATR has been to provide a tool for writing, testing, and comparing grammars of very different types in a powerful formalism with well-understood formal properties and a well-defined semantics (Shieber 1984). Thus PATR could be useful for writing grammars, designing rammar formalisms, and for exploring classes of such formalisms. The work on exploring categorial unification formalisms has not only benefitted from the features of PATR but it has in a way also influenced the development of the PATR formalism. It was, for instance, essential for the writing of categorial grammars to allow category variables in the context-free phrase structure part of the rules. How else could one formulate the rules of functional application. The implementation f this facility through Stuart Shieber, however, raised interesting problems in connection with the prediction aspect of the Earley-parser. Original Earley prediction works on category symbols. An answer to these problems was presented by Shieber (1985) who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer. Another example for the influence of the CUG efforts on the development of PATR is a new template notation introduced by Lauri Karttunen in his Interlisp-D version of PATR. Since categorial grammars exhibit an extensive embedding of categories within other categories, it is useful to unify templates not only with the whole lexical DAG but also with its categorial subgraphs. The @-notation permits this use of templates (Karttunen, 1986)3"
90,"In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements.","In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements. Automated language understanding requires the determination f the concept which a given use of a word represents, a process referred to as word sense disambiguation (WSD). WSD is typically effected in natural llanguage processing systems by utilizing semantic teature lists for each word in the system's lexicon, together with restriction mechanisms such as case role selection. However, it is often impractical to manually encode such information, especially for generalized text where the variety and meaning of words is potentially unrestricted. Furthermore, restriction mechanisms usually operate within a single sentence~ and thus the broader context cannot assist in the disambiguation process. in this paper, we describe a means tor automatically building Very Large Neural Networks (VLNNs) from definition texts in machine-readable dictionaries, and denmnstrate he use of these networks for WSD. Our method brings together two earlier, independent approaches to WSD: the use of machine-readable dictionaries and spreading and activation models. The automatic onstruction of VLNNs enables real-size experiments with neural networks, which in turn The authors would like to acknowledge the contributions of St~phanc tlari6 and Gavin Huntlcy to the work presented in this paper. provides insight into their behavior and design and can lead to possible improvements. 2.1. Machine-readable dictionaries Jbr WSD. There have been several attempts to exploit the information in maclfine-readable versions of everyday dictionaries ( ee, tor instance, Amsler, 1980; Calzolari, 1984; Chodorow, Byrd and Heidorn, 1985; Markowitz, Ahlswede and Evens, 1986; Byrd et al, 1987; V&onis, Ide and Wurbel, 1989), in which an enormous amount of lexical and semantic knowledge is already ""encoded"". Such information is not systematic or even complete, and its extraction from machine- readable dictionaries is not always straightforward. However, it has been shown that even in its base form, information from machine-readable dictionaries can be used, for example, to assist in the disambiguation f prepositional phrase attachment (Jensen and Bluet, 1987), or to find subject domains in texts (Walker and Amsler, 1986). The most general and well-known attempt to utilize information i machine-readable dictionaries for WSD is that of Lesk (1986), which computes the degree of overlap--that is, number of shared words--in definition texts of words that appear in a ten-word window of 1 389 context. The sense of a word with the greatest number of overlaps with senses of other words in the window is chosen as the correct one. For example, consider the definitions of pen and sheep from the Collins English Dictionary, the dictionary used in our experiments, in figure 1. Figure 1: Definitions of PEN, SHEEP, GOAT and PAGE in the Collins English Dictionary pen 1 1. an implement for writing or drawing using ink, formerly consisting of a sharpened and split quill, and now of a metal nib attached to a holder. 2. the writing end of such an implement; nib. 3. style of writing. 4. the pen. a. writing as an occupation, b. the written word. 5, the long horny internal shell of a squid. 6. to write or compose. pen 2 1. an enclosure in which domestic animals are kept. 2.any place of confinement. 3. a dock for servicing submarines. 4. to enclose or keep in a pen. pen 3 short for penitentiary. pen 4 a female swan. sheep L any of various bovid mammals of the genus O~is and related genera having transversely ribbed horns and a narrow face, There are many breeds of domestic sheep, raised for their wool and for meat. 2. :Barbary sheep. 3. a meek or timid person. 4. separate the sheep from the goats, to pick out the members of any group who are superior in some respects. goat 1. any sure-footed agile bovid mammal of the genus Capra, naturally inhabiting rough stony ground in Europe, Asia, and N Africa, typically having a brown-grey colouring and a beard. Domesticated varieties (C. hircus) are reared for milk, meat, and wool. 3. a lecherous man. 4. a bad or inferior member of any group 6. act (or play) the (giddy) goat. to fool around. 7. get (someone's) goat. to cause annoyance to (someone) page I 1. one side of one of the leaves of a book, newspaper, letter, etc. or the written or printed matter it bears. 2. such a leaf considered as a unit 3. an episode, phase, or period 4. Printing. the type as set up for printing a page. 6. to look through (a book, report, etc.); leaf through. page 2 1. a boy employed to run errands, carry messages, etc., for the guests in a hotel, club, etc. 2. a youth in attendance at official functions or ceremonies. 3. a. a boy in training for knighthood in personal attendance on a knight, b. a youth in the personal service of a person of rank. 4. an attendant at Congress or other legislative body. 5. a boy or girl employed in the debating chamber of the house of Commons, the Senate, or a legislative assembly to carry messages for members. 6. to call out the name of (a person). 7. to call (a person) by an electronic device, such as bleep, g. to act as a page to or attend as a page. If these two words appear together in context, the appropriate senses of pen (2.1: ""enclosure"") and sheep (1: ""mammal"") will be chosen because the definitions of these two senses have the word domestic in common. However, with one word as a basis, the relation is tenuous and wholly dependent upon a particular dictionary's wording. The method also fails to take into account less immediate r lationships between words. As a result, it will not determine the correct sense of pen in the context of goat. The correct sense of pen (2.1: enclosure ) and the correct sense of goat (1: mammal ) do not share any words in common in their definitions in the Collins English Dictionary; however, a strategy 390 which takes into account a longer path through definitions will find that animal is in the definition of pen 2.1, each of mammal and animal appear in the definition of the other, and mammal is in the definition of goat 1. Similarly, Lesk's method would also be unable to determine the correct sense of pen (1.1: writing utensil ) in the context of page, because seven of the thirteen senses of pen have the same number of overlaps with senses of page. Six of the senses of pen share only the word write with the correct sense of page (1.1: ""leaf of a book""). However, pen 1.1 also contains words such as draw and ink, and page 1.1 contains book, newspaper, letter, and print. These other words are heavily interconnected in a complex network which cannot be discovered by simply counting overlaps. Wilks et al (forthcoming) build on Lesk's method by computing the degree of overlap for related word-sets constructed using co-occurrence data from definition texts, but their method suffers from the same problems, in addition to combinatorial problems thai prevent disambiguating more than one word at a time. 2.2. Neural networks for WSD. Neural network approaches to WSD have been suggested (Cottrell and Small, 1983; Waltz and Pollack, 1985). These models consist of networks in which the nodes (""neurons"") represent words or concepts, connected by ""activatory"" links: the words activate the concepts to which they are semantically related, and vice versa. In addition, ""lateral"" inhibitory links usually interconnect competing senses of a given word. Initially, the nodes corresponding tothe words in the sentence to be analyzed are activated. These words activate their neighbors in the next cycle in turn, these neighbors activate their immediate neighbors, and so on. After a number of cycles, the network stabilizes in a state in which one sense for each input word is more activated than the others, using a parallel, analog, relaxation process. Neural network approaches to WSD seem able to capture most of what cannot be handled by overlap strategies such as Lesk's. However, the networks used in experiments o far are hand-coded and thus necessarily very small (at most, a few dozen words and concepts). Due to a lack of real-size data, it is not clear that he same neural net models will scale up for realistic application. Further, some approaches rely on ""context- setting"" nodes to prime particular word senses in order to force 1the correct interpretation? But as Waltz and Pollack point out, it is possible that such words (e.g., writing in the context of pen ) are not explicitly present in the text under analysis, but may be inferred by the reader from the presence of other, related words (e.g., page, book, inkwell, etc.). To solve this problem, words in such networks have been represented by sets of semantic ""microfeatures"" (Waltz and Pollack, 1985; Bookman, 1987) which correspond to fundamental semantic distinctions (animate/inanimate, edible/ inedible, threatening/safe, etc.), characteristic duration of events (second, minute, hour, day, etc.), locations (city, country, continent, etc.), and other similar distinctions that humans typically make about situations in the world. To be comprehensive, the authors uggest that these features must number in the thousands. Each concept iin the network is linked, via bidirectional activatory or inhibitory links, to only a subset of the complete microfeature s t. A given concept theoretically shares everal microfeatures with concepts to which it is closely related, and will therefore activate the nodes corresponding to closely related concepts when it is activated :itself. ttowever, such schemes are problematic due to the difficulties of designing an appropriate set of microfeatures, which in essence consists of designing semantic primitives. This becomes clear when one exmnines the sample microfeatures given by Waltz ~md Pollack: they specify micro.f carfares uch as CASINO and CANYON, but it is obviously questionable whether such concepts constitute fundamental semantic distinctions. More practically, it is simply difficult to imagine how vectors of several thousands of microfeamrcs for each one of the lens of thousands of words and hundreds of thousands of senses can be realistically encoded by hand. Our approach to WSD takes advantage of both strategies outlined above, but enables us to address solutions to their shortcomings. This work has been carried out in tile context of a joint project of Vassar College and the Groupe Reprdsentation et Traitement des Connaissances of the Centre National de la Recherche Scientifique (CNRS), which is concerned with the construction and exploitation of a large lexical data base of English and French. At present, the Vassar/CNRS data base includes, through the courtesy of several editors and research institutions, several English and French dictionaries (the Collins English Dictionary, the Oxford Advanced Learner's Dictionary, the COBUILD Dictionary, the Longman) Dictionary of Contemporary English, theWebster's 9th Dictionary, and the ZYZOMYS CD-ROM dictionary from Hachette Publishers) as well as several other lexical and textual materials (the Brown Corpus of American English, the CNRS BDLex data base, the MRC Psycholinguistic Data Base, etc.). We build VLNNs utilizing definitions in the Collins English Dictionary. Like Lesk and Wilks, we assume that there are significant semantic relations between a word and the words used to define it. The connections in the network reflect these relations. All of the knowledge represented in the network is automatically generated from a machine-readable dictionary, and therefore no hand coding is required. Further, the lexicon m~d the knowledge it contains potentially cover all of English (90,000 words), and as a result this information cml potentially be used to help dismnbiguate unrestricted text. 3.1. Topology of the network. In our model, words are complex units. Each word in the input is represented by a word node connected by excitatory links to sense nodes (figure 2) representing the different possible senses tbr that word in the Collins English Dictionary. Each sense node is in turn connected by excitatory links to word nodes rcpreseming the words in tile definition of that sense. This process is repeated a number of times, creating an increasingly complex and interconnected network. Ideally, the network would include the entire dictionary, but for practical reasons we limit the number of repetitions and thus restrict tile size of the network to a few thousand nodes and 10 to 20 thousand transitions. All words in the network are reduced to their lemmas, and grammatical words are excluded. The different sense nodes tor a given word are interconnected by lateral inhibitory links. 3 391 Figure 2. Topology of the network ~. , : ' .i \ [ ~ Word Node Sense Node ~ . Excitatory Link .......................... Inhibitory Link When the network is run, the input word nodes are activated first. Then each input word node sends activation to its sense nodes, which in turn send activation to the word nodes to which they are connected, and so on throughout he network for a number of cycles. At each cycle, word and sense nodes receive feedback from connected nodes. Competing sense nodes send inhibition to one another. Feedback and inhibition cooperate in a ""winner-take-all"" strategy to activate increasingly related word and sense nodes and deactivate the unrelated or weakly related nodes. Eventually, after a few dozen cycles, the network stabilizes in a configuration where only the sense nodes with the strongest relations to other nodes in the network are activated. Because of the ""winner-take-all"" strategy, at most one sense node per word will ultimately be activated. Our model does not use microfeatures, because, as we will show below, the context is taken into account by the number of nodes in the network and the extent to which they are heavily interconnected. So far, we do not consider the syntax of the input sentence, in order to locus on the semantic properties of the model. However, it is clear that syntactic information can assist in the disambiguation process in certain cases, and a network including a syntactic layer, such as that proposed by Waltz and Pol lack, would undoubtedly enhance the model's behavior. 3.2. Results. The network finds the correct sense in cases where Lesk's strategy succeeds. For example, if the input consists of pen and sheep, pen 2.1 and sheep 1 are correct ly act ivated. More interestingly, the network selects "" the appropriate senses in cases where Lesk's strategy fails. Figures 3 and 4 show the state of the network after being run with pen and goat, and pen and page, respectively. The figures represent only the most activated part of each network after 100 cycles. Over the course of the run, the network reinforces only a small cluster of the most semantically relevant words and senses, and filters out tile rest of the thousands of nodes. The correct sense for each word in each context (pen 2.1 with goat 1, and pen 1.1 withpage 1.1) is the only one activated at the end of the run. This model solves the context-setting problem mentioned above without any use of microfeatures. Sense 1.1 of pen would also be activated if it appeared in the context of a large number of other words--e.g., book, ink, inkwell, pencil, paper, write, draw, sketch, etc.--which ave a similar semantic relationship to pen. For example, figure 5 shows the state of the network after being run with pen and book. It is apparent that the subset of nodes activated is similar to those which were activated by page. 392 4 Figure 3. State of the network after being run with ""pen"" and ""goat"" \[ are the most activated } Figure 4. State of the network after being run with ""pen"" and ""page"" ~ \[ The darker nodes \] Figure 5. State of the network after being run with ""pen"" and ""book"" r The darker nodes \] ~ ~ , ook 393 The examples given here utilize only two words as input, in order to show clearly the behavior of the network. In fact, the performance of the network improves with additional input, since additional context can only contribute more to the disambiguation process. For example, given the sentence The young page put the sheep in the pen, the network correctly chooses the correct senses of page (2.3: ""a youth in personal service""), sheep (1), and pen (2.1). This example is particularly difficult, because page and sheep compete against each other to activate different senses of pen, as demonstrated in the examples above. However, the word young reinforces sense 2.3 of page, which enables sheep to win the struggle. Inter-sentential context could be used as well, by retaining the most activated nodes within the network during subsequent runs. By running various experiments on VLNNs, we have discovered that when the simple models proposed so far are scaled up, several improvements are necessary. We have, for instance, discovered that ""gang effects"" appear due to extreme imbalance among words having few senses and hence few connections, and words containing up to 80 senses and several hundred connections, and that therefore dampening is required. tn addition, we have found that is is necessary to treat a word node and its sense nodes as a complex, ecological unit rather than as separate ntities. In our model, word nodes corttrol the behavior of sense nodes by means of a differential neuron that prevents, for example, a sense node from becoming more activated than its master word node. Our experimentation with VLNNs has also shed light on the role of and need for various other parameters, uch as thresholds, decay, etc.","Our approach to WSD takes advantage of both strategies outlined above, but enables us to address solutions to their shortcomings. This work has been carried out in tile context of a joint project of Vassar College and the Groupe Reprdsentation et Traitement des Connaissances of the Centre National de la Recherche Scientifique (CNRS), which is concerned with the construction and exploitation of a large lexical data base of English and French. At present, the Vassar/CNRS data base includes, through the courtesy of several editors and research institutions, several English and French dictionaries (the Collins English Dictionary, the Oxford Advanced Learner's Dictionary, the COBUILD Dictionary, the Longman) Dictionary of Contemporary English, theWebster's 9th Dictionary, and the ZYZOMYS CD-ROM dictionary from Hachette Publishers) as well as several other lexical and textual materials (the Brown Corpus of American English, the CNRS BDLex data base, the MRC Psycholinguistic Data Base, etc.). We build VLNNs utilizing definitions in the Collins English Dictionary. Like Lesk and Wilks, we assume that there are significant semantic relations between a word and the words used to define it. The connections in the network reflect these relations. All of the knowledge represented in the network is automatically generated from a machine-readable dictionary, and therefore no hand coding is required. Further, the lexicon m~d the knowledge it contains potentially cover all of English (90,000 words), and as a result this information cml potentially be used to help dismnbiguate unrestricted text. 3.1. Topology of the network. In our model, words are complex units. Each word in the input is represented by a word node connected by excitatory links to sense nodes (figure 2) representing the different possible senses tbr that word in the Collins English Dictionary. Each sense node is in turn connected by excitatory links to word nodes rcpreseming the words in tile definition of that sense. This process is repeated a number of times, creating an increasingly complex and interconnected network. Ideally, the network would include the entire dictionary, but for practical reasons we limit the number of repetitions and thus restrict tile size of the network to a few thousand nodes and 10 to 20 thousand transitions. All words in the network are reduced to their lemmas, and grammatical words are excluded. The different sense nodes tor a given word are interconnected by lateral inhibitory links. 3 391 Figure 2. Topology of the network ~. , : ' .i \ [ ~ Word Node Sense Node ~ . Excitatory Link .......................... Inhibitory Link When the network is run, the input word nodes are activated first. Then each input word node sends activation to its sense nodes, which in turn send activation to the word nodes to which they are connected, and so on throughout he network for a number of cycles. At each cycle, word and sense nodes receive feedback from connected nodes. Competing sense nodes send inhibition to one another. Feedback and inhibition cooperate in a ""winner-take-all"" strategy to activate increasingly related word and sense nodes and deactivate the unrelated or weakly related nodes. Eventually, after a few dozen cycles, the network stabilizes in a configuration where only the sense nodes with the strongest relations to other nodes in the network are activated. Because of the ""winner-take-all"" strategy, at most one sense node per word will ultimately be activated. Our model does not use microfeatures, because, as we will show below, the context is taken into account by the number of nodes in the network and the extent to which they are heavily interconnected. So far, we do not consider the syntax of the input sentence, in order to locus on the semantic properties of the model. However, it is clear that syntactic information can assist in the disambiguation process in certain cases, and a network including a syntactic layer, such as that proposed by Waltz and Pol lack, would undoubtedly enhance the model's behavior. 3.2. Results. The network finds the correct sense in cases where Lesk's strategy succeeds. For example, if the input consists of pen and sheep, pen 2.1 and sheep 1 are correct ly act ivated. More interestingly, the network selects "" the appropriate senses in cases where Lesk's strategy fails. Figures 3 and 4 show the state of the network after being run with pen and goat, and pen and page, respectively. The figures represent only the most activated part of each network after 100 cycles. Over the course of the run, the network reinforces only a small cluster of the most semantically relevant words and senses, and filters out tile rest of the thousands of nodes. The correct sense for each word in each context (pen 2.1 with goat 1, and pen 1.1 withpage 1.1) is the only one activated at the end of the run. This model solves the context-setting problem mentioned above without any use of microfeatures. Sense 1.1 of pen would also be activated if it appeared in the context of a large number of other words--e.g., book, ink, inkwell, pencil, paper, write, draw, sketch, etc.--which ave a similar semantic relationship to pen. For example, figure 5 shows the state of the network after being run with pen and book. It is apparent that the subset of nodes activated is similar to those which were activated by page. 392 4 Figure 3. State of the network after being run with ""pen"" and ""goat"" \[ are the most activated } Figure 4. State of the network after being run with ""pen"" and ""page"" ~ \[ The darker nodes \] Figure 5. State of the network after being run with ""pen"" and ""book"" r The darker nodes \] ~ ~ , ook 393 The examples given here utilize only two words as input, in order to show clearly the behavior of the network. In fact, the performance of the network improves with additional input, since additional context can only contribute more to the disambiguation process. For example, given the sentence The young page put the sheep in the pen, the network correctly chooses the correct senses of page (2.3: ""a youth in personal service""), sheep (1), and pen (2.1). This example is particularly difficult, because page and sheep compete against each other to activate different senses of pen, as demonstrated in the examples above. However, the word young reinforces sense 2.3 of page, which enables sheep to win the struggle. Inter-sentential context could be used as well, by retaining the most activated nodes within the network during subsequent runs. By running various experiments on VLNNs, we have discovered that when the simple models proposed so far are scaled up, several improvements are necessary. We have, for instance, discovered that ""gang effects"" appear due to extreme imbalance among words having few senses and hence few connections, and words containing up to 80 senses and several hundred connections, and that therefore dampening is required. tn addition, we have found that is is necessary to treat a word node and its sense nodes as a complex, ecological unit rather than as separate ntities. In our model, word nodes corttrol the behavior of sense nodes by means of a differential neuron that prevents, for example, a sense node from becoming more activated than its master word node. Our experimentation with VLNNs has also shed light on the role of and need for various other parameters, uch as thresholds, decay, etc."
91,"Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries.","Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. There have been quite a number of recent papers on parallel text: Brown et al(1990, 1991, 1993), Chen (1993), Church (1993), Church et al(1993), Dagan et al(1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al(1992), Warwick- Armstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and English- Chinese. In previous work (Church et al 1993), we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980)), using charalign (Church, 1993), a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages uch as English and Japanese which are written in different alphabets. The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese. It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences. The K-vec method attempts to address this question.","There have been quite a number of recent papers on parallel text: Brown et al(1990, 1991, 1993), Chen (1993), Church (1993), Church et al(1993), Dagan et al(1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al(1992), Warwick- Armstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and English- Chinese. In previous work (Church et al 1993), we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980)), using charalign (Church, 1993), a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages uch as English and Japanese which are written in different alphabets. The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese. It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences. The K-vec method attempts to address this question."
92,"We present an efI\]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It con- rains a lexicon with over 90,000 entries, con- structed automatically b applying a set of ex- traction and conversion rules to entries from machine readable dictionaries.","We present an efI\]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It con- rains a lexicon with over 90,000 entries, con- structed automatically b applying a set of ex- traction and conversion rules to entries from machine readable dictionaries. Principle-based grammars, such as Govern- ment-Binding (GB) theory (Chomsky, 1981; Haegeman, 1991), offer many advantages over rule-based and unification-based grammars, such as the universality ofprinciples and mod- ularity of components in the grammar. Prin- ciples are constraints over X-bar structures. Most previous principle-based parsers, e.g., (Dorr, 1991; Font, 1991; Johnson, 1991), es- sentially generate all possible X-bar structures of a sentence and then use the principles to fil- ter out the illicit ones. The drawback of this approach is the inefficiency due 1;o the large number of candidate structures to be. filtered out. The problem persists even when w~rions techniques such as optimal ordering of princi- ples (Fong, 1991), and corontining (Dorr, 1991; Johnson, 1991) are used. This problem may also account for the fact that these parsers are experimental nd have limited coverage. This paper describes an efficient, broad- coverage, principle-based parser, called PRIN- CIPAR. The main innovation in PRINCIPAR is that it applies principles to descriptions o17 X- bar structures rather than the structures them- selves. X-bar structures of a sentence are only built when their descriptions have satisfied all the pri ncil)les. O dynamic data \ [~ static dala l)rocegsing module data flow Figure 1: '.Pile architecture of PRINCIPAR Figure I shows the architecture of PRIN- CIPAR. Sentence analysis is divided into three steps. The lexical analyser first converts the in- put sentence into a set of texical items. Then, a message passing algorithm for OB-parsing is used to construct a shared parse forest. Fi- nally, a parse tree retriever is used to enumer- ate the parse trees. The key idea of the parsing algorithm was presented in (tin, 199:1). This paper presents some implementation details and experimental results.","Principle-based grammars, such as Govern- ment-Binding (GB) theory (Chomsky, 1981; Haegeman, 1991), offer many advantages over rule-based and unification-based grammars, such as the universality ofprinciples and mod- ularity of components in the grammar. Prin- ciples are constraints over X-bar structures. Most previous principle-based parsers, e.g., (Dorr, 1991; Font, 1991; Johnson, 1991), es- sentially generate all possible X-bar structures of a sentence and then use the principles to fil- ter out the illicit ones. The drawback of this approach is the inefficiency due 1;o the large number of candidate structures to be. filtered out. The problem persists even when w~rions techniques such as optimal ordering of princi- ples (Fong, 1991), and corontining (Dorr, 1991; Johnson, 1991) are used. This problem may also account for the fact that these parsers are experimental nd have limited coverage. This paper describes an efficient, broad- coverage, principle-based parser, called PRIN- CIPAR. The main innovation in PRINCIPAR is that it applies principles to descriptions o17 X- bar structures rather than the structures them- selves. X-bar structures of a sentence are only built when their descriptions have satisfied all the pri ncil)les. O dynamic data \ [~ static dala l)rocegsing module data flow Figure 1: '.Pile architecture of PRINCIPAR Figure I shows the architecture of PRIN- CIPAR. Sentence analysis is divided into three steps. The lexical analyser first converts the in- put sentence into a set of texical items. Then, a message passing algorithm for OB-parsing is used to construct a shared parse forest. Fi- nally, a parse tree retriever is used to enumer- ate the parse trees. The key idea of the parsing algorithm was presented in (tin, 199:1). This paper presents some implementation details and experimental results."
93,"Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, has recently received much attention. Our work examines whether seman tic role information is beneficial to questionanswering. We introduce a general frame work for answer extraction which exploits semantic role annotations in the FrameNetparadigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate im provements over state-of-the-art models.","Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, has recently received much attention. Our work examines whether seman tic role information is beneficial to questionanswering. We introduce a general frame work for answer extraction which exploits semantic role annotations in the FrameNetparadigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate im provements over state-of-the-art models. Recent years have witnessed significant progress in developing methods for the automatic identificationand labeling of semantic roles conveyed by sentential constituents.1 The success of these methods, often referred to collectively as shallow semantic pars ing (Gildea and Jurafsky, 2002), is largely due to the availability of resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al, 2005), which document the surface realization of semantic roles in real world corpora. More concretely, in the FrameNet paradigm, themeaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic repre sentations of situations. Semantic roles (or frame1The approaches are too numerous to list; we refer the inter ested reader to Carreras and Ma`rquez (2005) for an overview.elements) are defined for each frame and correspond to salient entities present in the evoked situ ation. Predicates with similar semantics instantiate the same frame and are attested with the same roles. The FrameNet database lists the surface syntacticrealizations of semantic roles, and provides anno tated example sentences from the British National Corpus. For example, the frame Commerce Sell has three core semantic roles, namely Buyer, Goods, andSeller ? each expressed by an indirect object, a di rect object, and a subject (see sentences (1a)?(1c)). It can also be attested with non-core (peripheral) roles (e.g., Means, Manner, see (1d) and (1e)) thatare more generic and can be instantiated in sev eral frames, besides Commerce Sell. The verbs sell, vend, and retail can evoke this frame, but also the nouns sale and vendor. (1) a. [Lee]Seller sold a textbook [to Abby]Buyer. b. [Kim]Seller sold [the sweater]Goods. c. [My company]Seller has sold [more than three million copies]Goods. d. [Abby]Seller sold [the car]Goods [for cash]Means. e. [He]Seller [reluctanctly]Manner sold [his rock]Goods.By abstracting over surface syntactic configurations, semantic roles offer an important first step to wards deeper text understanding and hold promisefor a range of applications requiring broad cover age semantic processing. Question answering (QA) is often cited as an obvious beneficiary of semantic 12 role labeling (Gildea and Jurafsky, 2002; Palmer et al., 2005; Narayanan and Harabagiu, 2004). Faced with the question Q: What year did the U.S. buyAlaska? and the retrieved sentence S: . . .before Russia sold Alaska to the United States in 1867, a hypo thetical QA system must identify that United States is the Buyer despite the fact that it is attested in one instance as a subject and in another as an object. Once this information is known, isolating the correct answer (i.e., 1867 ) can be relatively straightforward.Although conventional wisdom has it that seman tic role labeling ought to improve answer extraction, surprising little work has been done to this effect (see Section 2 for details) and initial results have been mostly inconclusive or negative (Sun et al, 2005; Kaisser, 2006). There are at least two good reasons for these findings. First, shallow semanticparsers trained on declarative sentences will typically have poor performance on questions and generally on out-of-domain data. Second, existing re sources do not have exhaustive coverage and recallwill be compromised, especially if the question an swering system is expected to retrieve answers fromunrestricted text. Since FrameNet is still under development, its coverage tends to be more of a prob lem in comparison to other semantic role resources such as PropBank. In this paper we propose an answer extractionmodel which effectively incorporates FrameNetstyle semantic role information. We present an auto matic method for semantic role assignment which is conceptually simple and does not require extensive feature engineering. A key feature of our approachis the comparison of dependency relation paths at tested in the FrameNet annotations and raw text. We formalize the search for an optimal role assignment as an optimization problem in a bipartite graph. Thisformalization allows us to find an exact, globally op timal solution. The graph-theoretic framework goessome way towards addressing coverage problems related with FrameNet and allows us to formulate an swer extraction as a graph matching problem. As abyproduct of our main investigation we also exam ine the issue of FrameNet coverage and show howmuch it impacts performance in a TREC-style ques tion answering setting. In the following section we provide an overview of existing work on question answering systems that exploit semantic role-based lexical resources. Thenwe define our learning task and introduce our approach to semantic role assignment and answer ex traction in the context of QA. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. Question answering systems have traditionally de pended on a variety of lexical resources to bridge surface differences between questions and potential answers. WordNet (Fellbaum, 1998) is perhaps the most popular resource and has been employed in a variety of QA-related tasks ranging from query expansion, to axiom-based reasoning (Moldovan et al., 2003), passage scoring (Paranjpe et al, 2003), and answer filtering (Leidner et al, 2004). Besides WordNet, recent QA systems increasingly rely on syntactic information as a means of abstracting over word order differences and structural alternations (e.g., passive vs. active voice). Most syntax-based QA systems (Wu et al, 2005) incorporate some means of comparison between the tree representing the question with the subtree surrounding the answer candidate. The assumption here is that appropriate answers are more likely to have syntactic relationsin common with their corresponding question. Syntactic structure matching has been applied to passage retrieval (Cui et al, 2005) and answer extrac tion (Shen and Klakow, 2006). Narayanan and Harabagiu (2004) were the firstto stress the importance of semantic roles in an swering complex questions. Their system identifies predicate argument structures by merging semanticrole information from PropBank and FrameNet. Expected answers are extracted by performing probabilistic inference over the predicate argument struc tures in conjunction with a domain specific topicmodel. Sun et al (2005) incorporate semantic analy sis in their TREC05 QA system. They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. They find that semantic analysis does not boost performance due to the low recall of the semantic parser. Kaisser (2006) proposes a 13 SemStruc ac1SemStruc ac2 SemStruc aci SemStruc q Sent. Model I Q Model I Model II Answer Figure 1: Architecture of answer extraction question paraphrasing method based on FrameNet. Questions are assigned semantic roles by matching their dependency relations with those attested in the FrameNet annotations. The assignments are used to create question reformulations which are submitted to Google for answer extraction. The semantic role assignment module is not probabilistic, it relies onstrict matching, and runs into severe coverage prob lems. In line with previous work, our method exploitssyntactic information in the form of dependency re lation paths together with FrameNet-like semanticroles to smooth lexical and syntactic divergences be tween question and answer sentences. Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database. In contrast to Kaisser (2006), we model the semanticrole assignment and answer extraction tasks numerically, thereby alleviating the coverage problems en countered previously. We briefly summarize the architecture of the QA system we are working with before formalizing the mechanics of our FrameNet-based answer extractionmodule. In common with previous work, our over all approach consists of three stages: (a) determiningthe expected answer type of the question, (b) retrieving passages likely to contain answers to the question, and (c) performing a match between the ques tion words and retrieved passages in order to extract the answer. In this paper we focus on the last stage: question and answer sentences are normalized to aFrameNet-style representation and answers are re trieved by selecting the candidate whose semantic structure is most similar to the question.The architecture of our answer extraction mod ule is shown in Figure 1. Semantic structures for questions and sentences are automatically derived using the model described in Section 4 (Model I). Asemantic structure SemStruc = ?p,Set(SRA)? consists of a predicate p and a set of semantic role assignments Set(SRA). p is a word or phrase evoking a frame F of FrameNet. A semantic role assignment SRA is a ternary structure ?w,SR,s?, consist ing of frame element w, its semantic role SR, and score s indicating to what degree SR qualifies as a label for w.For a question q, we generate a semantic struc ture SemStrucq. Question words, such as what, who, when, etc., are considered expected answer phrases (EAPs). We require that EAPs are frame elementsof SemStrucq. Likely answer candidates are extracted from answer sentences following some pre processing steps detailed in Section 6. For each candidate ac, we derive its semantic structureSemStrucac and assume that ac is a frame element of SemStrucac. Question and answer seman tic structures are compared using a model based on graph matching detailed in Section 5 (Model II). We calculate the similarity of all derived pairs ?SemStrucq,SemStrucac? and select the candidate with the highest value as an answer for the question. Our method crucially exploits the annotated sen tences in the FrameNet database together with theoutput of a dependency parser. Our guiding assumption is that sentences that share dependency rela tions will also share semantic roles as long as they evoke the same or related frames. This is motivated by much research in lexical semantics (e.g., Levin (1993)) hypothesizing that the behavior of words,particularly with respect to the expression and interpretation of their arguments, is to a large ex tent determined by their meaning. We first describe how predicates are identified and then introduce our model for semantic role labeling. Predicate Identification Predicate candidates are identified using a simple look-up procedure whichcompares POS-tagged tokens against FrameNet entries. For efficiency reasons, we make the simplifying assumption that questions have only one predicate which we select heuristically: (1) verbs are pre 14 ferred to other parts of speech, (2) if there is more than one verb in the question, preference is given to the verb with the highest level of embedding in the dependency tree, (3) if no verbs are present, a nounis chosen. For example, in Q: Who beat Floyd Pat terson to take the title away?, beat, take away, and title are identified as predicate candidates and beat is selected the main predicate of the question. Foranswer sentences, we require that the predicate is ei ther identical or semantically related to the question predicate (see Section 5). In the example given above, the predicate beat evoques a single frame (i.e., Cause harm). However,predicates often have multiple meanings thus evo quing more than one frame. Knowing which is the appropriate frame for a given predicate impacts the semantic role assignment task; selecting the wrong frame will unavoidably result in erroneous semanticroles. Rather than disambiguiting polysemous pred icates prior to semantic role assignment, we performthe assignment for each frame evoqued by the pred icate. Semantic Role Assignment Before describing our approach to semantic role labeling we define dependency relation paths. A relation path R is a relation sequence ?r1,r2, ...,rL?, in which rl (l = 1,2, ...,L) is one of predefined dependency relations with suffix of traverse direction. An example of arelation path is R = ?sub jU ,ob jD?, where the sub scripts U and D indicate upward and downwardmovement in trees, respectively. Given an unannotated sentence whose roles we wish to label, we as sume that words or phrases w with a dependency path connecting them to p are frame elements. Eachframe element is represented by an unlabeled depen dency path Rw which we extract by traversing thedependency tree from w to p. Analogously, we ex tract from the FrameNet annotations all dependencypaths RSR that are labeled with semantic role infor mation and correspond to p. We next measure thecompatibility of labeled and unlabeled paths as fol lows: s(w,SR) = maxRSR?M [sim(Rw,RSR) ?P(RSR)] (2) where M is the set of dependency relation pathsfor SR in FrameNet, sim(Rw,RSR) the similarity be tween paths Rw and RSR weighted by the relative w SRw SR (a) (b) Figure 2: Sample original bipartite graph (a) and its subgraph with edge covers (b). In each graph, the left partition represents frame elements and the right partition semantic roles.frequency of RSR in FrameNet (P(RSR)). We consider both core and non-core semantic roles instan tiated by frames with at least one annotation in FrameNet. Core roles tend to have more annotations in Framenet and consequently are considered more probable. We measure sim(Rw,RSR), by adapting a string kernel to our task. Our hypothesis is that the more common substrings two dependency paths have, the more similar they are. The string kernel we used is similar to Leslie (2002) and defined asthe sum of weighted common dependency relation subsequences between Rw and RSR. For efficiency, we consider only unigram and bigram sub sequences. Subsequences are weighted by a metricakin to t f ? id f which measures the degree of asso ciation between a candidate SR and the dependency relation r present in the subsequence: weightSR(r) = fr ? log ( 1+ Nnr ) (3) where fr is the frequency of r occurring in SR; N is the total number of SRs evoked by a given frame; and nr is the number of SRs containing r. For each frame element we thus generate a set of semantic role assignments Set(SRA). This initialassignment can be usefully represented as a com plete bipartite graph in which each frame element (word or phrase) is connected to the semantic roleslicensed by the predicate and vice versa. (see Figure 2a). Edges are weighted and represent how com patible the frame elements and semantic roles are (see equation (2)). Now, for each frame element w 15 Q: Who discovered prions?S: 1997: Stanley B. Prusiner, United States, discovery of prions, ... SemStruc q p: discover Original SR assignments: Optimized SR assignments: 0.06 Cognizer. Phenomenon Ground State Evidence EAP prions 0000 0.01 0.1 0.05 0.05 0.02 0.06 Cognizer. Phenomenon Ground State Evidence EAP prions 0.1 0.05 0.05 0.02 SemStruc ac (ac: Stanley B. Prusiner) p: discovery Original SR assignments: Optimized SR assignments: 0.25 Cognizer. Phenomenon Topic Evidence ac prions 0.15 0.2 0.16 0.25 Cognizer. Phenomenon Topic Evidence ac prions 0.15 0.2 0.16 0.120.07 0 0 Figure 3: Semantic structures induced by our model for a question and answer sentence we could simply select the semantic role with the highest score. However, this decision procedure is local, i.e., it yields a semantic role assignment foreach frame element independently of all other ele ments. We therefore may end up with the same role being assigned to two frame elements or with frameelements having no role at all. We remedy this short coming by treating the semantic role assignment as a global optimization problem. Specifically, we model the interaction between all pairwise labeling decisions as a minimum weight bipartite edge cover problem (Eiter and Mannila,1997; Cormen et al, 1990). An edge cover is a sub graph of a bipartite graph so that each node is linked to at least one node of the other partition. This yields a semantic role assignment for all frame elements (see Figure 2b where frame elements and roles areadjacent to an edge). Edge covers have been success fully applied in several natural language processing tasks, including machine translation (Taskar et al, 2005) and annotation projection (Pado? and Lapata, 2006).Formally, optimal edge cover assignments are so lutions of following optimization problem: max E is edge cover ?(ndw,ndSR)?E s(nd w,ndSR)(4)where, s(ndw,ndSR) is the compatibility score be tween the frame element node ndw and semantic role node ndSR. Edge covers can be computed efficiently in cubic time using algorithms for the equivalent linear assignment problem. Our experiments used Jonker and Volgenant?s (1987) solver.2 Figure 3 shows the semantic role assignments generated by our model for the question Q: Whodiscovered prions? and the candidate answer sen tence S: 1997: Stanley B. Prusiner, United States,discovery of prions. Here we identify two predi cates, namely discover and discovery. The expectedanswer phrase (EAP) who and the answer candi date Stanley B. Prusiner are assigned the COGNIZERrole. Note that frame elements can bear multiple se mantic roles. By inducing a soft labeling we hope to render the matching of questions and answers morerobust, thereby addressing to some extent the cover age problems associated with FrameNet. We measure the similarity between a question and its candidate answer by matching their predicatesand semantic role assignments. Since SRs are framespecific, we prioritize frame matching to SR match ing. Two predicates match if they evoke the same frame or one of its hypernyms (or hyponyms). Thelatter are expressed by the Inherits From and Is In herited By relations in the frame definitions. If the predicates match, we examine whether the assignedsemantic roles match. Since we represent SR assignments as graphs with edge covers, we can also for malize SR matching as a graph matching problem. The similarity between two graphs is measured as the sum of similarities between their subgraphs.We first decompose a graph into subgraphs consist ing of one frame element node w and a set of SR nodes connected to it. The similarity between two subgraphs SubG1, and SubG2 is then formalized as: (5) Sim(SubG1,SubG2) = ? ndSR1 ? SubG1 ndSR2 ? SubG2 ndSR1 = ndSR2 1 |s(ndw,ndSR1 )? s(ndw,ndSR2 )|+1where, ndSR1 and ndSR2 are semantic role nodes con nected to a frame element node ndw in SubG1 and 2The software is available from http://www.magiclogic. com/assignment.html . 16 1757[11, 20] 2117[21, 50] 439[51, 100] 40[101, INF) 33800 1175[1, 5]1287[6, 10] Figure 4: Distribution of Numbers of Predicates and annotated sentences; each sub-pie, lists the number of predicates (above) with their corresponding range of annotated sentences (below) SubG2, respectively. s(ndw,ndsr1 ) and s(ndw,ndSR2 )are edge weights between two nodes in correspond ing subgraphs (see (2)). Our intuition here is that the more semantic roles two subgraphs share for a given frame element, the more similar they are and the closer their corresponding edge weights should be. Edge weights are normalized by dividing by the sum of all edges in a subgraph. Data All our experiments were performed on the TREC02?05 factoid questions. We excluded NIL questions since TREC doesn?t supply an answer for them. We used the FrameNet V1.3 lexical database.It contains 10,195 predicates grouped into 795 se mantic frames and 141,238 annotated sentences. Figure 4 shows the number of annotated sentences available for different predicates. As can be seen,there are 3,380 predicates with no annotated sentences and 1,175 predicates with less than 5 anno tated sentences. All FrameNet sentences, questions, and answer sentences were parsed using MiniPar (Lin, 1994), a robust dependency parser.As mentioned in Section 4 we extract depen dency relation paths by traversing the dependency tree from the frame element node to the predicate node. We used all dependency relations providedby MiniPar (42 in total). In order to increase cov erage, we combine all relation paths for predicates that evoke the same frame and are labeled with the same POS tag. For example, found and establish are both instances of the frame Intentionally createbut the database does not have any annotated sen tences for found.v. In default of not assigning anyrole labels for found.v, our model employs the rela tion paths for the semantically related establish.v. Preprocessing Here we summarize the steps ofour QA system preceding the assignment of semantic structure and answer extraction. For each ques tion, we recognize its expected answer type (e.g., in Q: Which record company is Fred Durst with? wewould expect the answer to be an ORGANIZATION ). Answer types are determined using classi fication rules similar to Li and Roth (2002). We alsoreformulate questions into declarative sentences fol lowing the strategy proposed in Brill et al (2002). The reformulated sentences are submitted as queries to an IR engine for retrieving sentences with relevant answers. Specifically, we use the Lemur Toolkit3, a state-of-the-art language model-driven search engine. We work only with the 50 top-rankedsentences as this setting performed best in previ ous experiments of our QA system. We also add toLemur?s output gold standard sentences, which contain and support an answer for each question. Specifically, documents relevant for each question are re trieved from the AQUAINT Corpus4 according to TREC supplied judgments. Next, sentences which match both the TREC provided answer pattern and at least one question key word are extracted and their suitability is manually judged by humans. The set of relevant sentences thus includes at least one sentence with an appropriate answer as well as sentences that do not contain any answer specific information. This setup is somewhat idealized, however it allows us toevaluate in more detail our answer extraction mod ule (since when an answer is not found, we know it is the fault of our system). Relevant sentences are annotated with their named entities using Lingpipe5, a MUC-based named entity recognizer. When we successfully classify a question with an expected answer type 3See http://www.lemurproject.org/ for details. 4This corpus consists of English newswire texts and is used as the main document collection in official TREC evaluations. 5The software is available from www.alias-i.com/ lingpipe/ 17 (e.g., ORGANIZATION in the example above), we assume that all NPs attested in the set of relevant sentences with the same answer type are candidate answers; in cases where no answer type is found (e.g., as in Q: What are prions made of? ), all NPs in the relevant answers set are considered candidate answers. Baseline We compared our answer extractionmethod to a QA system that exploits solely syntac tic information without making use of FrameNet or any other type of role semantic annotations. For each question, the baseline identifies key phrases deemed important for answer identification. These are verbs, noun phrases, and expected answer phrases (EAPs,see Section 3). All dependency relation paths con necting a key phrase and an EAP are compared tothose connecting the same key phrases and an answer candidate. The similarity of question and an swer paths is computed using a simplified version of the similarity measure6 proposed in Shen and Klakow (2006). Our second baseline employs Shalmaneser (Erkand Pado?, 2006), a publicly available shallow se mantic parser7, for the role labeling task instead of the graph-based model presented in Section 4. Thesoftware is trained on the FrameNet annotated sen tences using a standard feature set (see Carreras and Ma`rquez (2005) for details). We use Shalmaneser to parse questions and answer sentences. The parser makes hard decisions about the presence or absence of a semantic role. Unfortunately, this prevents usfrom using our method for semantic structure match ing (see Section 5) which assumes a soft labeling.We therefore came up with a simple matching strat egy suitable for the parser?s output. For questionand answer sentences matching in their frame as signment, phrases bearing the same semantic role asthe EAP are considered answer candidates. The latter are ranked according to word overlap (i.e., iden tical phrases are ranked higher than phrases with no6Shen and Klakow (2006) use a dynamic time warping al gorithm to calculate the degree to which dependency relationpaths are correlated. Correlations for individual relations are es timated from training data whereas we assume a binary value (1 for identical relations and 0 otherwise). The modification wasnecessary to render the baseline system comparable to our an swer extraction model which is unsupervised. 7The software is available from http://www.coli. uni-saarland.de/projects/salsa/shal/ . overlap at all). Our evaluation was motivated by the following ques tions: (1) How does the incompleteness of FrameNet impact QA performance on the TREC data sets? In particular, we wanted to examine whether there are questions for which in principle no answer can befound due to missing frame entries or missing an notated sentences. (2) Are all questions and theircorresponding answers amenable to a FrameNet style analysis? In other words, we wanted to assess whether questions and answers often evoke the sameor related frames (with similar roles). This is a prerequisite for semantic structure matching and ulti mately answer extraction. (3) Do the graph-basedmodels introduced in this paper bring any perfor mance gains over state-of-the-art shallow semanticparsers or more conventional syntax-based QA systems? Recall that our graph-based models were de signed especially for the QA answer extraction task. Our results are summarized in Tables 1?3. Table 1 records the number of questions to be answered forthe TREC02?05 datasets (Total). We also give infor mation regarding the number of questions which arein principle unanswerable with a FrameNet-style se mantic role analysis. Column NoFrame shows the number of questions which don?t have an appropriate frame or predicate in the database. For example, there is currently no predicate entry for sponsor or sink (e.g., Q: Who is the sponsor of the International Criminal Court? and Q: What date did the Lusitania sink? Column NoAnnot refers to questions for which no semantic role labeling is possible because annotated sentences for the relevant predicates are missing. For instance,there are no annotations for win (e.g., Q: What divi sion did Floyd Patterson win? ) or for hit (e.g., Q: What was the Beatles? first number one hit? Thisproblem is not specific to our method which admit tedly relies on FrameNet annotations for performingthe semantic role assignment (see Section 4). Shal low semantic parsers trained on FrameNet would also have trouble assigning roles to predicates for which no data is available. Finally, column NoMatch reports the number of questions which cannot be answered due to frame 18 Data Total NoFrame NoAnnot NoMatch Rest TREC02 444 87 (19.6) 29 (6.5) 176 (39.6) 152 (34.2) TREC03 380 55 (14.5) 30 (7.9) 183 (48.2) 112 (29.5) TREC04 203 47 (23.1) 14 (6.9) 67 (33.0) 75 (36.9) TREC05 352 70 (19.9) 23 (6.5) 145 (41.2) 114 (32.4) Table 1: Number of questions which cannot be answered using a FrameNet style semantic analysis; numbers in parentheses are percentages of Total (NoFrame: frames or predicates are missing; NoAnnot: annotated sentences are missing, NoMatch: questions and candidate answers evoke different frames. mismatches. Consider Q: What does AARP stand for? whose answer is found in S: The American Association of Retired Persons (AARP) qualify fordiscounts. The answer and the question evoke dif ferent frames; in fact here a semantic role analysis is not relevant for locating the right answer. As can be seen NoMatch cases are by far the most frequent. The number of questions remaining after excluding NoFrame, NoAnnot, and NoMatch are shown under the Rest heading in Table 1.These results indicate that FrameNet-based se mantic role analysis applies to approximately 35% of the TREC data. This means that an extraction module relying solely on FrameNet will have poor performance, since it will be unable to find answers for more than half of the questions beeing asked. We nevertheless examine whether our model brings any performance improvements on this limited dataset which is admittedly favorable towards a FrameNetstyle analysis. Table 2 shows the results of our an swer extraction module (SemMatch) together with two baseline systems. The first baseline uses only dependency relation path information (SynMatch),whereas the second baseline (SemParse) uses Shal maneser, a state-of-the-art shallow semantic parser for the role labeling task. We consider an answer correct if it is returned with rank 1. As can be seen,SemMatch is significantly better than both SynMatch and SemParse, whereas the latter is signifi cantly worse than SynMatch. Although promising, the results in Table 2 are not very informative, since they show performance gainson partial data. Instead of using our answer extrac tion model on its own, we next combined it with the syntax-based system mentioned above (SynMatch, see also Section 6 for details). If FrameNet is indeedhelpful for QA, we would expect an ensemble sys Model TREC02 TREC03 TREC04 TREC05 SemParse 13.16 8.92 17.33 13.16 SynMatch 35.53? 33.04? 40.00? 36.84? SemMatch 53.29?? 49.11?? 54.67?? 59.65?? Table 2: System Performance on subset of TRECdatasets (see Rest column in Table 1); ?: signifi cantly better than SemParse; ?: significantly better than SynMatch (p < 0.01, using a ?2 test). Model TREC02 TREC03 TREC04 TREC05 SynMatch 32.88? 30.70? 35.95? 34.38? +SemParse 25.23 23.68 28.57 26.70 +SemMatch 38.96?? 35.53?? 42.36?? 41.76?? Table 3: System Performance on TREC datasets (see Total column in Table 1); ?: significantly better than +SemParse; ?: significantly better than SynMatch (p < 0.01, using a ?2 test).tem to yield better performance over a purely syn tactic answer extraction module. The two systems were combined as follows. Given a question, we first pass it to our FrameNet model; if an answer is found,our job is done; if no answer is returned, the ques tion is passed on to SynMatch. Our results are givenin Table 3. +SemMatch and +SemParse are ensem ble systems using SynMatch together with the QAspecific role labeling method proposed in this pa per and Shalmaneser, respectively. We also compare these systems against SynMatch on its own.We can now attempt to answer our third ques tion concerning our model?s performance on theTREC data. Our experiments show that a FrameNet enhanced answer extraction module significantlyoutperforms a similar module that uses only syntactic information (compare SynMatch and +Sem Match in Table 3). Another interesting finding is that 19 the shallow semantic parser performs considerably worse in comparison to our graph-based models and the syntax-based system. Inspection of the parser?s output highlights two explanations for this. First, theshallow semantic parser has difficulty assigning ac curate semantic roles to questions (even when theyare reformulated as declarative sentences). And secondly, it tends to favor precision over recall, thus re ducing the number of questions for which answers can be found. A similar finding is reported in Sun et al. (2005) for a PropBank trained parser.","Our evaluation was motivated by the following ques tions: (1) How does the incompleteness of FrameNet impact QA performance on the TREC data sets? In particular, we wanted to examine whether there are questions for which in principle no answer can befound due to missing frame entries or missing an notated sentences. (2) Are all questions and theircorresponding answers amenable to a FrameNet style analysis? In other words, we wanted to assess whether questions and answers often evoke the sameor related frames (with similar roles). This is a prerequisite for semantic structure matching and ulti mately answer extraction. (3) Do the graph-basedmodels introduced in this paper bring any perfor mance gains over state-of-the-art shallow semanticparsers or more conventional syntax-based QA systems? Recall that our graph-based models were de signed especially for the QA answer extraction task. Our results are summarized in Tables 1?3. Table 1 records the number of questions to be answered forthe TREC02?05 datasets (Total). We also give infor mation regarding the number of questions which arein principle unanswerable with a FrameNet-style se mantic role analysis. Column NoFrame shows the number of questions which don?t have an appropriate frame or predicate in the database. For example, there is currently no predicate entry for sponsor or sink (e.g., Q: Who is the sponsor of the International Criminal Court? and Q: What date did the Lusitania sink? Column NoAnnot refers to questions for which no semantic role labeling is possible because annotated sentences for the relevant predicates are missing. For instance,there are no annotations for win (e.g., Q: What divi sion did Floyd Patterson win? ) or for hit (e.g., Q: What was the Beatles? first number one hit? Thisproblem is not specific to our method which admit tedly relies on FrameNet annotations for performingthe semantic role assignment (see Section 4). Shal low semantic parsers trained on FrameNet would also have trouble assigning roles to predicates for which no data is available. Finally, column NoMatch reports the number of questions which cannot be answered due to frame 18 Data Total NoFrame NoAnnot NoMatch Rest TREC02 444 87 (19.6) 29 (6.5) 176 (39.6) 152 (34.2) TREC03 380 55 (14.5) 30 (7.9) 183 (48.2) 112 (29.5) TREC04 203 47 (23.1) 14 (6.9) 67 (33.0) 75 (36.9) TREC05 352 70 (19.9) 23 (6.5) 145 (41.2) 114 (32.4) Table 1: Number of questions which cannot be answered using a FrameNet style semantic analysis; numbers in parentheses are percentages of Total (NoFrame: frames or predicates are missing; NoAnnot: annotated sentences are missing, NoMatch: questions and candidate answers evoke different frames. mismatches. Consider Q: What does AARP stand for? whose answer is found in S: The American Association of Retired Persons (AARP) qualify fordiscounts. The answer and the question evoke dif ferent frames; in fact here a semantic role analysis is not relevant for locating the right answer. As can be seen NoMatch cases are by far the most frequent. The number of questions remaining after excluding NoFrame, NoAnnot, and NoMatch are shown under the Rest heading in Table 1.These results indicate that FrameNet-based se mantic role analysis applies to approximately 35% of the TREC data. This means that an extraction module relying solely on FrameNet will have poor performance, since it will be unable to find answers for more than half of the questions beeing asked. We nevertheless examine whether our model brings any performance improvements on this limited dataset which is admittedly favorable towards a FrameNetstyle analysis. Table 2 shows the results of our an swer extraction module (SemMatch) together with two baseline systems. The first baseline uses only dependency relation path information (SynMatch),whereas the second baseline (SemParse) uses Shal maneser, a state-of-the-art shallow semantic parser for the role labeling task. We consider an answer correct if it is returned with rank 1. As can be seen,SemMatch is significantly better than both SynMatch and SemParse, whereas the latter is signifi cantly worse than SynMatch. Although promising, the results in Table 2 are not very informative, since they show performance gainson partial data. Instead of using our answer extrac tion model on its own, we next combined it with the syntax-based system mentioned above (SynMatch, see also Section 6 for details). If FrameNet is indeedhelpful for QA, we would expect an ensemble sys Model TREC02 TREC03 TREC04 TREC05 SemParse 13.16 8.92 17.33 13.16 SynMatch 35.53? 33.04? 40.00? 36.84? SemMatch 53.29?? 49.11?? 54.67?? 59.65?? Table 2: System Performance on subset of TRECdatasets (see Rest column in Table 1); ?: signifi cantly better than SemParse; ?: significantly better than SynMatch (p < 0.01, using a ?2 test). Model TREC02 TREC03 TREC04 TREC05 SynMatch 32.88? 30.70? 35.95? 34.38? +SemParse 25.23 23.68 28.57 26.70 +SemMatch 38.96?? 35.53?? 42.36?? 41.76?? Table 3: System Performance on TREC datasets (see Total column in Table 1); ?: significantly better than +SemParse; ?: significantly better than SynMatch (p < 0.01, using a ?2 test).tem to yield better performance over a purely syn tactic answer extraction module. The two systems were combined as follows. Given a question, we first pass it to our FrameNet model; if an answer is found,our job is done; if no answer is returned, the ques tion is passed on to SynMatch. Our results are givenin Table 3. +SemMatch and +SemParse are ensem ble systems using SynMatch together with the QAspecific role labeling method proposed in this pa per and Shalmaneser, respectively. We also compare these systems against SynMatch on its own.We can now attempt to answer our third ques tion concerning our model?s performance on theTREC data. Our experiments show that a FrameNet enhanced answer extraction module significantlyoutperforms a similar module that uses only syntactic information (compare SynMatch and +Sem Match in Table 3). Another interesting finding is that 19 the shallow semantic parser performs considerably worse in comparison to our graph-based models and the syntax-based system. Inspection of the parser?s output highlights two explanations for this. First, theshallow semantic parser has difficulty assigning ac curate semantic roles to questions (even when theyare reformulated as declarative sentences). And secondly, it tends to favor precision over recall, thus re ducing the number of questions for which answers can be found. A similar finding is reported in Sun et al. (2005) for a PropBank trained parser."
94,"In this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem. The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings. We describe the details of the mod- el and test the model on several bilingual corpora.","In this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem. The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings. We describe the details of the mod- el and test the model on several bilingual corpora. In this paper, we address the problem of word alignments for a bilingual corpus. In the recent years, there have been a number of papers con- sidering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993). In our approach, we use a first-order Hidden Markov model (HMM) (aelinek, 1976), which is similar, but not identical to those used in speech recognition. The key component of this approach is to make the alignment probabilities dependent not on the absolute position of the word align- ment, but on its relative position; i.e. we consider the differences in the index of the word positions rather than the index itself. The organization of the paper is as follows. After reviewing the statistical approach to ma- chine translation, we first describe the convention- al model (mixture model). We then present our first-order HMM approach in lull detail. Finally we present some experimental results and compare our model with the conventional model. The goal is the translation of a text given in some language F into a target language E. For conve- nience, we choose for the following exposition as language pair French and English, i.e. we are giv- en a French string f~ = fx ...fj...fJ, which is to be translated into an English string e / = el...ei...cl. Among all possible English strings, we will choose the one with the highest probability which is given by Bayes' decision rule: a{ = argmax{P,.(c{lAa)} q = argmax {Pr(ejt) . l ' r ( f  le\[)} el ~ Pr(e{) is the language model of the target lan- guage, whereas Pr(fJle{) is the string translation model. The argmax operation denotes the search problem. In this paper, we address the problem of introducing structures into the probabilistic de- pendencies in order to model the string translation probability Pr(f~ le{).","The goal is the translation of a text given in some language F into a target language E. For conve- nience, we choose for the following exposition as language pair French and English, i.e. we are giv- en a French string f~ = fx ...fj...fJ, which is to be translated into an English string e / = el...ei...cl. Among all possible English strings, we will choose the one with the highest probability which is given by Bayes' decision rule: a{ = argmax{P,.(c{lAa)} q = argmax {Pr(ejt) . l ' r ( f  le\[)} el ~ Pr(e{) is the language model of the target lan- guage, whereas Pr(fJle{) is the string translation model. The argmax operation denotes the search problem. In this paper, we address the problem of introducing structures into the probabilistic de- pendencies in order to model the string translation probability Pr(f~ le{)."
95,"We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994). In contrast to that work, our al- gorithm does not require in-depth, full, syn.. tactic parsing of text. Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream. Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components.","We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994). In contrast to that work, our al- gorithm does not require in-depth, full, syn.. tactic parsing of text. Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream. Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components. (l,appin and Leass, 1994) describe an algorithm for pronominal anaphora resolution with high rate of cor- rect analyses. While one of the strong points of this algorithm is that it operates primarily on syntactic in- formation ahme, this also turns out to be a limiting factor for its wide use: current state-of-the-art of prac- tically applicable parsing technology still falls short of robust and reliable delivery of syntactic analysis of real texts to the level of detail and precision that the filters a nd constraints described by I ,appin and l ,eass assume. We are particularly interested in a class of text pro- cessing applications, capable of delivery of content analysis to a depth inw~lving non-trivial amount of discourse processing, including anaphora resolution. The operational context prohibits us from making any assumptions concerning domain, style, and genre of input; as a result, we have developed a text processing framework which builds its capabilities entirely on the basis of a considerably shallower linguistic analysis of the input stream, thus trading off depth of base level analysis for breadth of cown:age. In this paper, we present work on modifying the lmp- pin/Leass algorithm in a way which enables it to work off a flat morpho-syntactic analysis of the sentences of a text, while retaining a degree of quality and accuracy in pronorainal anaphora resolution comparable to that reported in (Lappin and l,eass, 1994). The modifica- tions discussed below make the algorithm available to a wide range of text processing frameworks, which, due to the lack of full syntactic parsing capability, nor- really would have been unable to use this high preci- sion anap hora resolution tool. The work is additionally important, we feel, as it shows that informatkm about the content and logical structure of a text, in princi-. pie a core requirement for higher level semantic and discourse processes, can be effectively approximated by the right mix of constituent analysis and inferences about functional relations. The base level linguistic analysis for actaphora resolu- tion is the output of a part of speech tagger, augmented with syntactic function annotatkms for each input to. ken; this kind of analysis is generated by the mor- pbosyntactic tagging system described in (Voutilainen et al, 1992), (Karlsson et al, 1995) (hencehvth 1,1NC:- ~;olq'). In addition to extremely high levels of accuracy in recall and precision of tag assignment ((VoutiJainen et al, 1992) report 99.77?/,, overall recall and 95.54% overall preciskm, over a variety of text genres, and in comparison with other state-of-the-art tagging sys- tems), the primary motivation for adopting this system is the requirement todevelop a robust ext processor- with anaphora resolution being just one of its discourse analysis functkms capable of reliably handling arbi- trary kinds of input. The tagger provides a very simple analysis of the structure of the text: for each lexical item in each sen- tence, it provides a set of values which indicate the morphological, lexical, grammatical nd syntactic fea- tures of the item in tile context in which it appears. In addition, the modified algorithm we present requh:es annota tion of the input text stream by a simple position-- identification function which associates an integer with each token in a text sequentially (we will refer to a to- ken's integer value as its oJ~et). As an example, given the text ""For 1995 the company set up its headquar- ters in Hall \] l, the newest and most presti-. gious of CeBIT's 23 hal Is."" tile anaphora resolutkm algorithm would be presented with the h}llowing analysis tream. Note, in particu-. lar, the grammatical function information (e.g., @SUl~J, O)q.FMAINV) and the integer values (e.g., ""offt 39"") asso- cia ted with each token. ""For /o f f139"" ""for"" PREP @ADVL ""1995/o f f140 .... 1995"" NUM CARD @<P "" the/o f f l41"" ""the"" DET CENTRAL ART SG/PL @DN> ""company/o f f142"" ""company"" N NOM SG/PL @SUBJ ""set/off143"" ""set"" V PAST VF IN @+FMAINV ""up/of f144"" ""up"" ADV ADVL @ADVL "" i t s /o f f145 .... it"" PRON GEN SG3 @GN> ""headquar ters /o f f146 .... headquar ters"" N NOM SG/PL @OBJ "" in /o f f147 .... in"" PREP @<NOM @ADVL ""Ha l l /o f f148 .... hal l"" N NOM SG @NN> "" l l /o f f149"" ""Ii"" NUM CARD @<P ""$ , /o f f l50 .... ,"" PUNCT "" the/o f f l51"" ""the"" DET CENTRAL ART SG/PL @DN> ""newest /o f f152 .... new"" A SUP @PCOMPL-O ""and/of f153 .... and"" CC @CC ""most /o f f154"" ""much"" ADV SUP @AD-A> ""pres t ig ious /o f f155 .... p res t ig ious"" A ABS @<P ""of /o f f156 .... of"" PREP @<NOM-OF ""CeBIT ' s /o f f157"" ""cebit"" N GEN SG @GN> ""23/0f f158 .... 23"" NUM CARD @QN> ""ha l l s /o f f159 .... hal l"" N NOM PL @<P ""$ . /o f f160 .... PUNCT 2.1 Data collection. Although LINGSOFT does not provide specific infor- mation about constituent structure, partial constituen- cy-specifically, identification of sequences of tokens as phrasal units--can be inferred from the analysis by running the tagged text through a set of filters, which are stated as regular expressions over metatokens such as the ones illustrated above. For the purposes of anaphora resolution, the pri- mary data set consists of a complete listing of all noun phrases, reduced to modifier-head sequences. This data set is obtained by means of a phrasal grammar whose patterns characterize the composition of a noun phrase (NP) in terms of possible token sequences. The output of NP identification is a set of token/feature matrix/offset sequences, where offset value is deter- mined by the offset of the first token in the sequence. The offset indicates the position of the NP in the text, and so provides crucial information about precedence relations. A secondary data set consists of observations about the syntactic ontexts in which the NPs identified by the phrasal grammar appear. These observations are derived using a set of patterns designed to detect nom- inal sequences in two subordinate syntactic environ- ments: containment in an adverbial adjunct and con- tainment in an NP (i.e., containment in a prepositional or clausal complement of a noun, or containment in a relative clause). This is accomplished by running a set of patterns which identify NPs that occur locally to ad- verbs, relative pronouns, and noun-preposition r noun- complementizer sequences over the tagged text in con- junction with the basic NP patterns described above. Because the syntactic,patterns are stated as regular ex- pressions, misanalyses are inevitable. In practice, how- ever, the extent o which incorrect analyses of syntactic context affect the overall accuracy of the algorithm is not large; we will return to a discussion of this point in section 4. A third set of patterns identifies and tags occurrences of ""expletive"" it. These patterns target occurrences of the pronoun it in certain contexts, e.g., as the subject of members of a specific set of verbs (seem, appear, etc.), or as the subject of adjectives with clausal complements. Once the extraction procedures are complete and the results unified, a set of discourse referents--abstract ob- jects which represent the participants inthe discourse-- is generated from the set of NP observations. A particu- larly convenient implementation f discourse referents is to represent them as objects in the Common Lisp Object System, with slots which encode the following information parameters (where ADJUNCT and EMBED indicate whether a discourse referent was observed in either of the two syntactic ontexts discussed above): TEXT: text form TYPE: referential type (e.g., REF, PRO, RFLX) AGR: person, number, gender GFUN: grammatical function ADJUNCT: T o r NIL EMBED: T o r NIL POS: text position Note that each discourse referent contains information about itself and the context in which it appears, but the only information about its relation to other dis- course referents is in the form of precedence r lations (as determined by text position). The absence of explicit information about configurational relations marks the crucial difference between our algorithm and the Lap- pin/Leass algorithm. (Lappin and Leass, 1994) use configurational information in two ways: as a factor in the determination of the salience of a discourse refer- ent (discussed below), and as input to a set of disjoint reference filters. Our implementation seeks to perform exactly the same tasks by inferring hierarchical rela- tions from a less rich base. The modifications and assumptions required to accomplish this goal will be highlighted in the following discussion. 2.2 Anaphora resolution. Once the representation f the text has been recast as a set of discourse referents (ordered by offset value), it is sent to the anaphora resolution algorithm proper. The basic logic of the algorithm parallels that of the Lap- pin/Leass algorithm. The interpretation procedure in- volves moving through the text sentence by sentence and interpreting the discourse referents in each sen- tence from left to right. There are two possible in- terpretations of a discourse referent: either it is taken to introduce a new participant in the discourse, or it is taken to refer to a previously interpreted iscourse referent. Coreference is determined by first eliminating from consideration those discourse referents to which an anaphoric expression cannot possibly refer, then se- lecting the optimal antecedent from the candidates that remain, where optimality is determined by a salience measure. In order to present the details of anaphora resolution, we define below our notions--and implementations-- of coreference and salience. 2.2.1 Coreference As in the Lappin and Leass algorithm, the anaphor- antecedent relation is established between two dis- course referents (cf. (Helm, 1982), (Kamp, 1981)), @hile the more general notion of coreference is represented in terms of equivalence classes of anaphorically re- lated discourse referents, which we will refer to as ""COREF classes"". Thus, the problem of interpreting an anaphoric expression boils down to the problem of es- tablishing an anaphoric link between the anaphor and some previously interpreted iscourse referent (pos- sibly another anaphor); a consequence of establishing 114 this link is that the anaphor becomes a member of the COREF class already associated with its antecedent. In our implementation, COREF classes are repre- sented as objects in the Common Lisp Object System which contain information about the COREF class as a whole, including canonical form (typically deter- mined by the discourse referent which introduces the class), membership, and, most importantly, salience (discussed below). 1 The connection between a dis- course referent and its COREF class is mediated through the COREF object as follows: every discourse referent includes an information parameter which is a pointer to a COREF object; discourse referents which have been determined to be coreferential share the same COREF value (and so literally point to the same object). Imple- menting coreference in this way provides a means of getting from any discourse referent in a COREF class to information about the class as a whole. 2.2.2 Salience The information parameter of a COREF object most cru- cial to anaphora resolution is its salience, which is de- termined by the status of the members of the COREF class it re.presents with respect to 10 contextual, gram- matical, and syntactic onstraints. Following (Lappin and Leass, 1994), we will refer to these constraints as ""salience factors"". Individual salience factors are asso- ciated with numerical values; the overall salience, or ""salience weight"" of a COREF is the sum of the values of the salience factors that are satisfied by some member of the COREF class (note that values may be satisfied at most once by each member of the class). The salience factors used by our algorithm are defined below with their values. Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as deter- mined by a text-segmentation algorithm which follows (Hearst, 1994). SENT-S: 100 iff in the current sentence CNTX-S: 50 iff in the current context SUBJ-S: 80 iff GFUN = subject EXST-S: 70 iff in an existential construction POSS-S: 65 iff GFUN = possessive ACC-S: 50 iff GFUN = direct object DAT-S: 40 iff GFUN = indirect object OBLQ-S: 30 iff the complement of a preposition HEAD-S: 80 iff EMBED = NIL ARG-S: 50 iff ADJUNCT = NIL Note that the values of salience factors are arbitrary; what is crucial, as pointed out by (Lappin and Leass, 1994), is the relational structure imposed on the factors by these values. The relative ranking of the factors is justified both linguistically, as a reflection of the role of the functional hierarchy in determining anaphoric relations (cf. (Keenan and Comrie, 1977)), as well as by experimental results--both Lappin and Leass' and our own. For all factors except CNTX-S and POSS-S, we adopt the values derived from a series of experiments described in (Lappin and Leass, 1994) which used dif- ferent settings to determine the relative importance of 1The implementation of aCOREF object needs to be aware of po- tenlial circularities, thus a COREF does not actually contain its member discourse r ferents, but rather alisting of their offsets, each factor as a function of the overall success of the algorithm. Our values for CNTX-S and POSS-S were de- termined using similar tests. An important feature of our implementation of salience, following that of Lappin and Leass, is that it is variable: the salience of a COREF class decreases and increases according to the frequency of reference to the class. When an anaphoric link is established between a pronoun and a previously introduced iscourse refer- ent, the pronoun is added to the COREF class associated with the discourse referent, its COREF value is set to the COREF value of the antecedent (i.e., to the COREF ob- ject which represents he class), and the salience of the COREF object is recalculated according to how the new member satisfies the set of salience factors. This final step raises the overall salience of the COREF, since the new member will minimally satisfy SENT-S and CNTX-S. Salience is not stable, however: in order to realisti- cally represent the local prominence of discourse ref- erents in a text, a decay function is built into the algo- rithm, so that salience weight decreases over time. If new members are not added, the salience weight of a COREF eventually reduces to zero. The consequence of this variability in salience is that a very general heuris- tic for anaphora resolution is established: resolve a pronoun to the most salient candidate antecedent. 2.2.3 Interpretation As noted above, in terms of overall strategy, the resolu- tion procedure follows that of Lappin and Leass. The first step in interpreting the discourse referents in a new sentence isto decrease the salience weights of the COREF classes that have already been established by a factor of two. Next, the algorithm locates all non-anaphoric dis- course referents in the sentence under consideration, generates a new COREF class for each one, and calcu- lates its salience weight according to how the discourse referent satisfies the set of salience factors. The second step involves the interpretation f lexical anaphors (reflexives and reciprocals). A list of candi- date antecedent-anaphor pairs is generated for every lexical anaphor, based on the hypothesis that a lexical anaphor must refer to a coargument. In the absence of configurational information, coarguments are iden- tified using grammatical function information (as de- termined by LINGSOFT) and precedence relations. A reflexive can have one of three possible grammatical function values: direct object, indirect object, or oblique. In the first case, the closest preceding discourse referent with grammatical function value subject is identified as a possible antecedent. In the latter cases, both the clos- est preceding subject and the closest preceding direct object hat is not separated from the anaphor by a sub- ject are identified as possible antecedents. If more than one possible antecedent is located for a lexical anaphor, the one with the highest salience weight is determined to be the actual antecedent. Once an antecedent has been located, the anaphor is added to the COREF class associated with the antecedent, and the salience of the COREF class is recalculatec~ accordingly. The final step is the interpretation f pronouns. The basic resolution heuristic, as noted above, is quite sim- ple: generate a set of candidate antecedents, then es- tablish coreference with the candidate which has the greatest salience weight (in the event of a tie, the clos- est candidateis chosen). In order to generate the candi- date set, however, those discourse referents with which 115 a pronoun cannot refer must be eliminated from consid- eration. This is accomplished by running the overall candidate pool (the set of interpreted iscourse ref- erents whose salience values exceed an arbitrarily set threshold) through two sets of filters: a set of morpho- logical agreement filters, which eliminate from consid- eration any discourse referent which disagrees in per- son, numbeb or gender with the pronoun, and a set of disjoint reference filters. The determination f disjoint reference represents a significant point of divergence between our algorithm and the Lappin/Leass algorithm, because, as is well known, configurational relations play a prominent role in determining which constituents in a sentence a pro- noun may refer to. Three conditions are of particular relevance to the anaphora resolution algorithm: Condition \]: A pronoun cannot corefer with a coargument. Condition 2: A pronoun cannot corefer with a nonpronominal constituent which it both commands and precedes. Condition 3: A pronoun cannot corefer with a constituent which contains it. In the absence of configurafional information, our al- gorithm relies on inferences from grammatical func- tion and precedence todetermine disjoint reference. In practice, even without accurate information about con- stituent structure, the syntactic filters described below are extremely accurate (see the discussion of this point in section 4). Condition i is implemented bylocating all discourse referents with GFUN value direct object, indirect object, or oblique which follow a pronoun with GFUN value subject or direct object, as long as no subject intervenes (the hypothesis being that a subject indicates the beginning of the next clause). Discourse referents which satisfy these conditions are identified as disjoint. Condition 2 is implemented by locating for ev- ery non-adjunct and non-embedded pronoun the set of non-pronominal discourse referents in its sentence which follow it, and eliminating these as potential an- tecedents. In effect, the command relation is inferred from precedence and the information provided by the syntactic patterns: an argument which is neither con- tained in an adjunct nor embedded in another nominal commands those expressions which it precedes. Condition 3 makes use of the observation that a dis- course referent contains every object o its right with a non-nil EMBED value. The algorithm identifies as dis- joint a discourse referent and every pronoun which fol- lows it and has a non-nil EMBED value, until a discourse referent with EMBED value NIL is located (marking the end of the containment domain). Condiditon 3 also rules out coreference between a genitive pronoun and the NP it modifies. After the morphological nd syntactic filters have been applied, the set of discourse referents that remain constitute the set of candidate antecedents for the pro- noun. The candidate set is subjected to a final evalu- ation procedure which performs two functions: it de- creases the salience of candidates which the pronoun precedes (cataphora is penalized), and it increases the sa li ence of candida tes which satisfy either a locality or a parallelism condition (described below), both of which apply to intrasentential c ndidates. The h)cality heuristic isdesigned to negate the effects of subordinationwhen both candidate and anaphor ap- pear in the same subordinate context, the assumption being that the prominence of a candidate should be de- termined with respect o the position of the anaphor. This is a point of difference between our algorithm and the one described in (Lappin and Leass, 1994). The salience of a candidate which is determined tobe in the same subordinate context as a pronoun (determined as a function of precedence r lations and EMBED and ADJUNCT values) is temporarily increased to the level it would have were the candidate not in the subordi- nate context; the level is returned to normal after the anaphor is resolved. The parallelism heuristic rewards candidates which are such that the pair consisting of the GFUN values of candidate and anaphor are identical to GFUN values of a previously identified anaphor-antecedent pair. This parallelism heuristic differs from a similar one used by the Lappin/Leass algorithm, which rewards candi- dates whose grammatical function is identical to that of an anaphor. Once the generation and evaluation of the candidate set is complete, the candidates are ranked according to salience weight, and the candidate with the high- est salience weight is determined tobe the antecedent of the pronoun under consideration. In the event of a tie, the candidate which most immediately precedes the anaphor is selected as the antededent (where prece- dence is determined by comparing offset values). The COREF value of the pronoun is set to that of the an- tecedent, adding it to the the antecedent's COREF class, and the salience of the class is recalculated accordingly. The larger context from which the sample analysis in the beginning of Section 2 was taken is as follows: ""...while Apple and its PowerPC partners claimed some prime real estate on the show floor, Apple's most interesting offerings de- buted behind the scenes. Gone was the nar- row corner booth that Apple shoehorned its products into last year. For 1995 the com- pany set up its headquarters in Hall 11, the newest and most prestigious of CeNT's 23 halls."" The anaphora resolution algorithm generates the fol- lowing analysis for the first italicized pronoun. For each candidate, ~ the annotation i square brackets in- dicates its offset value, and the number to the right indicates its salience weight at the point of interpreta- tkm of the pronoun. ANA: its \[@off/\]33\] CND: Apple \[@of 1/131\] 432 Apple \[/aol f/10\] \] 352 its \[@off/\].03\] 352 App\]e's \[@offf/\] I 5\] 1352 prilne real estat(! \[@off/\]08\] 165 show f loor \ [ (aof f /1 \ ]2 l \]55 year \[@o~f/137 I 310/3 The candidate set illustrates several important points. First, the equality in salience weights of the candi- dates at offsets 101, 103, and 115 is a consequence of 2Note that our syntactic filters are quite capable of discarding a number of configurationally inappropriate antecedents, which appear to satisfy the precedence r lation. 116 the fact that these discourse referents are members of the same COP, Et ~' class. Their unification into a single class indicates both successful anaphora resolution (of the pronoun at offset 103), as well as the operation of higherqevel discourse processing designed to identify all references to a particular COREF class, not just the anaphoric ones (cf. (Kennedy and Boguraev, :1996)). The higher salience of the optimal candidate--which ix also a member of this COREF class--shows the effect of the locality heuristic described in section 2.2.3. Both the pronoun and the candidate appear in the same sub- ordinate context (within a relative clause); as a result the salience of the candidate (but not of the class to which it bekmgs) is temporarily boosted to negate the effect of subordinatkm. An abbreviated candidate set for the second itali- cized pronoun is given below: ANA: i t s {61of f /145\ ] CND: company \[(,)ot I / 142 \] :H,0 App l e ((,!of 17/ 13 / \] 192 it:~:; {(aof I / I 3 ~ \] 192 This set is interesting because it illustrates the promi- nent role of SENT-S in controlling salience: company ix correctly identified as the antecedent of the pronotm, despite the frequency of mention of members of the COREF class containing Apple and its, because it occurs in the same sentence as the anaphor. Of course, this ex- ample also indicates the need fl~r additional heuristics designed to connect company with Apple, since these discourse referents clearly make reference to the same object. We are currentlyworking towards this goal; see (Kennedy and Boguraev, \]996) for discussion. 'l'he following text segment illust rates the resolution of in tersen ten tia l a napho ra. ""Sun's prototype lntemet access device uses a 1-10-Mhz MicroSPARCprocesso~; and is diskless. Its dimensions are 5.5 inches x 9 inches x 2inches."" ANA: \]its \[\[aol f /347\ ] CNI): IAlte~:ileL access devic() \[(,!o~\[/33\[i\] 180 M i c KOf;PARCI)rOC e!s sot \[(4oEI /34\] \] 16!i ~;un 's \[<4o1 f /3 : t3 I \ [40 The first sentence in this fl'agment introduces three dis- course referents bearing different grammatical func- tions, none of which appear in subordinate contexts. Since the sentence in which the anaphor occurs does not contain any candidates (the discourse referent in- troduced by dimensions ix eliminated from considera- tion by both the morphok)gical nct disjoint reference filters), only those from the previous entence are con- sidered (each is compatible with the morphological requirements of the anaphor). These are ranked ac- cording to salience weight, where the crucial factor is grammatical function value. The result of the ranking is that Internet access device--the candidate which satis- fies the highest-weighted salience facto1, SUBl-S--is the optimal candidate, and so correctly identified as the an tecedent Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate of 75'70 accu- racy. The data set on which the evaluatkm was based consisted of 27 texts, taken from a random selection of genres, including press releases, product annotmce- meats, news stories, magazine articles, and other doc- uments existing as World Wide Web pages. Within these texts, we counted 3(16 third person anaphoric pro- nouns; of these, 231l were correctly resolved to the dis- course referent identified as the antecedent by the first author. 3 This rate of accuracy is clearly comparable to that of the Lappin/Leass algorithm, which (Lappin and Leass, \] 994) report as 85?/,,. Several observations about he results and the com- parison with (lmppin and I,eass, 1994) are in order. First, and most obviously, some deterioratkm in qual- ity is to be expected, given the relatively impoverished linguistic base we start with. Second, it is important to note that this is not just a matter of simple comparison. The results in (l.appin and Leass, 1994) describe the output of the procedttre applied to a singh,' text genre: computer manuals. Ar- guably, this is an example of a particularly well be- haved text; in any case, it is not clear how the figure would be normalized over a wide range of text types, some of them not completely 'clean', as is the case with our data. Third, close analysis of the most common types of error our algorithm currently makes reveals two spe- cific configurations in the input which confuse the pro- cedure and contribute to the error rate: gender mis- match (35% of errors) and certain long range contextttal (stylistic) phenomena, best exemplified by text contain- ing quoted passages in-line (14% of errors). Implementing a gender (dis-)agreement fil er is not technically complex; as noted above, the current algo- rithrn contains one. The persistence of gender mis- matches in the output simply reflects the lack of a con- sistent gender slot in the I,\[NGSOFT tagger output. Aug- menting the algorithm with a lexical database which includes more detailed gender information will result in improved accuracy. Ensuring proper interpretatkm of anaphors both within and outside of quoted text requires, in effect, a method of evaluating quoted speech separately from its surrotmdingcnntext. Al hough acomplex problem, we feel that this is possible, given that our input data stream embodies a richer notkm of position and con- text, as a resu\[t of an independent text segmentation procedure adapted from (\[ learst, 1994) (and discussed above in section 2.2.2). What is worth noting is the small number of errors which can be directly attributed to the absence of con- figurational inh~rmation. Of the 75 misinterpreted pro- nouns, only 2 inw~lved a failure to establish configu- ratkmally determined disjoint reference (both of these inw~lved Condition 3), and only an additional several errors could be tmambiguously traced to a failure to correctly identify the syntactic ontext in which a dis~ course referent appeared (as determined by a misfireof the salience factors ensitive to syntactic context, I lEAD- S and ARC:S). Overall, these considerations lead to two conchl-. sions. First, with the incorporation of more explicit morphological nd contextual information, it should 3The set of 306 ""anaphoric"" pronouns excluded 30 occurrences of ""expletive"" it not identified by the expletive patterns (prhnari ly occurrences in object position), as well as 6 occurrences of it which referred to a VP or propositional constituent. We are currently mfinin g the existing expletive patterns for improved accuracy. 117 be possible to increase the overall quality of our out- put, bringing it much closer in line with Lappin and Leass' results. Again, straight comparison would not be trivial, as e.g. quoted text passages are not a natural part of computer manuals, and are, on the other hand, an extremely common occurrence in the types of text we are dealing with. Second, and most importantly, the absence of ex- plicit configurational information does not result in a substantial degradation i the accuracy of an anaphora resolution algorithm that is otherwise similar to that described in (Lappin and Leass, 1994).","Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate of 75'70 accu- racy. The data set on which the evaluatkm was based consisted of 27 texts, taken from a random selection of genres, including press releases, product annotmce- meats, news stories, magazine articles, and other doc- uments existing as World Wide Web pages. Within these texts, we counted 3(16 third person anaphoric pro- nouns; of these, 231l were correctly resolved to the dis- course referent identified as the antecedent by the first author. 3 This rate of accuracy is clearly comparable to that of the Lappin/Leass algorithm, which (Lappin and Leass, \] 994) report as 85?/,,. Several observations about he results and the com- parison with (lmppin and I,eass, 1994) are in order. First, and most obviously, some deterioratkm in qual- ity is to be expected, given the relatively impoverished linguistic base we start with. Second, it is important to note that this is not just a matter of simple comparison. The results in (l.appin and Leass, 1994) describe the output of the procedttre applied to a singh,' text genre: computer manuals. Ar- guably, this is an example of a particularly well be- haved text; in any case, it is not clear how the figure would be normalized over a wide range of text types, some of them not completely 'clean', as is the case with our data. Third, close analysis of the most common types of error our algorithm currently makes reveals two spe- cific configurations in the input which confuse the pro- cedure and contribute to the error rate: gender mis- match (35% of errors) and certain long range contextttal (stylistic) phenomena, best exemplified by text contain- ing quoted passages in-line (14% of errors). Implementing a gender (dis-)agreement fil er is not technically complex; as noted above, the current algo- rithrn contains one. The persistence of gender mis- matches in the output simply reflects the lack of a con- sistent gender slot in the I,\[NGSOFT tagger output. Aug- menting the algorithm with a lexical database which includes more detailed gender information will result in improved accuracy. Ensuring proper interpretatkm of anaphors both within and outside of quoted text requires, in effect, a method of evaluating quoted speech separately from its surrotmdingcnntext. Al hough acomplex problem, we feel that this is possible, given that our input data stream embodies a richer notkm of position and con- text, as a resu\[t of an independent text segmentation procedure adapted from (\[ learst, 1994) (and discussed above in section 2.2.2). What is worth noting is the small number of errors which can be directly attributed to the absence of con- figurational inh~rmation. Of the 75 misinterpreted pro- nouns, only 2 inw~lved a failure to establish configu- ratkmally determined disjoint reference (both of these inw~lved Condition 3), and only an additional several errors could be tmambiguously traced to a failure to correctly identify the syntactic ontext in which a dis~ course referent appeared (as determined by a misfireof the salience factors ensitive to syntactic context, I lEAD- S and ARC:S). Overall, these considerations lead to two conchl-. sions. First, with the incorporation of more explicit morphological nd contextual information, it should 3The set of 306 ""anaphoric"" pronouns excluded 30 occurrences of ""expletive"" it not identified by the expletive patterns (prhnari ly occurrences in object position), as well as 6 occurrences of it which referred to a VP or propositional constituent. We are currently mfinin g the existing expletive patterns for improved accuracy. 117 be possible to increase the overall quality of our out- put, bringing it much closer in line with Lappin and Leass' results. Again, straight comparison would not be trivial, as e.g. quoted text passages are not a natural part of computer manuals, and are, on the other hand, an extremely common occurrence in the types of text we are dealing with. Second, and most importantly, the absence of ex- plicit configurational information does not result in a substantial degradation i the accuracy of an anaphora resolution algorithm that is otherwise similar to that described in (Lappin and Leass, 1994)."
96,"I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment disam- biguation, and present results colnparing peffo> mange of this algorithm with other corpus-based approaches to this problem.","I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment disam- biguation, and present results colnparing peffo> mange of this algorithm with other corpus-based approaches to this problem. Prel)ositioual phrase attachment disambiguation is a difficult problem. Take, for example, the sen- rouge: ( l ) Buy a ear \[p,o with a steering wheel\]. We would guess that the correct interpretation is that one should buy cars that come with steer- ing wheels, and not that one should use a steering wheel as barter for purchasing a car. \]n this case, we are helped by our world knowledge about auto- mobiles and automobile parts, and about typical methods of barter, which we can draw upon to cor- rectly disambignate he sentence. Beyond possibly needing such rich semantic or conceptual int'ornla- tion, A l tmann and Steedman (AS88) show that there a,re certain cases where a discourse model is needed to correctly disambiguate prepositional phrase atta.chment. However, while there are certainly cases of an> biguity that seem to need some deep knowledge, either linguistic or conceptual, one might ask whag sort of performance could 1oe achieved by a sys- tem thai uses somewhat superficial knowledge au- *Parts of this work done a.t the Computer and hP lbrmation Science Department, University of Penn- sylvania were supported by by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-C0031 PR\[ (first author) and by an IBM gradmtte fellowship (second author). This work was also supported at MIT by ARPA under Con- tract N000t4-89-J-la32= monitored through the Office of Naval resear<:h (lirst a.uthor). tomatically ~xtracted from a large corpus. Recent work has shown thai; this approach olds promise (H\]~,91, HR93). hi this paper we describe a new rule-based ap- proach to prepositional phrase attachment, disam- biguation. A set of silnple rules is learned au- tomatically to try to prediet proper attachment based on any of a number of possible contextual giles. Baseline l l indle and Rooth (IIR91, 1\[17{93) describe corpus-based approach to disambiguating between prepositional phrase attachlnent to the main verb and to the object nonn phrase (such as in the ex- ample sentence above). They first point out that simple attachment s rategies snch as right associa- tion (Kim73) and miuimal a.tbtchment (Fra78) do not work well i,l practice' (see (WFB90)). They then suggest using lexical preference, estimated from a large corpus of text, as a method of re- solving attachment ambiguity, a technique the}' call ""lexical association."" From a large corpus of pursed text, they first find all nonn phrase heads, and then record the verb (if' any) that precedes the head, and the preposition (if any) that follows it, as well as some other syntactic inforlnation about the sentence. An algorithm is then specified 1,o try to extract attachment information h'om this table of co-occurrences. I!'or instance, a table entry is cousidered a definite instance of the prepositional phrase attaching to the noun if: '\['he noun phrase occm:s in a context where no verb could license the prepositional phrase, specifically if the noun phrase is in a subjeet or other pre-verbal position. They specify seven different procedures for decid- ing whether a table entry is au instance of no attachment, sure noun attach, sm:e verb attach, or all ambiguous attach. Using these procedures, they are able to extract frequency information, 1198 counting t, he numl)e,r of times a ptu:ticular verb or ncmn a.ppe~u:s with a pal:tieuh~r l~reposition. These frequen(;ies erve a.s training d~t;a for the statistical model they use to predict correct i~ttachmenL To dismnbigu;~te s ntence (l), they would compute the likelihood of the preposition with giwm the verb buy, {rod eolltrast that with the likelihood of that preposition given I:he liOttll whed. ()he, problem wit;h this ,~pproa~ch is tll~tt it is limited in what rel~tionships are examined to make mi ~d;tachment decision. Simply extending t\[indle and l{,ooth's model to allow R)r relalion- ships such as tlml~ I)e.tweell the verb and the' ob- ject o\[' the preposition would i:esult ill too large a. parameter spa.ce, given ~my realistic quantity of traiuing data. Another prol)lem of the method, shared by ma.ny statistical approaches, is that the. model ~(:quired (Inring training is rel)reser~ted in a huge, t~d)le of probabilities, pl:ecludiug any stra.ightf'orward analysis of its workings. ' l~-ansformat ion-Based Er ror -Dr iven Learn ing Tra, nS\]bl'm~d;ion-lmsed errol:-dHven learlting is ~ sin@e learning a.lgorithm tlmt has t)eeu applied to a. number of natural la.ngm,ge prol)ie.ms, includ- Jllg l)a.t't O\[' speech tagging and syuta.cl, ic l)m:sing (1h:i92, \]h:i93a, Bri!)gb, Bri9d). Figure :1 illus- trates the learning l)l:OCC'SS, l:irsL, tlll;21nlola, ted text; is l)assed through the initial-st;ate mmota- tot. 'l'lw~ initial-stat, e area)tater can range in com- plexity from quite trivial (e.g. assigning rmtdom strll(:ttll:C) to quit, e sophistica.ted (e.g. assigning the output of a. I{nowledge-based ;/l/llot;~l, tol' that was created by hand). Ouce text has beeu passed through the iuitia.l-state almOl, at.or, it. is then (;ore- pared to the h'ugh,, as indicated ill a luamlally an- nota,teA eorl)llS , and transformations are le~u'ned that can be applied to the oul, put of the iuitial state remora, tot t;o make it, better resemble the :ruffs. So far, ouly ~ greedy search al)proach as been used: at eaeh itera.tion o\[' learning, t.he tra nsfo> nl~tion is found whose application results in the greatest iml)rovenmnt; ha.t transfk)rmation is then added to the ordered trmlsforlmLtiou list and the corpus is upd~d.ed by a.pplying the. learned trans formation. (See, (I{,Mg,\[) for a detailed discussiou of this algorithm in the context of machiue, le, aru-- iug issues.) Ottce 3,11 ordered list; of transform~tions i learned, new text, can be mmotated hy first aI> plying the initial state ~mnotator to it and then applying each o\[' the traaM'ormations, iu order. UNANNOTATI{D \] ""I'I~X'I' 1NH'\[AI, l STATE ANNOTATliD TEXT TI~.I j'\['l l , ~ e , N El( ~-~ RUI ,I-S Figure \[: Transfonm~tion-I~ased Error.-Driven l,earlfiUg. r lh:ansformation-B ased Prepos i t iona l Phrase At tachment We will now show how transformation-based e.rrol> driwm IGmfing can be used to resolve prep(~si- tiered phrase at, tachment ambiguity. The l)reposi- tioiml phrase a.tt~Munent |ea.riter learns tra.nsfor-- Ill~ttiollS \[?onl a C,)l:l>tls O\[ 4-tuples of the \['orm (v I11 I\] 1|9), where v is ~1 w;rl), nl is the head of its objecl, llolni \]phrase, i ) is the \])l'epositioll, and 11:2 is the head of the noun phrase, governed by the prel)c, sition (for e,-:anq~le, sce/v :1~' bo:q/,l o,/p the h711/~2). 1,'or all sentences that conlbrm to this pattern in the Penn Treeb~mk W{dl St, l:eet 3ourlml corpns (MSM93), such a 4-tuplc was formed, attd each :l-tuple was paired with the at~aehnteut de- cision used in the Treebauk parse) '\['here were 12,766 4q;ul)les in all, which were randomly split into 12,206 trnining s**mples and 500 test samples. \[n this e?periment (as in (\[II~,9\], I\]l{93)), tim at- tachment choice For l)repositional i)hrases was I)e- I,ween the oh.iecl~ mmn and l,he matrix verb. \[n the initial sl,~te mmotator, all prepositional phrases I \])at.terns were extra.clxxl usJ.ng tgrep, a. tree-based grep program written by Rich Pito. '\]'\]te 4-tuples were cxtract;ed autom~tk:ally, a.ud mista.kes were not. m~vn tta.lly pruned out. 1199 are attached to the object, noun. 2 This is tile at- tachment predicted by right association (Kim73). The allowable transforlnations are described by the following templates: ? Change the attachment location from X to Y if: - n l i sW - n2 is W - v isW -- p is W - n l is W1 and n2 is W2 - n l i sWl andv isW2 Here ""from X to Y"" can be either ""from nl to v"" or ""from v to nl ,"" W (W1, W2, etc.) can be any word, and the ellipsis indicates that the complete set of transformations permits matching on any combination of values for v, n l , p, and n2, with the exception of patterns that specify vahms for all four. For example, one allowable transformation would be Change the attachment location from nl to v if p is ""until"". Learning proceeds as follows. First, the train- ing set is processed according to the start state annotator, in this case attaching all prepositional phrases low (attached to nl) . Then, in essence, each possible transtbrmation is scored by apply- ing it to the corpus and cornputing the reduction (or increase) in error rate. in reality, the search is data driven, and so the vast majority of al- lowable transformations are not examined. The best-scoring transformation then becomes the first transformation i the learned list. It is applied to the training corpus, and learning continues on the modified corpus. This process is iterated until no rule can he found that reduces the error rate. In the experiment, a tol, al of 471 transfor- mations were learned - - Figure 3 shows the first twenty. 3 Initial accuracy on the test set is 64.0% when prepositional phrases are always attached to the object noun. After applying the transforma- tions, accuracy increases to 80.8%. Figure 2 shows a plot of test-set accuracy as a function of the nulnber of training instances. It is interesting to note that the accuracy curve has not yet, reached a 2If it is the case that attaching to the verb would be a better start state in some corpora, this decision could be parameterized. ZIn transformation #8, word token amount appears because it was used as the head noun for noun phrases representing percentage amounts, e.g. ""5%."" The rule captures the very regular appearance in the Penn Tree- bank Wall Street Journal corpus of parses like Sales for the yea,"" \[v'P rose \[Np5Yo\]\[pP in fiscal 1988\]\]. Accuracy 81.00 rl 80.00 !! 79,00 t 77.00 !--R . . . / - -F . . . %oo!1 / I 74:001 . . . _ _ t .... _ _ 73.00 j - 72.00 l l i _ __ / __ . ,?!>2 - 70.00 69.00 68.00 67.00 64.00 0.00 5.00 I q ! i t T!aining Size x 103 10.00 Figure 2: Accuracy as a function of l;raining corpus size (no word class information). plateau, suggesting that more training data wonld lead to further improvements. Adding Word Class In format ion In the above experiment, all trans\[brmations are. triggered hy words or groups of words, and it is surprising that good performance is achieved even in spite of the inevitable sparse data problems. There are a number of ways to address the sparse data problem. One of the obvious ways, mapping words to part of speech, seerns unlikely to help. h> stead, semanl, ic class information is an attracLive alternative. We incorporated the idea of using semantic ino tbrmation in the lbllowing way. Using the Word~ Net noun hierarchy (Milg0), each noun in the ffa{ning and test corpus was associated with a set containing the noun itself ph.ts the name of every semantic lass that noun appears in (if any). 4 The transformation template is modified so that in ad- dition to asking if a nmm matches ome word W, 4Class names corresponded to unique ""synonynl set"" identifiers within the WordNet noun database. A noun ""appears in"" a class if it falls within the hy- ponym (IS-A) tree below that class. In the experiments reported here we used WordNet version :l.2. 1200 1 2 4 5 (3 7 8 9 10 II 12 :13 \]4 15 \[6 17 \[8 119 2()_ Change Att{:~ehment Location l""r~m~ To ( ;oMit ion N1 V P is at N\ ] \ / P is as N1 V I ) is iulo N:I \/ P is ,l}'om N:I V P is with N\] V N2 is year N 1 V P is by I? is i~ and N I V NI ix amounl N \[ \/ \]' is lhrough NI V \]) is d'urb~g NI V V ix p,ul N1 V N2 is mou.lk N\[ V 1' is ulldcr NJ V 1 ) is after V is have and N1 V I' is b~ N:\[ V P is wilk.oul V NI P is of V is buy and N1 \/ P is for N:I V P is beJbl""( V is have and NI V P is o~ x/ L( V v ~ V v / V V ,/ Figure 3: The \[irst 20 transforntat;ions learned tbr preposil;ional phrase ~ttachme, n|;. it: (~an a/so ask if"" it is a~ member of some class C. s This al)proaeh I;o data. sparseness i similar to tllat of (l{,es93b, li, l\[93), where {~ method ix proposed for using WordNet in conjunction with a corpus to ohtain class-based statisl, ie,q. ()lit' method here is ltlllC\]l simpler, however, in I;hat we a.re only us- ing Boolean values to indieal;e whel;her ~ word can be a member of' a class, rather than esl, imat ing ~ filll se{, of joint probabil it ies involving (:lasses. Since the tr;ulsformation-based al)l/roach with classes cCm gener~dize ill a way that the approach without classes is ml~l)le to, we woldd expect f'cwer l;ransf'ormal;ions to be necessary, l!;xperimeaH, ally, this is indeed the case. In a second experiment;, l;raining a.ml testing were era:tied out on the same samples as i , the previous experiment, bul; I;his t ime using the ext, ende, d tra ns lbrmat ion t(;ml)la.tes for word classes. A total of 266 transformations were learned. Applying l.hese transt'ormai.ions to the test set l'eslllted in a.n accuracy of' 81.8%. \[n figure 4 we show tile lirst 20 tra.nsform{~l, ions lem'ned using ilOllll classes. Class descriptions arc surrounded by square bracl{ets. (; 'Phe first; grans- Ibrmation st~l.cs thai. if"" N2 is a. nomt I, hal; describes time (i.e. ix a. member of WordNet class that in- cludes tim nouns ""y(;ar,"" ""month,"" ""week,"" and others), thell the preltositiomd phrase should be al;tache(\[ t,() the w;rb, since, tim(; is \]nlMl more l ikely Io modify a yet'It (e.g. le,vc lh(: re(cling iu an hour) thaJl a, lloun. This exlw, r iment also demonstrates how rely \[~?~l;ul:e-based lexicon or word classiflcat, ion scheme cau triviaJly be incorlJorated into the learner, by exLencling l;ransfot'nlal,iolls to allow thent to make l'efel'eAlc(? |;o it WOl:(\[ g i l t \ [ { l i ly O\[' its features. \],valuation against Other Algorithms In (lIl~91, HR93), tra.inittg is done on a superset el' sentence types ttsed ill train- ing the transforlJ~atiolFbased learner. The transformation-based learner is I, rained on sen- tences containing v, n\[ and p, whereas the algo- r i thm describe.d by l l indle and I~,ooth ca.n zdso use sentences (;ontailfing only v and p, (n' only nl and i1. \[11 their lmper, they tra.in on ow~r 200,000 sen- Lettces with prel)ositions f'rotn the Associated Press (APt newswire, trod I;hey quote a.n accuracy of 78- 80% on AP test &~ta.. ~' For reasons of ~: u n- t ime c\[lk:icn(:y, transfonmLl, ions tmddng re\['crence 1:o tile classes of both n l a,nd n2 were IlOI; p(~l?lXiitl, tR(I. GI;or expository purposes, the u.iqm'. WordNet id('.ntilicrs luwe been replaced by words Lh~LL describe the cont, cnt of the class. 1207 (~lml~.ge \] At tachment , / Location / # li'rom t 'Fo \[ Condition 1 N1 V N2 is \[time\]","Prel)ositioual phrase attachment disambiguation is a difficult problem. Take, for example, the sen- rouge: ( l ) Buy a ear \[p,o with a steering wheel\]. We would guess that the correct interpretation is that one should buy cars that come with steer- ing wheels, and not that one should use a steering wheel as barter for purchasing a car. \]n this case, we are helped by our world knowledge about auto- mobiles and automobile parts, and about typical methods of barter, which we can draw upon to cor- rectly disambignate he sentence. Beyond possibly needing such rich semantic or conceptual int'ornla- tion, A l tmann and Steedman (AS88) show that there a,re certain cases where a discourse model is needed to correctly disambiguate prepositional phrase atta.chment. However, while there are certainly cases of an> biguity that seem to need some deep knowledge, either linguistic or conceptual, one might ask whag sort of performance could 1oe achieved by a sys- tem thai uses somewhat superficial knowledge au- *Parts of this work done a.t the Computer and hP lbrmation Science Department, University of Penn- sylvania were supported by by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-C0031 PR\[ (first author) and by an IBM gradmtte fellowship (second author). This work was also supported at MIT by ARPA under Con- tract N000t4-89-J-la32= monitored through the Office of Naval resear<:h (lirst a.uthor). tomatically ~xtracted from a large corpus. Recent work has shown thai; this approach olds promise (H\]~,91, HR93). hi this paper we describe a new rule-based ap- proach to prepositional phrase attachment, disam- biguation. A set of silnple rules is learned au- tomatically to try to prediet proper attachment based on any of a number of possible contextual giles. Baseline l l indle and Rooth (IIR91, 1\[17{93) describe corpus-based approach to disambiguating between prepositional phrase attachlnent to the main verb and to the object nonn phrase (such as in the ex- ample sentence above). They first point out that simple attachment s rategies snch as right associa- tion (Kim73) and miuimal a.tbtchment (Fra78) do not work well i,l practice' (see (WFB90)). They then suggest using lexical preference, estimated from a large corpus of text, as a method of re- solving attachment ambiguity, a technique the}' call ""lexical association."" From a large corpus of pursed text, they first find all nonn phrase heads, and then record the verb (if' any) that precedes the head, and the preposition (if any) that follows it, as well as some other syntactic inforlnation about the sentence. An algorithm is then specified 1,o try to extract attachment information h'om this table of co-occurrences. I!'or instance, a table entry is cousidered a definite instance of the prepositional phrase attaching to the noun if: '\['he noun phrase occm:s in a context where no verb could license the prepositional phrase, specifically if the noun phrase is in a subjeet or other pre-verbal position. They specify seven different procedures for decid- ing whether a table entry is au instance of no attachment, sure noun attach, sm:e verb attach, or all ambiguous attach. Using these procedures, they are able to extract frequency information, 1198 counting t, he numl)e,r of times a ptu:ticular verb or ncmn a.ppe~u:s with a pal:tieuh~r l~reposition. These frequen(;ies erve a.s training d~t;a for the statistical model they use to predict correct i~ttachmenL To dismnbigu;~te s ntence (l), they would compute the likelihood of the preposition with giwm the verb buy, {rod eolltrast that with the likelihood of that preposition given I:he liOttll whed. ()he, problem wit;h this ,~pproa~ch is tll~tt it is limited in what rel~tionships are examined to make mi ~d;tachment decision. Simply extending t\[indle and l{,ooth's model to allow R)r relalion- ships such as tlml~ I)e.tweell the verb and the' ob- ject o\[' the preposition would i:esult ill too large a. parameter spa.ce, given ~my realistic quantity of traiuing data. Another prol)lem of the method, shared by ma.ny statistical approaches, is that the. model ~(:quired (Inring training is rel)reser~ted in a huge, t~d)le of probabilities, pl:ecludiug any stra.ightf'orward analysis of its workings. ' l~-ansformat ion-Based Er ror -Dr iven Learn ing Tra, nS\]bl'm~d;ion-lmsed errol:-dHven learlting is ~ sin@e learning a.lgorithm tlmt has t)eeu applied to a. number of natural la.ngm,ge prol)ie.ms, includ- Jllg l)a.t't O\[' speech tagging and syuta.cl, ic l)m:sing (1h:i92, \]h:i93a, Bri!)gb, Bri9d). Figure :1 illus- trates the learning l)l:OCC'SS, l:irsL, tlll;21nlola, ted text; is l)assed through the initial-st;ate mmota- tot. 'l'lw~ initial-stat, e area)tater can range in com- plexity from quite trivial (e.g. assigning rmtdom strll(:ttll:C) to quit, e sophistica.ted (e.g. assigning the output of a. I{nowledge-based ;/l/llot;~l, tol' that was created by hand). Ouce text has beeu passed through the iuitia.l-state almOl, at.or, it. is then (;ore- pared to the h'ugh,, as indicated ill a luamlally an- nota,teA eorl)llS , and transformations are le~u'ned that can be applied to the oul, put of the iuitial state remora, tot t;o make it, better resemble the :ruffs. So far, ouly ~ greedy search al)proach as been used: at eaeh itera.tion o\[' learning, t.he tra nsfo> nl~tion is found whose application results in the greatest iml)rovenmnt; ha.t transfk)rmation is then added to the ordered trmlsforlmLtiou list and the corpus is upd~d.ed by a.pplying the. learned trans formation. (See, (I{,Mg,\[) for a detailed discussiou of this algorithm in the context of machiue, le, aru-- iug issues.) Ottce 3,11 ordered list; of transform~tions i learned, new text, can be mmotated hy first aI> plying the initial state ~mnotator to it and then applying each o\[' the traaM'ormations, iu order. UNANNOTATI{D \] ""I'I~X'I' 1NH'\[AI, l STATE ANNOTATliD TEXT TI~.I j'\['l l , ~ e , N El( ~-~ RUI ,I-S Figure \[: Transfonm~tion-I~ased Error.-Driven l,earlfiUg. r lh:ansformation-B ased Prepos i t iona l Phrase At tachment We will now show how transformation-based e.rrol> driwm IGmfing can be used to resolve prep(~si- tiered phrase at, tachment ambiguity. The l)reposi- tioiml phrase a.tt~Munent |ea.riter learns tra.nsfor-- Ill~ttiollS \[?onl a C,)l:l>tls O\[ 4-tuples of the \['orm (v I11 I\] 1|9), where v is ~1 w;rl), nl is the head of its objecl, llolni \]phrase, i ) is the \])l'epositioll, and 11:2 is the head of the noun phrase, governed by the prel)c, sition (for e,-:anq~le, sce/v :1~' bo:q/,l o,/p the h711/~2). 1,'or all sentences that conlbrm to this pattern in the Penn Treeb~mk W{dl St, l:eet 3ourlml corpns (MSM93), such a 4-tuplc was formed, attd each :l-tuple was paired with the at~aehnteut de- cision used in the Treebauk parse) '\['here were 12,766 4q;ul)les in all, which were randomly split into 12,206 trnining s**mples and 500 test samples. \[n this e?periment (as in (\[II~,9\], I\]l{93)), tim at- tachment choice For l)repositional i)hrases was I)e- I,ween the oh.iecl~ mmn and l,he matrix verb. \[n the initial sl,~te mmotator, all prepositional phrases I \])at.terns were extra.clxxl usJ.ng tgrep, a. tree-based grep program written by Rich Pito. '\]'\]te 4-tuples were cxtract;ed autom~tk:ally, a.ud mista.kes were not. m~vn tta.lly pruned out. 1199 are attached to the object, noun. 2 This is tile at- tachment predicted by right association (Kim73). The allowable transforlnations are described by the following templates: ? Change the attachment location from X to Y if: - n l i sW - n2 is W - v isW -- p is W - n l is W1 and n2 is W2 - n l i sWl andv isW2 Here ""from X to Y"" can be either ""from nl to v"" or ""from v to nl ,"" W (W1, W2, etc.) can be any word, and the ellipsis indicates that the complete set of transformations permits matching on any combination of values for v, n l , p, and n2, with the exception of patterns that specify vahms for all four. For example, one allowable transformation would be Change the attachment location from nl to v if p is ""until"". Learning proceeds as follows. First, the train- ing set is processed according to the start state annotator, in this case attaching all prepositional phrases low (attached to nl) . Then, in essence, each possible transtbrmation is scored by apply- ing it to the corpus and cornputing the reduction (or increase) in error rate. in reality, the search is data driven, and so the vast majority of al- lowable transformations are not examined. The best-scoring transformation then becomes the first transformation i the learned list. It is applied to the training corpus, and learning continues on the modified corpus. This process is iterated until no rule can he found that reduces the error rate. In the experiment, a tol, al of 471 transfor- mations were learned - - Figure 3 shows the first twenty. 3 Initial accuracy on the test set is 64.0% when prepositional phrases are always attached to the object noun. After applying the transforma- tions, accuracy increases to 80.8%. Figure 2 shows a plot of test-set accuracy as a function of the nulnber of training instances. It is interesting to note that the accuracy curve has not yet, reached a 2If it is the case that attaching to the verb would be a better start state in some corpora, this decision could be parameterized. ZIn transformation #8, word token amount appears because it was used as the head noun for noun phrases representing percentage amounts, e.g. ""5%."" The rule captures the very regular appearance in the Penn Tree- bank Wall Street Journal corpus of parses like Sales for the yea,"" \[v'P rose \[Np5Yo\]\[pP in fiscal 1988\]\]. Accuracy 81.00 rl 80.00 !! 79,00 t 77.00 !--R . . . / - -F . . . %oo!1 / I 74:001 . . . _ _ t .... _ _ 73.00 j - 72.00 l l i _ __ / __ . ,?!>2 - 70.00 69.00 68.00 67.00 64.00 0.00 5.00 I q ! i t T!aining Size x 103 10.00 Figure 2: Accuracy as a function of l;raining corpus size (no word class information). plateau, suggesting that more training data wonld lead to further improvements. Adding Word Class In format ion In the above experiment, all trans\[brmations are. triggered hy words or groups of words, and it is surprising that good performance is achieved even in spite of the inevitable sparse data problems. There are a number of ways to address the sparse data problem. One of the obvious ways, mapping words to part of speech, seerns unlikely to help. h> stead, semanl, ic class information is an attracLive alternative. We incorporated the idea of using semantic ino tbrmation in the lbllowing way. Using the Word~ Net noun hierarchy (Milg0), each noun in the ffa{ning and test corpus was associated with a set containing the noun itself ph.ts the name of every semantic lass that noun appears in (if any). 4 The transformation template is modified so that in ad- dition to asking if a nmm matches ome word W, 4Class names corresponded to unique ""synonynl set"" identifiers within the WordNet noun database. A noun ""appears in"" a class if it falls within the hy- ponym (IS-A) tree below that class. In the experiments reported here we used WordNet version :l.2. 1200 1 2 4 5 (3 7 8 9 10 II 12 :13 \]4 15 \[6 17 \[8 119 2()_ Change Att{:~ehment Location l""r~m~ To ( ;oMit ion N1 V P is at N\ ] \ / P is as N1 V I ) is iulo N:I \/ P is ,l}'om N:I V P is with N\] V N2 is year N 1 V P is by I? is i~ and N I V NI ix amounl N \[ \/ \]' is lhrough NI V \]) is d'urb~g NI V V ix p,ul N1 V N2 is mou.lk N\[ V 1' is ulldcr NJ V 1 ) is after V is have and N1 V I' is b~ N:\[ V P is wilk.oul V NI P is of V is buy and N1 \/ P is for N:I V P is beJbl""( V is have and NI V P is o~ x/ L( V v ~ V v / V V ,/ Figure 3: The \[irst 20 transforntat;ions learned tbr preposil;ional phrase ~ttachme, n|;. it: (~an a/so ask if"" it is a~ member of some class C. s This al)proaeh I;o data. sparseness i similar to tllat of (l{,es93b, li, l\[93), where {~ method ix proposed for using WordNet in conjunction with a corpus to ohtain class-based statisl, ie,q. ()lit' method here is ltlllC\]l simpler, however, in I;hat we a.re only us- ing Boolean values to indieal;e whel;her ~ word can be a member of' a class, rather than esl, imat ing ~ filll se{, of joint probabil it ies involving (:lasses. Since the tr;ulsformation-based al)l/roach with classes cCm gener~dize ill a way that the approach without classes is ml~l)le to, we woldd expect f'cwer l;ransf'ormal;ions to be necessary, l!;xperimeaH, ally, this is indeed the case. In a second experiment;, l;raining a.ml testing were era:tied out on the same samples as i , the previous experiment, bul; I;his t ime using the ext, ende, d tra ns lbrmat ion t(;ml)la.tes for word classes. A total of 266 transformations were learned. Applying l.hese transt'ormai.ions to the test set l'eslllted in a.n accuracy of' 81.8%. \[n figure 4 we show tile lirst 20 tra.nsform{~l, ions lem'ned using ilOllll classes. Class descriptions arc surrounded by square bracl{ets. (; 'Phe first; grans- Ibrmation st~l.cs thai. if"" N2 is a. nomt I, hal; describes time (i.e. ix a. member of WordNet class that in- cludes tim nouns ""y(;ar,"" ""month,"" ""week,"" and others), thell the preltositiomd phrase should be al;tache(\[ t,() the w;rb, since, tim(; is \]nlMl more l ikely Io modify a yet'It (e.g. le,vc lh(: re(cling iu an hour) thaJl a, lloun. This exlw, r iment also demonstrates how rely \[~?~l;ul:e-based lexicon or word classiflcat, ion scheme cau triviaJly be incorlJorated into the learner, by exLencling l;ransfot'nlal,iolls to allow thent to make l'efel'eAlc(? |;o it WOl:(\[ g i l t \ [ { l i ly O\[' its features. \],valuation against Other Algorithms In (lIl~91, HR93), tra.inittg is done on a superset el' sentence types ttsed ill train- ing the transforlJ~atiolFbased learner. The transformation-based learner is I, rained on sen- tences containing v, n\[ and p, whereas the algo- r i thm describe.d by l l indle and I~,ooth ca.n zdso use sentences (;ontailfing only v and p, (n' only nl and i1. \[11 their lmper, they tra.in on ow~r 200,000 sen- Lettces with prel)ositions f'rotn the Associated Press (APt newswire, trod I;hey quote a.n accuracy of 78- 80% on AP test &~ta.. ~' For reasons of ~: u n- t ime c\[lk:icn(:y, transfonmLl, ions tmddng re\['crence 1:o tile classes of both n l a,nd n2 were IlOI; p(~l?lXiitl, tR(I. GI;or expository purposes, the u.iqm'. WordNet id('.ntilicrs luwe been replaced by words Lh~LL describe the cont, cnt of the class. 1207 (~lml~.ge \] At tachment , / Location / # li'rom t 'Fo \[ Condition 1 N1 V N2 is \[time\]"
97,"computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability. We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,","computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability. We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al., Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997), and many more. Therefore, much attention has been focused on automatically computing them based on a corpus of relation instances. Resnik (1996) presented the earliest work in this area, describing an information-theoretic approach that inferred selectional preferences based on the WordNet hypernym hierarchy. Recent work (Erk, 2007; Bergsma et al., 2008) has moved away from generalization to known classes, instead utilizing distributional similarity between nouns to generalize beyond observed relation-argument pairs. This avoids problems like WordNet’s poor coverage of proper nouns and is shown to improve performance. These methods, however, no longer produce the generalized class for an argument. In this paper we describe a novel approach to computing selectional preferences by making use of unsupervised topic models. Our approach is able to combine benefits of both kinds of methods: it retains the generalization and humaninterpretability of class-based approaches and is also competitive with the direct methods on predictive tasks. Unsupervised topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) and its variants are characterized by a set of hidden topics, which represent the underlying semantic structure of a document collection. For our problem these topics offer an intuitive interpretation – they represent the (latent) set of classes that store the preferences for the different relations. Thus, topic models are a natural fit for modeling our relation data. In particular, our system, called LDA-SP, uses LinkLDA (Erosheva et al., 2004), an extension of LDA that simultaneously models two sets of distributions for each topic. These two sets represent the two arguments for the relations. Thus, LDA-SP is able to capture information about the pairs of topics that commonly co-occur. This information is very helpful in guiding inference. We run LDA-SP to compute preferences on a massive dataset of binary relations r(a1, a2) extracted from the Web by TEXTRUNNER (Banko and Etzioni, 2008). Our experiments demonstrate that LDA-SP significantly outperforms state of the art approaches obtaining an 85% increase in recall at precision 0.9 on the standard pseudodisambiguation task. Additionally, because LDA-SP is based on a formal probabilistic model, it has the advantage that it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical coverage), and word sense ambiguity. Because of these limitations researchers have investigated non-class based approaches, which attempt to directly classify a given noun-phrase as plausible/implausible for a relation. Of these, the similarity based approaches make use of a distributional similarity measure between arguments and evaluate a heuristic scoring function: Erk (2007) showed the advantages of this approach over Resnik’s information-theoretic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbased approaches. Probably the closest to our work is a model proposed by Rooth et al. (1999), in which each class corresponds to a multinomial over relations and arguments and EM is used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Recently, Bergsma et. al. (2008) proposed the first discriminative approach to selectional preferences. Their insight that pseudo-negative examples could be used as training data allows the application of an SVM classifier, which makes use of many features in addition to the relation-argument co-occurrence frequencies used by other methods. They automatically generated positive and negative examples by selecting arguments having high and low mutual information with the relation. Since it is a discriminative approach it is amenable to feature engineering, but needs to be retrained and tuned for each task. On the other hand, generative models produce complete probability distributions of the data, and hence can be integrated with other systems and tasks in a more principled manner (see Sections 4.2.2 and 4.3.1). Additionally, unlike LDA-SP Bergsma et al.’s system doesn’t produce human-interpretable topics. Finally, we note that LDA-SP and Bergsma’s system are potentially complimentary – the output of LDA-SP could be used to generate higher-quality training data for Bergsma, potentially improving their results. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight two systems, developed independently of our own, which apply LDA-style models to similar tasks. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Keller and Lapata’s (2003) Web hit-count based system. Van Durme and Gildea (2009) proposed applying LDA to general knowledge templates extracted using the KNEXT system (Schubert and Tong, 2003). In contrast, our work uses LinkLDA and focuses on modeling multiple arguments of a relation (e.g., the subject and direct object of a verb). We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2. At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in Section 4, it is the best model for our relation data. We are given a set R of binary relations and a corpus D = {r(a1, a2)} of extracted instances for these relations. 2 Our task is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. We first describe the straightforward application of LDA to modeling our corpus of extracted relations. In this case two separate LDA models are used to model a1 and a2 independently. In the generative model for our data, each relation r has a corresponding multinomial over topics 0r, drawn from a Dirichlet. For each extraction, a hidden topic z is first picked according to 0r, and then the observed argument a is chosen according to the multinomial 0, Readers familiar with topic modeling terminology can understand our approach as follows: we treat each relation as a document whose contents consist of a bags of words corresponding to all the noun phrases observed as arguments of the relation in our corpus. Formally, LDA generates each argument in the corpus of relations as follows: for each topic t = 1... T do Generate Ot according to symmetric Dirichlet distribution Dir(,q). for each relation r = 1... |R |do Generate 0r according to Dirichlet distribution Dir(α). for each tuple i = 1... Nr do Generate zr,i from Multinomial(Or). Generate the argument ar,i from multiOne weakness of IndependentLDA is that it doesn’t jointly model a1 and a2 together. Clearly this is undesirable, as information about which topics one of the arguments favors can help inform the topics chosen for the other. For example, class pairs such as (team, game), (politician, political issue) form much more plausible selectional preferences than, say, (team, political issue), (politician, game). As a more tightly coupled alternative, we first propose JointLDA, whose graphical model is depicted in Figure 1. The key difference in JointLDA (versus LDA) is that instead of one, it maintains two sets of topics (latent distributions over words) denoted by Q and -y, one for classes of each argument. A topic id k represents a pair of topics, Qk and -yk, that co-occur in the arguments of extracted relations. Common examples include (Person, Location), (Politician, Political issue), etc. The hidden variable z = k indicates that the noun phrase for the first argument was drawn from the multinomial Qk, and that the second argument was drawn from -yk. The per-relation distribution 0r is a multinomial over the topic ids and represents the selectional preferences, both for arg1s and arg2s of a relation r. Although JointLDA has many desirable properties, it has some drawbacks as well. Most notably, in JointLDA topics correspond to pairs of multinomials (Qk, -yk); this leads to a situation in which multiple redundant distributions are needed to represent the same underlying semantic class. For example consider the case where we we need to represent the following selectional preferences for our corpus of relations: (person, location), (person, organization), and (person, crime). Because JointLDA requires a separate pair of multinomials for each topic, it is forced to use 3 separate multinomials to represent the class person, rather than learning a single distribution representing person and choosing 3 different topics for a2. This results in poor generalization because the data for a single class is divided into multiple topics. In order to address this problem while maintaining the sharing of influence between a1 and a2, we next present LinkLDA, which represents a compromise between IndependentLDA and JointLDA. LinkLDA is more flexible than JointLDA, allowing different topics to be chosen for a1, and a2, however still models the generation of topics from the same distribution for a given relation. Figure 2 illustrates the LinkLDA model in the plate notation, which is analogous to the model in (Erosheva et al., 2004). In particular note that each ai is drawn from a different hidden topic zi, however the zi’s are drawn from the same distribution 0r for a given relation r. To facilitate learning related topic pairs between arguments we employ a sparse prior over the per-relation topic distributions. Because a few topics are likely to be assigned most of the probability mass for a given relation it is more likely (although not necessary) that the same topic number k will be drawn for both arguments. When comparing LinkLDA with JointLDA the better model may not seem immediately clear. On the one hand, JointLDA jointly models the generation of both arguments in an extracted tuple. This allows one argument to help disambiguate the other in the case of ambiguous relation strings. LinkLDA, however, is more flexible; rather than requiring both arguments to be generated from one of IZI possible pairs of multinomials (Qz, -yz), LinkLDA allows the arguments of a given extraction to be generated from IZ12 possible pairs. Thus, instead of imposing a hard constraint that z1 = z2 (as in JointLDA), LinkLDA simply assigns a higher probability to states in which z1 = z2, because both hidden variables are drawn from the same (sparse) distribution 0r. LinkLDA can thus re-use argument classes, choosing different combinations of topics for the arguments if it fits the data better. In Section 4 we show experimentally that LinkLDA outperforms JointLDA (and IndependentLDA) by wide margins. We use LDA-SP to refer to LinkLDA in all the experiments below. For all the models we use collapsed Gibbs sampling for inference in which each of the hidden variables (e.g., zr,i,1 and zr,i,2 in LinkLDA) are sampled sequentially conditioned on a fullassignment to all others, integrating out the parameters (Griffiths and Steyvers, 2004). This produces robust parameter estimates, as it allows computation of expectations over the posterior distribution as opposed to estimating maximum likelihood parameters. In addition, the integration allows the use of sparse priors, which are typically more appropriate for natural language data. In all experiments we use hyperparameters α = 771 = 772 = 0.1. We generated initial code for our samplers using the Hierarchical Bayes Compiler (Daume III, 2007). There are several advantages to using topic models for our task. First, they naturally model the class-based nature of selectional preferences, but don’t take a pre-defined set of classes as input. Instead, they compute the classes automatically. This leads to better lexical coverage since the issue of matching a new argument to a known class is side-stepped. Second, the models naturally handle ambiguous arguments, as they are able to assign different topics to the same phrase in different contexts. Inference in these models is also scalable – linear in both the size of the corpus as well as the number of topics. In addition, there are several scalability enhancements such as SparseLDA (Yao et al., 2009), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999; Erk, 2007; Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the best model with the known state of the art approaches to selectional preferences. Secondly, we show significant improvements to performance at an end-task of textual inference in Section 4.3. Finally, we report on the quality of a large database of Wordnet-based preferences obtained after manually associating our topics with Wordnet classes (Section 4.4). For all experiments we make use of a corpus of r(a1, a2) tuples, which was automatically extracted by TEXTRUNNER (Banko and Etzioni, 2008) from 500 million Web pages. To create a generalization corpus from this large dataset. We first selected 3,000 relations from the middle of the tail (we used the 2,0005,000 most frequent ones)3 and collected all instances. To reduce sparsity, we discarded all tuples containing an NP that occurred fewer than 50 times in the data. This resulted in a vocabulary of about 32,000 noun phrases, and a set of about 2.4 million tuples in our generalization corpus. We inferred topic-argument and relation-topic multinomials (Q, 'y, and 0) on the generalization corpus by taking 5 samples at a lag of 50 after a burn in of 750 iterations. Using multiple samples introduces the risk of topic drift due to lack of identifiability, however we found this to not be a problem in practice. During development we found that the topics tend to remain stable across multiple samples after sufficient burn in, and multiple samples improved performance. Table 1 lists sample topics and high ranked words for each (for both arguments) as well as relations favoring those topics. We first compare the three LDA-based approaches to each other and two state of the art similarity based systems (Erk, 2007) (using mutual information and Jaccard similarity respectively). These similarity measures were shown to outperform the generative model of Rooth et al. (1999), as well as class-based methods such as Resnik’s. In this pseudo-disambiguation experiment an observed tuple is paired with a pseudo-negative, which has both arguments randomly generated from the whole vocabulary (according to the corpus-wide distribution over arguments). The task is, for each relation-argument pair, to determine whether it is observed, or a random distractor. For this experiment we gathered a primary corpus by first randomly selecting 100 high-frequency relations not in the generalization corpus. For each relation we collected all tuples containing arguments in the vocabulary. We held out 500 randomly selected tuples as the test set. For each tuprobable values according to the multinomial distributions for each argument (Qt and -yt). The middle column reports a few relations whose inferred topic distributions Br assign highest probability to t. ple r(a1, a2) in the held-out set, we removed all tuples in the training set containing either of the rel-arg pairs, i.e., any tuple matching r(a1, *) or r(*, a2). Next we used collapsed Gibbs sampling to infer a distribution over topics, Br, for each of the relations in the primary corpus (based solely on tuples in the training set) using the topics from the generalization corpus. For each of the 500 observed tuples in the testset we generated a pseudo-negative tuple by randomly sampling two noun phrases from the distribution of NPs in both corpora. Our prediction system needs to determine whether a specific relation-argument pair is admissible according to the selectional preferences or is a random distractor (D). Following previous work, we perform this experiment independently for the two relation-argument pairs (r, a1) and (r, a2). We first compute the probability of observing a1 for first argument of relation r given that it is not a distractor, P(a1|r, -,D), which we approximate by its probability given an estimate of the parameters inferred by our model, marginalizing over hidden topics t. The analysis for the second A simple application of Bayes Rule gives the probability that a particular argument is not a distractor. Here the distractor-related probabilities are independent of r, i.e., P(D|r) = P(D), P(a1|D, r) = P(a1|D), etc. We estimate P(a1|D) according to their frequency in the generalization corpus. Figure 3 plots the precision-recall curve for the pseudo-disambiguation experiment comparing the three different topic models. LDA-SP, which uses LinkLDA, substantially outperforms both IndependentLDA and JointLDA. Next, in figure 4, we compare LDA-SP with mutual information and Jaccard similarities using both the generalization and primary corpus for computation of similarities. We find LDA-SP significantly outperforms these methods. Its edge is most noticed at high precisions; it obtains 85% more recall at 0.9 precision compared to mutual information. Overall LDA-SP obtains an 15% increase in the area under precision-recall curve over mutual information. All three systems’ AUCs are shown in Table 2; LDA-SP’s improvements over both Jaccard and mutual information are highly significant with a significance level less than 0.01 using a paired t-test. In addition to a superior performance in selectional preference evaluation LDA-SP also produces a set of coherent topics, which can be useful in their own right. For instance, one could use them for tasks such as set-expansion (Carlson et al., 2010) or automatic thesaurus induction (EtWe now evaluate LDA-SP’s ability to improve performance at an end-task. We choose the task of improving textual entailment by learning selectional preferences for inference rules and filtering inferences that do not respect these. This application of selectional preferences was introduced by Pantel et. al. (2007). For now we stick to inference rules of the form r1(a1, a2) ==>' r2(a1, a2), though our ideas are more generally applicable to more complex rules. As an example, the rule (X defeats Y) ==>- (X plays Y) holds when X and Y are both sports teams, however fails to produce a reasonable inference if X and Y are Britain and Nazi Germany respectively. In order for an inference to be plausible, both relations must have similar selectional preferences, and further, the arguments must obey the selectional preferences of both the antecedent r1 and the consequent r2.4 Pantel et al. (2007) made use of these intuitions by producing a set of classbased selectional preferences for each relation, then filtering out any inferences where the arguments were incompatible with the intersection of these preferences. In contrast, we take a probabilistic approach, evaluating the quality of a specific inference by measuring the probability that the arguments in both the antecedent and the consequent were drawn from the same hidden topic in our model. Note that this probability captures both the requirement that the antecedent and consequent have similar selectional preferences, and that the arguments from a particular instance of the rule’s application match their overlap. We use zrz,j to denote the topic that generates the jth argument of relation ri. The probability that the two arguments a1, a2 were drawn from the same hidden topic factorizes as follows due to the conditional independences in our model:5 4Similarity-based and discriminative methods are not applicable to this task as they offer no straightforward way to compare the similarity between selectional preferences of two relations. 5Note that all probabilities are conditioned on an estimate of the parameters 0, ,Q, 7 from our model, which are omitted for compactness. To compute each of these factors we simply marginalize over the hidden topics: where P(z = t|a) can be computed using Bayes rule. For example, In order to evaluate LDA-SP’s ability to filter inferences based on selectional preferences we need a set of inference rules between the relations in our corpus. We therefore mapped the DIRT Inference rules (Lin and Pantel, 2001), (which consist of pairs of dependency paths) to TEXTRUNNER relations as follows. We first gathered all instances in the generalization corpus, and for each r(a1, a2) created a corresponding simple sentence by concatenating the arguments with the relation string between them. Each such simple sentence was parsed using Minipar (Lin, 1998). From the parses we extracted all dependency paths between nouns that contain only words present in the TEXTRUNNER relation string. These dependency paths were then matched against each pair in the DIRT database, and all pairs of associated relations were collected producing about 26,000 inference rules. Following Pantel et al. (2007) we randomly sampled 100 inference rules. We then automatically filtered out any rules which contained a negation, or for which the antecedent and consequent contained a pair of antonyms found in WordNet (this left us with 85 rules). For each rule we collected 10 random instances of the antecedent, and generated the consequent. We randomly sampled 300 of these inferences to hand-label. In figure 5 we compare the precision and recall of LDA-SP against the top two performing systems described by Pantel et al. (ISP.IIM-V and ISP.JIM, both using the CBC clusters (Pantel, 2003)). We find that LDA-SP achieves both higher precision and recall than ISP.IIM-V. It is also able to achieve the high-precision point of ISP.JIM and can trade precision to get a much larger recall. Top 10 Inference Rules Ranked by LDA-SP antecedent consequent KL-div will begin at will start at 0.014999 shall review shall determine 0.129434 may increase may reduce 0.214841 walk from walk to 0.219471 consume absorb 0.240730 shall keep shall maintain 0.264299 shall pay to will notify 0.290555 may apply for may obtain 0.313916 copy download 0.316502 should pay must pay 0.371544 Bottom 10 Inference Rules Ranked by LDA-SP antecedent consequent KL-div lose to shall take 10.011848 should play could do 10.028904 could play get in 10.048857 will start at move to 10.060994 shall keep will spend 10.105493 should play get in 10.131299 shall pay to leave for 10.131364 shall keep return to 10.149797 shall keep could do 10.178032 shall maintain have spent 10.221618 Table 3: Top 10 and Bottom 10 ranked inference rules ranked by LDA-SPafter automatically filtering out negations and antonyms (using WordNet). In addition we demonstrate LDA-SP’s ability to rank inference rules by measuring the Kullback Leibler Divergence6 between the topicdistributions of the antecedent and consequent, Br1 and Bre respectively. Table 3 shows the top 10 and bottom 10 rules out of the 26,000 ranked by KL Divergence after automatically filtering antonyms (using WordNet) and negations. For slight variations in rules (e.g., symmetric pairs) we mention only one example to show more variety. Finally we explore LDA-SP’s ability to produce a repository of human interpretable class-based selectional preferences. As an example, for the relation was born in, we would like to infer that the plausible arguments include (person, location) and (person, date). Since we already have a set of topics, our task reduces to mapping the inferred topics to an equivalent class in a taxonomy (e.g., WordNet). We experimented with automatic methods such as Resnik’s, but found them to have all the same problems as directly applying these approaches to the SP task.7 Guided by the fact that we have a relatively small number of topics (600 total, 300 for each argument) we simply chose to label them manually. By labeling this small number of topics we can infer class-based preferences for an arbitrary number of relations. In particular, we applied a semi-automatic scheme to map topics to WordNet. We first applied Resnik’s approach to automatically shortlist a few candidate WordNet classes for each topic. We then manually picked the best class from the shortlist that best represented the 20 top arguments for a topic (similar to Table 1). We marked all incoherent topics with a special symbol 0. This process took one of the authors about 4 hours to complete. To evaluate how well our topic-class associations carry over to unseen relations we used the same random sample of 100 relations from the pseudo-disambiguation experiment.8 For each argument of each relation we picked the top two topics according to frequency in the 5 Gibbs samples. We then discarded any topics which were labeled with 0; this resulted in a set of 236 predictions. A few examples are displayed in table 4. We evaluated these classes and found the accuracy to be around 0.88. We contrast this with Pantel’s repository,9 the only other released database of selectional preferences to our knowledge. We evaluated the same 100 relations from his website and tagged the top 2 classes for each argument and evaluated the accuracy to be roughly 0.55. We emphasize that tagging a pair of class-based preferences is a highly subjective task, so these results should be treated as preliminary. Still, these early results are promising. We wish to undertake a larger scale study soon. We have presented an application of topic modeling to the problem of automatically computing selectional preferences. Our method, LDA-SP, learns a distribution over topics for each relation while simultaneously grouping related words into these topics. This approach is capable of producing human interpretable classes, however, avoids the drawbacks of traditional class-based approaches (poor lexical coverage and ambiguity). LDA-SP achieves state-of-the-art performance on predictive tasks such as pseudo-disambiguation, and filtering incorrect inferences. Because LDA-SP generates a complete probabilistic model for our relation data, its results are easily applicable to many other tasks such as identifying similar relations, ranking inference rules, etc. In the future, we wish to apply our model to automatically discover new inference rules and paraphrases. Finally, our repository of selectional preferences for 10,000 relations is available at http://www.cs.washington.edu/ research/ldasp.","We have presented an application of topic modeling to the problem of automatically computing selectional preferences. Our method, LDA-SP, learns a distribution over topics for each relation while simultaneously grouping related words into these topics. This approach is capable of producing human interpretable classes, however, avoids the drawbacks of traditional class-based approaches (poor lexical coverage and ambiguity). LDA-SP achieves state-of-the-art performance on predictive tasks such as pseudo-disambiguation, and filtering incorrect inferences. Because LDA-SP generates a complete probabilistic model for our relation data, its results are easily applicable to many other tasks such as identifying similar relations, ranking inference rules, etc. In the future, we wish to apply our model to automatically discover new inference rules and paraphrases. Finally, our repository of selectional preferences for 10,000 relations is available at http://www.cs.washington.edu/ research/ldasp."
98,"If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further by combining word representations. You can download word features, for use in existing NLP systems, as well as our here:","If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further by combining word representations. You can download word features, for use in existing NLP systems, as well as our here: By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy. Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al. (2009) achieve state-of-the-art accuracy. However, these approaches dictate a particular choice of model and training regime. It can be tricky and time-consuming to adapt an existing supervised NLP system to use these semi-supervised techniques. It is preferable to use a simple and general method to adapt existing supervised NLP systems to be semi-supervised. One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a significant increase in accuracy. But which word features are good for what tasks? Should we prefer certain word features? Can we combine them? A word representation is a mathematical object associated with each word, often a vector. Each dimension’s value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature. Conventionally, supervised lexicalized NLP approaches take a word and convert it to a symbolic ID, which is then transformed into a feature vector using a one-hot representation: The feature vector has the same length as the size of the vocabulary, and only one dimension is on. However, the one-hot representation of a word suffers from data sparsity: Namely, for words that are rare in the labeled training data, their corresponding model parameters will be poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov & Roth, 2009; Huang & Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk & Gauvain, 2002; Mnih & Hinton, 2007; Collobert & Weston, 2008), on the other hand, induce dense real-valued low-dimensional word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representations have never been systematically compared in a controlled way. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking. We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1. Distributional word representations are based upon a cooccurrence matrix F of size WxC, where W is the vocabulary size, each row Fw is the initial representation of word w, and each column Fc is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). Fw has dimensionality W, which can be too large to use Fw as features for word w in a supervised model. One can map F to matrix f of size W x d, where d << C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F. The self-organizing semantic map (Ritter & Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund & Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2·W types of context (columns): The first or second W are counted if the word c occurs within a window of 10 to the left or right of the word w, respectively. f is chosen by taking the 200 columns (out of 140K in F) with the highest variances. ICA is another technique to transform F into f. (V¨ayrynen & Honkela, 2004; V¨ayrynen & Honkela, 2005; V¨ayrynen et al., 2007). ICA is expensive, and the largest vocabulary size used in these works was only 10K. As far as we know, ICA methods have not been used when the size of the vocab W is 100K or more. Explicitly storing cooccurrence matrix F can be memory-intensive, and transforming F to f can be time-consuming. It is preferable that F never be computed explicitly, and that f be constructed incrementally. ˇReh˚uˇrek and Sojka (2010) describe an incremental approach to inducing LSA and LDA topic models over 270 millions word tokens with a vocabulary of 315K word types. This is similar in magnitude to our experiments. Another incremental approach to constructing f is using a random projection: Linear mapping g is multiplying F by a random matrix chosen a priori. This random indexing method is motivated by the Johnson-Lindenstrauss lemma, which states that for certain choices of random matrix, if d is sufficiently large, then the original distances between words in F will be preserved in f (Sahlgren, 2005). Kaski (1998) uses this technique to produce 100-dimensional representations of documents. Sahlgren (2001) was the first author to use random indexing using narrow context. Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing. However, like all the works cited above, Sahlgren (2006) only uses distributional representation to improve existing systems for one-shot classification tasks, such as IR, WSD, semantic knowledge tests, and text categorization. It is not well-understood what settings are appropriate to induce distributional word representations for structured prediction tasks (like parsing and MT) and sequence labeling tasks (like chunking and NER). Previous research has achieved repeated successes on these tasks using clustering representations (Section 3) and distributed representations (Section 4), so we focus on these representations in our work. Another type of word representation is to induce a clustering over words. Clustering methods and distributional methods can overlap. For example, Pereira et al. (1993) begin with a cooccurrence matrix and transform this matrix into a clustering. The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov & Roth, 2009), PCFG parsing (Candito & Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, which emits a multinomial distribution over possible vocabulary words. They perform hard clustering using the Viterbi algorithm. (Alternately, they could keep the soft clustering, with the representation for a particular word token being the posterior probability distribution over the states.) However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha & Pereira, 2003). Goldberg et al. (2009) use an HMM to assign POS tags to words, which in turns improves the accuracy of the PCFG-based Hebrew parser. Deschacht and Moens (2009) use a latent-variable language model to improve semantic role labeling. Another approach to word representation is to learn a distributed representation. (Not to be confused with distributional representations.) A distributed representation is dense, lowdimensional, and real-valued. Distributed word representations are called word embeddings. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin & Bengio, 2005; Collobert & Weston, 2008; Mnih & Hinton, 2009) and allow scaling to very large training corpora. Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic. For each training update, we read an n-gram x = (w1, ... , wn) from the corpus. The model concatenates the learned embeddings of the n words, giving e(w1) ® ... ® e(wn), where e is the lookup table and ® is concatenation. We also create a corrupted or noise n-gram x˜ = (w1, ... , wn_q, ˜wn), where ˜wn # wn is chosen uniformly from the vocabulary.1 For convenience, we write e(x) to mean e(w1) ® ... ® e(wn). We predict a score s(x) for x by passing e(x) through a single hidden layer neural network. The training criterion is that n-grams that are present in the training corpus like x must have a score at least some margin higher than corrupted n-grams like ˜x. Specifically: L(x) = max(0, 1− s(x) + s(˜x)). We minimize this loss stochastically over the n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table. We implemented the approach of Collobert and Weston (2008), with the following differences: The log-bilinear model (Mnih & Hinton, 2007) is a probabilistic and linear neural model. Given an n-gram, the model concatenates the embeddings of the n − 1 first words, and learns a linear model to predict the embedding of the last word. The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing. Mnih and Hinton (2009) speed up model evaluation during training and testing by using a hierarchy to exponentially filter down the number of computations that are performed. This hierarchical evaluation technique was first proposed by Morin and Bengio (2005). The model, combined with this optimization, is called the hierarchical log-bilinear (HLBL) model. n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted. We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang & Buchholz, 2000). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set: We use CRFsuite because it makes it simple to modify the feature generation code, so one can easily add new features. We use SGD optimization, and enable negative state features and negative transition features. (“feature.possible transitions=1, feature.possible states=1”) Table 1 shows the features in the baseline chunker. As you can see, the Brown and embedding features are unigram features, and do not participate in conjunctions like the word features and tag features do. Koo et al. (2008) sees further accuracy improvements on dependency parsing when using word representations in compound features. The data comes from the Penn Treebank, and is newswire from the Wall Street Journal in 1989. Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for development. We trained models on the 7936 training partition sentences, and evaluated their F1 on the development set. After choosing hyperparameters to maximize the dev F1, we would retrain the model using these hyperparameters on the full 8936 sentence training set, and evaluate on test. One hyperparameter was l2-regularization sigma, which for most models was optimal at 2 or 3.2. The word embeddings also required a scaling hyperparameter, as described in Section 7.2. NER is typically treated as a sequence prediction problem. Following Ratinov and Roth (2009), we use the regularized averaged perceptron model. Ratinov and Roth (2009) describe different sequence encoding like BILOU and BIO, and show that the BILOU encoding outperforms BIO, and the greedy inference performs competitively to Viterbi while being significantly faster. Accordingly, we use greedy inference and BILOU text chunk representation. We use the publicly available implementation from Ratinov and Roth (2009) (see the end of this paper for the URL). In our baseline experiments, we remove gazetteers and non-local features (Krishnan & Manning, 2006). However, we also run experiments that include these features, to understand if the information they provide mostly overlaps with that of the word representations. After each epoch over the training set, we measured the accuracy of the model on the development set. Training was stopped after the accuracy on the development set did not improve for 10 epochs, generally about 50–80 epochs total. The epoch that performed best on the development set was chosen as the final model. We use the following baseline set of features When using the lexical features, we normalize dates and numbers. For example, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD**DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. This delexicalization is performed separately from using the word representation. That is, if we have induced an embedding for 12/3/2008 , we will use the embedding of 12/3/2008 , and *DD*/*D*/*DDDD* in the baseline features listed above. Unlike in our chunking experiments, after we chose the best model on the development set, we used that model on the test set too. (In chunking, after finding the best hyperparameters on the development set, we would combine the dev and training set and training a model over this combined set, and then evaluate on test.) The standard evaluation benchmark for NER is the CoNLL03 shared task dataset drawn from the Reuters newswire. The training set contains 204K words (14K sentences, 946 documents), the test set contains 46K words (3.5K sentences, 231 documents), and the development set contains 51K words (3.3K sentences, 216 documents). We also evaluated on an out-of-domain (OOD) dataset, the MUC7 formal run (59K words). MUC7 has a different annotation standard than the CoNLL03 data. It has several NE types that don’t appear in CoNLL03: money, dates, and numeric quantities. CoNLL03 has MISC, which is not present in MUC7. To evaluate on MUC7, we perform the following postprocessing steps prior to evaluation: These postprocessing steps will adversely affect all NER models across-the-board, nonetheless allowing us to compare different models in a controlled manner. Unlabeled data is used for inducing the word representations. We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences. We left case intact in the corpus. By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers. We use a preprocessing technique proposed by Liang, (2005, p. 51), which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a–z. We assume that whitespace is not counted, although this is not specified in Liang’s thesis. We call this preprocessing step cleaning. In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training. For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary—a curriculum training strategy (Elman, 1993; Bengio et al., 2009; Spitkovsky et al., 2010)—would provide better results than cleaning. After cleaning, there are 37 million words (58% of the original) in 1.3 million sentences (41% of the original). The cleaned RCV1 corpus has 269K word types. This is the vocabulary size, i.e. how many word representations were induced. Note that cleaning is applied only to the unlabeled data, not to the labeled data used in the supervised tasks. RCV1 is a superset of the CoNLL03 corpus. For this reason, NER results that use RCV1 word representations are a form of transductive learning. The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al., 2008; Ratinov & Roth, 2009). We also induced 100, 320, and 3200 Brown clusters, for comparison. (Because Brown clustering scales quadratically in the number of clusters, inducing 10000 clusters would have been prohibitive.) Because Brown clusters are hierarchical, we can use cluster supersets as features. We used clusters at path depth 4, 6, 10, and 20 (Ratinov & Roth, 2009). These are the prefixes used in Table 1. The Collobert and Weston (2008) (C&W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. One of the difficulties in inducing these embeddings is that there is no stopping criterion defined, and that the quality of the embeddings can keep improving as training continues. Collobert (p.c.) simply leaves one computer training his embeddings indefinitely. We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows. In comparison to Turian et al. (2009), we use improved C&W embeddings in this work: formly in the range [-0.01, +0.01], not [-1,+1]. For rare words, which are typically updated only 143 times per epoch2, and given that our embedding learning rate was typically 1e-6 or 1e-7, this means that rare word embeddings will be concentrated around zero, instead of spread out randomly. The HLBL embeddings were trained for 100 epochs (7 days).3 Unlike our Collobert and Weston (2008) embeddings, we did not extensively tune the learning rates for HLBL. We used a learning rate of 1e-3 for both model parameters and embedding parameters. We induced embeddings with 100 dimensions over 5-gram windows, and embeddings with 50 dimensions over 5-gram windows. Embeddings were induced over one pass approach using a random tree, not two passes with an updated tree and embeddings re-estimation. Like many NLP systems, the baseline system contains only binary features. The word embeddings, however, are real numbers that are not necessarily in a bounded range. If the range of the word embeddings is too large, they will exert more influence than the binary features. We generally found that embeddings had zero mean. We can scale the embeddings by a hyperparameter, to control their standard deviation. Assume that the embeddings are represented by a matrix E: c- is a scaling constant that sets the new standard deviation after scaling the embeddings. work. In Turian et al. (2009), we were not able to prescribe a default value for scaling the embeddings. However, these curves demonstrate that a reasonable choice of scale factor is such that the embeddings have a standard deviation of 0.1. There are capacity controls for the word representations: number of Brown clusters, and number of dimensions of the word embeddings. Figure 2 shows the effect on the validation F1 as we vary the capacity of the word representations. In general, it appears that more Brown clusters are better. We would like to induce 10000 Brown clusters, however this would take several months. In Turian et al. (2009), we hypothesized on the basis of solely the HLBL NER curve that higher-dimensional word embeddings would give higher accuracy. Figure 2 shows that this hypothesis is not true. For NER, the C&W curve is almost flat, and we were suprised to find the even 25-dimensional C&W word embeddings work so well. For chunking, 50-dimensional embeddings had the highest validation F1 for both C&W and HLBL. These curves indicates that the optimal capacity of the word embeddings is task-specific. gazetteers to the baseline. To speed up training, in combined experiments (C&W plus another word representation), we used the 50-dimensional C&W embeddings, not the 200-dimensional ones. In the last section, we show how many unlabeled words were used. Table 2 shows the final chunking results and Table 3 shows the final NER F1 results. We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and—for NER—Lin and Wu (2009). Tables 2 and 3 show that accuracy can be increased further by combining the features from different types of word representations. But, if only one word representation is to be used, Brown clusters have the highest accuracy. Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings. On chunking, there is only a minute difference between Brown clusters and the embeddings. Combining representations leads to small increases in the test F1. In comparison to chunking, combining different word representations on NER seems gives larger improvements on the test F1. On NER, Brown clusters are superior to the word embeddings. Since much of the NER F1 is derived from decisions made over rare words, we suspected that Brown clustering has a superior representation for rare words. Brown makes a single hard clustering decision, whereas the embedding for a rare word is close to its initial value since it hasn’t received many training updates (see Footnote 2). Figure 3 shows the total number of per-token errors incurred on the test set, depending upon the frequency of the word token in the unlabeled data. For NER, Figure 3 (b) shows that most errors occur on rare words, and that Brown clusters do indeed incur fewer errors for rare words. This supports our hypothesis that, for rare words, Brown clustering produces better representations than word embeddings that haven’t received sufficient training updates. For chunking, Brown clusters and C&W embeddings incur almost identical numbers of errors, and errors are concentrated around the more common words. We hypothesize that non-rare words have good representations, regardless of the choice of word representation technique. For tasks like chunking in which a syntactic decision relies upon looking at several token simultaneously, compound features that use the word representations might increase accuracy more (Koo et al., 2008). Using word representations in NER brought larger gains on the out-of-domain data than on the in-domain data. We were surprised by this result, because the OOD data was not even used during the unsupervised word representation induction, as was the in-domain data. We are curious to investigate this phenomenon further. Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO). They find a lowdimensional projection of the input features that gives good linear classifiers over auxiliary tasks. These auxiliary tasks are sometimes specific to the supervised task, and sometimes general language modeling tasks like “predict the missing word”. Suzuki and Isozaki (2008) present a semisupervised extension of CRFs. (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.) One of the advantages of the semi-supervised learning approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and Suzuki and Isozaki (2008). Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. Since they can scale to millions of phrases, and they train over 800B unlabeled words, they achieve state-of-the-art accuracy on NER using their phrase clusters. This suggests that extending word representations to phrase representations is worth further investigation. Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando & Zhang, 2005; Suzuki & Isozaki, 2008; Suzuki et al., 2009). Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. Ours is the first work to systematically compare different word representations in a controlled way. We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. We also found that combining different word representations can improve accuracy further. Error analysis indicates that Brown clustering induces better representations for rare words than C&W embeddings that have not received many training updates. Another contribution of our work is a default method for setting the scaling parameter for word embeddings. With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning. Future work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features. Replicating our experiments You can visit http://metaoptimize.com/ projects/wordreprs/ to find: The word representations we induced, which you can download and use in your experiments; The code for inducing the word representations, which you can use to induce word representations on your own data; The NER and chunking system, with code for replicating our experiments.","Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando & Zhang, 2005; Suzuki & Isozaki, 2008; Suzuki et al., 2009). Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. Ours is the first work to systematically compare different word representations in a controlled way. We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. We also found that combining different word representations can improve accuracy further. Error analysis indicates that Brown clustering induces better representations for rare words than C&W embeddings that have not received many training updates. Another contribution of our work is a default method for setting the scaling parameter for word embeddings. With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning. Future work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features. Replicating our experiments You can visit http://metaoptimize.com/ projects/wordreprs/ to find: The word representations we induced, which you can download and use in your experiments; The code for inducing the word representations, which you can use to induce word representations on your own data; The NER and chunking system, with code for replicating our experiments."
99,"We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.","We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm. Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of O(n4) time and O(n3) space. We evaluate our parsers on the Penn WSJ Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc et al., 2001), achieving unlabeled attachment scores of 93.04% and 87.38%. In summary, we make three main contributions: The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes. In dependency grammar, syntactic relationships are represented as head-modifier dependencies: directed arcs between a head, which is the more “essential” word in the relationship, and a modifier, which supplements the meaning of the head. For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier). A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence. Every dependency tree is rooted at a special “*” token, allowing the selection of the sentential head to be modeled as if it were a dependency. For a sentence x, we define dependency parsing as a search for the highest-scoring analysis of x: Here, y(x) is the set of all trees compatible with x and SCORE(x, y) evaluates the event that tree y is the analysis of sentence x. Since the cardinality of y(x) grows exponentially with the length of the sentence, directly solving Eq. 1 is impractical. A common strategy, and one which forms the focus of this paper, is to factor each dependency tree into small parts, which can be scored in isolation. Factored parsing can be formalized as follows: That is, we treat the dependency tree y as a set of parts p, each of which makes a separate contribution to the score of y. For certain factorizations, efficient parsing algorithms exist for solving Eq. 1. We define the order of a part according to the number of dependencies it contains, with analogous terminology for factorizations and parsing algorithms. In the remainder of this paper, we focus on factorizations utilizing the following parts: Specifically, Sections 4.1, 4.2, and 4.3 describe parsers that, respectively, factor trees into grandchild parts, grand-sibling parts, and a mixture of grand-sibling and tri-sibling parts. Our new third-order dependency parsers build on ideas from existing parsing algorithms. In this section, we provide background on two relevant parsers from previous work. The first type of parser we describe uses a “firstorder” factorization, which decomposes a dependency tree into its individual dependencies. Eisner (2000) introduced a widely-used dynamicprogramming algorithm for first-order parsing; as it is the basis for many parsers, including our new algorithms, we summarize its design here. The Eisner (2000) algorithm is based on two interrelated types of dynamic-programming structures: complete spans, which consist of a headword and its descendents on one side, and incomplete spans, which consist of a dependency and the region between the head and modifier. Formally, we denote a complete span as Ch,e where h and a are the indices of the span’s headword and endpoint. An incomplete span is denoted as Ih,,,t where h and m are the index of the head and modifier of a dependency. Intuitively, a complete span represents a “half-constituent” headed by h, whereas an incomplete span is only a partial half-constituent, since the constituent can be extended by adding more modifiers to m. Each type of span is created by recursively combining two smaller, adjacent spans; the constructions are specified graphically in Figure 2. An incomplete span is constructed from a pair of complete spans, indicating the division of the range [h, m] into constituents headed by h and m. A complete span is created by “completing” an incomplete span with the other half of m’s constituent. The point of concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction. In order to parse a sentence x, it suffices to find optimal constructions for all complete and incomplete spans defined on x. This can be accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling spans, which represent the region between successive modifiers of some head. Formally, we denote a sibling span as S,,,,, where s and m are a pair of modifiers involved in a sibling relationship. Modified versions of sibling spans will play an important role in the new parsing algorithms described in Section 4. Figure 3 provides a graphical specification of the second-order parsing algorithm. Note that incomplete spans are constructed in a new way: the second-order parser combines a smaller incomplete span, representing the next-innermost dependency, with a sibling span that covers the region between the two modifiers. Sibling parts (h, m, s) can thus be obtained from Figure 3(b). Despite the use of second-order parts, each derivation is still defined by a span and split point, so the parser requires O(n3) time and O(n2) space. In this section we describe our new third-order dependency parsing algorithms. Our overall method is characterized by the augmentation of each span with a “grandparent” index: an index external to the span whose role will be made clear below. This section presents three parsing algorithms based on this idea: Model 0, a second-order parser, and Models 1 and 2, which are third-order parsers. The first parser, Model 0, factors each dependency tree into a set of grandchild parts—pairs of dependencies connected head-to-tail. Specifically, a grandchild part is a triple of indices (g, h, m) where (g, h) and (h, m) are dependencies.3 In order to parse this factorization, we augment both complete and incomplete spans with grandparent indices; for brevity, we refer to these augmented structures as g-spans. Formally, we denote a complete g-span as Ch,e, where Ch,e is a normal complete span and g is an index lying outside the range [h, e], with the implication that (g, h) is a dependency. Incomplete g-spans are defined analogously and are denoted as Ih,.. Figure 4 depicts complete and incomplete gspans and provides a graphical specification of the SCOREG is the scoring function for grandchild parts. We use the g-span identities as shorthand for their chart entries (e.g., Igi,j refers to the entry containing the maximum score of that g-span). Model 0 dynamic-programming algorithm. The algorithm resembles the first-order parser, except that every recursive construction must also set the grandparent indices of the smaller g-spans; fortunately, this can be done deterministically in all cases. For example, Figure 4(a) depicts the decomposition of Cgh,e into an incomplete half and a complete half. The grandparent of the incomplete half is copied from Cgh,e while the grandparent of the complete half is set to h, the head of m as defined by the construction. Clearly, grandchild parts (g, h, m) can be read off of the incomplete g-spans in Figure 4(b,d). Moreover, since each derivation copies the grandparent index g into successively smaller g-spans, grandchild parts will be produced for all grandchildren of g. Model 0 can be parsed by adapting standard top-down or bottom-up chart parsing techniques. For concreteness, Figure 5 provides a pseudocode sketch of a bottom-up chart parser for Model 0; although the sketch omits many details, it suffices for the purposes of illustration. The algorithm progresses from small widths to large in the usual manner, but after defining the endpoints (i, j) there is an additional loop that enumerates all possible grandparents. Since each derivation is defined by three fixed indices (the g-span) and one free index (the split point), the complexity of the algorithm is O(n4) time and O(n3) space. Note that the grandparent indices cause each gspan to have non-contiguous structure. For example, in Figure 4(a) the words between g and h will be controlled by some other g-span. Due to these discontinuities, the correctness of the Model 0 dynamic-programming algorithm may not be immediately obvious. While a full proof of correctness is beyond the scope of this paper, we note that each structure on the right-hand side of Figure 4 lies completely within the structure on the left-hand side. This nesting of structures implies, in turn, that the usual properties required to ensure the correctness of dynamic programming hold. We now describe our first third-order parsing algorithm. Model 1 decomposes each tree into a set of grand-sibling parts—combinations of sibling parts and grandchild parts. Specifically, a grand-sibling is a 4-tuple of indices (g, h, m, s) where (h, m, s) is a sibling part and (g, h, m) and (g, h, s) are grandchild parts. For example, in Figure 1, the words “must,” “report,” “sales,” and “immediately” form a grand-sibling part. In order to parse this factorization, we introduce sibling g-spans Shm,s, which are composed of a normal sibling span Sm,s and an external index h, with the implication that (h, m, s) forms a valid sibling part. Figure 6 provides a graphical specification of the dynamic-programming algorithm for Model 1. The overall structure of the algorithm resembles the second-order sibling parser, with the addition of grandparent indices; as in Model 0, the grandparent indices can be set deterministically in all cases. Note that the sibling g-spans are crucial: they allow grand-sibling parts (g, h, m, s) to be read off of Figure 6(b), while simultaneously propagating grandparent indices to smaller g-spans. Like Model 0, Model 1 can be parsed via adaptations of standard chart-parsing techniques; we omit the details for brevity. Despite the move to third-order parts, each derivation is still defined by a g-span and a split point, so that parsing requires only O(n4) time and O(n3) space. Higher-order parsing algorithms have been proposed which extend the second-order sibling factorization to parts containing multiple siblings (McDonald and Pereira, 2006, also see Section 6 for discussion). In this section, we show how our g-span-based techniques can be combined with a third-order sibling parser, resulting in a parser that captures both grand-sibling parts and tri-sibling parts—4-tuples of indices (h, m, s, t) such that both (h, m, s) and (h, s, t) are sibling parts. In order to parse this factorization, we introduce a new type of dynamic-programming structure: sibling-augmented spans, or s-spans. Formally, we denote an incomplete s-span as Ih,m,s where Ih,m is a normal incomplete span and s is an index lying in the strict interior of the range [h, m], such that (h, m, s) forms a valid sibling part. Figure 7 provides a graphical specification of the Model 2 parsing algorithm. An incomplete s-span is constructed by combining a smaller incomplete s-span, representing the next-innermost pair of modifiers, with a sibling g-span, covering the region between the outer two modifiers. As in Model 1, sibling g-spans are crucial for propagating grandparent indices, while allowing the recovery of tri-sibling parts (h, m, s, t). Figure 7(b) shows how an incomplete s-span can be converted into an incomplete g-span by exchanging the internal sibling index for an external grandparent index; in the process, grand-sibling parts (g, h, m, s) are enumerated. Since every derivation is defined by an augmented span and a split point, Model 2 can be parsed in O(n4) time and O(n3) space. It should be noted that unlike Model 1, Model 2 produces grand-sibling parts only for the outermost pair of grandchildren,4 similar to the behavior of the Carreras (2007) parser. In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras (2007) algorithm by “demoting” each third-order part into a second-order part: SCOREGS(x, g, h, m, s) = SCOREG(x, g, h, m) SCORETS(x, h, m, s, t) = SCORES(x, h, m, s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively. The emulated version has the same computational complexity as the original, so there is no practical reason to prefer it over the original. Nevertheless, the relationship illustrated above highlights the efficiency of our approach: we are able to recover third-order parts in place of second-order parts, at no additional cost. The technique of grandparent-index augmentation has proven fruitful, as it allows us to parse expressive third-order factorizations while retaining an efficient O(n4) runtime. In fact, our thirdorder parsing algorithms are “optimally” efficient in an asymptotic sense. Since each third-order part is composed of four separate indices, there are 0(n4) distinct parts. Any third-order parsing algorithm must at least consider the score of each part, hence third-order parsing is Q(n4) and it follows that the asymptotic complexity of Models 1 and 2 cannot be improved. The key to the efficiency of our approach is a fundamental asymmetry in the structure of a directed tree: a head can have any number of modifiers, while a modifier always has exactly one head. Factorizations like that of Carreras (2007) obtain grandchild parts by augmenting spans with the indices of modifiers, leading to limitations on the grandchildren that can participate in the factorization. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). We briefly outline a few extensions to our algorithms; we hope to explore these in future work. Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities. Our parsers are easily extended to labeled dependencies. Direct integration of labels into Models 1 and 2 would result in third-order parts composed of three labeled dependencies, at the cost of increasing the time and space complexities by factors of O(L3) and O(L2), respectively, where L bounds the number of labels per dependency. If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by adapting methods in Eisner (2000). Complexity would increase by factors of O(54) time and O(53) space, where 5 bounds the number of senses per word. If more vertical context is desired, the dynamicprogramming structures can be extended with additional ancestor indices, resulting in a “spine” of ancestors above each span. Each additional ancestor lengthens the vertical scope of the factorization (e.g., from grand-siblings to “great-grandsiblings”), while increasing complexity by a factor of O(n). Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n). Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm. Efficiently encoding this kind of information into a sentence-independent grammar transformation would be challenging at best. Eisner (2000) defines dependency parsing models where each word has a set of possible “senses” and the parser recovers the best joint assignment of syntax and senses. Our new parsing algorithms could be implemented by defining the “sense” of each word as the index of its head. However, when parsing with senses, the complexity of the Eisner (2000) parser increases by factors of O(53) time and O(52) space (ibid., Section 4.2). Since each word has n potential heads, a direct application of the word-sense parser leads to time and space complexities of O(n6) and O(n4), respectively, in contrast to our O(n4) and O(n3).5 Eisner (2000) also uses head automata to score or recognize the dependents of each head. An interesting question is whether these automata could be coerced into modeling the grandparent indices used in our parsing algorithms. However, note that the head automata are defined in a sentenceindependent manner, with two automata per word in the vocabulary (ibid., Section 2). The automata are thus analogous to the rules of a CFG and attempts to use them to model grandparent indices would face difficulties similar to those already described for grammar transformations in CFGs. It should be noted that third-order parsers have previously been proposed by McDonald and Pereira (2006), who remarked that their secondorder sibling parser (see Figure 3) could easily be extended to capture m > 1 successive modifiers in O(nm+1) time (ibid., Section 2.2). To our knowledge, however, Models 1 and 2 are the first third-order parsing algorithms capable of modeling grandchild parts. In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3). Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts, with complexities of O(n4) time and O(n3) space. An important limitation of the parser’s factorization is that it only defines grandchild parts for outermost grandchildren: (g, h, m) is scored only when m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8 Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as: SCOREPART(x, p) = w · f(x, p) Here, f is a feature-vector mapping and w is a vector of associated parameters. Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations. For example, Model 1 defines feature mappings for dependencies, siblings, grandchildren, and grand-siblings, so that the score of a dependency parse is given by: Above, y is simultaneously decomposed into several different types of parts; trivial modifications to the Model 1 parser allow it to evaluate all of the necessary parts in an interleaved fashion. A similar treatment of Model 2 yields five feature mappings: the four above plus ftsib(x, h, m, s, t), which represents tri-sibling parts. The lower-order feature mappings fdep, fsib, and fgch are based on feature sets from previous work (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras, 2007), to which we added lexicalized versions of several features. For example, fdep contains lexicalized “in-between” features that depend on the head and modifier words as well as a word lying in between the two; in contrast, previous work has generally defined in-between features for POS tags only. As another example, our second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams only. Our third-order feature mappings fgsib and ftsib consist of four types of features. First, we define 4-gram features that characterize the four relevant indices using words and POS tags; examples include POS 4-grams and mixed 4-grams with one word and three POS tags. Second, we define 4gram context features consisting of POS 4-grams augmented with adjacent POS tags: for example, fgsib(x, g, h, m, s) includes POS 7-grams for the tags at positions (g, h, m, s, g+1, h+1, m+1). Third, we define backed-offfeatures that track bigram and trigram interactions which are absent in the lower-order feature mappings: for example, ftsib(x, h, m, s, t) contains features predicated on the trigram (m, s, t) and the bigram (m, t), neither of which exist in any lower-order part. Fourth, noting that coordinations are typically annotated as grand-siblings (e.g., “report purchases and sales” in Figure 1), we define coordination features for certain grand-sibling parts. For example, fgsib(x, g, h, m, s) contains features examining the implicit head-modifier relationship (g, m) that are only activated when the POS tag of s is a coordinating conjunction. Finally, we make two brief remarks regarding the use of POS tags. First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags. There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select paon English parsing. For each beam value, parsers were trained on the English training set and evaluated on the English validation set; the same beam value was applied to both training and validation data. Pass = %dependencies surviving the beam in training data, Orac = maximum achievable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m  |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m  |x). Table 1 provides information on the behavior of the pruning method. Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et al. (2008), and additive gains might be realized by applying their cluster-based feature sets to our enriched factorizations. In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3). Interestingly, grandchild interactions appear to provide important information: for example, when Model 2 is used without grandchild-based features (“Model 2, no-G” in Table 3), its accuracy suffers noticeably. In addition, it seems that grandchild interactions are particularly useful in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features. We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions. Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data. The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006). There are several possibilities for further research involving our third-order parsing algorithms. One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4. A second area for future work lies in applications of dependency parsing. While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008). Finally, in the hopes that others in the NLP community may find our parsers useful, we provide a free distribution of our implementation.2","We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions. Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data. The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006). There are several possibilities for further research involving our third-order parsing algorithms. One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4. A second area for future work lies in applications of dependency parsing. While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008). Finally, in the hopes that others in the NLP community may find our parsers useful, we provide a free distribution of our implementation.2"
