# Extractive and Abstractive Text Summarization for Long Documents 
*A combination of extractive and abstractive text summarization for summarizing long scientific texts*


<img src='https://drive.google.com/uc?id=1caGEZmT4ODf0R7zNZLqYh-8plpv55sIh'>

![divider](../assets/gradient-divider.png?raw=true)


## üìë Contextualization

In an increasingly information-dependent world, the ability to provide the most important and accurate information in the least amount of time is exceedingly valuable. Text summarization can provide this value. It is the process of summarizing a certain document in order to get the most important information from the original one. Essentially, text summarization produces a concise summary which preserves the valuable information and meaning of a document.

There are two general types of text summarization: Extractive and Abstractive summarization.

### Extractive Summarization

Extractive summarization, from the word itself, is a method of extracting a subset of words that contain the most important information in a text. This approach takes into consideration the most important parts of document sentences and uses them to form the summarization. Then, algorithms to give weights to these parts and rank them based on similarity and importance are used.

The general workflow for extractive summarization goes like: 

**Text input --> Get similar sentences --> Assign weights to sentences --> Rank sentences --> Choose sentences with highest ranks to form the summary**

### Abstractive Summarization

In contrast, abstractive summarization aims to **abstract** and use words that did not appear in the input document based on the semantic information of the text. This means abstractive summarization produces a new summary. Abstractive summarization interprets and examines the document using advanced NLP techniques and generates a new concise summary based the most important information in the text. 

The general workflow for abstractive summarization goes like:

**Text input --> Understand the context of the document --> Use semantic understanding --> Abstract and create a new summary**

In general, abstractive summarization is desired more than extractive summarization because it is akin to how a human would summarize a text by first understanding its meaning and putting it into his/her own words. However, given the challenges in semantic representation, extractive summarization often gives better results. 

![divider](../assets/gradient-divider.png?raw=true)

## üíº The Project

In addition to the advantages and disadvantages of these two summarization techniques, there is also difficulty in summarizing long text documents. For example, in this Github issue [Bart now enforces maximum sequence length in Summarization Pipeline](https://github.com/huggingface/transformers/issues/4224), there are limits to the maximum length of a text document for abstractive summarization of some transformer models like BART. Given this, I researched on how to solve this problem and came across this paper: [Combination of abstractive and extractive approaches for summarization of long scientific texts](https://arxiv.org/abs/2006.05354) which applied extractive summarization to get a summary with the important extracted information from the text and then performed abstractive summarization on the extracted summary along with the scientific paper's abstract and conclusion. 

While I won't be going as detailed as the paper, in this project, I still aim to apply extractive and abstractive summarization in order to summarize long scientific documents. 

![divider](../assets/gradient-divider.png?raw=true)

## üìÅ The Dataset

The dataset I will use for this project consists of 100 scientific papers from the WING NUS group's Scisumm corpus found at this [github link](https://github.com/WING-NUS/scisumm-corpus). According to the authors, [Scisumm](https://cs.stanford.edu/~myasu/projects/scisumm_net/) is a summary of scientific papers should ideally incorporate the impact of the papers on the research community reflected by citations. To facilitate research in citation-aware scientific paper summarization (Scisumm), the CL-Scisumm shared task has been organized since 2014 for papers in the computational linguistics and NLP domain. 

![divider](../assets/gradient-divider.png?raw=true)

## ‚ùó The Methodology

The project workflow consists of three main steps: data collection and preprocessing, modelling, and model deployment. 

### Data Collection and Preprocessing

In this step, I choose 100 scientific papers from the Scisumm corpus. I selectively decide which papers to include because the project requires papers which explicitly have an `abstract` and a `conclusion` in the .xml file. Some papers, after investigation, did not have an `abstract` section and instead was found directly in the text section of the document. This will lead to extraction errors as the .xml extraction pipeline was explicitly designed for xml documents which explicity have an `abstract` and `conclusion` section. 

For the data preprocessing step, I create a data cleaning and preprocessing functions with the following capabilities:
* Lemmatization
* Stopword removal
* Lowercase
* Punctuation cleaning
* Emoji cleaning
* Number cleaning
* Weblinks cleaning
* Unnecessary spaces removal

I gave the user the freedom to choose which cleaning to apply by creating a unified function where every cleaning step is a boolean. For the purpose of this project, I do not lemmatize, remove stopwords, lowercase, and remove punctuations so that the summarization will still have its semantic context in place. 

### Model Training

For modelling, I perform both extractive and abstractive summarization. For extractive summarization, I use the BERT transformer model and customize it to use the pre-trained weights of the **sciBERT** model which specializes in scientific texts, which fit our purpose. For every text, I determine the optimal number of sentences for the extracted summary.

For abstractive summarization, I first concatenate the abstract, extractive summary, and conclusion together since much of the important information can be found in them. Then, I use the **facebook-BART-large-cnn** transformer model to perform the abstraction. 

### Model Deployment

For deployment, I use Streamlit to create a simple user interface which requires a long text input to summarize.

![divider](../assets/gradient-divider.png?raw=true)

## üìî Jupyter Notebooks

There are a total of 4 Jupyter notebooks included in this project. There are as follows:
* `01-gc-data-collection.ipynb`
* `02-gc-data-cleaning-and-preprocessing.ipynb`
* `03-gglc-feature-explorations-and-text-summarization.ipynb`
* `04-gc-model-deployment-with-streamlit-and-localtunnel.ipynb`

Each Jupyter notebook contains explorations related to the notebook's title. They are meant to showcase my perspective as I was creating individual components of the project and are reference points regarding the project workflow from start to finish. 

![divider](../assets/gradient-divider.png?raw=true)

## Project Organization

Shown below is the structure of the project inspired from [cookiecutter's data science project template](https://drivendata.github.io/cookiecutter-data-science/).

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for text summarization.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data.
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks containing the explorations performed in this project
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the project
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts for text summarization
    ‚îÇ¬†  ‚îú‚îÄ‚îÄ deployment     <- Scripts for project deployment
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox


![divider](../assets/gradient-divider.png?raw=true)

## Usage

There are two main use cases for this project. 

**First is summarizing a `.csv` file or any file with the following format:** 

| index | abstract  | full_text | conclusion |
| :-----: | :-: | :-: | :-: |
| 0 | Lorem | Ipsum | Lorem |
| 1 | Ipsum | Lorem | Ipsum |

**Second is deploying a local Streamlit model which takes as input the abstract, full text, and conclusion of a long scientific article and generates a summary** 

Whichever use case is to be executed, I recommend executing the step-by-step instructions below in a **virtual environment**. A virtual environment is a tool used to separate the dependencies of different projects by creating **individual environments** for each. Simply put, virtual environments protect the user from library conflicts since different projects will require different library versions or even none of the libraries a current project is using at all and vice versa.

To create a virtual environment, the module `virtualenv` can be used. `virtualenv` creates a separate folder with all the required executables for Python projects. 

### Install the virtualenv package

To install `virtualenv`, make sure [pip](https://pip.pypa.io/en/stable/installation/) is intalled in your device.

Install `virtualenv` with pip:

```python
pip install virtualenv
```

### Create the virtual environment

To create a virtual environment, a path must be declared. For this project, we can use the name `text_summarizer`:

```python
virtualenv text_summarizer
```

### Activate the virtual environment

The virtual environment can be activated using the commands:

#### Mac OS/ Linux

```bash
source text_summarizer/bin/activate
```

#### Windows

```bash
text_summarizer\Scripts\activate
```

Upon activation, you should see the virtual environment name in parenthesis in the terminal i.e. (text_summarizer). 

### Deactivate the virtual environment

To deactivate the virtual environment and use your original Python environment, simply:

```bash
deactivate
```

For a more complete explanation, check the the [virtualenv documentation](https://virtualenv.pypa.io/en/latest/).

![divider](../assets/gradient-divider.png?raw=true)

## Install Requirements

After activating your virtual environment, **clone the repository into it:**

```bash
git clone https://github.com/gersongerardcruz/extractive_and_abstractive_text_summarization.git
```

Then, move into the directory of the repository:

```bash
cd extractive_and_abstractive_text_summarization
```

Then, install the necessary requirements:

```bash
pip install -r requirements.txt
```

This command will install all dependencies needed for this project into your virtual environment. 

### Text Summarization on `.csv` files

The repository contains two raw data files: one of the complete 100 texts in `top100.csv` and another of the first five scientific texts in `top5.csv`. The `top5.csv` file is meant to be a test file for you to experience how the summarization works. 

### Cleaning and Processing the Data

The first step in summarization is processing the data. To process the data, execute the `src/data/make_dataset.py` and output the resulting data into the `processed/` folder. The `make_dataset.py` requires two arguments: input file path and output file path as shown in the code block below:

```bash
py src/data/make_dataset.py data/raw/top5.csv data/processed/top5_cleaned.csv
```

### Performing Text Summarization

After processing the raw data, text summarization can be performed by executing the `src/models/summarize.py` which performs text summarization on a `.csv` file of the prescribed format. The `summarize.py` also requires the arguments: input file path and output file path. 

```bash
py src/models/summarize.py data/processed/top5_cleaned.csv results.csv
```

This command will perform text summarization on the cleaned csv file and outputs a summarized version in the `results.csv` file. 

<img src='https://drive.google.com/uc?id=1GkUMwRkLPQBW4qWzqlp3vxKdfCG7uPHQ'>

### Text Summarization via Streamlit

The deployment file is found in `src/deployment/deploy.py`. To run this file using streamlit, simply run the command:

```bash
streamlit run src/deployment/deploy.py
```

This will redirect you to a new tab containing the locally hosted summarizer via streamlit as shown below:

![streamlit_deploy](../assets/streamlit_deploy.png?raw=true)

Simply fill-out the fields with the abstract, full text, and conclusion of the texts in the cleaned .csv file, and click `Summarize`. This will run the text summarization found in `deploy.py` and return the summarized version of the text below as shown in the screenshot below:

![streamlit_summarize](../assets/streamlit_summarize.png?raw=true)

![streamlit_results](../assets/streamlit_results.png?raw=true)

To deploy the app, simply follow the instructions found in the [streamlit deployment documentation](https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app)

![divider](../assets/gradient-divider.png?raw=true)

## References

Project Organization


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
